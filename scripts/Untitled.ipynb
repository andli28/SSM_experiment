{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99ae2c19-3ef5-47dc-9701-58d428750f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 07:27:23 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 16384, 'disable_log_stats': True, 'model': 'ibm-ai-platform/Bamba-9B-v1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 07:27:24 [model.py:547] Resolved architecture: BambaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-17 07:27:24 [model.py:1907] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=4096 or model_max_length=None in model's config.json). VLLM_ALLOW_LONG_MAX_MODEL_LEN must be used with extreme caution. If the model uses relative position encoding (RoPE), positions exceeding derived_max_model_len lead to nan. If the model uses absolute position encoding, positions exceeding derived_max_model_len will cause a CUDA array out-of-bounds error.\n",
      "INFO 12-17 07:27:24 [model.py:1510] Using max model len 16384\n",
      "INFO 12-17 07:27:24 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 12-17 07:27:24 [config.py:297] Hybrid or mamba-based model detected: disabling prefix caching since it is not yet supported.\n",
      "INFO 12-17 07:27:24 [config.py:308] Hybrid or mamba-based model detected: setting cudagraph mode to FULL_AND_PIECEWISE in order to optimize performance.\n",
      "INFO 12-17 07:27:24 [config.py:376] Setting attention block size to 528 tokens to ensure that attention page size is >= mamba page size.\n",
      "INFO 12-17 07:27:24 [config.py:397] Padding mamba page size by 0.69% to ensure that mamba page size and attention page size are exactly equal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 12-17 07:27:32 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:34] Checking if TPU platform is available.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:58] Checking if CUDA platform is available.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:78] Confirmed CUDA platform is available.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:106] Checking if ROCm platform is available.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:127] Checking if XPU platform is available.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:153] Checking if CPU platform is available.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:58] Checking if CUDA platform is available.\n",
      "DEBUG 12-17 07:27:32 [platforms/__init__.py:78] Confirmed CUDA platform is available.\n",
      "INFO 12-17 07:27:32 [platforms/__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:34 [v1/engine/core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:34 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///insomnia001/depts/edu/COMS-E6998-015/dwz2107/tmp/248903ae-1683-4a89-b29e-1ede5b29e50f'], outputs=['ipc:///insomnia001/depts/edu/COMS-E6998-015/dwz2107/tmp/bb0fa46b-d65e-4824-b123-2add6afcba3b'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:34 [v1/engine/core.py:487] Has DP Coordinator: False, stats publish address: None\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:34 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:34 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:34 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:34 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='ibm-ai-platform/Bamba-9B-v1', speculative_config=None, tokenizer='ibm-ai-platform/Bamba-9B-v1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ibm-ai-platform/Bamba-9B-v1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:36 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:37 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:37 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:37 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14e36b5b3ad0>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:37 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.197.96.88:56525 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:37 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:37 [distributed/parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:38 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:38 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:38 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.bamba.BambaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:38 [v1/worker/gpu_model_runner.py:2602] Starting to load model ibm-ai-platform/Bamba-9B-v1...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:38 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:38 [platforms/cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:39 [compilation/backends.py:42] Using InductorAdaptor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:39 [config/compilation.py:649] enabled custom ops: Counter()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:39 [config/compilation.py:650] disabled custom ops: Counter({'column_parallel_linear': 93, 'rms_norm': 65, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'mamba_mixer2': 29, 'mixer2_gated_rms_norm': 29, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:39 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:27:39 [model_executor/model_loader/weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:07<00:22,  7.61s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:15<00:15,  7.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:27:55 [model_executor/models/utils.py:186] Loaded weight lm_head.weight with shape torch.Size([128256, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:20<00:06,  6.74s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:26<00:00,  6.19s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:26<00:00,  6.57s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:05 [model_executor/model_loader/default_loader.py:267] Loading weights took 26.47 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:06 [v1/worker/gpu_model_runner.py:2653] Model loading took 18.2459 GiB and 27.099465 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:06 [compilation/decorators.py:256] Start compiling function <code object forward at 0x113f6430, file \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/models/bamba.py\", line 305>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] Traced files (to be considered for compilation cache):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/nn/modules/container.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/attention/layer.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/distributed/communication_op.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/custom_op.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/mamba/mamba_mixer2.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/model_executor/models/bamba.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:08 [compilation/backends.py:501] /insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/platforms/interface.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:09 [compilation/backends.py:548] Using cache directory: /insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:09 [compilation/backends.py:559] Dynamo bytecode transform time: 3.34 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:10 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.1 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:10 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:10 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:10 [compilation/backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:10 [compilation/backends.py:203] Store the 0-th graph for dynamic shape from inductor via handle ('fkwob7rjciu24vbywseeo4pql3w75gqjnrnajavt2tqlac6pvgvb', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/cf/ccfjkpg55jvaovficvo6lbgcnkhpunpjiokx2faolcknehnql43k.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:11 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.2 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:11 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:11 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.4 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:11 [compilation/backends.py:203] Store the 1-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:11 [compilation/backends.py:203] Store the 2-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:12 [compilation/backends.py:203] Store the 3-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:12 [compilation/backends.py:203] Store the 4-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:12 [compilation/backends.py:203] Store the 5-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:12 [compilation/backends.py:203] Store the 6-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:12 [compilation/backends.py:203] Store the 7-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:12 [compilation/backends.py:203] Store the 8-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:13 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.3 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:13 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:13 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:14 [compilation/backends.py:203] Store the 9-th graph for dynamic shape from inductor via handle ('fiy3ydjpuxtgcc4pkzw3uwzza4gpjhkf67zcvu2oq4x5v37nkhac', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/zl/czlpv55c3ikz5nnjdnbwgg5cjgs7vflnuu6ctd6idhpd4buawerj.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:14 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.2 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:14 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:14 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:14 [compilation/backends.py:203] Store the 10-th graph for dynamic shape from inductor via handle ('fu5xzwwmsz3vkp2fjb5b5rdkpssiqqx72yyzfuq2jotm2ynmljmb', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ax/cax6tk7efetkibd5igofzkcmbb5bko5zjrmgraeicweukfuu6dtn.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:14 [compilation/backends.py:203] Store the 11-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:15 [compilation/backends.py:203] Store the 12-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:15 [compilation/backends.py:203] Store the 13-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:15 [compilation/backends.py:203] Store the 14-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:15 [compilation/backends.py:203] Store the 15-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:15 [compilation/backends.py:203] Store the 16-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:15 [compilation/backends.py:203] Store the 17-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:16 [compilation/backends.py:203] Store the 18-th graph for dynamic shape from inductor via handle ('fiy3ydjpuxtgcc4pkzw3uwzza4gpjhkf67zcvu2oq4x5v37nkhac', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/zl/czlpv55c3ikz5nnjdnbwgg5cjgs7vflnuu6ctd6idhpd4buawerj.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:16 [compilation/backends.py:203] Store the 19-th graph for dynamic shape from inductor via handle ('fu5xzwwmsz3vkp2fjb5b5rdkpssiqqx72yyzfuq2jotm2ynmljmb', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ax/cax6tk7efetkibd5igofzkcmbb5bko5zjrmgraeicweukfuu6dtn.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:16 [compilation/backends.py:203] Store the 20-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:16 [compilation/backends.py:203] Store the 21-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:16 [compilation/backends.py:203] Store the 22-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:16 [compilation/backends.py:203] Store the 23-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:17 [compilation/backends.py:203] Store the 24-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:17 [compilation/backends.py:203] Store the 25-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:17 [compilation/backends.py:203] Store the 26-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:17 [compilation/backends.py:203] Store the 27-th graph for dynamic shape from inductor via handle ('fiy3ydjpuxtgcc4pkzw3uwzza4gpjhkf67zcvu2oq4x5v37nkhac', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/zl/czlpv55c3ikz5nnjdnbwgg5cjgs7vflnuu6ctd6idhpd4buawerj.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/backends.py:203] Store the 28-th graph for dynamic shape from inductor via handle ('fu5xzwwmsz3vkp2fjb5b5rdkpssiqqx72yyzfuq2jotm2ynmljmb', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ax/cax6tk7efetkibd5igofzkcmbb5bko5zjrmgraeicweukfuu6dtn.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/backends.py:203] Store the 29-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/backends.py:203] Store the 30-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/backends.py:203] Store the 31-th graph for dynamic shape from inductor via handle ('ffq2ojuexa4jpyfetharlgigg6teo57tn2y6p6gde7mzv367dtoi', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/g7/cg7utzxdpgco5pjl577raum3eishvtarcrhdiule7vaeedxwf4xk.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.1 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:18 [compilation/backends.py:203] Store the 32-th graph for dynamic shape from inductor via handle ('ffpgn7qqultizkbp2nf6l4qhebnxlj267dr4s4boijo7i4vvtrcn', '/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/zn/cznh7qlndnolh5xlceqzftcruyni7gysa6wxblqp2itkqykn7plz.py')\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:18 [compilation/backends.py:218] Compiling a graph for dynamic shape takes 9.17 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:19 [compilation/backends.py:602] Computation graph saved to /insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/backbone/computation_graph.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:23 [compilation/monitor.py:34] torch.compile takes 12.51 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:24 [v1/worker/gpu_worker.py:284] Initial free memory: 47.10 GiB; Requested memory: 0.90 (util), 42.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:24 [v1/worker/gpu_worker.py:291] Free memory after profiling: 28.73 GiB (total), 24.30 GiB (within requested)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:24 [v1/worker/gpu_worker.py:297] Memory profiling takes 18.04 seconds. Total non KV cache memory: 19.36GiB; torch peak memory increase: 1.08GiB; non-torch forward increase memory: 0.04GiB; weights memory: 18.25GiB.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:24 [v1/worker/gpu_worker.py:298] Available KV cache memory: 23.30 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m WARNING 12-17 07:28:24 [v1/core/kv_cache_utils.py:982] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:24 [v1/core/kv_cache_utils.py:1087] GPU KV cache size: 184,800 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:24 [v1/core/kv_cache_utils.py:1091] Maximum concurrency for 16,384 tokens per request: 91.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|▏         | 1/67 [00:00<00:10,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:25 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:25 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 3/67 [00:00<00:10,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:25 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:25 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|▋         | 5/67 [00:00<00:09,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:25 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:25 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|█         | 7/67 [00:01<00:09,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:25 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:26 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|█▎        | 9/67 [00:01<00:08,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:26 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:26 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▋        | 11/67 [00:01<00:08,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:26 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:26 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:01<00:07,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:26 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:26 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 15/67 [00:02<00:07,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:27 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:27 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 17/67 [00:02<00:07,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:27 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:27 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 19/67 [00:02<00:06,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:27 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:27 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:02<00:05,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:27 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:03<00:05,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 25/67 [00:03<00:04,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 27/67 [00:03<00:04,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 29/67 [00:03<00:04,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:28 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▋     | 31/67 [00:04<00:04,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 34/67 [00:04<00:03,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:04<00:02, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:29 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 42/67 [00:05<00:02, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 46/67 [00:05<00:01, 13.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 50/67 [00:05<00:01, 14.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 54/67 [00:05<00:00, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:30 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 58/67 [00:06<00:00, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|█████████▎| 62/67 [00:06<00:00, 16.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:31 [compilation/cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:06<00:00, 10.10it/s]\n",
      "Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:32 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:27,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:32 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:32 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:01<00:11,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:06,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:04,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:02,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:02,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:33 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:34 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:35 [compilation/cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:36 [v1/worker/gpu_model_runner.py:3480] Graph capturing finished in 12 secs, took 0.68 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:36 [v1/worker/gpu_worker.py:393] Free memory on device (47.1/47.4 GiB) on startup. Desired GPU memory utilization is (0.9, 42.66 GiB). Actual usage is 18.25 GiB for weight, 1.08 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.68 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=24133454950` (22.48 GiB) to fit into requested memory, or `--kv-cache-memory=28894258688` (26.91 GiB) to fully utilize gpu memory. Current kv cache memory in use is 23.3 GiB.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m INFO 12-17 07:28:36 [v1/engine/core.py:210] init engine (profile, create kv cache, warmup model) took 30.15 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:37 [v1/engine/core.py:737] EngineCore waiting for work.\n",
      "INFO 12-17 07:28:37 [llm.py:306] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:28:37 [v1/engine/core.py:737] EngineCore waiting for work.\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER NOTEBOOK: MCQ decoding + parsing sanity test for vLLM + Jamba/Mamba/etc.\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\"\n",
    "os.environ[\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\"] = \"1\"\n",
    "\n",
    "from typing import Optional, List, Dict, Any\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# -----------------------\n",
    "# Config (edit these)\n",
    "# -----------------------\n",
    "MODEL = \"ibm-ai-platform/Bamba-9B-v1\"   # or \"ai21labs/Jamba-7B\", \"mistralai/Mamba-Codestral-7B-v0.1\", etc.\n",
    "CTX_LEN = 16384\n",
    "DTYPE = \"float16\"\n",
    "TP = 1\n",
    "GPU_MEM_UTIL = 0.90\n",
    "\n",
    "USE_CHAT = True   # for Jamba; set False for some completion-only models\n",
    "MAX_NEW_MAIN = 4  # MCQ: 2-4 is plenty\n",
    "MAX_NEW_REP = 1\n",
    "\n",
    "# -----------------------\n",
    "# Prompt builder\n",
    "# -----------------------\n",
    "DEFAULT_0SHOT = \"\"\"{doc}\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "A. {a}\n",
    "B. {b}\n",
    "C. {c}\n",
    "D. {d}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def build_prompt(doc: str, q: str, A: str, B: str, C: str, D: str) -> str:\n",
    "    p = DEFAULT_0SHOT.format(doc=doc.strip(), q=q.strip(), a=A.strip(), b=B.strip(), c=C.strip(), d=D.strip())\n",
    "    p += \"\\n\\nReturn exactly one letter: A, B, C, or D.\"\n",
    "    return p\n",
    "\n",
    "def should_use_chat(name: str) -> bool:\n",
    "    s = (name or \"\").lower()\n",
    "    return any(k in s for k in [\"instruct\", \"chat\", \"assistant\", \"jamba\"])\n",
    "\n",
    "def maybe_apply_chat_template(tok, prompt: str, use_chat: bool) -> str:\n",
    "    if not use_chat:\n",
    "        return prompt\n",
    "    if hasattr(tok, \"apply_chat_template\"):\n",
    "        return tok.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    return prompt\n",
    "\n",
    "# -----------------------\n",
    "# Answer extractor (fixed)\n",
    "# -----------------------\n",
    "ANSWER_PATTERNS = [\n",
    "    re.compile(r\"(?:final\\s*answer|answer)\\s*[:\\-]\\s*\\(?([A-D])\\)?\", re.IGNORECASE),\n",
    "    re.compile(r\"the\\s+correct\\s+answer\\s+is\\s*\\(?([A-D])\\)?\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    if not response:\n",
    "        return None\n",
    "    text = response.replace(\"*\", \"\").strip()\n",
    "    tail = text[-512:]\n",
    "    for pat in ANSWER_PATTERNS:\n",
    "        m = pat.search(tail)\n",
    "        if m:\n",
    "            return m.group(1).upper()\n",
    "    m = re.search(r\"\\b([A-D])\\b\", tail)\n",
    "    return m.group(1).upper() if m else None\n",
    "\n",
    "# -----------------------\n",
    "# Load tokenizer + vLLM\n",
    "# -----------------------\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL,\n",
    "    trust_remote_code=True,\n",
    "    dtype=DTYPE,\n",
    "    tensor_parallel_size=TP,\n",
    "    max_model_len=CTX_LEN,\n",
    "    gpu_memory_utilization=GPU_MEM_UTIL,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tests: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"id\": \"toy_1\",\n",
    "        \"doc\": \"Video PreTraining (VPT) learns to act by watching unlabeled videos and uses a small labeled set to train an inverse dynamics model to label actions.\",\n",
    "        \"q\": \"Which of the following statements is correct?\",\n",
    "        \"A\": \"Both contractor data and data crawled from the Internet are used to train VPT agents to model state-action pairs.\",\n",
    "        \"B\": \"All machine learning methods involved in the two articles are related to neural network deep learning.\",\n",
    "        \"C\": \"Both voyager and VPT control Minecraft agents by predicting the actions of simulated mouse and keyboard operations in each given state.\",\n",
    "        \"D\": \"VPT's modeling of action space is approximate rather than precise.\",\n",
    "        \"gold\": \"D\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d84e1aef-62db-442d-8b1e-0977b11ba334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m DEBUG 12-17 07:34:08 [v1/engine/core.py:743] EngineCore loop active.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                  | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [logging_utils/dump_input.py:69] Dumping input data for V1 LLM engine (v0.11.0) with config: model='ibm-ai-platform/Bamba-9B-v1', speculative_config=None, tokenizer='ibm-ai-platform/Bamba-9B-v1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ibm-ai-platform/Bamba-9B-v1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":\"/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/backbone\"}, \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [logging_utils/dump_input.py:76] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=3,prompt_token_ids_len=5133,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([34, 35, 36, 37, 38, 39, 40, 41, 42, 43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53]),num_computed_tokens=0,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=[], resumed_from_preemption=[], new_token_ids=[], new_block_ids=[], num_computed_tokens=[]), num_scheduled_tokens={3: 5133}, total_num_scheduled_tokens=5133, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], finished_req_ids=[], free_encoder_mm_hashes=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710] EngineCore encountered a fatal error.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 701, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     engine_core.run_busy_loop()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 728, in run_busy_loop\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     self._process_engine_step()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 754, in _process_engine_step\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     outputs, model_executed = self.step_fn()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]                               ^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 284, in step\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     model_output = self.execute_model_with_error_logging(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 270, in execute_model_with_error_logging\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     raise err\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 261, in execute_model_with_error_logging\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     return model_fn(scheduler_output)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 103, in execute_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     output = self.collective_rpc(\"execute_model\",\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 447, in execute_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     output = self.model_runner.execute_model(scheduler_output,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2413, in execute_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     ) = self._bookkeeping_sync(scheduler_output, sampler_output,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2144, in _bookkeeping_sync\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     valid_sampled_token_ids = self._to_list(sampled_token_ids)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4159, in _to_list\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     self.transfer_event.synchronize()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/cuda/streams.py\", line 231, in synchronize\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710]     super().synchronize()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710] torch.AcceleratorError: CUDA error: device-side assert triggered\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m ERROR 12-17 07:34:09 [v1/engine/core.py:710] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [64,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [65,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [66,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [67,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [72,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [73,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [74,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [75,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [80,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [81,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [82,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [83,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [88,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [89,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [90,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "/insomnia001/home/dwz2107/.cache/vllm/torch_compile_cache/66dcbb55ec/rank_0_0/inductor_cache/ii/ciil7rhth7sqj5ecr2awv7wlwey2bil2uiejvrqlufo3xiowouz7.py:37: unknown: block: [8338,0,0], thread: [91,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp10, [XBLOCK]) < 4096` failed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/shared/apps/anaconda/2023.09/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/shared/apps/anaconda/2023.09/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 701, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     engine_core.run_busy_loop()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 728, in run_busy_loop\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     self._process_engine_step()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 754, in _process_engine_step\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     outputs, model_executed = self.step_fn()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m                               ^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 284, in step\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     model_output = self.execute_model_with_error_logging(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 270, in execute_model_with_error_logging\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     raise err\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 261, in execute_model_with_error_logging\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     return model_fn(scheduler_output)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 103, in execute_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     output = self.collective_rpc(\"execute_model\",\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 447, in execute_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     output = self.model_runner.execute_model(scheduler_output,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2413, in execute_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     ) = self._bookkeeping_sync(scheduler_output, sampler_output,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2144, in _bookkeeping_sync\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     valid_sampled_token_ids = self._to_list(sampled_token_ids)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4159, in _to_list\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     self.transfer_event.synchronize()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m   File \"/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/torch/cuda/streams.py\", line 231, in synchronize\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m     super().synchronize()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m torch.AcceleratorError: CUDA error: device-side assert triggered\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2290876)\u001b[0;0m \n"
     ]
    },
    {
     "ename": "EngineDeadError",
     "evalue": "EngineCore encountered an issue. See stack trace (above) for the root cause.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEngineDeadError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# IMPORTANT: vLLM expects a list of prompts\u001b[39;00m\n\u001b[32m    106\u001b[39m raw = add_filler_tokens(raw, tok, n_tokens=\u001b[32m5000\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m out = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp_main\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    109\u001b[39m text = out.outputs[\u001b[32m0\u001b[39m].text \u001b[38;5;28;01mif\u001b[39;00m out.outputs \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m pred = extract_answer(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:401\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    390\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    391\u001b[39m     prompts, lora_request)\n\u001b[32m    393\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    394\u001b[39m     prompts=prompts,\n\u001b[32m    395\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    398\u001b[39m     priority=priority,\n\u001b[32m    399\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:1600\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1598\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1599\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1602\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:265\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    268\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/insomnia001/depts/edu/COMS-E6998-015/dwz2107/envs/ssm-venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:670\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    668\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.outputs_queue.get()\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m670\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outputs.wave_complete \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    672\u001b[39m     \u001b[38;5;28mself\u001b[39m.engines_running = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mEngineDeadError\u001b[39m: EngineCore encountered an issue. See stack trace (above) for the root cause."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1217 07:34:09.862728493 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# -------------------------\n",
    "# Config for BAMBA\n",
    "# -------------------------\n",
    "MODEL = \"ibm-ai-platform/Bamba-9B-v1\"  # adjust if your hf_id differs\n",
    "USE_CHAT = False  # IMPORTANT: Bamba is completion-style\n",
    "MAX_NEW_MAIN = 8\n",
    "MAX_NEW_REP = 16\n",
    "\n",
    "# -------------------------\n",
    "# Answer patterns + extractor\n",
    "# -------------------------\n",
    "ANSWER_PATTERNS = [\n",
    "    re.compile(r\"(?:final\\s*answer|answer)\\s*[:\\-]\\s*\\(?\\s*([A-D])\\s*\\)?\", re.IGNORECASE),\n",
    "    re.compile(r\"the\\s+correct\\s+answer\\s+is\\s*[:\\-]?\\s*\\(?\\s*([A-D])\\s*\\)?\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    if not response:\n",
    "        return None\n",
    "    text = response.replace(\"*\", \"\").strip()\n",
    "    tail = text[-512:]\n",
    "\n",
    "    m = re.search(r\"\\\\boxed\\s*\\{\\s*([A-D])\\s*\\}\", tail, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "\n",
    "    for pat in ANSWER_PATTERNS:\n",
    "        m = pat.search(tail)\n",
    "        if m:\n",
    "            return m.group(1).upper()\n",
    "\n",
    "    m = re.search(r\"\\b([A-D])\\b\", tail)\n",
    "    return m.group(1).upper() if m else None\n",
    "\n",
    "# -------------------------\n",
    "# Prompt builders\n",
    "# -------------------------\n",
    "DEFAULT_0SHOT = \"\"\"{doc}\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "A. {a}\n",
    "B. {b}\n",
    "C. {c}\n",
    "D. {d}\n",
    "\n",
    "Final answer:\"\"\"\n",
    "\n",
    "def add_filler_tokens(prompt: str, tok, n_tokens: int = 5000) -> str:\n",
    "    \"\"\"\n",
    "    Appends ~n_tokens of harmless filler text, measured in tokenizer tokens.\n",
    "    \"\"\"\n",
    "    filler_unit = \" lorem\"  # stable, non-special\n",
    "    unit_ids = tok.encode(filler_unit, add_special_tokens=False)\n",
    "    if not unit_ids:\n",
    "        raise RuntimeError(\"Tokenizer produced no tokens for filler_unit\")\n",
    "\n",
    "    reps = (n_tokens // len(unit_ids)) + 1\n",
    "    filler_text = filler_unit * reps\n",
    "\n",
    "    # Trim to exactly n_tokens\n",
    "    ids = tok.encode(filler_text, add_special_tokens=False)[:n_tokens]\n",
    "    filler_text = tok.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "    return prompt + \"\\n\\n[FILLER]\\n\" + filler_text\n",
    "\n",
    "def build_prompt(doc: str, q: str, A: str, B: str, C: str, D: str) -> str:\n",
    "    return DEFAULT_0SHOT.format(\n",
    "        doc=doc.strip(), q=q.strip(), a=A.strip(), b=B.strip(), c=C.strip(), d=D.strip()\n",
    "    )\n",
    "\n",
    "def build_reprompt(q: str, A: str, B: str, C: str, D: str, prev: str) -> str:\n",
    "    # Keep it short; include choices so A/B/C/D is grounded.\n",
    "    return (\n",
    "        \"Return exactly one letter: A, B, C, or D.\\n\\n\"\n",
    "        f\"Question: {q.strip()}\\n\"\n",
    "        f\"Prev: {prev.strip()[:300]}\\n\"\n",
    "        f\"A. {A.strip()}\\n\"\n",
    "        f\"B. {B.strip()}\\n\"\n",
    "        f\"C. {C.strip()}\\n\"\n",
    "        f\"D. {D.strip()}\\n\\n\"\n",
    "        \"Final answer:\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer + sampling params\n",
    "# -------------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "sp_main = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=MAX_NEW_MAIN, seed=0)\n",
    "sp_rep  = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=MAX_NEW_REP, seed=42)\n",
    "\n",
    "# -------------------------\n",
    "# Run tests (expects `llm` and `tests` already defined)\n",
    "# tests items must have: id, doc, q, A, B, C, D, (optional) gold\n",
    "# -------------------------\n",
    "for t in tests:\n",
    "    raw = build_prompt(t[\"doc\"], t[\"q\"], t[\"A\"], t[\"B\"], t[\"C\"], t[\"D\"])\n",
    "\n",
    "    # IMPORTANT: vLLM expects a list of prompts\n",
    "    raw = add_filler_tokens(raw, tok, n_tokens=5000)\n",
    "\n",
    "    out = llm.generate([raw], sp_main)[0]\n",
    "    text = out.outputs[0].text if out.outputs else \"\"\n",
    "    pred = extract_answer(text)\n",
    "\n",
    "    rep_text = \"\"\n",
    "    if pred is None:\n",
    "        rep_raw = build_reprompt(t[\"q\"], t[\"A\"], t[\"B\"], t[\"C\"], t[\"D\"], text)\n",
    "        print(rep_raw)\n",
    "        rep_out = llm.generate([rep_raw], sp_rep)[0]\n",
    "        rep_text = rep_out.outputs[0].text if rep_out.outputs else \"\"\n",
    "        pred = extract_answer(rep_text)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"id:\", t[\"id\"])\n",
    "    print(\"PROMPT (last 400 chars):\\n\", raw[-400:])\n",
    "    print(\"\\nMAIN OUTPUT:\\n\", repr(text))\n",
    "    print(\"MAIN pred:\", pred)\n",
    "    if rep_text:\n",
    "        print(\"\\nREPROMPT OUTPUT:\\n\", repr(rep_text))\n",
    "        print(\"REP pred:\", pred)\n",
    "    if \"gold\" in t:\n",
    "        print(\"gold:\", t[\"gold\"], \"correct:\", (pred == t[\"gold\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f682a8e-3e28-4df8-a60f-c84e3405a13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538b066-26b1-4551-9a56-7f119a658cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
