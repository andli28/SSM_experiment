{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 0, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI have subdomain.example.com that I use for development purposes. My web application solution contains a web API etc, that I need to call from external systems, hence I am not using localhost.\nI now need to test for SSL and need a certificate for my subdomain.example.com development domain name.\nI have tried creating a self-signed certificate as outlined in http://technet.microsoft.com/en-us/library/cc753127(v=ws.10).aspx, but this certificate only works for localhost. Can this certificate be used for my purpose or will I have to create a self-signed for my development subdomain? If I have to create a self-signed certification for my development subdomain, what utility or online service (Free) can I use for this?\n\n\nCandidate answers:\nA1:\n\nI had this error occur when using visual studio. This occurs when you have the Cryptographic Network provider settings set to OpenSSL in the Visual Studio Options window. When I changed the setting to Secure Channel it solved it for me. This setting must have been set for me when I upgraded my VS.\n\n\nA2:\n\naddition to Sam.Rueby's Answer:\nThere is not a StoreName enumeration value for Web Hosting, but Web Hosting store can be accessed by using StoreName value as \"WebHosting\".\nvar store = new X509Store( \"WebHosting\", StoreLocation.LocalMachine);\n\n\n\nA3:\n\nI'm not sure what kind of scheduled tasks you mean. If you mean stuff like \"every hour, refresh foo.xml\" type tasks, then use the Windows Scheduled Tasks system. (The \"at\" command, or via the controller.) Have it either run a console app or request a special page that kicks off the process.\nEdit: I should add, this is an OK way to get your IIS app running at scheduled points too. So suppose you want to check your DB every 30 minutes and email reminders to users about some data, you can use scheduled tasks to request this page and hence get IIS processing things.\nIf your needs are more complex, you might consider creating a Windows Service and having it run a loop to do whatever processing you need. This also has the benefit of separating out the code for scaling or management purposes. On the downside, you need to deal with Windows services.\n\n\nA4:\n\nThis library works like a charm\nhttp://www.codeproject.com/KB/cs/tsnewlib.aspx\nIt allows you to manage Windows scheduled tasks directly through your .NET code.\n\n\nA5:\n\nI agree with CyptoGuy that IIS.NET states the the difference is that Web Hosting is designed to scale to more certificates. I think I found another important difference, which is that I have yet to find a way to access the Web Hosting store from .NET code. Specifically because there is not a StoreName enumeration value for Web Hosting: http://msdn.microsoft.com/query/dev12.query?appId=Dev12IDEF1&l=EN-US&k=k%28System.Security.Cryptography.X509Certificates.StoreName%29;k%28TargetFrameworkMoniker-.NETFramework\n\n\nA6:\n\nTo fix the especific error SSL certificate problem: unable to get local issuer certificate in git \nI had the same issue with Let's Encrypt certificates .\nAn web site with https we just to need :\nSSLEngine On\nSSLCertificateFile /etc/letsencrypt/live/example.com/cert.pem\nSSLCertificateKeyFile /etc/letsencrypt/live/example.com/privkey.pem\nInclude /etc/letsencrypt/options-ssl-apache.conf\n\nbut git pull says : \nfatal: unable to access 'https://example.com/git/demo.git/': SSL certificate problem: unable to get local issuer certificate\n\nTo fix it, we need also add: \nSSLCertificateChainFile /etc/letsencrypt/live/example.com/chain.pem\n\n\n\nA7:\n\nOne thing that messed me up was the format of the path (on my Windows PC).  I originally had this:\ngit config --global http.sslCAInfo C:\\certs\\cacert.pem\n\nBut that failed with the \"unable to get local issuer certificate\" error.\nWhat finally worked was this:\ngit config --global http.sslCAInfo \"C:\\\\certs\\\\cacert.pem\"\n\n\n\nA8:\n\nOne option would be to set up a windows service and get that to call your scheduled task.\nIn winforms I've used Timers put don't think this would work well in ASP.NET\n\n\nA9:\n\nIn my case, as I have installed the ConEmu Terminal for Window 7, it creates the ca-bundle during installation at C:\\Program Files\\Git\\mingw64\\ssl\\certs.\nThus, I have to run the following commands on terminal to make it work:\n$ git config --global http.sslbackend schannel\n$ git config --global http.sslcainfo /mingw64/ssl/certs/ca-bundle.crt\n\nHence, my C:\\Program Files\\Git\\etc\\gitconfig contains the following:\n[http]\n    sslBackend = schannel\n    sslCAinfo = /mingw64/ssl/certs/ca-bundle.crt\n\nAlso, I chose same option as mentioned here when installing the Git.\nHope that helps!\n\n\nA10:\n\nDownload and install local certificate. Probably it is published at your company site. For instance, *.cer file.\n\nRight click it and select Install Certificate. 'Certificate Inport Wizard' will appear. Select Local Machine. Press Next, confirm.\n\nSelect Place all certificates in the following store, press Browse and select Trusted Root Certification Authorities, OK, Finish.\n\n\n\nAlso you can check if other applications can fetch, pull or push data. For instance, in Android Studio or probably IDEA you should select in Settings this checkbox: Use credential helper.\n\n\nA11:\n\nI faced this issue as well. And finally got resolved by getting guidance from this MSDN Blog.\nUpdate\nActually you need to add the certificate in git's certificates file curl-ca-bundel.cert that resides in Git\\bin directory.\nSteps\n\nOpen your github page in browser, and click over lock icon in address bar.\nIn the opened little popup up navigate to 'view certificate' link, it will open a popup window.\nIn which navigate to certificates tab (3rd in my case). Select the top node that is root certificate. And press copy certificate button in the bottom and save the file.\nIn file explorer navigate Git\\bin directory and open curl-ca-bundle.crt in text editor.\nOpen the exported certificate file (in step 3) in text editor as well.\nCopy all of the content from exported certificate to the end of curl-ca-bundle.crt, and save.\n\nFinally check the status. Please note that backup curl-ca-bundle.crt file before editing to remain on safe side.\n\n\nA12:\n\nsolved my problem\ngit config --global http.sslBackend schannel\n\n\nA13:\n\nJan 2021 - Got around this in VS2019 by setting Menu > Git > Settings > Git Global Settings > Cryptographic Network Provider > [Secure Channel] instead of [OpenSSL]\nGit SSL certificate problem unable to get local issuer certificate (fix)\nPS: Didn't need to set --global or --local http.sslVerify false. I was cloning an Azure DevOps repo which wasn't using any self signed certs.. This seems like an issue with either VS2019 or Git for Windows.. They need to fix it !!\n\n\nA14:\n\nAll of my tasks (which need to be scheduled) for a website are kept within the website and called from a special page.  I then wrote a simple Windows service which calls this page every so often.  Once the page runs it returns a value.  If I know there is more work to be done, I run the page again, right away, otherwise I run it in a little while.  This has worked really well for me and keeps all my task logic with the web code.  Before writing the simple Windows service, I used Windows scheduler to call the page every x minutes.   \nAnother convenient way to run this is to use a monitoring service like Pingdom.  Point their http check to the page which runs your service code.  Have the page return results which then can be used to trigger Pingdom to send alert messages when something isn't right.\n\n\nA15:\n\nJust for future quick reference, the commands are:\nImport-Module WebAdministration\nStop-WebSite 'Default Web Site'\nStart-WebSite 'Default Web Site'\n\n\n\nA16:\n\ngit config --global http.sslVerify false\n\n\nA17:\n\nI got this error when trying to \"clone\" the project.  One work-around is to just use the \"download as zip\" on the webpage, which, for me, achieved what I wanted to do.\n\n\nA18:\n\nI have resolved the issue by adding below entry in ${HOME}/.gitconfig file\n[remote \"origin\"]\nproxy=\nIn most case it will happen when proxy enabled in your machine so above mentioned entry will fix this problem.\n\n\nA19:\n\nI've just had the same issue but using sourcetree on windows Same steps for normal GIT on Windows as well. Following the following steps I was able to solve this issue.\n\nObtain the server certificate tree\nThis can be done using chrome.\nNavigate to be server address.\nClick on the padlock icon and view the certificates.\nExport all of the certificate chain as base64 encoded files (PEM) format.\nAdd the certificates to the trust chain of your GIT trust config file\nRun \"git config --list\".\nfind the \"http.sslcainfo\" configuration this shows where the certificate trust file is located.\nCopy all the certificates into the trust chain file including the \"- -BEGIN- -\" and the \"- -END- -\".\nMake sure you add the entire certificate Chain to the certificates file\n\nThis should solve your issue with the self-signed certificates and using GIT.\nI tried using the \"http.sslcapath\" configuration but this did not work. Also if i did not include the whole chain in the certificates file then this would also fail. If anyone has pointers on these please let me know as the above has to be repeated for a new install.\nIf this is the system GIT then you can use the options in TOOLS -> options\nGIt tab to use the system GIT and this then solves the issue in sourcetree as well.\n\n\nA20:\n\nI've found this to be easy for all involved:\n\nCreate a webservice method such as DoSuchAndSuchProcess\nCreate a console app that calls this webmethod.\nSchedule the console app in the task scheduler. \n\nUsing this methodology all of the business logic is contained in your web app, but you have the reliability of the windows task manager, or any other commercial task manager to kick it off and record any return information such as an execution report. Using a web service instead of posting to a page has a bit of an advantage because it's easier to get return data from a webservice.\n\n\nA21:\n\nWhy reinvent the wheel, use the Threading and the Timer class.\n    protected void Application_Start()\n    {\n        Thread thread = new Thread(new ThreadStart(ThreadFunc));\n        thread.IsBackground = true;\n        thread.Name = \"ThreadFunc\";\n        thread.Start();\n    }\n\n    protected void ThreadFunc()\n    {\n        System.Timers.Timer t = new System.Timers.Timer();\n        t.Elapsed += new System.Timers.ElapsedEventHandler(TimerWorker);\n        t.Interval = 10000;\n        t.Enabled = true;\n        t.AutoReset = true;\n        t.Start();\n    }\n\n    protected void TimerWorker(object sender, System.Timers.ElapsedEventArgs e)\n    {\n        //work args\n    }\n\n\n\nA22:\n\nI've used Abidar successfully in an ASP.NET project (here's some background information).\nThe only problem with this method is that the tasks won't run if the ASP.NET web application is unloaded from memory (ie. due to low usage).  One thing I tried is creating a task to hit the web application every 5 minutes, keeping it alive, but this didn't seem to work reliably, so now I'm using the Windows scheduler and basic console application to do this instead.\nThe ideal solution is creating a Windows service, though this might not be possible (ie. if you're using a shared hosting environment).  It also makes things a little easier from a maintenance perspective to keep things within the web application.\n\n\nA23:\n\nUpdate IIS 10.0+ (2017+)\nYou can use the newer IISAdministration module with the Start-IISSite cmdlet like this:\nImport-Module IISAdministration\nStart-IISSite -Name \"Default Web Site\"\n\nOr by using Get-IISSite, you can then start / stop like this:\nImport-Module IISAdministration\n$site = Get-IISSite \"Default Web Site\"\n$site.Start()\n$site.Stop()\n\n\n\nA24:\n\nCreate a custom Windows Service.\nI had some mission-critical tasks set up as scheduled console apps and found them difficult to maintain. I created a Windows Service with a 'heartbeat' that would check a schedule in my DB every couple of minutes. It's worked out really well.\nHaving said that, I still use scheduled console apps for most of my non-critical maintenance tasks. If it ain't broke, don't fix it.\n\n\nA25:\n\nTo completely detail out the summary of all the above answers.\nReason\nThis problem is occuring because git cannot complete the https handshake with the git server were the repository you are trying to access is present.\nSolution\nSteps to get the certificate from the github server\n\nOpen the github you are trying to access in the browser\nPress on the lock icon in the address bar > click on 'certificate'\nGo to 'Certification Path' tab > select the top most node in the hierarchy of certificates > click on 'view certificate'\nNow click on 'Details' and click on 'Copy to File..' > Click 'Next' > Select 'Base 64 encoded X509 (.CER)' > save it to any of your desired path.\n\nSteps to add the certificate to local git certificate store\n\nNow open the certificate you saved in the notepad and copy the content along with --Begin Certificate-- and --end certificate--\n\nTo find the path were all the certificates are stored for your git, execute the following command in cmd.\ngit config --list\n\nCheck for the key 'http.sslcainfo', the corresponding value will be path.\n\n\n\nNote: If u can't find the key http.sslcainfo check for Git's default path: C:\\Program Files\\Git\\mingw64\\ssl\\certs\n\n\nNow open 'ca-bundle.crt' present in that path.\n\n\nNote 1 : open this file administrator mode otherwise you will not be able to save it after update. (Tip - you can use Notepad++ for this\npurpose)\nNote 2 : Before modifying this file please keep a backup elsewhere.\n\n\nNow copy the contents of file mentioned in step 1 to the file in step 4 at end file, like how other certificates are placed in ca-bundle.crt.\nNow open a new terminal and now you should be able to perform operations related to the git server using https.\n\n\n\nA26:\n\nWhen using Windows, the problem resides that git by default uses the \"Linux\" crypto backend. Starting with Git for Windows 2.14, you can configure Git to use SChannel, the built-in Windows networking layer as the crypto backend. To do that, just run the following command in the GIT client:\ngit config --global http.sslbackend schannel\n\nThis means that it will use the Windows certificate storage mechanism and you don't need to explicitly configure the curl CA storage (http.sslCAInfo) mechanism.\n\n\nA27:\n\nIn case of github Repositories (or any none-self-signed certs), choosing below while installing Git-on-windows, resolved the issue.\n\n\n\nA28:\n\ngit config --global http.sslbackend secure-transport\n\n(had to do that after update to Big S\u044er)\n\n\nA29:\n\nIn my case, I had to use different certificates for different git repositories.\nFollow steps below (If you have a certificate of your repository, you can read from step 5)\n\nGo to remote repository's site. Ex: github.com, bitbucket.org, tfs.example...\n\nClick Lock icon on the upper left side and click Certificate.\n\nGo to Certification Path tab and double click to .. Root Certificate\n\nGo to Details tab and click Copy to file.\n\nExport/Copy certificate to wherever you want. Ex: C:\\certs\\example.cer\n\nOpen git bash at your local repository folder and type:\n$ git config http.sslCAInfo \"C:\\certs\\example.cer\"\n\n\nNow you can use different certificates for each repository.\nRemember, calling with the --global parameter will also change the certificates of git repositories in other folders, so you should not use the --global parameter when executing this command.\n\n\nA30:\n\nThis technique by Jeff Atwood for Stackoverflow is the simplest method I've come across. It relies on the \"cache item removed\" callback mechanism build into ASP.NET's cache system\nUpdate: Stackoverflow has outgrown this method. It only works while the website is running but it's a very simple technique that is useful for many people.\nAlso check out Quartz.NET\n\n\nA31:\n\nWith IIS's self-signed certificate feature, you cannot set the common name (CN) for the certificate, and therefore cannot create a certificate bound to your choice of subdomain.\nOne way around the problem is to use makecert.exe, which is bundled with the .Net 2.0 SDK. On my server it's at:\nC:\\Program Files\\Microsoft.Net\\SDK\\v2.0 64bit\\Bin\\makecert.exe\n\nYou can create a signing authority and store it in the LocalMachine certificates repository as follows (these commands must be run from an Administrator account or within an elevated command prompt):\nmakecert.exe -n \"CN=My Company Development Root CA,O=My Company,\n OU=Development,L=Wallkill,S=NY,C=US\" -pe -ss Root -sr LocalMachine\n -sky exchange -m 120 -a sha1 -len 2048 -r\n\nYou can then create a certificate bound to your subdomain and signed by your new authority:\n(Note that the the value of the -in parameter must be the same as the CN value used to generate your authority above.)\nmakecert.exe -n \"CN=subdomain.example.com\" -pe -ss My -sr LocalMachine\n -sky exchange -m 120 -in \"My Company Development Root CA\" -is Root\n -ir LocalMachine -a sha1 -eku 1.3.6.1.5.5.7.3.1\n\nYour certificate should then appear in IIS Manager to be bound to your site as explained in Tom Hall's post.\nAll kudos for this solution to Mike O'Brien for his excellent blog post at http://www.mikeobrien.net/blog/creating-self-signed-wildcard\n\n\nA32:\n\nYou might have a DNS issue and not a certificate issue, so before you disable SSL verification in your Git shell you should rule out a DNS problem. Cases such as these have been mentioned in Q&A forums such as https-issues-possibly-related-to-dns. If you are using WSL on Windows as your terminal, then you can try running sudo echo nameserver 8.8.8.8 > /etc/resolv.conf and then issue the git commands to see if that makes a difference. This does not seem to be a permanent DNS fix (lasting only the lifetime of your terminal session), but it could help you determine whether it is a DNS issue and not a certificate issue. You could also check this document on configuring your network to use a public DNS. Again, this is only to help you determine if your DNS settings might need adjusting in order to help resolve the certificate issues.\n\n\nA33:\n\n\nDownload certificate from this link:\nhttps://github.com/bagder/ca-bundle\nAdd it to C:\\Program Files\\Git\\bin and C:\\Program Files\\Git\\mingw64\\bin\n\nThen try something like: git clone https://github.com/heroku/node-js-getting-started.git\n\n\nA34:\n\nTo get access to system modules, Powershell needs to be run like this:  \n[path]\\powershell.exe -NoExit -ImportSystemModules\n\nI found the above on this iis forum.\n\n\nA35:\n\nWe use console applications also.  If you use logging tools like Log4net you can properly monitor their execution.  Also, I'm not sure how they are more difficult to maintain than a web page, given you may be sharing some of the same code libraries between the two if it is designed properly.\nIf you are against having those tasks run on a timed basis, you could have a web page in your administrative section of your website that acts as a queue.  User puts in a request to run the task, it in turn inserts a blank datestamp record on MyProcessQueue table and your scheduled task is checking every X minutes for a new record in MyProcessQueue.  That way, it only runs when the customer wants it to run.\nHope those suggestions help.\n\n\nA36:\n\nTo avoid disabling ssl verification entirely or duplicating / hacking the bundled CA certificate file used by git, you can export the host's certificate chain into a file, and make git use it:\ngit config --global http.https://the.host.com/.sslCAInfo c:/users/me/the.host.com.cer\n\nIf that does not work, you can disable ssl verification only for the host:\ngit config --global http.https://the.host.com/.sslVerify false\n\nNote : Subjected to possible man in the middle attacks when ssl verification is turned off.\n\n\nA37:\n\nfrom IIS.NET:\n\nThe key difference between Web Hosting store and Personal store is that Web Hosting store is designed to scale to higher numbers of certificates.\n\nFor lightweight scenarios (when you have less than 20-30 certificates) you can use any, Personal or WebHosting store.\nWhen you have 30+ different certificates, you should use Web Hosting store for performance reasons.\n\n\nA38:\n\nOpen Git Bash and run the command if you want to completely disable SSL verification.\ngit config --global http.sslVerify false\n\nNote: This solution opens you to attacks like man-in-the-middle attacks.\nTherefore turn on verification again as soon as possible:\ngit config --global http.sslVerify true\n\n\n\nA39:\n\nAdding to Keith's answer, you can perform this remotely using Invoke-Command.\nImport-Module WebAdministration\n$siteName = \"Default Web Site\"\n$serverName = \"name\"\n$block = {Stop-WebSite $args[0]; Start-WebSite $args[0]};  \n$session = New-PSSession -ComputerName $serverName\nInvoke-Command -Session $session -ScriptBlock $block -ArgumentList $siteName \n\n\n\nA40:\n\nThis works for me. I opened cmd line and ran following command. and pulled again.\n\ngit config --global http.sslVerify false\n\n\n\nA41:\n\nI've had the same problem from Azure DevOps (Visual Studio). Finally I've decided to clone my repo using SSH protocol because of i've prefered it instead of disabling SSL verification.\nYou only need to generate a SSH Key, you can do it so... SSH documentation\nssh-keygen\n\nAnd then, import your public key on yout git host (like Azure Devops, Github, Bitbucket, Gitlab, etc.)\n\n\nA42:\n\nA New Task Scheduler Class Library for .NET \nNote: Since this library was created, Microsoft has introduced a new task scheduler (Task Scheduler 2.0) for Windows Vista. This library is a wrapper for the Task Scheduler 1.0 interface, which is still available in Vista and is compatible with Windows XP, Windows Server 2003 and Windows 2000.\nhttp://www.codeproject.com/KB/cs/tsnewlib.aspx\n\n\nA43:\n\nYou can easily create a Windows Service that runs code on interval using the 'ThreadPool.RegisterWaitForSingleObject' method. It is really slick and quite easy to get set up. This method is a more streamlined approach then to use any of the Timers in the Framework.\nHave a look at the link below for more information:\nRunning a Periodic Process in .NET using a Windows Service:\nhttp://allen-conway-dotnet.blogspot.com/2009/12/running-periodic-process-in-net-using.html\n\n\nA44:\n\nThe problem is that git by default using the \"Linux\" crypto backend.\nBeginning with Git for Windows 2.14, you can now configure Git to use SChannel, the built-in Windows networking layer as the crypto backend. This means that it will use the Windows certificate storage mechanism and you do not need to explicitly configure the curl CA storage mechanism: https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx\nJust execute:\ngit config --global http.sslbackend schannel\n\nThat should help.\nUsing schannel is by now the standard setting when installing git for Windows, also it is recommended to not checkout repositories by SSH anmore if possible, as https is easier to configure and less likely to be blocked by a firewall it means less chance of failure.\n\n\nA45:\n\nAn answer to  Using makecert for Development SSL fixed this for me.\nI do not know why, but the certificate created by the simple 'Create Self Signed Certificate' link in IIS Manager does not do the trick.  I followed the approach in the linked question of creating and installing a self-signed CA Root; then using that to issue a Server Authentication Certificate for my server.  I installed both of them in IIS.\nThat gets my situation the same as the blog post referenced in the original question.  Once the root certificate was copy/pasted into curl-ca-bundle.crt the git/curl combo were satisfied.\n\n\nA46:\n\nIf you own the server you should use the windows task scheduler. Use AT /? from the command line to see the options. \nOtherwise, from a web based environment, you might have to do something nasty like set up a different machine to make requests to a certain page on a timed interval.\n\n\nA47:\n\nHere's another way: \n1) Create a \"heartbeat\" web script that is responsible for launching the tasks if they are DUE or overdue to be launched. \n2) Create a scheduled process somewhere (preferrably on the same web server) that hits the webscript and forces it to run at a regular interval.  (e.g. windows schedule task that quietly launches the heatbeat script using IE or whathaveyou)\nThe fact that the task code is contained within a web script is purely for the sake of keeping the code within the web application code-base (the assumption is that both are dependent on each other), which would be easier for web developers to manage.\nThe alternate approach is to create an executable server script / program that does all the schedule work itself and run the executable itself as a scheduled task. This can allow for fundamental decoupling between the web application and the scheduled task. Hence if you need your scheduled tasks to run even in the even that the web app / database might be down or inaccessible, you should go with this approach. \n\n\nA48:\n\nI had this issue as well.  In my case, I was trying to get a post-receive Git hook to update a working copy on a server with each push.  Tried to follow the instructions in the blog you linked to.  Didn't work for me as well and overriding the settings on a per-user basis didn't seem to work either.\nWhat I ended up having to do was disable SSL verification (as the article mentions) for Git as a whole.  Not the perfect solution, but it'll work until I can figure out a better one.\nI edited the Git config text file (with my favorite line-ending neutral app like Notepad++) located at:\n\nC:\\Program Files (x86)\\Git\\etc\\gitconfig\n\nIn the [http] block, I added an option to disable sslVerify.  It looked like this when I was done:\n[http]\n    sslVerify = false\n    sslCAinfo = /bin/curl-ca-bundle.crt\n\nThat did the trick. \nNOTE: \n\nThis disables SSL verification and is not recommended as a long term solution.\nYou can disable this per-repository which still isn't great, but localizes the setting.\nWith the advent of LetsEncrypt.org, it is now fairly simple, automated and free to set up SSL as an alternative to self-signed certs and negates the need to turn off sslVerify.\n\n\n\nA49:\n\nI have also recently faced this issue, and you get this issue usually when you are behind some corporate firewall, and then git cannot complete the https handshake with the git repository that you are trying to access. One of the way to solve it, is to use this command:\ngit config --global http.sslverify false\n\nBut as others have said this pretty much disables the ssl verification. The only secure way is to get the SSL certificate and add it locally to your Git folder, specifically in the file ca-bundle.crt.\nOne way to solve it, is to use the package pcer, to install it you need to do the following:\nnpm i -g pcer\n\nThen you execute the following:\npcer alias git \"C:\\Program Files\\Git\\mingw64\\etc\\ssl\\certs\\ca-bundle.crt\"\n\nThis would make git point to the above path, and then you can fetch the certificate by doing the following:\npcer fetch github.com -l git\n\nThen you can perform git clone and it would work normally. For example:\n\nFor more information:\nhttps://www.npmjs.com/package/pcer\nhttps://github.com/PeterHdd/pcer\nNote, I'm the owner of this package\n\n\nA50:\n\nUsing PowerShell\nFrom Windows 8.1 and Windows Server 2012 R2 (Windows PowerShell 4.0) and upwards, you can create a self-signed certificate using the new New-SelfSignedCertificate cmdlet:\nExamples:\nNew-SelfSignedCertificate -DnsName www.mydomain.example -CertStoreLocation cert:\\LocalMachine\\My\n\nNew-SelfSignedCertificate -DnsName subdomain.mydomain.example -CertStoreLocation cert:\\LocalMachine\\My\n\nNew-SelfSignedCertificate -DnsName *.mydomain.example -CertStoreLocation cert:\\LocalMachine\\My\n\nUsing the IIS Manager\n\nLaunch the IIS Manager\nAt the server level, under IIS, select Server Certificates\nOn the right hand side under Actions select Create Self-Signed Certificate\nWhere it says \"Specify a friendly name for the certificate\" type in an appropriate name for reference.\n\nExamples: www.domain.example or subdomain.domain.example\n\n\nThen, select your website from the list on the left hand side\nOn the right hand side under Actions select Bindings\nAdd a new HTTPS binding and select the certificate you just created (if your certificate is a wildcard certificate you'll need to specify a hostname)\nClick OK and test it out.\n\n\n\nA51:\n\nI found that the following to stop individual websites on a remote server to work:\nInvoke-Command -Computername $servername -Scriptblock { \n    (Import-Module WebAdministration); \n    Stop-Website -Name \"WebsiteName\"; \n    Stop-Website -Name \"AnotherWebsiteName\"\n}\n\nI had some of the errors above until Import-Module was put in ()\n\n\nA52:\n\nError\n\npush failed\nfatal: unable to access\nSSL certificate problem: unable to get local issuer certificate\n\nReason\nAfter committing files on a local machine, the \"push fail\" error can occur when the local Git connection parameters are outdated (e.g. HTTP change to HTTPS).\nSolution\n\nOpen the .git folder in the root of the local directory\nOpen the config file in a code editor or text editor (VS Code, Notepad, Textpad)\nReplace HTTP links inside the file with the latest HTTPS or SSH link available from the web page of the appropriate Git repo (clone button)\nExamples:\nurl = http://git.[host]/[group/project/repo_name]     (actual path)\n\nreplace it with either\nurl = ssh://git@git.[host]:/[group/project/repo_name] (new path SSH)\nurl = https://git.[host]/[group/project/repo_name]    (new path HTTPS)\n\n\n\n\n\nA53:\n\nAdditionally, if your application uses SQL SERVER you can use the SQL Agent to schedule your tasks.  This is where we commonly put re-occurring code that is data driven (email reminders, scheduled maintenance, purges, etc...).  A great feature that is built in with the SQL Agent is failure notification options, which can alert you if a critical task fails.\n\n\nA54:\n\nI have had this issue before, and solve it using the following config.\n\n[http \"https://your.domain\"]\n   sslCAInfo=/path/to/your/domain/priviate-certificate\n\nSince git 2.3.1, you can put https://your.domain after http to indicate the following certificate is only for it.\n\n\nA55:\n\nUse Windows Scheduler to run a web page.\nTo prevent malicous user or search engine spiders to run it, when you setup the scheduled task, simply call the web page with a querystring, ie : mypage.aspx?from=scheduledtask\nThen in the page load, simply use a condition :\n    if (Request.Querystring[\"from\"] == \"scheduledtask\")\n    {\n    //executetask\n    }\nThis way no search engine spider or malicious user will be able to execute your scheduled task.\n\n\nA56:\n\nkiddailey I think was pretty close, however I would not disable ssl verification but rather rather just supply the local certificate:\nIn the Git config file\n[http]\n    sslCAinfo = /bin/curl-ca-bundle.crt\n\nOr via command line:\ngit config --global http.sslCAinfo /bin/curl-ca-bundle.crt\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 1, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI have a PHP script that may be placed on a windows system or a linux system. I need to run different commands in either case. \nHow can I detect which environment I am in? (preferably something PHP rather than clever system hacks)\nUpdate\nTo clarify, the script is running from the command line.\n\n\nCandidate answers:\nA1:\n\nYou could start your php script from the command line (i.e. bash) by using   \nnohup php myscript.php & \nthe & puts your process in the background.\nEdit:\nYes, there are some drawbacks, but not possible to control? That's just wrong.\nA simple kill processid will stop it. And it's still the best and simplest solution. \n\n\nA2:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA3:\n\nCheck out https://github.com/shaneharter/PHP-Daemon\nThis is an object-oriented daemon library. It has built-in support for things like logging and error recovery, and it has support for creating background workers. \n\n\nA4:\n\nThere is more than one way to solve this problem.\nI do not know the specifics but perhaps there is another way to trigger the PHP process.  For instance if you need the code to run based on events in a SQL database you could setup a trigger to execute your script.   This is really easy to do under PostgreSQL: http://www.postgresql.org/docs/current/static/external-pl.html .\nHonestly I think your best bet is to create a Damon process using nohup.  nohup allows the command to continue to execute even after the user has logged out:\nnohup php myscript.php &\n\nThere is however a very serious problem.   As you said PHP's memory manager is complete garbage,  it was built with the assumption that a script is only executing for a few seconds and then exists.   Your PHP script will start to use GIGABYTES of memory after only a few days.  You MUST ALSO create a cron script that runs every 12 or maybe 24 hours that kills and re-spawns your php script like this:\nkillall -3 php\nnohup php myscript.php &\n\nBut what if the script was in the middle of a job?  Well kill -3 is an interrupt, its the same as doing a ctrl+c  on the CLI.   Your php script can catch this interrupt and exit gracefully using the PHP pcntl library: http://php.oregonstate.edu/manual/en/function.pcntl-signal.php\nHere is an example:\nfunction clean_up() {\n  GLOBAL $lock;\n  mysql_close();\n  fclose($lock)\n  exit();\n}\npcntl_signal(SIGINT, 'clean_up');\n\nThe idea behind the $lock is that the PHP script can open a file with a fopen(\"file\",\"w\");.  Only one process can have a write lock on a file,  so using this you can make sure that only one copy of your PHP script is running. \nGood Luck!\n\n\nA5:\n\nI recently had a need for a cross-platform solution (Windows, Mac, and Linux) to the problem of running PHP scripts as daemons.  I solved the problem by writing my own C++ based solution and making binaries:\nhttps://github.com/cubiclesoft/service-manager/\nFull support for Linux (via sysvinit), but also Windows NT services and Mac OSX launchd.\nIf you just need Linux, then a couple of the other solutions presented here work well enough and, depending on the flavor.  There is also Upstart and systemd these days, which have fallbacks to sysvinit scripts.  But half of the point of using PHP is that it is cross-platform in nature, so code written in the language has a pretty good chance of working everywhere as-is.  Deficiencies start showing up when certain external native OS-level aspects enter the picture such as system services, but you'll get that problem with most scripting languages.\nAttempting to catch signals as someone here suggested in PHP userland is not a good idea.  Read the documentation on pcntl_signal() carefully and you will quickly learn that PHP handles signals using some rather unpleasant methods (specifically, 'ticks') that chew up a bunch of cycles for something rarely seen by processes (i.e. signals).  Signal handling in PHP is also only barely available on POSIX platforms and support differs based on the version of PHP.  It initially sounds like a decent solution but it falls pretty short of being truly useful.\nPHP has also been getting better about memory leak issues as time progresses.  You still have to be careful (the DOM XML parser tends to leak still) but I rarely see runaway processes these days and the PHP bug tracker is pretty quiet by comparison to the days of yore.\n\n\nA6:\n\nYou can check if the directory separator is / (for unix/linux/mac) or \\ on windows. The constant name is DIRECTORY_SEPARATOR.\nif (DIRECTORY_SEPARATOR === '/') {\n    // unix, linux, mac\n}\n\nif (DIRECTORY_SEPARATOR === '\\\\') {\n    // windows\n}\n\n\n\nA7:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA8:\n\nAs others have already mentioned, running PHP as a daemon is quite easy, and can be done using a single line of command. But the actual problem is keeping it running and managing it. I've had the same problem quite some time ago and although there are plenty of solutions already available, most of them have lots of dependencies or are difficult to use and not suitable for basic usages. I wrote a shell script that can manage a any process/application including PHP cli scripts. It can be set as a cronjob to start the application and will contain the application and manage it. If it's executed again, for example via the same cronjob, it check if the app is running or not, if it does then simply exits and let its previous instance continue managing the application.\nI uploaded it to github, feel free to use it : https://github.com/sinasalek/EasyDeamonizer\nEasyDeamonizer\nSimply watches over your application (start, restart, log, monitor, etc). a generic script to make sure that your appliation remains running properly. Intentionally it uses process name instread of pid/lock file to prevent all its side effects and keep the script as simple and as stirghforward as possible, so it always works even when EasyDaemonizer itself is restarted.\nFeatures\n\nStarts the application and optionally a customized delay for each start\nMakes sure that only one instance is running\nMonitors CPU usage and restarts the app automatically when it reaches the defined threshold\nSetting EasyDeamonizer to run via cron to run it again if it's halted for any reason\nLogs its activity\n\n\n\nA9:\n\nKevin van Zonneveld wrote a very nice detailed article on this, in his example he makes use of the System_Daemon PEAR package (last release date on 2009-09-02).\n\n\nA10:\n\nyou can check pm2 here is, http://pm2.keymetrics.io/\ncreate a ssh file, such as worker.sh put into your php script that you will deal with.\nworker.sh\nphp /path/myscript.php\n\ndaemon start\npm2 start worker.sh\n\nCheers, that is it.\n\n\nA11:\n\nSearch extension in \n\n/etc/php5/apache2/php.ini\n\n\n\nA12:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA13:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA14:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\nA15:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\nA16:\n\nChecking for installed php modules and packages\nIn addition to running \nphp -m\n\nto get the list of installed php modules, you will probably find it helpful to get the list of the currently installed php packages in Ubuntu:\nsudo dpkg --get-selections | grep -v deinstall | grep php\n\nThis is helpful since Ubuntu makes php modules available via packages.\nYou can then install the needed modules by selecting from the available Ubuntu php packages, which you can view by running:\nsudo apt-cache search php | grep \"^php5-\"\n\nOr, for Ubuntu 16.04 and higher:\nsudo apt-cache search php | grep \"^php7\"\n\nAs you have mentioned, there is plenty of information available on the actual installation of the packages that you might require, so I won't go into detail about that here.\nRelated: Enabling / disabling installed php modules\nIt is possible that an installed module has been disabled. In that case, it won't show up when running php -m, but it will show up in the list of installed Ubuntu packages.\nModules can be enabled/disabled via the php5enmod tool (phpenmod on later distros) which is part of the php-common package.\nUbuntu 12.04: \nEnabled modules are symlinked in /etc/php5/conf.d\nUbuntu 12.04: (with PHP 5.4+) \nTo enable an installed module:\nphp5enmod <modulename>\n\nTo disable an installed module:\nphp5dismod <modulename>\n\nUbuntu 16.04 (php7) and higher:\nTo enable an installed module:\nphpenmod <modulename>\n\nTo disable an installed module:\nphpdismod <modulename>\n\nReload Apache\nRemember to reload Apache2 after enabling/disabling:\nservice apache2 reload\n\n\n\nA17:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA18:\n\nWith new  systemd you can create a service.\nYou must create a file or a symlink in  /etc/systemd/system/, eg. myphpdaemon.service and place content like this one, myphpdaemon will be the name of the service:\n[Unit]\nDescription=My PHP Daemon Service\n#May your script needs MySQL or other services to run, eg. MySQL Memcached\nRequires=mysqld.service memcached.service \nAfter=mysqld.service memcached.service\n\n[Service]\nUser=root\nType=simple\nTimeoutSec=0\nPIDFile=/var/run/myphpdaemon.pid\nExecStart=/usr/bin/php -f /srv/www/myphpdaemon.php arg1 arg2> /dev/null 2>/dev/null\n#ExecStop=/bin/kill -HUP $MAINPID #It's the default you can change whats happens on stop command\n#ExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\n\nRestart=on-failure\nRestartSec=42s\n\nStandardOutput=null #If you don't want to make toms of logs you can set it null if you sent a file or some other options it will send all PHP output to this one.\nStandardError=/var/log/myphpdaemon.log\n[Install]\nWantedBy=default.target\n\nYou will be able to start, get status, restart and stop  the services using the command\nsystemctl <start|status|restart|stop|enable> myphpdaemon\nYou can use the PHP native server using php -S 127.0.0.1:<port> or run it as a script. Using a PHP script you should have a kind of \"forever loop\" to continue running.\n<?php\ngc_enable();//\nwhile (!connection_aborted() || PHP_SAPI == \"cli\") {\n  \n  //Code Logic\n  \n  //sleep and usleep could be useful\n    if (PHP_SAPI == \"cli\") {\n        if (rand(5, 100) % 5 == 0) {\n            gc_collect_cycles(); //Forces collection of any existing garbage cycles\n        }\n    }\n}\n\nWorking example:\n[Unit]\nDescription=PHP APP Sync Service\nRequires=mysqld.service memcached.service\nAfter=mysqld.service memcached.service\n\n[Service]\nUser=root\nType=simple\nTimeoutSec=0\nPIDFile=/var/run/php_app_sync.pid\nExecStart=/bin/sh -c '/usr/bin/php -f /var/www/app/private/server/cron/app_sync.php  2>&1 > /var/log/app_sync.log'\nKillMode=mixed\n\nRestart=on-failure\nRestartSec=42s\n\n[Install]\nWantedBy=default.target\n\nIf your PHP routine should be executed once in a cycle (like a diggest)  you may use a shell or bash script to be invoked into systemd service file instead of PHP directly, for example:\n#!/usr/bin/env bash\nscript_path=\"/app/services/\"\n\nwhile [ : ]\ndo\n#    clear\n    php -f \"$script_path\"${1}\".php\" fixedparameter ${2}  > /dev/null 2>/dev/null\n    sleep 1\ndone\n\nIf you chose these option you should change the KillMode to mixed to processes, bash(main) and PHP(child) be killed.\nExecStart=/app/phpservice/runner.sh phpfile parameter  > /dev/null 2>/dev/null\nKillMode=process\n\nThis method also is effective if you're facing a memory leak.\n\nNote: Every time that you change your \"myphpdaemon.service\" you must\nrun `systemctl daemon-reload', but do worry if you do not do, it will be\nalerted when is needed.\n\n\n\nA19:\n\nAnother option is to use Upstart. It was originally developed for Ubuntu (and comes packaged with it by default), but is intended to be suitable for all Linux distros.\nThis approach is similar to Supervisord and daemontools, in that it automatically starts the daemon on system boot and respawns on script completion.\nHow to set it up:\nCreate a new script file at /etc/init/myphpworker.conf. Here is an example:\n# Info\ndescription \"My PHP Worker\"\nauthor      \"Jonathan\"\n\n# Events\nstart on startup\nstop on shutdown\n\n# Automatically respawn\nrespawn\nrespawn limit 20 5\n\n# Run the script!\n# Note, in this example, if your PHP script returns\n# the string \"ERROR\", the daemon will stop itself.\nscript\n    [ $(exec /usr/bin/php -f /path/to/your/script.php) = 'ERROR' ] && ( stop; exit 1; )\nend script\n\nStarting & stopping your daemon:\nsudo service myphpworker start\nsudo service myphpworker stop\n\nCheck if your daemon is running:\nsudo service myphpworker status\n\nThanks\nA big thanks to Kevin van Zonneveld, where I learned this technique from.\n\n\nA20:\n\nTo check if this extensions are enabled or not, you can create a php file i.e. info.php and write the following code there:\n<?php \necho \"GD: \", extension_loaded('gd') ? 'OK' : 'MISSING', '<br>';\necho \"XML: \", extension_loaded('xml') ? 'OK' : 'MISSING', '<br>';\necho \"zip: \", extension_loaded('zip') ? 'OK' : 'MISSING', '<br>';\n?>\n\nThat's it.\n\n\nA21:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA22:\n\nPerhaps the easiest way to see which extensions are (compiled and) loaded (not in cli) is to have a server run the following:\n<?php\n$ext = get_loaded_extensions();\nasort($ext);\nforeach ($ext as $ref) {\n    echo $ref . \"\\n\";\n}\n\nPHP cli does not necessarily have the same extensions loaded.\n\n\nA23:\n\nCore Predefined Constants: http://us3.php.net/manual/en/reserved.constants.php which has the PHP_OS (string) constant.\nOr if you want to detect the OS of the client:\n<?php\n    echo $_SERVER['HTTP_USER_AGENT'] . \"\\n\\n\";\n\n    $browser = get_browser(null, true);\n    print_r($browser);\n?>\n\nFrom http://us3.php.net/manual/en/function.get-browser.php\n\nAccording to your edit you can refer to this dublicate PHP Server Name from Command Line\nYou can use\nstring php_uname ([ string $mode = \"a\" ] )\n\nSo\nphp_uname(\"s\")\n\n\n's': Operating system name. eg.\n  FreeBSD.\n\nWould do the trick for you, see here http://php.net/manual/en/function.php-uname.php\n\n\nA24:\n\nYou can\n\nUse nohup as Henrik suggested.\nUse screen and run your PHP program as a regular process inside that. This gives you more control than using nohup. \nUse a daemoniser like http://supervisord.org/ (it's written in Python but can daemonise any command line program and give you a remote control to manage it). \nWrite your own daemonise wrapper like Emil suggested but it's overkill IMO.\n\nI'd recommend the simplest method (screen in my opinion) and then if you want more features or functionality, move to more complex methods. \n\n\nA25:\n\nIf you can - grab a copy of Advanced Programming in the UNIX Environment. The entire chapter 13 is devoted to daemon programming. Examples are in C, but all the function you need have wrappers in PHP (basically the pcntl and posix extensions).\nIn a few words - writing a daemon (this is posible only on *nix based OS-es - Windows uses services) is like this:\n\nCall umask(0) to prevent permission issues.\nfork() and have the parent exit.\nCall setsid().\nSetup signal processing of SIGHUP (usually this is ignored or used to signal the daemon to reload its configuration) and SIGTERM (to tell the process to exit gracefully).\nfork() again and have the parent exit.\nChange the current working dir with chdir().\nfclose() stdin, stdout and stderr and don't write to them. The corrrect way is to redirect those to either /dev/null or a file, but I couldn't find a way to do it in PHP. It is possible when you launch the daemon to redirect them using the shell (you'll have to find out yourself how to do that, I don't know :).\nDo your work!\n\nAlso, since you are using PHP, be careful for cyclic references, since the PHP garbage collector, prior to PHP 5.3, has no way of collecting those references and the process will memory leak, until it eventually crashes.\n\n\nA26:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA27:\n\nFor information on php extensions etc, on site.\n\nCreate a new file and name it info.php (or some other name.php)\nWrite this code in it:\n <?php\n   phpinfo ();\n ?>\n\nSave the file in the root (home)of the site\nOpen the file in your browser. For example: example.com/info.php\nAll the php information on your site will be displayed.\n\n\n\nA28:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\nA29:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA30:\n\nI run a large number of PHP daemons.  \nI agree with you that PHP is not the best (or even a good) language for doing this, but the daemons share code with the web-facing components so overall it is a good solution for us.\nWe use daemontools for this.  It is smart, clean and reliable.  In fact we use it for running all of our daemons.\nYou can check this out at http://cr.yp.to/daemontools.html.\nEDIT: A quick list of features.\n\nAutomatically starts the daemon on reboot\nAutomatically restart dameon on failure\nLogging is handled for you, including rollover and pruning\nManagement interface: 'svc' and 'svstat'\nUNIX friendly (not a plus for everyone perhaps)\n\n\n\nA31:\n\nYou can view which modules (compiled in) are available via terminal through php -m\n\n\nA32:\n\nAnother quick way to see if a module is enabled / disabled vs only installed or not is to use phpquery command.\nFor example, on my Linux Mint machine, if I want to see if xdebug is enabled I would run:\n phpquery -v 8.1 -s apache2 -m xdebug\n\n-v - is to specify for which version you want\n-s - to specify the environment (apache2 or cli)\n-m - the module you are interested into.\nThe response for the above example was (in my case):\nxdebug (Enabled for apache2 by maintainer script)\n\nHere some more examples.\n\n\nA33:\n\nTested on PHP 8.1:\nCheck extension INSTALLED (in Operative System):\nphp -m\n\nCheck extension LOADED (in CLI):\n# set EXT value with the desired extension name.\nEXT=gmp php -r \"echo getenv('EXT'),': ', extension_loaded( getenv('EXT') ) ? 'Loaded' : 'Not loaded', PHP_EOL;\"\n\nCheck extension LOADED (in PHP-FPM):\necho \"<?php phpinfo(); ?>\" > public/info.php\n\nand then visit you-running-php-fpm-host/info.php; remember to delete the file afterwards!\n\n\nA34:\n\nThe php_uname function can be used to detect this.\necho php_uname();\n\n\n\nA35:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA36:\n\nCheck the value of the PHP_OS constantDocs.\nIt will give you various values on Windows like WIN32, WINNT or Windows.\nSee as well: Possible Values For: PHP_OS and php_unameDocs:\nif (strtoupper(substr(PHP_OS, 0, 3)) === 'WIN') {\n    echo 'This is a server using Windows!';\n} else {\n    echo 'This is a server not using Windows!';\n}\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "   (vs. \"Yes, I have a problem with the way the code is written.\n  (i.e. the code is not well-"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 2, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nUsing PHP, I want to convert UNIX timestamps to date strings similar to this: 2008-07-17T09:24:17Z\nHow do I convert a timestamp such as 1333699439 to 2008-07-17T09:24:17Z?\n\n\nCandidate answers:\nA1:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA2:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA3:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA4:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\nA5:\n\nIn addition to the already great and respected answers in here, I would like to expand on PHP having no setters/getters.\nPHP does not have getter and setter syntax. It provides subclassed or magic methods to allow \"hooking\" and overriding the property lookup process, as pointed out by Dave.\nMagic allows us lazy programmers to do more with less code at a time at which we are actively engaged in a project and know it intimately, but usually at the expense of readability.\nPerformance Every unnecessary function, that results from forcing a getter/setter-like code-architecture in PHP, involves its own memory stack-frame upon invocation and is wasting CPU cycles. \nReadability: The codebase incurs bloating code-lines, which impacts code-navigation as more LOC mean more scrolling,.\nPreference: Personally, as my rule of thumb, I take the failure of static code analysis \nas a sign to avoid going down the magical road as long as obvious long-term benefits elude me at that time.\nFallacies:\nA common argument is readability. For instance that $someobject->width is easier to read than $someobject->width(). However unlike a planet's circumference or width, which can be assumed to be  static, an object's instance such as $someobject, which requires a width function, likely takes a measurement of the object's instance width.\nTherefore readability increases mainly because of assertive naming-schemes and not by hiding the function away that outputs a given property-value.\n__get / __set uses:\n\npre-validation and pre-sanitation of property values\nstrings e.g.\n\"\nsome {mathsobj1->generatelatex} multi\nline text {mathsobj1->latexoutput}\nwith lots of variables for {mathsobj1->generatelatex}\n some reason\n\"\n\nIn this case generatelatex would adhere to a naming scheme of actionname + methodname\nspecial, obvious cases\n$dnastringobj->homeobox($one_rememberable_parameter)->gattaca->findrelated()\n$dnastringobj->homeobox($one_rememberable_parameter)->gttccaatttga->findrelated()\n\n\nNote: PHP chose not to implement getter/setter syntax. I am not claiming that getters/setter are generally bad.\n\n\nA6:\n\nValidating + Formatting/Deriving Values\nSetters let you to validate data and getters let you format or derive data. Objects allow you to encapsulate data and its validation and formatting code into a neat package that encourages DRY.\nFor example, consider the following simple class that contains a birth date.\nclass BirthDate {\n\n    private $birth_date;\n\n    public function getBirthDate($format='Y-m-d') {\n        //format $birth_date ...\n        //$birth_date = ...\n        return $birth_date;\n    }\n\n    public function setBirthDate($birth_date) {                   \n        //if($birth_date is not valid) throw an exception ...          \n        $this->birth_date = $birth_date;\n    }\n\n    public function getAge() {\n        //calculate age ...\n        return $age;\n    }\n\n    public function getDaysUntilBirthday() {\n        //calculate days until birth days\n        return $days;\n    }\n}\n\nYou'll want to validate that the value being set is\n\nA valid date\nNot in the future\n\nAnd you don't want to do this validation all over your application (or over multiple applications for that matter). Instead, it's easier to make the member variable protected or private (in order to make the setter the only access point) and to validate in the setter because then you'll know that the object contains a valid birth date no matter which part of the application the object came from and if you want to add more validation then you can add it in a single place.\nYou might want to add multiple formatters that operate on the same member variable i.e. getAge() and getDaysUntilBirthday() and you might want to enforce a configurable format in getBirthDate() depending on locale. Therefore I prefer consistently accessing values via getters as opposed to mixing $date->getAge() with $date->birth_date.\ngetters and setters are also useful when you extend objects. For example, suppose your application needed to allow 150+ year birth dates in some places but not in others. One way to solve the problem without repeating any code would be to extend the BirthDate object and put the additional validation in the setter.\nclass LivingBirthDate extends BirthDate {\n\n    public function setBirthDate($birth_date) {\n        //if $birth_date is greater than 150 years throw an exception\n        //else pass to parent's setter\n        return parent::setBirthDate($birth_date);\n    }\n}\n\n\n\nA7:\n\nYou can use php magic methods __get and __set.\n<?php\nclass MyClass {\n  private $firstField;\n  private $secondField;\n\n  public function __get($property) {\n    if (property_exists($this, $property)) {\n      return $this->$property;\n    }\n  }\n\n  public function __set($property, $value) {\n    if (property_exists($this, $property)) {\n      $this->$property = $value;\n    }\n\n    return $this;\n  }\n}\n?>\n\n\n\nA8:\n\nGoogle already published a guide on optimization of PHP and the conclusion was:\nNo getter and setter Optimizing PHP\nAnd no, you must not use magic methods.  For PHP, Magic Methods are evil. Why?\n\nThey are hard to debug.\nThere is a negative performance impact.\nThey require writing more code.\n\nPHP is not Java, C++, or C#. PHP is different and plays with different rules. \n\n\nA9:\n\nEncapsulation is important in any OO language, popularity has nothing to do with it. In dynamically typed languages, like PHP, it is especially useful because there is little ways to ensure a property is of a specific type without using setters.\nIn PHP, this works:\nclass Foo {\n   public $bar; // should be an integer\n}\n$foo = new Foo;\n$foo->bar = \"string\";\n\nIn Java, it doesn't:\nclass Foo {\n   public int bar;\n}\nFoo myFoo = new Foo();\nmyFoo.bar = \"string\"; // error\n\nUsing magic methods (__get and __set) also works, but only when accessing a property that has lower visibility than the current scope can access. It can easily give you headaches when trying to debug, if it is not used properly.\n\n\nA10:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA11:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA12:\n\nThere are many ways to create sourcecode in a netbeans-convention. This is nice. It makes thinks such easyer === FALSE. Just use the traditionel, specially if you are not sure which one of the properties should be encapsuled and which one not. I know, it is a boi.... pla... code, but for debugging-works and many other thinks it is the better, clear way. Dont spend to much time with thousend of arts how to make simple getters and setters. You cannot implement too some design patterns like the demeter-rule and so on, if you use magics. In specific situation you can use magic_calls or for small, fast and clear solutions. Sure you could make solutions for design-patters in this way too, but why to make you live more difficult. \n\n\nA13:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\nA14:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA15:\n\nAssuming you are using PHP5.3 then the modern way of handling dates is via the native DateTime class. To get the current time you can just call\n$currentTime = new DateTime();\n\nTo create a DateTime object from a specific timestamp (i.e. not now)\n$currentTime = DateTime::createFromFormat( 'U', $timestamp );\n\nTo get a formatted string you can then call\n$formattedString = $currentTime->format( 'c' );\n\nSee the manual page here\n\n\nA16:\n\nTry gmdate like this:\n<?php\n$timestamp=1333699439;\necho gmdate(\"Y-m-d\\TH:i:s\\Z\", $timestamp);\n?>\n\n\n\nA17:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA18:\n\nWhy use getters and setters?\n\nScalability: It's easier refactor a getter than search all the var assignments in a project code.\nDebugging: You can put breakpoints at setters and getters.\nCleaner: Magic functions are not good solution for writting less, your IDE will not suggest the code. Better use templates for fast-writting getters.\n\n\n\n\nA19:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA20:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA21:\n\nI made an experiment using the magic method __call.\nNot sure if I should post it (because of all the \"DO NOT USE MAGIC METHODS\" warnings in the other answers and comments) but i'll leave it here.. just in case someone find it useful.\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = substr($_name, 4);\n\n    if (isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nJust add that method above in your class, now you can type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_foo(); // return \"bar\"\n$C->get_bom(); // return \"bim\"\n\n// as setter\n$C->set_foo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_bom(\"zam\"); // set \"zam\" as new value of bom\n\n\n\nThis way you can get/set everything in your class if it exist so, if you need it for only a few specific elements, you could use a \"whitelist\" as filter.  \nExample:\nprivate $callWhiteList = array(\n    \"foo\" => \"foo\",\n    \"fee\" => \"fee\",\n    // ...\n);\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = $this->callWhiteList[substr($_name, 4)];\n\n    if (!is_null($varName) && isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nNow you can only get/set \"foo\" and \"fee\".\nYou can also use that \"whitelist\" to assign custom names to access to your vars.\nFor example,\nprivate $callWhiteList = array(\n    \"myfoo\" => \"foo\",\n    \"zim\" => \"bom\",\n    // ...\n);\n\nWith that list you can now type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // private $callWhiteList = array( ... )\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_myfoo(); // return \"bar\"\n$C->get_zim(); // return \"bim\"\n\n// as setter\n$C->set_myfoo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_zim(\"zam\"); // set \"zam\" as new value of bom\n\n\n.\n.\n.\nThat's all.\n\nDoc:\n__call() is triggered when invoking inaccessible methods in an object context. \n\n\nA22:\n\nIf you preffer to use the __call function, you can use this method. It works with\n\nGET => $this->property()\nSET => $this->property($value)\nGET => $this->getProperty()\nSET => $this->setProperty($value)\n\nkalsdas\npublic function __call($name, $arguments) {\n\n    //Getting and setting with $this->property($optional);\n\n    if (property_exists(get_class($this), $name)) {\n\n\n        //Always set the value if a parameter is passed\n        if (count($arguments) == 1) {\n            /* set */\n            $this->$name = $arguments[0];\n        } else if (count($arguments) > 1) {\n            throw new \\Exception(\"Setter for $name only accepts one parameter.\");\n        }\n\n        //Always return the value (Even on the set)\n        return $this->$name;\n    }\n\n    //If it doesn't chech if its a normal old type setter ot getter\n    //Getting and setting with $this->getProperty($optional);\n    //Getting and setting with $this->setProperty($optional);\n    $prefix = substr($name, 0, 3);\n    $property = strtolower($name[3]) . substr($name, 4);\n    switch ($prefix) {\n        case 'get':\n            return $this->$property;\n            break;\n        case 'set':\n            //Always set the value if a parameter is passed\n            if (count($arguments) != 1) {\n                throw new \\Exception(\"Setter for $name requires exactly one parameter.\");\n            }\n            $this->$property = $arguments[0];\n            //Always return the value (Even on the set)\n            return $this->$name;\n        default:\n            throw new \\Exception(\"Property $name doesn't exist.\");\n            break;\n    }\n}\n\n\n\nA23:\n\nWell, PHP does have magic methods __get, __set, __isset & __unset, which is always a start. Alas proper (get it?) OO properties is more than magic methods. The main problem with PHP's implementation is that magic methods are called for all inaccessible properties. Which means you have to Repeat Yourself (eg. by calling property_exists()) in the magic methods when determining if name is actually a property of your object. And you can't really solve this general problem with a base class unless all your classes inherit from ie. ClassWithProperties, since PHP lacks multiple inheritance.\nIn contrast, Python new style classes gives you property(), which lets you explicitly define all your properties. C# has special syntax. \nhttp://en.wikipedia.org/wiki/Property_(programming)\n\n\nA24:\n\nGenerally speaking, the first way is more popular overall because those with prior programming knowledge can easily transition to PHP and get work done in an object-oriented fashion. The first way is more universal. My advice would be to stick with what is tried and true across many languages. Then, when and if you use another language, you'll be ready to get something accomplished (instead of spending time reinventing the wheel).\n\n\nA25:\n\nuse date function  date ( string $format [, int $timestamp = time() ] )\nUse date('c',time()) as format to convert to ISO 8601 date (added in PHP 5) - 2012-04-06T12:45:47+05:30\nuse date(\"Y-m-d\\TH:i:s\\Z\",1333699439) to get 2012-04-06T13:33:59Z\nHere are some of the formats date function supports\n<?php\n$today = date(\"F j, Y, g:i a\");                 // March 10, 2001, 5:16 pm\n$today = date(\"m.d.y\");                         // 03.10.01\n$today = date(\"j, n, Y\");                       // 10, 3, 2001\n$today = date(\"Ymd\");                           // 20010310\n$today = date('h-i-s, j-m-y, it is w Day');     // 05-16-18, 10-03-01, 1631 1618 6 Satpm01\n$today = date('\\i\\t \\i\\s \\t\\h\\e jS \\d\\a\\y.');   // it is the 10th day.\n$today = date(\"D M j G:i:s T Y\");               // Sat Mar 10 17:16:18 MST 2001\n$today = date('H:m:s \\m \\i\\s\\ \\m\\o\\n\\t\\h');     // 17:03:18 m is month\n$today = date(\"H:i:s\");                         // 17:16:18\n?>\n\n\n\nA26:\n\nclass MyClass {\n    private $firstField;\n    private $secondField;\n    private $thirdField;\n\n    public function __get( $name ) {\n        if( method_exists( $this , $method = ( 'get' . ucfirst( $name  ) ) ) )\n            return $this->$method();\n        else\n            throw new Exception( 'Can\\'t get property ' . $name );\n    }\n\n    public function __set( $name , $value ) {\n        if( method_exists( $this , $method = ( 'set' . ucfirst( $name  ) ) ) )\n            return $this->$method( $value );\n        else\n            throw new Exception( 'Can\\'t set property ' . $name );\n    }\n\n    public function __isset( $name )\n    {\n        return method_exists( $this , 'get' . ucfirst( $name  ) ) \n            || method_exists( $this , 'set' . ucfirst( $name  ) );\n    }\n\n    public function getFirstField() {\n        return $this->firstField;\n    }\n\n    protected function setFirstField($x) {\n        $this->firstField = $x;\n    }\n\n    private function getSecondField() {\n        return $this->secondField;\n    }\n}\n\n$obj = new MyClass();\n\necho $obj->firstField; // works\n$obj->firstField = 'value'; // works\n\necho $obj->getFirstField(); // works\n$obj->setFirstField( 'value' ); // not works, method is protected\n\necho $obj->secondField; // works\necho $obj->getSecondField(); // not works, method is private\n\n$obj->secondField = 'value'; // not works, setter not exists\n\necho $obj->thirdField; // not works, property not exists\n\nisset( $obj->firstField ); // returns true\nisset( $obj->secondField ); // returns true\nisset( $obj->thirdField ); // returns false\n\nReady!\n\n\nA27:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\nA28:\n\nAfter reading the other advices, I'm inclined to say that:\nAs a GENERIC rule, you will not always define setters for ALL properties, specially \"internal\" ones (semaphores, internal flags...). Read-only properties will not have setters, obviously, so some properties will only have getters; that's where __get() comes to shrink the code:\n\ndefine a __get() (magical global getters) for all those properties which are alike,\ngroup them in arrays so:\n\nthey'll share common characteristics: monetary values will/may come up properly formatted, dates in an specific layout (ISO, US, Intl.), etc.\nthe code itself can verify that only existing & allowed properties are being read using this magical method.\nwhenever you need to create a new similar property, just declare it and add its name to the proper array and it's done. That's way FASTER than defining a new getter, perhaps with some lines of code REPEATED again and again all over the class code.\n\n\nYes! we could write a private method to do that, also, but then again, we'll have MANY methods declared (++memory) that end up calling another, always the same, method. Why just not write a SINGLE method to rule them all...? [yep! pun absolutely intended! :)]\nMagic setters can also respond ONLY to specific properties, so all date type properties can be screened against invalid values in one method alone. If date type properties were listed in an array, their setters can be defined easily. Just an example, of course. there are way too many situations.\nAbout readability... Well... That's another debate: I don't like to be bound to the uses of an IDE (in fact, I don't use them, they tend to tell me (and force me) how to write... and I have my likes about coding \"beauty\"). I tend to be consistent about naming, so using ctags and a couple of other aids is sufficient to me... Anyway: once all this magic setters and  getters are done, I write the other setters that are too specific or \"special\" to be generalized in a __set() method. And that covers all I need about getting and setting properties. Of course: there's not always a common ground, or there are such a few properties that is not worth the trouble of coding a magical method, and then there's still the old good traditional setter/getter pair.\nProgramming languages are just that: human artificial languages. So, each of them has its own intonation or accent, syntax and flavor, so I won't pretend to write a Ruby or Python code using the same \"accent\" than Java or C#, nor I would write a JavaScript or PHP to resemble Perl or SQL... Use them the way they're meant to be used.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "X,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 3, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI (sort of) already know the answer to this question.  But I figured it is one that gets asked so frequently on the R Users list, that there should be one solid good answer.  To the best of my knowledge there is no multiline comment functionality in R.  So, does anyone have any good workarounds?\nWhile quite a bit of work in R usually involves interactive sessions (which casts doubt on the need for multiline comments), there are times when I've had to send scripts to colleagues and classmates, much of which involves nontrivial blocks of code.  And for people coming from other languages it is a fairly natural question.  \nIn the past I've used quotes. Since strings support linebreaks, running an R script with\n\"\nHere's my multiline comment.\n\n\"\na <- 10\nrocknroll.lm <- lm(blah blah blah)\n ...\n\nworks fine.  Does anyone have a better solution?\n\n\nCandidate answers:\nA1:\n\nAnother dplyr pipe compatible option with tidyrmethod replace_na that works for several columns:\nrequire(dplyr)\nrequire(tidyr)\n\nm <- matrix(sample(c(NA, 1:10), 100, replace = TRUE), 10)\nd <- as.data.frame(m)\n\nmyList <- setNames(lapply(vector(\"list\", ncol(d)), function(x) x <- 0), names(d))\n\ndf <- d %>% replace_na(myList)\n\nYou can easily restrict to e.g. numeric columns:\nd$str <- c(\"string\", NA)\n\nmyList <- myList[sapply(d, is.numeric)]\n\ndf <- d %>% replace_na(myList)\n\n\n\nA2:\n\nReplace is.na & NULL in data frame.\n\ndata frame with colums\n\nA$name[is.na(A$name)]<-0\nOR\nA$name[is.na(A$name)]<-\"NA\"\n\nwith all data frame\n\ndf[is.na(df)]<-0\n\nwith replace na with blank in data frame\n\ndf[is.na(df)]<-\"\"\n\nreplace NULL to NA\n\ndf[is.null(df)] <- NA\n\n\nA3:\n\nI wan to add a next solution which using a popular Hmisc package.\nlibrary(Hmisc)\ndata(airquality)\n# imputing with 0 - all columns\n# although my favorite one for simple imputations is Hmisc::impute(x, \"random\")\n> dd <- data.frame(Map(function(x) Hmisc::impute(x, 0), airquality))\n> str(dd[[1]])\n 'impute' Named num [1:153] 41 36 12 18 0 28 23 19 8 0 ...\n - attr(*, \"names\")= chr [1:153] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"imputed\")= int [1:37] 5 10 25 26 27 32 33 34 35 36 ...\n> dd[[1]][1:10]\n  1   2   3   4   5   6   7   8   9  10 \n 41  36  12  18  0*  28  23  19   8  0* \n\nThere could be seen that all imputations metadata are allocated as attributes. Thus it could be used later.\n\n\nA4:\n\nThis does come up on the mailing list fairly regularly, see for example this recent thread on r-help.  The consensus answer usually is the one shown above: that given that the language has no direct support, you have to either\n\nwork with an editor that has region-to-comment commands, and most advanced R editors do \nuse the if (FALSE) constructs suggested earlier but note that it still requires complete parsing and must hence be syntactically correct\n\n\n\nA5:\n\nThis is a more flexible solution. It works no matter how large your data frame is, or zero is indicated by 0 or zero or whatsoever.\nlibrary(dplyr) # make sure dplyr ver is >= 1.00\n\ndf %>%\n    mutate(across(everything(), na_if, 0)) # if 0 is indicated by `zero` then replace `0` with `zero`\n\n\n\nA6:\n\nI know the question is already answered, but doing it this way might be more useful to some:\nDefine this function:\nna.zero <- function (x) {\n    x[is.na(x)] <- 0\n    return(x)\n}\n\nNow whenever you need to convert NA's in a vector to zero's you can do:\nna.zero(some.vector)\n\n\n\nA7:\n\nWould've commented on @ianmunoz's post but I don't have enough reputation.  You can combine dplyr's mutate_each and replace to take care of the NA to 0 replacement.  Using the dataframe from @aL3xa's answer...\n> m <- matrix(sample(c(NA, 1:10), 100, replace = TRUE), 10)\n> d <- as.data.frame(m)\n> d\n\n    V1 V2 V3 V4 V5 V6 V7 V8 V9 V10\n1   4  8  1  9  6  9 NA  8  9   8\n2   8  3  6  8  2  1 NA NA  6   3\n3   6  6  3 NA  2 NA NA  5  7   7\n4  10  6  1  1  7  9  1 10  3  10\n5  10  6  7 10 10  3  2  5  4   6\n6   2  4  1  5  7 NA NA  8  4   4\n7   7  2  3  1  4 10 NA  8  7   7\n8   9  5  8 10  5  3  5  8  3   2\n9   9  1  8  7  6  5 NA NA  6   7\n10  6 10  8  7  1  1  2  2  5   7\n\n> d %>% mutate_each( funs_( interp( ~replace(., is.na(.),0) ) ) )\n\n    V1 V2 V3 V4 V5 V6 V7 V8 V9 V10\n1   4  8  1  9  6  9  0  8  9   8\n2   8  3  6  8  2  1  0  0  6   3\n3   6  6  3  0  2  0  0  5  7   7\n4  10  6  1  1  7  9  1 10  3  10\n5  10  6  7 10 10  3  2  5  4   6\n6   2  4  1  5  7  0  0  8  4   4\n7   7  2  3  1  4 10  0  8  7   7\n8   9  5  8 10  5  3  5  8  3   2\n9   9  1  8  7  6  5  0  0  6   7\n10  6 10  8  7  1  1  2  2  5   7\n\nWe're using standard evaluation (SE) here which is why we need the underscore on \"funs_.\"  We also use lazyeval's interp/~ and the . references \"everything we are working with\", i.e. the data frame.  Now there are zeros!\n\n\nA8:\n\nif you want to assign a new name after changing the NAs in a specific column in this case column V3, use you can do also like this \nmy.data.frame$the.new.column.name <- ifelse(is.na(my.data.frame$V3),0,1)\n\n\n\nA9:\n\nIf you want to replace NAs in factor variables, this might be useful:\nn <- length(levels(data.vector))+1\n\ndata.vector <- as.numeric(data.vector)\ndata.vector[is.na(data.vector)] <- n\ndata.vector <- as.factor(data.vector)\nlevels(data.vector) <- c(\"level1\",\"level2\",...,\"leveln\", \"NAlevel\") \n\nIt transforms a factor-vector into a numeric vector and adds another artifical numeric factor level, which is then transformed back to a factor-vector with one extra \"NA-level\" of your choice.\n\n\nA10:\n\nWith dplyr 0.5.0, you can use coalesce function which can be easily integrated into %>% pipeline by doing coalesce(vec, 0). This replaces all NAs in vec with 0:\nSay we have a data frame with NAs:\nlibrary(dplyr)\ndf <- data.frame(v = c(1, 2, 3, NA, 5, 6, 8))\n\ndf\n#    v\n# 1  1\n# 2  2\n# 3  3\n# 4 NA\n# 5  5\n# 6  6\n# 7  8\n\ndf %>% mutate(v = coalesce(v, 0))\n#   v\n# 1 1\n# 2 2\n# 3 3\n# 4 0\n# 5 5\n# 6 6\n# 7 8\n\n\n\nA11:\n\nAnother example using imputeTS package:\nlibrary(imputeTS)\nna.replace(yourDataframe, 0)\n\n\n\nA12:\n\nThis simple function extracted from Datacamp could help:\nreplace_missings <- function(x, replacement) {\n  is_miss <- is.na(x)\n  x[is_miss] <- replacement\n\n  message(sum(is_miss), \" missings replaced by the value \", replacement)\n  x\n}\n\nThen\nreplace_missings(df, replacement = 0)\n\n\n\nA13:\n\nSee my comment in @gsk3 answer. A simple example:\n> m <- matrix(sample(c(NA, 1:10), 100, replace = TRUE), 10)\n> d <- as.data.frame(m)\n   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10\n1   4  3 NA  3  7  6  6 10  6   5\n2   9  8  9  5 10 NA  2  1  7   2\n3   1  1  6  3  6 NA  1  4  1   6\n4  NA  4 NA  7 10  2 NA  4  1   8\n5   1  2  4 NA  2  6  2  6  7   4\n6  NA  3 NA NA 10  2  1 10  8   4\n7   4  4  9 10  9  8  9  4 10  NA\n8   5  8  3  2  1  4  5  9  4   7\n9   3  9 10  1  9  9 10  5  3   3\n10  4  2  2  5 NA  9  7  2  5   5\n\n> d[is.na(d)] <- 0\n\n> d\n   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10\n1   4  3  0  3  7  6  6 10  6   5\n2   9  8  9  5 10  0  2  1  7   2\n3   1  1  6  3  6  0  1  4  1   6\n4   0  4  0  7 10  2  0  4  1   8\n5   1  2  4  0  2  6  2  6  7   4\n6   0  3  0  0 10  2  1 10  8   4\n7   4  4  9 10  9  8  9  4 10   0\n8   5  8  3  2  1  4  5  9  4   7\n9   3  9 10  1  9  9 10  5  3   3\n10  4  2  2  5  0  9  7  2  5   5\n\nThere's no need to apply apply. =)\nEDIT\nYou should also take a look at norm package. It has a lot of nice features for missing data analysis. =)\n\n\nA14:\n\nYou can use replace()\nFor example:\n> x <- c(-1,0,1,0,NA,0,1,1)\n> x1 <- replace(x,5,1)\n> x1\n[1] -1  0  1  0  1  0  1  1\n\n> x1 <- replace(x,5,mean(x,na.rm=T))\n> x1\n[1] -1.00  0.00  1.00  0.00  0.29  0.00 1.00  1.00\n\n\n\nA15:\n\nNo need to use any library.\ndf <- data.frame(a=c(1,3,5,NA))\n\ndf$a[is.na(df$a)] <- 0\n\ndf\n\n\n\nA16:\n\nThe dplyr hybridized options are now around 30% faster than the Base R subset reassigns. On a 100M datapoint dataframe mutate_all(~replace(., is.na(.), 0)) runs a half a second faster than the base R d[is.na(d)] <- 0 option. What one wants to avoid specifically is using an ifelse() or an if_else(). (The complete 600 trial analysis ran to over 4.5 hours mostly due to including these approaches.) Please see benchmark analyses below for the complete results.\nIf you are struggling with massive dataframes, data.table is the fastest option of all: 40% faster than the standard Base R approach. It also modifies the data in place, effectively allowing you to work with nearly twice as much of the data at once.\n\nA clustering of other helpful tidyverse replacement approaches\nLocationally:\n\nindex mutate_at(c(5:10), ~replace(., is.na(.), 0))\ndirect reference mutate_at(vars(var5:var10), ~replace(., is.na(.), 0))\nfixed match mutate_at(vars(contains(\"1\")), ~replace(., is.na(.), 0))\n\n\nor in place of contains(), try ends_with(),starts_with()\n\n\npattern match mutate_at(vars(matches(\"\\\\d{2}\")), ~replace(., is.na(.), 0))\n\nConditionally:\n(change just single type and leave other types alone.)\n\nintegers mutate_if(is.integer, ~replace(., is.na(.), 0))\nnumbers mutate_if(is.numeric, ~replace(., is.na(.), 0))\nstrings mutate_if(is.character, ~replace(., is.na(.), 0))\n\n\n##The Complete Analysis -\nUpdated for dplyr 0.8.0: functions use purrr format ~ symbols: replacing deprecated funs() arguments.\n###Approaches tested:\n# Base R: \nbaseR.sbst.rssgn   <- function(x) { x[is.na(x)] <- 0; x }\nbaseR.replace      <- function(x) { replace(x, is.na(x), 0) }\nbaseR.for          <- function(x) { for(j in 1:ncol(x))\n    x[[j]][is.na(x[[j]])] = 0 }\n\n# tidyverse\n## dplyr\ndplyr_if_else      <- function(x) { mutate_all(x, ~if_else(is.na(.), 0, .)) }\ndplyr_coalesce     <- function(x) { mutate_all(x, ~coalesce(., 0)) }\n\n## tidyr\ntidyr_replace_na   <- function(x) { replace_na(x, as.list(setNames(rep(0, 10), as.list(c(paste0(\"var\", 1:10)))))) }\n\n## hybrid \nhybrd.ifelse     <- function(x) { mutate_all(x, ~ifelse(is.na(.), 0, .)) }\nhybrd.replace_na <- function(x) { mutate_all(x, ~replace_na(., 0)) }\nhybrd.replace    <- function(x) { mutate_all(x, ~replace(., is.na(.), 0)) }\nhybrd.rplc_at.idx<- function(x) { mutate_at(x, c(1:10), ~replace(., is.na(.), 0)) }\nhybrd.rplc_at.nse<- function(x) { mutate_at(x, vars(var1:var10), ~replace(., is.na(.), 0)) }\nhybrd.rplc_at.stw<- function(x) { mutate_at(x, vars(starts_with(\"var\")), ~replace(., is.na(.), 0)) }\nhybrd.rplc_at.ctn<- function(x) { mutate_at(x, vars(contains(\"var\")), ~replace(., is.na(.), 0)) }\nhybrd.rplc_at.mtc<- function(x) { mutate_at(x, vars(matches(\"\\\\d+\")), ~replace(., is.na(.), 0)) }\nhybrd.rplc_if    <- function(x) { mutate_if(x, is.numeric, ~replace(., is.na(.), 0)) }\n\n# data.table   \nlibrary(data.table)\nDT.for.set.nms   <- function(x) { for (j in names(x))\n    set(x,which(is.na(x[[j]])),j,0) }\nDT.for.set.sqln  <- function(x) { for (j in seq_len(ncol(x)))\n    set(x,which(is.na(x[[j]])),j,0) }\nDT.nafill        <- function(x) { nafill(df, fill=0)}\nDT.setnafill     <- function(x) { setnafill(df, fill=0)}\n\n###The code for this analysis:\nlibrary(microbenchmark)\n# 20% NA filled dataframe of 10 Million rows and 10 columns\nset.seed(42) # to recreate the exact dataframe\ndfN <- as.data.frame(matrix(sample(c(NA, as.numeric(1:4)), 1e7*10, replace = TRUE),\n                            dimnames = list(NULL, paste0(\"var\", 1:10)), \n                            ncol = 10))\n# Running 600 trials with each replacement method \n# (the functions are excecuted locally - so that the original dataframe remains unmodified in all cases)\nperf_results <- microbenchmark(\n    hybrd.ifelse     = hybrd.ifelse(copy(dfN)),\n    dplyr_if_else    = dplyr_if_else(copy(dfN)),\n    hybrd.replace_na = hybrd.replace_na(copy(dfN)),\n    baseR.sbst.rssgn = baseR.sbst.rssgn(copy(dfN)),\n    baseR.replace    = baseR.replace(copy(dfN)),\n    dplyr_coalesce   = dplyr_coalesce(copy(dfN)),\n    tidyr_replace_na = tidyr_replace_na(copy(dfN)),\n    hybrd.replace    = hybrd.replace(copy(dfN)),\n    hybrd.rplc_at.ctn= hybrd.rplc_at.ctn(copy(dfN)),\n    hybrd.rplc_at.nse= hybrd.rplc_at.nse(copy(dfN)),\n    baseR.for        = baseR.for(copy(dfN)),\n    hybrd.rplc_at.idx= hybrd.rplc_at.idx(copy(dfN)),\n    DT.for.set.nms   = DT.for.set.nms(copy(dfN)),\n    DT.for.set.sqln  = DT.for.set.sqln(copy(dfN)),\n    times = 600L\n)\n\n###Summary of Results\n\n> print(perf_results)\nUnit: milliseconds\n              expr       min        lq     mean   median       uq      max neval\n      hybrd.ifelse 6171.0439 6339.7046 6425.221 6407.397 6496.992 7052.851   600\n     dplyr_if_else 3737.4954 3877.0983 3953.857 3946.024 4023.301 4539.428   600\n  hybrd.replace_na 1497.8653 1706.1119 1748.464 1745.282 1789.804 2127.166   600\n  baseR.sbst.rssgn 1480.5098 1686.1581 1730.006 1728.477 1772.951 2010.215   600\n     baseR.replace 1457.4016 1681.5583 1725.481 1722.069 1766.916 2089.627   600\n    dplyr_coalesce 1227.6150 1483.3520 1524.245 1519.454 1561.488 1996.859   600\n  tidyr_replace_na 1248.3292 1473.1707 1521.889 1520.108 1570.382 1995.768   600\n     hybrd.replace  913.1865 1197.3133 1233.336 1238.747 1276.141 1438.646   600\n hybrd.rplc_at.ctn  916.9339 1192.9885 1224.733 1227.628 1268.644 1466.085   600\n hybrd.rplc_at.nse  919.0270 1191.0541 1228.749 1228.635 1275.103 2882.040   600\n         baseR.for  869.3169 1180.8311 1216.958 1224.407 1264.737 1459.726   600\n hybrd.rplc_at.idx  839.8915 1189.7465 1223.326 1228.329 1266.375 1565.794   600\n    DT.for.set.nms  761.6086  915.8166 1015.457 1001.772 1106.315 1363.044   600\n   DT.for.set.sqln  787.3535  918.8733 1017.812 1002.042 1122.474 1321.860   600\n\n\n###Boxplot of Results\nggplot(perf_results, aes(x=expr, y=time/10^9)) +\n    geom_boxplot() +\n    xlab('Expression') +\n    ylab('Elapsed Time (Seconds)') +\n    scale_y_continuous(breaks = seq(0,7,1)) +\n    coord_flip()\n\n\nColor-coded Scatterplot of Trials (with y-axis on a log scale)\nqplot(y=time/10^9, data=perf_results, colour=expr) + \n    labs(y = \"log10 Scaled Elapsed Time per Trial (secs)\", x = \"Trial Number\") +\n    coord_cartesian(ylim = c(0.75, 7.5)) +\n    scale_y_log10(breaks=c(0.75, 0.875, 1, 1.25, 1.5, 1.75, seq(2, 7.5)))\n\n\nA note on the other high performers\nWhen the datasets get larger, Tidyr''s replace_na had historically pulled out in front. With the current collection of 100M data points to run through, it performs almost exactly as well as a Base R For Loop. I am curious to see what happens for different sized dataframes.\nAdditional examples for the mutate and summarize _at and _all function variants can be found here: https://rdrr.io/cran/dplyr/man/summarise_all.html\nAdditionally, I found helpful demonstrations and collections of examples here: https://blog.exploratory.io/dplyr-0-5-is-awesome-heres-why-be095fd4eb8a\nAttributions and Appreciations\nWith special thanks to:\n\nTyler Rinker and Akrun for demonstrating microbenchmark.\nalexis_laz for working on helping me understand the use of local(), and (with Frank's patient help, too) the role that silent coercion plays in speeding up many of these approaches.\nArthurYip for the poke to add the newer coalesce() function in and update the analysis.\nGregor for the nudge to figure out the data.table functions well enough to finally include them in the lineup.\nBase R For loop: alexis_laz\ndata.table For Loops: Matt_Dowle\nRoman for explaining what is.numeric() really tests.\n\n(Of course, please reach over and give them upvotes, too if you find those approaches useful.)\nNote on my use of Numerics:  If you do have a pure integer dataset, all of your functions will run faster. Please see alexiz_laz's work for more information. IRL, I can't recall encountering a data set containing more than 10-15% integers, so I am running these tests on fully numeric dataframes.\nHardware Used\n3.9 GHz CPU with 24 GB RAM\n\n\nA17:\n\nAnother option is to use collapse::replace_NA. By default, replace_NA replaces NAs with 0s.\nlibrary(collapse)\nreplace_NA(df)\n\nFor only some columns:\nreplace_NA(df, cols = c(\"V1\", \"V5\")) \n#Alternatively, one can use a function, indices or a logical vector to select the columns\n\n\nIt's also faster than any other answer (see this answer for a comparison):\nset.seed(42) # to recreate the exact dataframe\ndfN <- as.data.frame(matrix(sample(c(NA, as.numeric(1:4)), 1e7*10, replace = TRUE),\n                            dimnames = list(NULL, paste0(\"var\", 1:10)), \n                            ncol = 10))\n\nmicrobenchmark(collapse = replace_NA(dfN))\n\n# Unit: milliseconds\n#      expr      min      lq     mean  median       uq     max neval\n#  collapse 508.9198 621.405 751.3413 714.835 859.5437 1298.69   100\n\n\n\nA18:\n\nThe cleaner package has an na_replace() generic, that at default replaces numeric values with zeroes, logicals with FALSE, dates with today, etc.:\nlibrary(dplyr)\nlibrary(cleaner)\n\nstarwars %>% na_replace()\nna_replace(starwars)\n\nIt even supports vectorised replacements:\nmtcars[1:6, c(\"mpg\", \"hp\")] <- NA\nna_replace(mtcars, mpg, hp, replacement = c(999, 123))\n\nDocumentation: https://msberends.github.io/cleaner/reference/na_replace.html\n\n\nA19:\n\nFor a single vector:\nx <- c(1,2,NA,4,5)\nx[is.na(x)] <- 0\n\nFor a data.frame, make a function out of the above, then apply it to the columns.\nPlease provide a reproducible example next time as detailed here:\nHow to make a great R reproducible example?\n\n\nA20:\n\nIt is also possible to use tidyr::replace_na.\n    library(tidyr)\n    df <- df %>% mutate_all(funs(replace_na(.,0)))\n\nEdit (dplyr > 1.0.0):\ndf %>% mutate(across(everything(), .fns = ~replace_na(.,0))) \n\n\n\nA21:\n\ndplyr example:\nlibrary(dplyr)\n\ndf1 <- df1 %>%\n    mutate(myCol1 = if_else(is.na(myCol1), 0, myCol1))\n\nNote: This works per selected column, if we need to do this for all column, see @reidjax's answer using mutate_each.\n\n\nA22:\n\nDedicated functions, nafill and setnafill, for that purpose is in data.table.\nWhenever available, they distribute columns to be computed on multiple threads.\nlibrary(data.table)\n\nans_df <- nafill(df, fill=0)\n\n# or even faster, in-place\nsetnafill(df, fill=0)\n\n\n\nA23:\n\nAn easy way to write it is with if_na from hablar:\nlibrary(dplyr)\nlibrary(hablar)\n\ndf <- tibble(a = c(1, 2, 3, NA, 5, 6, 8))\n\ndf %>% \n  mutate(a = if_na(a, 0))\n\nwhich returns:\n      a\n  <dbl>\n1     1\n2     2\n3     3\n4     0\n5     5\n6     6\n7     8\n\n\n\nA24:\n\nTo replace all NAs in a dataframe you can use:\ndf %>% replace(is.na(.), 0)\n\n\nA25:\n\nI can think of two options. The first option is to use an editor that allows to block comment and uncomment (eg. Eclipse). The second option is to use an if statement. But that will only allow you to 'comment' correct R syntax.  Hence a good editor is the prefered workaround.\nif(FALSE){\n     #everything in this case is not executed\n\n}\n\n\n\nA26:\n\nThis is not exactly a new solution, but I like to write inline lambdas that handle things that I can't quite get packages to do. In this case,\ndf %>%\n   (function(x) { x[is.na(x)] <- 0; return(x) })\n\nBecause R does not ever \"pass by object\" like you might see in Python, this solution does not modify the original variable df, and so will do quite the same as most of the other solutions, but with much less need for intricate knowledge of particular packages.\nNote the parens around the function definition! Though it seems a bit redundant to me, since the function definition is surrounded in curly braces, it is required that inline functions are defined within parens for magrittr.\n\n\nA27:\n\ndplyr >= 1.0.0\nIn newer versions of dplyr:\n\nacross() supersedes the family of \"scoped variants\" like summarise_at(), summarise_if(), and summarise_all().\n\ndf <- data.frame(a = c(LETTERS[1:3], NA), b = c(NA, 1:3))\n\nlibrary(tidyverse)\n\ndf %>% \n  mutate(across(where(anyNA), ~ replace_na(., 0)))\n\n  a b\n1 A 0\n2 B 1\n3 C 2\n4 0 3\n\nThis code will coerce 0 to be character in the first column. To replace NA based on column type you can use a purrr-like formula in where:\ndf %>% \n  mutate(across(where(~ anyNA(.) & is.character(.)), ~ replace_na(., \"0\")))\n\n\n\nA28:\n\nIf we are trying to replace NAs when exporting, for example when writing to csv, then we can use:\n  write.csv(data, \"data.csv\", na = \"0\")\n\n\n\nA29:\n\nMore general approach of using replace() in matrix or vector to replace NA to 0\nFor example:\n> x <- c(1,2,NA,NA,1,1)\n> x1 <- replace(x,is.na(x),0)\n> x1\n[1] 1 2 0 0 1 1\n\nThis is also an alternative to using ifelse() in dplyr\ndf = data.frame(col = c(1,2,NA,NA,1,1))\ndf <- df %>%\n   mutate(col = replace(col,is.na(col),0))\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "X, to be fromg.\n   (v.s. \"No morethat\"\n  "}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 4, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI have an array int arr[5] that is passed to a function fillarr(int arr[]):\nint fillarr(int arr[])\n{\n    for(...);\n    return arr;\n}\n\n\nHow can I return that array?\nHow will I use it, say I returned a pointer how am I going to access it?\n\n\n\nCandidate answers:\nA1:\n\nAs others have noted, the code is illegitimate under C++11, although it was valid under earlier versions.  Consequently, a compiler for C++11 is required to issue at least one diagnostic, but behavior of the compiler or the remainder of the build system is unspecified beyond that.  Nothing in the Standard would forbid a compiler from exiting abruptly in response to an error, leaving a partially-written object file which a linker might think was valid, yielding a broken executable.\nAlthough a good compiler should always ensure before it exits that any object file it is expected to have produced will be either valid, non-existent, or recognizable as invalid, such issues fall outside the jurisdiction of the Standard.  While there have historically been (and may still be) some platforms where a failed compilation can result in legitimate-appearing executable files that crash in arbitrary fashion when loaded (and I've had to work with systems where link errors often had such behavior), I would not say that the consequences of syntax errors are generally unpredictable.  On a good system, an attempted build will generally either produce an executable with a compiler's best effort at code generation, or won't produce an executable at all.  Some systems will leave behind the old executable after a failed build, since in some cases being able to run the last successful build may be useful, but that can also lead to confusion.\nMy personal preference would be for disk-based systems to to rename the output file, to allow for the rare occasions when that executable would be useful while avoiding the confusion that can result from mistakenly believing one is running new code, and for embedded-programming systems to allow a programmer to specify for each project a program that should be loaded if a valid executable is not available under the normal name [ideally something which which safely indicates the lack of a useable program].  An embedded-systems tool-set would generally have no way of knowing what such a program should do, but in many cases someone writing \"real\" code for a system will have access to some hardware-test code that could easily be adapted to the purpose.  I don't know that I've seen the renaming behavior, however, and I know that I haven't seen the indicated programming behavior.\n\n\nA2:\n\nFirstly you have an error in the code, you mean to have Thing *thing(new Thing());, and only return thing;.\n\nUse shared_ptr<Thing>. Deref it as tho it was a pointer. It will be deleted for you when the last reference to the Thing contained goes out of scope.\nThe first solution is very common in naive libraries. It has some performance, and syntactical overhead, avoid it if possible\nUse the second solution only if you can guarantee no exceptions will be thrown, or when performance is absolutely critical (you will be interfacing with C or assembly before this even becomes relevant).\n\n\n\nA3:\n\n\nIt also states that the pointer will continue to point to the same location until it is reassigned or set to NULL.\n\nThat is definitely misleading information.\n\nClearly the address that the pointer is pointing to changes when delete is called!\nWhy is this happening? Does this have something to do with Visual Studio specifically?\n\nThis is clearly within the language specifications. ptr is not valid after the call to delete. Using ptr after it has been deleted is cause for undefined behavior. Don't do it. The run time environment is free to do whatever it wants to with ptr after the call to delete.\n\nAnd if delete can change the address it is pointing to anyways, why wouldn't delete automatically set the pointer to NULL instead of some random address???\n\nChanging the value of the pointer to any old value is within the language specification. As far as changing it to NULL, I would say, that would be bad. The program would behave in a more sane manner if the value of the pointer were set to NULL. However, that will hide the problem. When the program is compiled with different optimization settings or ported to a different environment, the problem will likely show up in the most inopportune moment.\n\n\nA4:\n\nint *fillarr(int arr[])\n\nYou can still use the result like\nint *returned_array = fillarr(some_other_array);\nif(returned_array[0] == 3)\n    do_important_cool_stuff();\n\n\n\nA5:\n\nSame reasoning when using a C-style static array rather than a std::vector. And for that, I kindly refer you to here.\n\n\nA6:\n\nDid you try to use smart pointers (if Thing is really big and heavy object), like shared_ptr:\n\n\n    std::shared_ptr calculateThing()\n    {\n        std::shared_ptr<Thing> thing(new Thing);\n        // .. some calculations\n        return thing;\n    }\n    \n    // ...\n    {\n        std::shared_ptr<Thing> thing = calculateThing();\n        // working with thing\n    \n        // shared_ptr frees thing \n    }\n\n\n\n\nA7:\n\nI'm going to assume that you know that std::array is compile-time fixed in size, while std::vector is variable size. Also, I'll assume you know that std::array doesn't do dynamic allocation. So instead, I'll answer why you would use std::array instead of std::vector.\nHave you ever found yourself doing this:\nstd::vector<SomeType> vecName(10);\n\nAnd then you never actually increase the size of the std::vector? If so, then std::array is a good alternative.\nBut really, std::array (coupled with initializer lists) exists to make C-style arrays almost entirely worthless. They don't generally compete with std::vectors; they compete more with C-style arrays.\nThink of it as the C++ committee doing their best to kill off almost all legitimate use of C-style arrays.\n\n\nA8:\n\nThe answer is: it depends on what C++ standard you're compiling against. All the code is perfectly well-formed across all standards\u2021 with the exception of this line:\nchar * s = \"My String\";\n\nNow, the string literal has type const char[10] and we're trying to initialize a non-const pointer to it. For all other types other than the char family of string literals, such an initialization was always illegal. For example:\nconst int arr[] = {1};\nint *p = arr; // nope!\n\nHowever, in pre-C++11, for string literals, there was an exception in \u00a74.2/2:\n\nA string literal (2.13.4) that is not a wide string literal can be converted to an rvalue of type \u201cpointer to char\u201d; [...]. In either case, the result is a pointer to the first element of the array. This conversion is considered only when there is an explicit appropriate pointer target type, and not when there is a general need to convert from an lvalue to an rvalue. [Note: this conversion is deprecated. See Annex D. ] \n\nSo in C++03, the code is perfectly fine (though deprecated), and has clear, predictable behavior.\nIn C++11, that block does not exist - there is no such exception for string literals converted to char*, and so the code is just as ill-formed as the int* example I just provided. The compiler is obligated to issue a diagnostic, and ideally in cases such as this that are clear violations of the C++ type system, we would expect a good compiler to not just be conforming in this regard (e.g. by issuing a warning) but to fail outright. \nThe code should ideally not compile - but does on both gcc and clang (I assume because there's probably lots of code out there that would be broken with little gain, despite this type system hole being deprecated for over a decade). The code is ill-formed, and thus it does not make sense to reason about what the behavior of the code might be. But considering this specific case and the history of it being previously allowed, I do not believe it to be an unreasonable stretch to interpret the resulting code as if it were an implicit const_cast, something like:\nconst int arr[] = {1};\nint *p = const_cast<int*>(arr); // OK, technically\n\nWith that, the rest of the program is perfectly fine, as you never actually touch s again. Reading a created-const object via a non-const pointer is perfectly OK. Writing a created-const object via such a pointer is undefined behavior:\nstd::cout << *p; // fine, prints 1\n*p = 5;          // will compile, but undefined behavior, which\n                 // certainly qualifies as \"unpredictable\"\n\nAs there is no modification via s anywhere in your code, the program is fine in C++03, should fail to compile in C++11 but does anyway - and given that the compilers allow it, there's still no undefined behavior in it\u2020. With allowances that the compilers are still [incorrectly] interpreting the C++03 rules, I see nothing that would lead to \"unpredictable\" behavior. Write to s though, and all bets are off. In both C++03 and C++11. \n\n\u2020Though, again, by definition ill-formed code yields no expectation of reasonable behavior\n\u2021Except not, see Matt McNabb's answer\n\n\nA9:\n\nI noticed that the address stored in ptr was always being overwritten with 00008123...\nThis seemed odd, so I did a little digging and found this Microsoft blog post containing a section discussing \"Automated pointer sanitization when deleting C++ objects\".\n\n...checks for NULL are a common code construct meaning that an existing check for NULL combined with using NULL as a sanitization value could fortuitously hide a genuine memory safety issue whose root cause really does needs addressing.\nFor this reason we have chosen 0x8123 as a sanitization value \u2013 from an operating system perspective this is in the same memory page as the zero address (NULL), but an access violation at 0x8123 will better stand out to the developer as needing more detailed attention.\n\nNot only does it explain what Visual Studio does with the pointer after it is deleted, it also answers why they chose NOT to set it to NULL automatically!\n\nThis \"feature\" is enabled as part of the \"SDL checks\" setting.  To enable/disable it go to: PROJECT -> Properties -> Configuration Properties -> C/C++ -> General -> SDL checks\nTo confirm this:\nChanging this setting and rerunning the same code produces the following output:\nptr = 007CBC10\nptr = 007CBC10\n\n\n\"feature\" is in quotes because in a case where you have two pointers to the same location, calling delete  will only sanitize ONE of them.  The other one will be left pointing to the invalid location...\n\nUPDATE:\nAfter 5 more years of C++ programming experience I realize this entire issue is basically a moot point. If you are a C++ programmer and are still using new and delete to manage raw pointers instead of using smart pointers (which circumvent this entire issue) you may want to consider a change in career path to become a C programmer. ;)\n\n\nA10:\n\nThere is 1 use case where plain C function pointers are the right answer: Comparison.\nstd::function has no proper concept of (in)equality. The only supported comparison is to nullptr, AKA the 'empty' state. Whereas function pointers are just pointers, so \"==\" works and does what you usually want: Return true for callbacks pointing to the exact same code (or both null), otherwise compare false. So if you really need to support comparing your callbacks, you need plain old function pointers or a custom functor type (a struct with an \"operator ()\") instead of std::function.\n\n\nA11:\n\n\nI don't want to return a copied value because it's inefficient\n\nProve it.\nLook up RVO and NRVO, and in C++0x move-semantics. In most cases in C++03, an out parameter is just a good way to make your code ugly, and in C++0x you'd actually be hurting yourself by using an out parameter.\nJust write clean code, return by value. If performance is a problem, profile it (stop guessing), and find what you can do to fix it. It likely won't be returning things from functions.\n\nThat said, if you're dead set on writing like that, you'd probably want to do the out parameter. It avoids dynamic memory allocation, which is safer and generally faster. It does require you have some way to construct the object prior to calling the function, which doesn't always make sense for all objects.\nIf you want to use dynamic allocation, the least that can be done is put it in a smart pointer. (This should be done all the time anyway) Then you don't worry about deleting anything, things are exception-safe, etc. The only problem is it's likely slower than returning by value anyway!\n\n\nA12:\n\nThe only slightly wrong thing that I see with this program is that you're not supposed to assign a string literal to a mutable char pointer, though this is often accepted as a compiler extension.\nOtherwise, this program appears well-defined to me:\n\nThe rules that dictate how character arrays become character pointers when passed as parameters (such as with cout << s2) are well-defined.\nThe array is null-terminated, which is a condition for operator<< with a char* (or a const char*).\n#include <iostream> includes <ostream>, which in turn defines operator<<(ostream&, const char*), so everything appears to be in place.\n\n\n\nA13:\n\nUse std::function to store arbitrary callable objects. It allows the user to provide whatever context is needed for the callback; a plain function pointer does not.\nIf you do need to use plain function pointers for some reason (perhaps because you want a C-compatible API), then you should add a void * user_context argument so it's at least possible (albeit inconvenient) for it to access state that's not directly passed to the function.\n\n\nA14:\n\nAfter deleting the pointer the memory to which it points may still be valid. To manifest this error, the pointer value is set to an obvious value. This really helps the debugging process. If the value were set to NULL, it may never show up as potential bug in the program flow. So it may hide a bug when you test later against NULL.\nAnother point is, that some run time optimizer may check that value and change its results.\nIn earlier times MS set the value to 0xcfffffff.\n\n\nA15:\n\nstd::function may bring VMT to the code in some cases, which has some impact on performance.\n\n\nA16:\n\n\nI don't want to return a copied value because it's inefficient\n\nThis may not be true. Compilers can do optimisation to prevent this copying.\nFor example, GCC does this optimisation. In the following program, neither move constructor nor copy constructor are called, since no copying or moving is done. Also, notice the address of c. Even though the object c is instantiated inside the function f(), c resides in the stack frame of main().\nclass C {\npublic:\n    int c = 5;\n    C() {}\n    C(const C& c) { \n        cout << \"Copy constructor \" << endl;\n    }\n    C(const C&& c)  noexcept {\n        cout << \"Move Constructor\" << endl;\n    }\n};\n\nC f() {\n    int beforeC;\n    C c;\n    int afterC;\n\n    cout << &beforeC << endl;   //0x7ffee02f26ac\n    cout << &c << endl;         //0x7ffee02f2710 (notice: even though c is instantiated inside f(), c resides in the stack frame of main()\n    cout << &afterC << endl;    //0x7ffee02f26a8\n\n    return c;\n}\n\nC g() {\n    C c = f(); ///neither copy constructor nor move constructor of C are called, since none is done\n    cout << &c << endl;  //0x7ffee02f2710\n    return c;\n}\n\nint main() {\n    int beforeC;\n    C c = g();    ///neither copy constructor nor move constructor of C are called, since none is done\n    int afterC;\n\n    cout << &beforeC << endl; //0x7ffee02f2718 \n    cout << &c << endl;       //0x7ffee02f2710 (notice:even though c is returned from f,it resides in the stack frame of main)\n    cout << &afterC << endl;  //0x7ffee02f270c\n    return 0;\n}\n\n\n\nA17:\n\nstd::array \n\nis an aggregate\nis fixed-size\nrequires that its\nelements be default constructible (vs\ncopy (C++03) or move (C++0x)\nconstructible)\nis linearly\nswappable (vs constant time)\nis linearly movable (vs constant time)\npotentially pays one less indirection than std::vector\n\nA good use case is when doing things 'close-to-the-metal', while keeping the niceties of C++ and keeping all the bad things of raw arrays out of the way.\n\n\nA18:\n\nYou see the side-effects of the /sdl compile option.  Turned on by default for VS2015 projects, it enables additional security checks beyond those provided by /gs.  Use Project > Properties > C/C++ > General > SDL checks setting to alter it.\nQuoting from the MSDN article:\n\n\nPerforms limited pointer sanitization. In expressions that do not involve dereferences and in types that have no user-defined destructor, pointer references are set to a non-valid address after a call to delete. This helps to prevent the reuse of stale pointer references.\n\n\nDo keep in mind that setting deleted pointers to NULL is a bad practice when you use MSVC.  It defeats the help you get from both the Debug Heap and this /sdl option, you can no longer detect invalid free/delete calls in your program.\n\n\nA19:\n\nI'm sure a C++ expert will come along with a better answer, but personally I like the second approach. Using smart pointers helps with the problem of forgetting to delete and as you say, it looks cleaner than having to create an object before hand (and still having to delete it if you want to allocate it on the heap).\n\n\nA20:\n\nI use my own personal hand coded Array<> template class, which has a simpler API compared with std::array or std::vector. For example: \nTo use a dynamic Array:\nArray<>  myDynamicArray; // Note array size is not given at compile time\nmyDynamicArray.resize(N); // N is a run time value\n\n...\n\nTo use a static Array, fixed size at compile time:\nArray<100> myFixedArry;\n\nI believe it has a better syntax than std::array, or std::vector. Also extremely efficient. \n\n\nA21:\n\nYou can't predict the behaviour of the compiler, for reasons noted above.  (It should fail to compile, but may not.)\nIf compilation succeeds, then the behaviour is well-defined.  You certainly can predict the behaviour of the program.\nIf it fails to compile, there is no program.  In a compiled language, the program is the executable, not the source code.  If you don't have an executable, you don't have a program, and you can't talk about behaviour of something that doesn't exist.\nSo I'd say your prof's statement is wrong.  You can't predict the behaviour of the compiler when faced with this code, but that's distinct from the behaviour of the program.  So if he's going to pick nits, he'd better make sure he's right.  Or, of course, you might have misquoted him and the mistake is in your translation of what he said.\n\n\nA22:\n\nOne quick way to determine if a copy constructor is being called is to add logging to your class's copy constructor:\nMyClass::MyClass(const MyClass &other)\n{\n    std::cout << \"Copy constructor was called\" << std::endl;\n}\n\nMyClass someFunction()\n{\n    MyClass dummy;\n    return dummy;\n}\n\nCall someFunction; the number of \"Copy constructor was called\" lines that you will get will vary between 0, 1, and 2. If you get none, then your compiler has optimised the return value out (which it is allowed to do). If you get don't get 0, and your copy constructor is ridiculously expensive, then search for alternative ways to return instances from your functions.\n\n\nA23:\n\nI believe, you are running some sort of debug mode and VS is attempting to repoint your pointer to some known location, so that further attempt to dereference it could be traced and reported. Try compiling/running the same program in release mode.\nPointers are usually not changed inside delete for the sake of efficiency and to avoid giving a false idea of safety. Setting delete pointer to pre-defined value will do no good in most of complex scenarios, since the pointer being deleted is likely to be only one of several pointing to this location.\nAs a matter of fact, the more I think about it, the more I find that VS is at fault when doing so, as usual. What if the pointer is const? Is it still gonna change it?\n\n\nA24:\n\nIn this case, your array variable arr can actually also be treated as a pointer to the beginning of your array's block in memory, by an implicit conversion.  This syntax that you're using:\nint fillarr(int arr[])\n\nIs kind of just syntactic sugar.  You could really replace it with this and it would still work:\nint fillarr(int* arr)\n\nSo in the same sense, what you want to return from your function is actually a pointer to the first element in the array:\nint* fillarr(int arr[])\n\nAnd you'll still be able to use it just like you would a normal array:\nint main()\n{\n  int y[10];\n  int *a = fillarr(y);\n  cout << a[0] << endl;\n}\n\n\n\nA25:\n\nThe other answers answer based on technical merits.  I'll give you an answer based on experience.\nAs a very heavy X-Windows developer who always worked with function pointer callbacks with void* pvUserData arguments, I started using std::function with some trepidation.\nBut I find out that combined with the power of lambdas and the like, it has freed up my work considerably to be able to, at a whim, throw multiple arguments in, re-order them, ignore parameters the caller wants to supply but I don't need, etc.  It really makes development feel looser and more responsive, saves me time, and adds clarity.\nOn this basis I'd recommend anyone to try using std::function any time they'd normally have a callback.  Try it everywhere, for like six months, and you may find you hate the idea of going back.\nYes there's some slight performance penalty, but I write high-performance code and I'm willing to pay the price.  As an exercise, time it yourself and try to figure out whether the performance difference would ever matter, with your computers, compilers and application space.\n\n\nA26:\n\nThe behaviour of the program is non-existent, because it is ill-formed.\nchar* s = \"My String\";\n\nThis is illegal. Prior to 2011, it had been deprecated for 12 years.\nThe correct line is:\nconst char* s = \"My String\";\n\nOther than that, the program is fine. Your professor should drink less whiskey!\n\n\nA27:\n\nJust return a object like this:\nThing calculateThing() \n{\n   Thing thing();\n   // do calculations and modify thing\n   return thing;\n}\n\nThis will invoke the copy constructor on Things, so you might want to do your own implementation of that. Like this:\nThing(const Thing& aThing) {}\n\nThis might perform a little slower, but it might not be an issue at all.\nUpdate\nThe compiler will probably optimize the call to the copy constructor, so there will be no extra overhead. (Like dreamlax pointed out in the comment).\n\n\nA28:\n\nJust create the object and return it\nThing calculateThing() {\n    Thing thing;\n    // do calculations and modify thing\n     return thing;\n}\n\nI think you'll do yourself a favor if you forget about optimization and just write readable code (you'll need to run a profiler later - but don't pre-optimize).\n\n\nA29:\n\nIn short, use std::function unless you have a reason not to.\nFunction pointers have the disadvantage of not being able to capture some context. You won't be able to for example pass a lambda function as a callback which captures some context variables (but it will work if it doesn't capture any). Calling a data member of an object (i.e. non-static) is thus also not possible, since the object (this-pointer) needs to be captured.(1)\nstd::function (since C++11) is primarily to store a function (passing it around doesn't require it to be stored). Hence if you want to store the callback for example in a data member, it's probably your best choice. But also if you don't store it, it's a good \"first choice\" although it has the disadvantage of introducing some (very small) overhead when being called (so in a very performance-critical situation it might be a problem but in most it should not). It is very \"universal\": if you care a lot about consistent and readable code as well as don't want to think about every choice you make (i.e. want to keep it simple), use std::function for every function you pass around.\nThink about a third option: If you're about to implement a small function which then reports something via the provided callback function, consider a template parameter, which can then be any callable object, i.e. a function pointer, a functor, a lambda, a std::function, ... Drawback here is that your (outer) function becomes a template and hence needs to be implemented in the header. On the other hand you get the advantage that the call to the callback can be inlined, as the client code of your (outer) function \"sees\" the call to the callback will the exact type information being available.\nExample for the version with the template parameter (write & instead of && for pre-C++11):\ntemplate <typename CallbackFunction>\nvoid myFunction(..., CallbackFunction && callback) {\n    ...\n    callback(...);\n    ...\n}\n\n\nAs you can see in the following table, all of them have their advantages and disadvantages:\n\n\n\n\n\nfunction ptr\nstd::function\ntemplate param\n\n\n\n\ncan capture context variables\nno1\nyes\nyes\n\n\nno call overhead (see comments)\nyes\nno\nyes\n\n\ncan be inlined (see comments)\nno\nno\nyes\n\n\ncan be stored in a class member\nyes\nyes\nno2\n\n\ncan be implemented outside of header\nyes\nyes\nno\n\n\nsupported without C++11 standard\nyes\nno3\nyes\n\n\nnicely readable (my opinion)\nno\nyes\n(yes)\n\n\n\n\n(1) Workarounds exist to overcome this limitation, for example passing the additional data as further parameters to your (outer) function: myFunction(..., callback, data) will call callback(data). That's the C-style \"callback with arguments\", which is possible in C++ (and by the way heavily used in the WIN32 API) but should be avoided because we have better options in C++.\n(2) Unless we're talking about a class template, i.e. the class in which you store the function is a template. But that would mean that on the client side the type of the function decides the type of the object which stores the callback, which is almost never an option for actual use cases.\n(3) For pre-C++11, use boost::function\n\n\nA30:\n\nOther answers have covered that this program is ill-formed in C++11 due to the assignment of a const char array to a char *. \nHowever the program was ill-formed prior to C++11 also. \nThe operator<< overloads are in <ostream>.  The requirement for iostream to include ostream was added in C++11.\nHistorically, most implementations had iostream include ostream anyway, perhaps for ease of implementation or perhaps in order to provide a better QoI.\nBut it would be conforming for iostream to only define the ostream class without defining the operator<< overloads. \n\n\nA31:\n\nstd::array is just a class version of the classic C array.  That means its size is fixed at compile time and it will be allocated as a single chunk (e.g. taking space on the stack).  The advantage it has is slightly better performance because there is no indirection between the object and the arrayed data.\nstd::vector is a small class containing pointers into the heap.  (So when you allocate a std::vector, it always calls new.)  They are slightly slower to access because those pointers have to be chased to get to the arrayed data...  But in exchange for that, they can be resized and they only take a trivial amount of stack space no matter how large they are.\n[edit]\nAs for when to use one over the other, honestly std::vector is almost always what you want.  Creating large objects on the stack is generally frowned upon, and the extra level of indirection is usually irrelevant.  (For example, if you iterate through all of the elements, the extra memory access only happens once at the start of the loop.)\nThe vector's elements are guaranteed to be contiguous, so you can pass &vec[0] to any function expecting a pointer to an array; e.g., C library routines.  (As an aside, std::vector<char> buf(8192); is a great way to allocate a local buffer for calls to read/write or similar without directly invoking new.)\nThat said, the lack of that extra level of indirection, plus the compile-time constant size, can make std::array significantly faster for a very small array that gets created/destroyed/accessed a lot.\nSo my advice would be:  Use std::vector unless (a) your profiler tells you that you have a problem and (b) the array is tiny.\n\n\nA32:\n\ndelete ptr;\ncout << \"ptr = \" << ptr << endl;\n\nIn general even reading (like you do above, note: this is different from dereferencing) values of invalid pointers (pointer becomes invalid for example when you delete it) is implementation defined behaviour. This was introduced in CWG #1438. See also here.\nPlease note that before that reading values of invalid pointers was undefined behaviour, so what you have above would be undefined behaviour, which means anything could happen. \n\n\nA33:\n\nvoid (*callbackFunc)(int); may be a C style callback function, but it is a horribly unusable one of poor design.\nA well designed C style callback looks like void (*callbackFunc)(void*, int); -- it has a void* to allow the code that does the callback to maintain state beyond the function.  Not doing this forces the caller to store state globally, which is impolite.\nstd::function< int(int) > ends up being slightly more expensive than int(*)(void*, int) invocation in most implementations.  It is however harder for some compilers to inline.  There are std::function clone implementations that rival function pointer invocation overheads (see 'fastest possible delegates' etc) that may make their way into libraries.\nNow, clients of a callback system often need to set up resources and dispose of them when the callback is created and removed, and to be aware of the lifetime of the callback.  void(*callback)(void*, int) does not provide this.\nSometimes this is available via code structure (the callback has limited lifetime) or through other mechanisms (unregister callbacks and the like).\nstd::function provides a means for limited lifetime management (the last copy of the object goes away when it is forgotten).\nIn general, I'd use a std::function unless performance concerns manifest.  If they did, I'd first look for structural changes (instead of a per-pixel callback, how about generating a scanline processor based off of the lambda you pass me?  which should be enough to reduce function-call overhead to trivial levels.).  Then, if it persists, I'd write a delegate based off fastest possible delegates, and see if the performance problem goes away.\nI would mostly only use function pointers for legacy APIs, or for creating C interfaces for communicating between different compilers generated code.  I have also used them as internal implementation details when I am implementing jump tables, type erasure, etc: when I am both producing and consuming it, and am not exposing it externally for any client code to use, and function pointers do all I need.\nNote that you can write wrappers that turn a std::function<int(int)> into a int(void*,int) style callback, assuming there are proper callback lifetime management infrastructure.  So as a smoke test for any C-style callback lifetime management system, I'd make sure that wrapping a std::function works reasonably well.\n\n\nA34:\n\nThe only reason to avoid std::function is support of legacy compilers that lack support for this template, which has been introduced in C++11.\nIf supporting pre-C++11 language is not a requirement, using std::function gives your callers more choice in implementing the callback, making it a better option compared to \"plain\" function pointers. It offers the users of your API more choice, while abstracting out the specifics of their implementation for your code that performs the callback.\n\n\nA35:\n\nstd::array has a fixed (compile time) size, while std::vector can grow.\nAs such, std::array is like using a C array, while std::vector is like dynamically allocating memory.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "   (vs (vs (vs (v (v (v (v (v (v (v (v (v (v (v (v ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 5, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI would like to have users click a link, then it selects the HTML text in another element (not an input).\nBy \"select\" I mean the same way you would select text by dragging your mouse over it. This has been a bear to research because everyone talks about \"select\" or \"highlight\" in other terms.\nIs this possible? My code so far:\nHTML:\n<a href=\"javascript:\" onclick=\"SelectText('xhtml-code')\">Select Code</a>\n<code id=\"xhtml-code\">Some Code here </code>\n\nJS:\nfunction SelectText(element) {\n    $(\"#\" + element).select();\n}\n\nAm I missing something blatantly obvious?\n\n\nCandidate answers:\nA1:\n\nThe differences between NPM and NPX are as below:\ni) NPM is used to install the packages while NPX is used to execute the packages.\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages used by npx don't need to be taken care of as they are not installed globally.\n\n\nA2:\n\nNPX is a tool for creating and executing some features in a new project\nNPM is the package manager that contains all of libraries\n\n\nA3:\n\nI resolve this problem on NodeJS like this:\nvar util = require('util');\n\n// Our circular object\nvar obj = {foo: {bar: null}, a:{a:{a:{a:{a:{a:{a:{hi: 'Yo!'}}}}}}}};\nobj.foo.bar = obj;\n\n// Generate almost valid JS object definition code (typeof string)\nvar str = util.inspect(b, {depth: null});\n\n// Fix code to the valid state (in this example it is not required, but my object was huge and complex, and I needed this for my case)\nstr = str\n    .replace(/<Buffer[ \\w\\.]+>/ig, '\"buffer\"')\n    .replace(/\\[Function]/ig, 'function(){}')\n    .replace(/\\[Circular]/ig, '\"Circular\"')\n    .replace(/\\{ \\[Function: ([\\w]+)]/ig, '{ $1: function $1 () {},')\n    .replace(/\\[Function: ([\\w]+)]/ig, 'function $1(){}')\n    .replace(/(\\w+): ([\\w :]+GMT\\+[\\w \\(\\)]+),/ig, '$1: new Date(\"$2\"),')\n    .replace(/(\\S+): ,/ig, '$1: null,');\n\n// Create function to eval stringifyed code\nvar foo = new Function('return ' + str + ';');\n\n// And have fun\nconsole.log(JSON.stringify(foo(), null, 4));\n\n\n\nA4:\n\nwith npx:\nnpx create-react-app MyApp\n\nearlier:\nnpm install -g create-react-app\ncreate-react-app MyApp\n\nEarlier it was necessary to install a CLI tool locally before using it, with npx that is not required anymore.\n\n\nA5:\n\nHere's an example of NPX in action: npx cowsay hello\nIf you type that into your bash terminal you'll see the result. The benefit of this is that npx has temporarily installed cowsay. There is no package pollution since cowsay is not permanently installed. This is great for one off packages where you want to avoid package pollution. \nAs mentioned in other answers, npx is also very useful in cases where (with npm) the package needs to be installed then configured before running. E.g. instead of using npm to install and then configure the json.package file and then call the configured run command just use npx instead. A real example:\nnpx create-react-app my-app\n\n\nA6:\n\nnpm is package manager or installer on the other hand Packages used by npx are not installed globally so you have to carefree for the pollution for the long term.\n\n\nA7:\n\nHere is the simple definition.\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages.\n\n\nA8:\n\nOne approach is to strip object and functions from main object. And stringify the simpler form\nfunction simpleStringify (object){\n    // stringify an object, avoiding circular structures\n    // https://stackoverflow.com/a/31557814\n    var simpleObject = {};\n    for (var prop in object ){\n        if (!object.hasOwnProperty(prop)){\n            continue;\n        }\n        if (typeof(object[prop]) == 'object'){\n            continue;\n        }\n        if (typeof(object[prop]) == 'function'){\n            continue;\n        }\n        simpleObject[prop] = object[prop];\n    }\n    return JSON.stringify(simpleObject); // returns cleaned up JSON\n};\n\nif you are using node js use inspect()\nimport {inspect} from \"util\";\nconsole.log(inspect(object));\n\n\n\nA9:\n\nNPM stands for Node Package Manager. NPM is Node.JS's default package manager. It's written in Javascript. The role of NPM is to manage the package and modules of node.js.\nNPX stands for Node Package Execute. NPX comes with npm, when npm is installed above the 5.2.0 version, it gets installed automatically. NPX is an npm package runner and its role is to execute the package from the registry without even installing that package.\nNow, the differences between NPM and NPX are as below:\n\ni) NPM is used to install the packages while NPX is used to execute the packages.\n\n\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages which are used by npx don't need to be taken care of as they are not installed globally.\n\n\n\nA10:\n\nPlain Javascript\n\n\nfunction selectText(nodeId) {\n    const node = document.getElementById(nodeId);\n\n    if (document.body.createTextRange) {\n        const range = document.body.createTextRange();\n        range.moveToElementText(node);\n        range.select();\n    } else if (window.getSelection) {\n        const selection = window.getSelection();\n        const range = document.createRange();\n        range.selectNodeContents(node);\n        selection.removeAllRanges();\n        selection.addRange(range);\n    } else {\n        console.warn(\"Could not select text in node: Unsupported browser.\");\n    }\n}\n\nconst clickable = document.querySelector('.click-me');\nclickable.addEventListener('click', () => selectText('target'));\n<div id=\"target\"><p>Some text goes here!</p><p>Moar text!</p></div>\n<p class=\"click-me\">Click me!</p>\n\n\n\nHere is a working demo. For those of you looking for a jQuery plugin, I made one of those too.\n\njQuery (original answer)\nI have found a solution for this in this thread. I was able to modify the info given and mix it with a bit of jQuery to create a totally awesome function to select the text in any element, regardless of browser:\nfunction SelectText(element) {\n    var text = document.getElementById(element);\n    if ($.browser.msie) {\n        var range = document.body.createTextRange();\n        range.moveToElementText(text);\n        range.select();\n    } else if ($.browser.mozilla || $.browser.opera) {\n        var selection = window.getSelection();\n        var range = document.createRange();\n        range.selectNodeContents(text);\n        selection.removeAllRanges();\n        selection.addRange(range);\n    } else if ($.browser.safari) {\n        var selection = window.getSelection();\n        selection.setBaseAndExtent(text, 0, text, 1);\n    }\n}\n\n\n\nA11:\n\nNPX:\nFrom https://www.futurehosting.com/blog/npx-makes-life-easier-for-node-developers-plus-node-vulnerability-news/:\n\nWeb developers can have dozens of projects on their development\n  machines, and each project has its own particular set of npm-installed\n  dependencies. A few years back, the usual advice for dealing with CLI\n  applications like Grunt or Gulp was to install them locally in each\n  project and also globally so they could easily be run from the command\n  line.\nBut installing globally caused as many problems as it solved. Projects\n  may depend on different versions of command line tools, and polluting\n  the operating system with lots of development-specific CLI tools isn\u2019t\n  great either. Today, most developers prefer to install tools locally\n  and leave it at that.\nLocal versions of tools allow developers to pull projects from GitHub\n  without worrying about incompatibilities with globally installed\n  versions of tools. NPM can just install local versions and you\u2019re good\n  to go. But project specific installations aren\u2019t without their\n  problems: how do you run the right version of the tool without\n  specifying its exact location in the project or playing around with\n  aliases?\nThat\u2019s the problem npx solves. A new tool included in NPM 5.2, npx is\n  a small utility that\u2019s smart enough to run the right application when\n  it\u2019s called from within a project.\nIf you wanted to run the project-local version of mocha, for example,\n  you can run npx mocha inside the project and it will do what you\n  expect.\nA useful side benefit of npx is that it will automatically install npm\n  packages that aren\u2019t already installed. So, as the tool\u2019s creator Kat\n  March\u00e1n points out, you can run npx benny-hill without having to deal\n  with Benny Hill polluting the global environment.\nIf you want to take npx for a spin, update to the most recent version\n  of npm.\n\n\n\nA12:\n\nnpx is a npm package runner (x probably stands for eXecute). One common way to use npx is to download and run a package temporarily or for trials.\ncreate-react-app is an npm package that is expected to be run only once in a project's lifecycle. Hence, it is preferred to use npx to install and run it in a single step.\nAs mentioned in the main page https://www.npmjs.com/package/npx, npx can run commands in the PATH or from node_modules/.bin by default.\nNote:\nWith some digging, we can find that create-react-app points to a Javascript file (possibly to /usr/lib/node_modules/create-react-app/index.js on Linux systems) that is executed within the node environment. This is simply a global tool that does some checks. The actual setup is done by react-scripts, whose latest version is installed in the project. Refer https://github.com/facebook/create-react-app for more info.\n\n\nA13:\n\nHave a look at the Selection object (Gecko engine) and the TextRange object (Trident engine.) I don't know about any JavaScript frameworks that have cross-browser support for this implemented, but I've never looked for it either, so it's possible that even jQuery has it.\n\n\nA14:\n\nIntroducing npx: an npm package runner\nNPM - Manages packages but doesn't make life easy executing any.NPX - A tool for executing Node packages.\n\nNPX comes bundled with NPM version 5.2+ \n\nNPM by itself does not simply run any package. It doesn't run any package as a matter of fact. If you want to run a package using NPM, you must specify that package in your package.json file.\nWhen executables are installed via NPM packages, NPM links to them:\n\nlocal installs have \"links\" created at ./node_modules/.bin/ directory.\nglobal installs have \"links\" created from the global bin/ directory (e.g. /usr/local/bin) on Linux or at %AppData%/npm on Windows.\n\nDocumentation you should read\n\nNPM:\nOne might install a package locally on a certain project:\nnpm install some-package\n\nNow let's say you want NodeJS to execute that package from the command line:\n$ some-package\n\nThe above will fail. Only globally installed packages can be executed by typing their name only.\nTo fix this, and have it run, you must type the local path:\n$ ./node_modules/.bin/some-package\n\nYou can technically run a locally installed package by editing your packages.json file and adding that package in the scripts section:\n{\n  \"name\": \"whatever\",\n  \"version\": \"1.0.0\",\n  \"scripts\": {\n    \"some-package\": \"some-package\"\n  }\n}\n\nThen run the script using npm run-script (or npm run):\nnpm run some-package\n\n\nNPX:\nnpx will check whether <command> exists in $PATH, or in the local project binaries, and execute it. So, for the above example, if you wish to execute the locally-installed package some-package all you need to do is type:\nnpx some-package\n\nAnother major advantage of npx is the ability to execute a package which wasn't previously installed:\n$ npx create-react-app my-app\n\nThe above example will generate a react app boilerplate within the path the command had run in, and ensures that you always use the latest version of a generator or build tool without having to upgrade each time you\u2019re about to use it.\n\nUse-Case Example:\nnpx command may be helpful in the script section of a package.json file,\nwhen it is unwanted to define a dependency which might not be commonly used or any other reason:\n\"scripts\": {\n    \"start\": \"npx gulp@3.9.1\",\n    \"serve\": \"npx http-server\"\n}\n\nCall with: npm run serve\n\nRelated questions:\n\nHow to use package installed locally in node_modules?\nNPM: how to source ./node_modules/.bin folder?\nHow do you run a js file using npm scripts?\n\n\n\nA15:\n\nSimple answer is like\nNPX: is used to execute any node package without installing the package on our machine.\nNPM: is used to install any node js package in our machine. We can use \"require(\"package-name')\" when we install any package using NPM. but we can not import the package when we use NPX.\nExample: You should run npm i axios\nin this case you are installing axios package in your local machine\nand npx create-react-app 'app-name'\nhere you are executing the create-react-app package directly on your machine without installing it's files.\n\n\nA16:\n\nNPM stands for Node Package Manager.\nIt comes pre-installed with Node.js. NPM helps to manage packages in your projects as dependencies.\nWhen using NPM, there are two ways to install a package into your local computer.\n\nLocally: When a package is installed locally, it is installed in\n./node_modules/.bin/ of the local project directory.\n\nGlobally: A global package is installed in the user environment\npath. /usr/local/bin for Linux and AppData%/npm for Windows.\n\n\nTo execute a locally installed package, it should be specified in the package.json scripts block as shown below.\n\"scripts\": {\n    \"your-package\":  \"your-package-name\"\n}\n\nThen, you can execute the package with:\nnpm run your-package-name\n\nNPX is an NPM package executor.\nCurrently, NPX is bundled with NPM when you install the NPM version 5.2.0 or higher.\nWhy NPX over NPM?\n\nNo need to edit the package.json file with node_modules paths.\n\nYou can directly execute the tool from the command line.\n\n\n\n\nA17:\n\nIt means that the object you pass in the request (I guess it is pagedoc) has a circular reference, something like:\nvar a = {};\na.b = a;\n\nJSON.stringify cannot convert structures like this.\nN.B.: This would be the case with DOM nodes, which have circular references, even if they are not attached to the DOM tree. Each node has an ownerDocument which refers to document in most cases. document has a reference to the DOM tree at least through document.body and document.body.ownerDocument refers back to document again, which is only one of multiple circular references in the DOM tree.\n\n\nA18:\n\nArray.prototype.map() index:\nOne can access the index Array.prototype.map() via the second argument of the callback function. Here is an example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst map = array.map((x, index) => {\n  console.log(index);\n  return x + index;\n});\n\nconsole.log(map);\n\n\n\nOther arguments of Array.prototype.map():\n\nThe third argument of the callback function exposes the array on which map was called upon\nThe second argument of Array.map() is a object which will be the this value for the callback function. Keep in mind that you have to use the regular function keyword in order to declare the callback since an arrow function doesn't have its own binding to the this keyword.\n\nFor example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst thisObj = { prop1: 1 }\n\nconst map = array.map((x, index, array) => {\n  console.log(array);\n  console.log(this)\n}, thisObj);\n\n\n\n\n\nA19:\n\nNPM - NODE PACKAGE MANAGER\n\nit's a package manager.\n\nnode.js  package can be installed using NPM\n\n\nNPX - NODE PACKAGE EXECUTE\n\nit execute the  node.js  package\n\ndoesn't matter whether you installed  node.js  package locally or\nglobally.\n\nNPX will temporarily install package and run it.\n\nNPX is used when you want to execute package for single time.\n\n\n.\n\n\nA20:\n\nHere's an example of what your app creation might look like using npx\n\nnpx create-react-app project-name --template all\n\n\n\nA21:\n\nYou will be able to get the current iteration's index for the map method through its 2nd parameter.\nExample:\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nlist.map((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return currElement; //equivalent to list[index]\n});\n\nOutput:\nThe current iteration is: 0 <br>The current element is: h\n\nThe current iteration is: 1 <br>The current element is: e\n\nThe current iteration is: 2 <br>The current element is: l\n\nThe current iteration is: 3 <br>The current element is: l \n\nThe current iteration is: 4 <br>The current element is: o\n\nSee also: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array/map\n\nParameters\ncallback - \n    Function that produces an element of the new Array, taking three arguments:\n1) currentValue\n        The current element being processed in the array.\n2) index\n      The index of the current element being processed in the array.\n3) array\n      The array map was called upon.\n\n\n\nA22:\n\n\nsuppose you have an array like\n\n\n\n   const arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    \n    \n    arr.map((myArr, index) => {\n      console.log(`your index is -> ${index} AND value is ${myArr}`);\n    })\n\n\n\n> output will be\n index is -> 0 AND value is 1\n index is -> 1 AND value is 2\n index is -> 2 AND value is 3\n index is -> 3 AND value is 4\n index is -> 4 AND value is 5\n index is -> 5 AND value is 6\n index is -> 6 AND value is 7\n index is -> 7 AND value is 8\n index is -> 8 AND value is 9\n\n\n\nA23:\n\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages. \nIt doesn't matter whether you installed that package globally or locally. NPX will temporarily install it and run it. NPM also can run packages if you configure a package.json file and include it in the script section. \n\nSo remember this, if you want to check/run a node package  quickly without installing locally or globally use NPX.\n\nnpM -  Manager\nnpX -  Execute - easy to remember\n\n\nA24:\n\nI normally use the circular-json npm package to solve this.\n// Felix Kling's example\nvar a = {};\na.b = a;\n// load circular-json module\nvar CircularJSON = require('circular-json');\nconsole.log(CircularJSON.stringify(a));\n//result\n{\"b\":\"~\"}\n\nNote: circular-json has been deprecated, I now use flatted (from the creator of CircularJSON):\n// ESM\nimport {parse, stringify} from 'flatted/esm';\n\n// CJS\nconst {parse, stringify} = require('flatted/cjs');\n\nconst a = [{}];\na[0].a = a;\na.push(a);\n\nstringify(a); // [[\"1\",\"0\"],{\"a\":\"0\"}]\n\nfrom: https://www.npmjs.com/package/flatted\n\n\nA25:\n\nThis thread (dead link) contains really wonderful stuff. But I'm not able to do it right on this page using FF 3.5b99 + FireBug due to \"Security Error\".\nYipee!! I was able to select whole right hand sidebar with this code hope it helps you:\n    var r = document.createRange();\n    var w=document.getElementById(\"sidebar\");  \n    r.selectNodeContents(w);  \n    var sel=window.getSelection(); \n    sel.removeAllRanges(); \n    sel.addRange(r); \n\nPS:- I was not able to use objects returned by jquery selectors like\n   var w=$(\"div.welovestackoverflow\",$(\"div.sidebar\"));\n   \n   //this throws **security exception**\n\n   r.selectNodeContents(w);\n\n\n\nA26:\n\nSimplest Definition:\nNPX\n\nThe npx stands for Node Package Execute and it comes with the npm,\nwhen you installed npm above 5.2.0 version then automatically npx will\ninstalled. It is an npm package runner that can execute any package\nthat you want from the npm registry without even installing that\npackage.\n\nNPM\n\nnpm is a package manager for the JavaScript programming language\nmaintained by npm, Inc. npm is the default package manager for the\nJavaScript runtime environment Node.js. It consists of a command line\nclient, also called npm, and an online database of public and paid-for\nprivate packages\n\n\n\nA27:\n\nBased on zainengineer's answer... Another approach is to make a deep copy of the object and strip circular references and stringify the result.\n\n\nfunction cleanStringify(object) {\r\n    if (object && typeof object === 'object') {\r\n        object = copyWithoutCircularReferences([object], object);\r\n    }\r\n    return JSON.stringify(object);\r\n\r\n    function copyWithoutCircularReferences(references, object) {\r\n        var cleanObject = {};\r\n        Object.keys(object).forEach(function(key) {\r\n            var value = object[key];\r\n            if (value && typeof value === 'object') {\r\n                if (references.indexOf(value) < 0) {\r\n                    references.push(value);\r\n                    cleanObject[key] = copyWithoutCircularReferences(references, value);\r\n                    references.pop();\r\n                } else {\r\n                    cleanObject[key] = '###_Circular_###';\r\n                }\r\n            } else if (typeof value !== 'function') {\r\n                cleanObject[key] = value;\r\n            }\r\n        });\r\n        return cleanObject;\r\n    }\r\n}\r\n\r\n// Example\r\n\r\nvar a = {\r\n    name: \"a\"\r\n};\r\n\r\nvar b = {\r\n    name: \"b\"\r\n};\r\n\r\nb.a = a;\r\na.b = b;\r\n\r\nconsole.log(cleanStringify(a));\r\nconsole.log(cleanStringify(b));\n\n\n\n\n\nA28:\n\nNPM: NPM stands for Node Package Manager and is the default package manager for Node.js. It was developed by Isaac Z. Schlueter and was originally released on January 12, 2010. It is entirely written in JavaScript. It consists of a command-line client npm which manages all node.js packages and modules. When node.js is installed, it is included in the installation.\nnpm run your-package-name\n\nNPX is a tool that use to execute packages.\nNPX is an acronym for Node Package Execute The NPX package comes with npm, so when you install npm above 5.2.0, NPX will be installed automatically.\nIt is an npm package runner that can execute any package that you want from the npm registry without even installing that package. The npx is useful during a single time use package. If you have installed npm below 5.2.0 then npx is not installed in your system.\nRun the following command to determine if npx is installed:\nnpx -v\n\nThe following command can be run if npx is not installed.\nnpm install -g npx\n\nUse npx to execute the package:\nnpx your-package-name\n\n\n\n\nA29:\n\nNPM => Is a JS package manager.\nNPX => Is a tool for executing Node packages and execute npm package binaries.\nIt is easy to remember:\n-npm stands for MANAGER\n-npx stands for EXECUTE\n\n\nA30:\n\nSimple Definition:\nnpm - Javascript package manager \nnpx - Execute npm package binaries\n\n\nA31:\n\nnpm is a tool that use to install packages and npx is a tool that use to execute packages.\nnpm-If you wish to run package through npm then you have to specify that package in your package.json and install it locally.\nnpx-A package can be executable without installing the package. It is an npm package runner so if any packages aren\u2019t already installed it will install them automatically.\n\n\nA32:\n\nUsing Ramda:\nimport {addIndex, map} from 'ramda';\n\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nconst mapIndexed = addIndex(map);\nmapIndexed((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return 'X';\n}, list);\n\n\n\nA33:\n\nAs per the JSON docs at Mozilla, JSON.stringify has a second parameter replacer which can be used to filter/ignore children items while parsing the tree. However, perhaps you can avoid the circular references.\nIn Node.js we cannot. So we can do something like this:\nfunction censor(censor) {\n  var i = 0;\n  \n  return function(key, value) {\n    if(i !== 0 && typeof(censor) === 'object' && typeof(value) == 'object' && censor == value) \n      return '[Circular]'; \n    \n    if(i >= 29) // seems to be a harded maximum of 30 serialized objects?\n      return '[Unknown]';\n    \n    ++i; // so we know we aren't using the original object anymore\n    \n    return value;  \n  }\n}\n\nvar b = {foo: {bar: null}};\n\nb.foo.bar = b;\n\nconsole.log(\"Censoring: \", b);\n\nconsole.log(\"Result: \", JSON.stringify(b, censor(b)));\n\nThe result:\nCensoring:  { foo: { bar: [Circular] } }\nResult: {\"foo\":{\"bar\":\"[Circular]\"}}\n\nUnfortunately there seems to be a maximum of 30 iterations before it automatically assumes it's circular. Otherwise, this should work. I even used areEquivalent from here, but JSON.stringify still throws the exception after 30 iterations. Still, it's good enough to get a decent representation of the object at a top level, if you really need it. Perhaps somebody can improve upon this though? In Node.js for an HTTP request object, I'm getting:\n{\n\"limit\": null,\n\"size\": 0,\n\"chunks\": [],\n\"writable\": true,\n\"readable\": false,\n\"_events\": {\n    \"pipe\": [null, null],\n    \"error\": [null]\n},\n\"before\": [null],\n\"after\": [],\n\"response\": {\n    \"output\": [],\n    \"outputEncodings\": [],\n    \"writable\": true,\n    \"_last\": false,\n    \"chunkedEncoding\": false,\n    \"shouldKeepAlive\": true,\n    \"useChunkedEncodingByDefault\": true,\n    \"_hasBody\": true,\n    \"_trailer\": \"\",\n    \"finished\": false,\n    \"socket\": {\n        \"_handle\": {\n            \"writeQueueSize\": 0,\n            \"socket\": \"[Unknown]\",\n            \"onread\": \"[Unknown]\"\n        },\n        \"_pendingWriteReqs\": \"[Unknown]\",\n        \"_flags\": \"[Unknown]\",\n        \"_connectQueueSize\": \"[Unknown]\",\n        \"destroyed\": \"[Unknown]\",\n        \"bytesRead\": \"[Unknown]\",\n        \"bytesWritten\": \"[Unknown]\",\n        \"allowHalfOpen\": \"[Unknown]\",\n        \"writable\": \"[Unknown]\",\n        \"readable\": \"[Unknown]\",\n        \"server\": \"[Unknown]\",\n        \"ondrain\": \"[Unknown]\",\n        \"_idleTimeout\": \"[Unknown]\",\n        \"_idleNext\": \"[Unknown]\",\n        \"_idlePrev\": \"[Unknown]\",\n        \"_idleStart\": \"[Unknown]\",\n        \"_events\": \"[Unknown]\",\n        \"ondata\": \"[Unknown]\",\n        \"onend\": \"[Unknown]\",\n        \"_httpMessage\": \"[Unknown]\"\n    },\n    \"connection\": \"[Unknown]\",\n    \"_events\": \"[Unknown]\",\n    \"_headers\": \"[Unknown]\",\n    \"_headerNames\": \"[Unknown]\",\n    \"_pipeCount\": \"[Unknown]\"\n},\n\"headers\": \"[Unknown]\",\n\"target\": \"[Unknown]\",\n\"_pipeCount\": \"[Unknown]\",\n\"method\": \"[Unknown]\",\n\"url\": \"[Unknown]\",\n\"query\": \"[Unknown]\",\n\"ended\": \"[Unknown]\"\n}\n\nI created a small Node.js module to do this here: https://github.com/ericmuyser/stringy Feel free to improve/contribute!\n\n\nA34:\n\nAccording to the jQuery documentation of select(): \n\nTrigger the select event of each matched element. This causes all of the functions that have been bound to that select event to be executed, and calls the browser's default select action on the matching element(s).\n\nThere is your explanation why the jQuery select() won't work in this case.\n\n\nA35:\n\nIn my case I simply forgot to use async/await thing while building the route:\napp.get('/products', async (req, res) => {\n    const products = await Product.find();\n    res.send(products );\n});\n\n\n\nA36:\n\nnpm - package manager\nnpx - Execute npm package\nThis is a difference with it.\n\n\nA37:\n\nNPM vs. NPX\nNPM stands for the Node Package Manager. A text based program for Nodejs package management.\nWhile NPX is a Node Package Runner. Its function is to execute the Nodejs package\nNPX will execute binary files from the Nodejs package, both installed and not.\nEven NPX can also help us use certain versions of Nodejs without having to use nvm (node.js version management), nave (node.js virtual environment), and nvm (node.js version management).\n\n\nA38:\n\nnpm - Package manager for JavaScript, just like: pip (Python), Maven (Java), NuGet (.NET), Composer (PHP), RubyGems (Ruby), ...\nnpx - runs a command of a package without installing it explicitly.\nUse cases:\n\nYou don't want to install packages neither globally nor locally.\nYou don't have permission to install it globally.\nJust want to test some commands.\nSometime, you want to have a script command (generate, convert something, ...) in package.json to execute something without installing these packages as project's dependencies.\n\nSyntax:\nnpx [options] [-p|--package <package>] <command> [command-arg]...\n\nPackage is optional:\nnpx   -p uglify-js         uglifyjs --output app.min.js app.js common.js\n      +----------------+   +--------------------------------------------+\n      package (optional)   command, followed by arguments\n\nFor example:\nStart a HTTP Server      : npx http-server\nLint code                : npx eslint ./src\n                         # Run uglifyjs command in the package uglify-js\nMinify JS                : npx -p uglify-js uglifyjs -o app.min.js app.js common.js\nMinify CSS               : npx clean-css-cli -o style.min.css css/bootstrap.css style.css\nMinify HTML              : npx html-minifier index-2.html -o index.html --remove-comments --collapse-whitespace\nScan for open ports      : npx evilscan 192.168.1.10 --port=10-9999\nCast video to Chromecast : npx castnow http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\n\nMore about command:\n\nhttps://docs.npmjs.com/files/package.json#bin\nhttps://github.com/mishoo/UglifyJS2/blob/master/package.json#L17\n\n\n\nA39:\n\nSimply npm is the Node Package Manager and\nnpx is the executeable version that run npm packages\n\n\nA40:\n\nFor my case I was getting that error when I was using async function on my server-side to fetch documents using mongoose. It turned out that the reason was I forgot to put await before calling find({}) method. Adding that part fixed my issue. \n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n  ,    ;  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 6, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nAfter updating to Xcode 5.1, I can no longer build my project for the 64-bit simulator, receiving this error:\n No architectures to compile for (ONLY_ACTIVE_ARCH=YES, active arch=x86_64, VALID_ARCHS=i386).\n\nThese are my target build settings:\n\nI tried changing \"Build Active Architecture\" to No, as well as adding \"i386\" to the \"Valid Architectures\", unfortunately neither worked.\nThanks for any suggestions!\n\n\nCandidate answers:\nA1:\n\nAs specified in another answer, you can also just not specify the distribution certificate to sign with and it will package correctly. TestFlight would need to update their app to do this.\n\n\nA2:\n\nNo need to quit the simulator or reset all content every time .\nJust close the app on which you are working if running on background .\nit works for me .\n\n\nA3:\n\nXcode 4.6 will prompt you to reinstall any older versions of the iOS Simulator if you just delete the SDK. To avoid that, you must also delete the Xcode cache. Then you won't be forced to reinstall the older SDK on launch.\nTo remove the iOS 5.0 simulator, delete these and then restart Xcode:\n\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/PhoneSimulator5.0.sdk\n~/Library/Caches/com.apple.dt.Xcode\n\nFor example, after doing a clean install of Xcode, I installed the iOS 5.0 simulator from Xcode preferences. Later, I decided that 5.1 was enough but couldn't remove the 5.0 version. Xcode kept forcing me to reinstall it on launch. After removing both the cache file and the SDK, it no longer asked.\n\n\nA4:\n\n\n\nWhat does the ENABLE_BITCODE actually do, will it be a non-optional requirement in the future?\n\n\nI'm not sure at what level you are looking for an answer at, so let's take a little trip. Some of this you may already know.\nWhen you build your project, Xcode invokes clang for Objective-C targets and swift/swiftc for Swift targets. Both of these compilers compile the app to an intermediate representation (IR), one of these IRs is bitcode. From this IR, a program called LLVM takes over and creates the binaries needed for x86 32 and 64 bit modes (for the simulator) and arm6/arm7/arm7s/arm64 (for the device). Normally, all of these different binaries are lumped together in a single file called a fat binary.\nThe ENABLE_BITCODE option cuts out this final step. It creates a version of the app with an IR bitcode binary. This has a number of nice features, but one giant drawback: it can't run anywhere. In order to get an app with a bitcode binary to run, the bitcode needs to be recompiled (maybe assembled or transcoded\u2026 I'm not sure of the correct verb) into an x86 or ARM binary.\nWhen a bitcode app is submitted to the App Store, Apple will do this final step and create the finished binaries.\nRight now, bitcode apps are optional, but history has shown Apple turns optional things into requirements (like 64 bit support). This usually takes a few years, so third party developers (like Parse) have time to update.\n\n\ncan I use the above method without any negative impact and without compromising a future appstore submission?\n\n\nYes, you can turn off ENABLE_BITCODE and everything will work just like before. Until Apple makes bitcode apps a requirement for the App Store, you will be fine.\n\n\nAre there any performance impacts if I enable / disable it?\n\n\nThere will never be negative performance impacts for enabling it, but internal distribution of an app for testing may get more complicated.\nAs for positive impacts\u2026 well that's complicated.\nFor distribution in the App Store, Apple will create separate versions of your app for each machine architecture (arm6/arm7/arm7s/arm64) instead of one app with a fat binary. This means the app installed on iOS devices will be smaller.\nIn addition, when bitcode is recompiled (maybe assembled or transcoded\u2026 again, I'm not sure of the correct verb), it is optimized. LLVM is always working on creating new a better optimizations. In theory, the App Store could recreate the separate version of the app in the App Store with each new release of LLVM, so your app could be re-optimized with the latest LLVM technology.\n\n\nA5:\n\nMake sure to select \"All\" to find the enable bitcode build settings:\n\n\n\nA6:\n\nBitcode makes crash reporting harder. Here is a quote from HockeyApp (which also true for any other crash reporting solutions):\n\nWhen uploading an app to the App Store and leaving the \"Bitcode\" checkbox enabled, Apple will use that Bitcode build and re-compile it on their end before distributing it to devices. This will result in the binary getting a new UUID and there is an option to download a corresponding dSYM through Xcode.\n\nNote: the answer was edited on Jan 2016 to reflect most recent changes\n\n\nA7:\n\n@vj9 thx. I update to xcode 7 . It show me the same error. Build well after set \"NO\"\n\nset \"NO\" it works well.\n\n\n\nA8:\n\nBe careful if you skip installs.\nI accidentally changed Skip Install on the main target, when i meant to change it on a dependency .\nWhen changing it back I had to do a clean.\n\n\nA9:\n\nI wrote up one-line bash script that would delete ALL your simulators:\nxcrun simctl list devices | grep -E -o -i \"([0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12})\" | xargs -L1 xcrun simctl delete\n\n\nxcrun simctl list devices will list all the simulators installed on your machine\ngrep -E -o -i \"([0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12})\" will grab the device UUID\nxargs -L1 xcrun simctl delete will attempt to delete the device for each UUID it found\n\nIf you want to see everything it'll execute, you can add echo before xcrun, i.e.\nxcrun simctl list devices | grep -E -o -i \"([0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12})\" | xargs -L1 echo xcrun simctl delete\n\n\n\nA10:\n\nThis worked for me:\niOS Simulator -> Reset Contents and Settings... -> Reset \n\n\nreferenced from Qiita: Unable to run app in Simulator \u30a8\u30e9\u30fc\u306e\u5bfe\u5fdc\u65b9\u6cd5.\n\n\nA11:\n\nIn addition to @childno.de answer, your Mac directory \n\n/private/var/db/receipts/ \n\nmay still contains obsolete iPhoneSimulatorSDK .bom and .plist files like this:\n\n/private/var/db/receipts/com.apple.pkg.iPhoneSimulatorSDK8_4.bom\n/private/var/db/receipts/com.apple.pkg.iPhoneSimulatorSDK8_4.plist\n\nThese could make your Downloads tab of Xcode's preferences show a tick (\u221a) for that obsolete simulator version.\nTo purge the unwanted simulators, you can do a search using this bash command from your Mac terminal:\n\nsudo find / -name \"*PhoneSimulator*\"\n\nThen go to corresponding directories to manually delete unwanted SimulatorSDKs\n\n\nA12:\n\n\nStarting with Xcode 14, bitcode is no longer required for watchOS and\ntvOS applications, and the App Store no longer accepts bitcode\nsubmissions from Xcode 14.\nXcode no longer builds bitcode by default and generates a warning\nmessage if a project explicitly enables bitcode: \u201cBuilding with\nbitcode is deprecated. Please update your project and/or target\nsettings to disable bitcode.\u201d The capability to build with bitcode\nwill be removed in a future Xcode release. IPAs that contain bitcode\nwill have the bitcode stripped before being submitted to the App\nStore. Debug symbols can only be downloaded from App Store Connect /\nTestFlight for existing bitcode submissions and are no longer\navailable for submissions made with Xcode 14. (86118779)\n\n\n\nA13:\n\nDid you try to just delete the 4.3 SDK from within the Xcode Package?\n\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs\n\nplease also delete the corresponding .dmg file in\n\n~/Library/Caches/com.apple.dt.Xcode/Downloads\n\nto prevent Xcode from re-installing the same package again.\n\nfor XCode >= 6 see  @praveen-matanam 's answer\n\n\nA14:\n\nIn short,\n\n\nsystem libraries, link them; \n3rd party libraries, embed them.\n\n\nwhy?\n\n\nif you try to embed system libraries, you won't find them in the popup list;\nif you link 3rd party libraries, you'll probably get a crash.\n\n\n\n\nA15:\n\nThe answer by Tim Gostony no longer works since the release of Xcode 7. Now the App Store submission process fails when resource rules are present. The solution is to clear your Code Signing Resource Rules Path and replace xcrun with xcodebuild tool:\nxcodebuild -exportArchive -archivePath [path to archive] -exportPath [path to output directory] -exportOptionsPlist [path to options.plist file]\n\nThe simplest Options.plist for exporting ad-hoc distribution ipa files looks like this:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>iCloudContainerEnvironment</key>\n    <string>Production</string>\n    <key>teamID</key>\n    <string>[YOUR TEAM ID]</string>\n    <key>method</key>\n    <string>ad-hoc</string>\n</dict>\n</plist>\n\nThere are other options available for this plist file regarding bitcode, app thinning, etc. That's why I think xcodebuild tool is the right tool for exporting ipa files for iOS 9 and above. \nMore details about options plist are available with xcodebuild -help command.\n\n\nA16:\n\nI just quit the simulator , cleaned up the project and ran again it worked.\n\n\nA17:\n\nJust closing the iOS Simulator worked for me.\nThere is not always the need to reset the iOS Simulator.  I did not even had to close Xcode.\n\n\nA18:\n\nRun this command in terminal to remove simulators that can't be accessed from the current version of Xcode (8+?) in use on your machine.\nxcrun simctl delete unavailable\nAlso if you're looking to reclaim simulator related space Michael Tsai found that deleting sim logs saved him 30 GB.\n~/Library/Logs/CoreSimulator\n\n\n\nA19:\n\nI emailed TestFlight support and got this response:\n\nOur team is currently investigating this issue with the TestFlight Desktop app. In the meantime, please use Xcode to create the IPA file and then upload it using the desktop app or the TestFlight website.\n\nThe suggested workaround did work.\n\n\nA20:\n\nAnswer valid through Xcode 7, 8, 9, 10, 11 and 12+\n\nYou can not archive, because from target you have not selected device or Generic iOS device or Any iOS Device , still there is a simulator.\nNothing generic or wrong with Xcode 7. This happens with all Xcode versions.\n\n\nProblem - Why archived button is grayed out?\n\n\n\n\nSolution - Why archived button is not grayed out?\n\n\nIn Xcode 12 it is named Any iOS Device\n\n\n\nA21:\n\nI had the same error message after upgrading to XCode 5.1.  Are you using CocoaPods?  If so, this should fix the problem:\n\nDelete the \"Pods\" project from the workspace in the left pane of Xcode and close Xcode.\nRun \"pod install\" from the command line to recreate the \"Pods\" project.\nRe-open Xcode and make sure \"Build Active Architecture Only\" is set to \"No\" in the build settings of both the \"Pods\" project and your own project.\nClean and build.\n\n\n\nA22:\n\n\nCan I use the linked answer to circumvent the issue without any negative impact and without compromising a future AppStore submission?\n\nyes\n\nWhat does the ENABLE_BITCODE actually do, will it be a non-optional requirement in the future?\n\nENABLE_BITCODE adds an intermediate representation of your code into binary. For watchOS, tvOS now it is mandatory\n\nAre there any performance impacts if I enable / disable it?\n\nIt has an impact on Xcode build and memory footprint when you archive your project\n[Bitcode]\n\n\nA23:\n\nIn XCode open Window - Devices, then select and remove the outdated simulators.\n\n\nA24:\n\nOn Yosemite w/ XCode 6.4 even using the SDKROOT patch the codesigning fails.  The following article explains how to patch the XCode script to get around this.  Note that this is patching XCode, so it is version specific, but fixes the problem.\nhttp://www.jayway.com/2015/05/21/fixing-your-ios-build-scripts\n\n\nA25:\n\nThe answer from Alistra work for me but I doesn't want to change a script which is not mine (a future Xcode release might change this file and the correction will be lost).\n\n diff PackageApplication PackageApplicationFixed 155,157c155,156\n<-     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements,resource-rules\",\n<-                          \"--sign\", $opt{sign},\n<-                          \"--resource-rules=$destApp/ResourceRules.plist\");\n---\n->     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements\",\n->                          \"--sign\", $opt{sign});\n\n\nI think answer from Vladimir Grigorov is the best if you have an archive using :\n\nxcodebuild -exportArchive -archivePath [path to archive] -exportPath [path to output directory] -exportOptionsPlist [path to options.plist file]\n\n\nIn my case, I doesn't have the archive, because I modify the application after build it and I need to change the Bundle Id and signing certificate.\nThe solution I found is to call codesign myself before used PackageApplication and ask PackageApplication to not sign. Like this :\nreplace :\n\n /usr/bin/xcrun -sdk iphoneos PackageApplication -v \"<app_path>\" -o \"<ipa_path>\" --sign \"<provisioning_profile.certificateSubject>\" --embed \"<provisioning_profile.path>\"\n\nby :\n\n/bin/cp -rpfv \"<provisioning_profile.path>\" \"<app_path>/embedded.mobileprovision\"\n/usr/bin/codesign -v -vvvv -f -s \"<provisioning_profile.certificateSubject>\" --entitlements=\"<entitlement_path>\" \"<app_path>\"\n/usr/bin/xcrun -sdk iphoneos PackageApplication -v \"<app_path>\" -o \"<ipa_path>\"\n\nDon't forget to embedded the .mobileprovision file using to sign with cp.\n\n\nA26:\n\nBitcode is a new feature of iOS 9\n\nBitcode is an intermediate representation of a compiled program. Apps you upload to iTunes Connect that contain bitcode will be compiled and linked on the App Store. Including bitcode will allow Apple to re-optimize your app binary in the future without the need to submit a new version of your app to the store.\nNote: For iOS apps, bitcode is the default, but optional. If you provide bitcode, all apps and frameworks in the app bundle need to include bitcode. For watchOS apps, bitcode is required\n\nSo you should disabled bitcode until all the frameworks of your app have bitcode enabled.\n\n\nA27:\n\nThe following patch for PackageApplications fixed it for me, I removed resource-rules as it says it's deprecated on 10.10.\nTestflight builds work without it. Appstore builds too.\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin\n % diff PackageApplication PackageApplicationFixed \n155,157c155,156\n<     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements,resource-rules\",\n<                          \"--sign\", $opt{sign},\n<                          \"--resource-rules=$destApp/ResourceRules.plist\");\n---\n>     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements\",\n>                          \"--sign\", $opt{sign});\n\n\n\nA28:\n\nDisconnect hardware keyboard.\nIn ios simulator main menu\nHardware -> Keyboard -> Disconnect Hardware keyboard.\n\n\nA29:\n\nI had the same problem.  I was running out of space.\nDeleting old device simulators did NOT help.\nMy space issue was caused by xCode.  It kept a copy of every iOS version on my macOS since I had installed xCode.\nDelete the iOS version you don't want and free up disk space.  I saved 50GB+ of space.\n\n\n\nNOTE -> can't you see ~/Library inside Finder?  It is hidden by default.  Use Terminal and type cd ~/Library/Developer/Xcode/iOS\\ DeviceSupport/ or google how to see hidden folders.\n\n\nNOTE -> if you have multiple users on a single macOS machine, make sure to find the directory ONLY with the user account that originally installed xCode.\n\n\nA30:\n\nAs mentioned in the Xcode release notes, this error (along with a couple others) results from a timing bug.  The workaround is to just retry.\nSee https://developer.apple.com/library/prerelease/mac/releasenotes/DeveloperTools/RN-Xcode/Chapters/xc6_release_notes.html#//apple_ref/doc/uid/TP40001051-CH4-SW1\n\nTesting on iOS simulator may produce an error indicating that the application could not be installed or launched.\n  Re-run testing or start another integration.\n\nAlso see:\nhttps://devforums.apple.com/thread/248879?tstart=0\n\n\nA31:\n\nmacOS Ventura 2023\nSame as the previous version search storage in your spotlight search tool and then double click the Developer option.\nThe interface looks a bit different now\n\nmacOS Big Sur & Monterey before 2023\nWe can use the Storage Management tool:\n\u2318 + Space bar and search storage.\nUnder Developer: you can find Xcode Cache and build files and also old version of iOS support files that you can delete.\n\nTested on macOS Big Sur & Monterey\n\n\nA32:\n\nI wish I knew why it works, but here's a fix that worked for me:\n\nFound the fix !\nClick on your project > Targets > Select your target > Build Settings\n  >\nCode Signing Resource Rules Path\nand add :\n$(SDKROOT)/ResourceRules.plist\n\n\n\nA33:\n\nReseting the Xcode will delete some bad junk which simulator has cached, so reseting is good option to resolve this issue.\niOS Simulator -> Reset Contents and Settings.\n\n\nA34:\n\nJust quit the xcode6.0 beta or else and run project again, it worked for me.\n\n\nA35:\n\nOctober 2020 update\nAs was mentioned, you can use xcrun to do a few things:\n\nxcrun simctl list devices or xcrun simctl list --json to list all simulators\nxcrun simctl delete <device udid> to delete specific device\nxcrun simctl delete unavailable to remove old devices for runtimes that are no longer supported\n\nMore things you can do with xcrun (see code snippet)\n\n\n- `xcrun simctl boot <device udid>` to launch (multiple) simulators\n- `xcrun simctl io booted recordVideo \u2014 type=mp4 ./test.mp4` to record simulator video\n- `xcrun simctl io booted screenshot ./screen.png` to make screenshot of simulator\n- `xcrun simctl openurl booted https://google.com` to open URL in simulator\n- `xcrun simctl addmedia booted ./test.mp4` to upload photo or video file (for photos app)\n- `xcrun simctl get_app_container booted <your apps bundle identifier>` to find the app container (where identifier is like *com.bundle.identifier*)\n- `xcrun simctl help` to explore **more** commands\n\n\n\nOriginal Answer\nSeptember 2017, Xcode 9\nRuntimes\nYou will find them here:\n/Library/Developer/CoreSimulator/Profiles/Runtimes\n\n\nDevices\nTo delete devices go here:\n~/Library/Developer/CoreSimulator/Devices\n\nMuch easier to delete them use Xcode:\nXcode->Window->Devices and Simulators\n\nHelping Xcode \"forget\" about runtimes and prevent from re-installing them - delete .dmg file(s) here:\n~/Library/Caches/com.apple.dt.Xcode/Downloads\n\nI hope it will help someone \ud83d\ude42\n\n\nA36:\n\nOpen your app scheme setting and edit it as shown in attached screenshot,\nActually you have to disable the \"Launch Due to background fetch\". Worked for me.\nSteps,\n1. Select your target and select \"edit scheme\"\n2. Select \"Options\" tab on right side of detail window\n3. Uncheck \"Launch Due to background fetch\" checkbox.\n\n\nA37:\n\nAdd:\nArchitectures: $(ARCHS_STANDARD_INCLUDING_64_BIT)\nValid architectures: arm64 armv7 armv7s\n\n\nA38:\n\nThis happened to me once when I added some assets as symbolic links (through ln -sf). Even installing to devices was failing.\nAfter I removed the links and made a hard copy, it worked and error was gone.\n\n\nA39:\n\nThis problem probably started after we enabled background mode in our app.My solution :\n1) Go to simulator\n2) Hold cmd + shift\n3) Tap 'H' key two times\n4) Slide the application off to stop it running in the background.\n5) Run again. \n(Very tiring to have to repeat the above step each time I build and run)\n\n\nA40:\n\nIn Xcode 6+ you can simply go to Menu > Window > Devices > Simulators and delete a simulator  you don't need.\n\n\nA41:\n\nfollowing some of the answers here, deleting some simulators from my Xcode Menu > Window > Devices > Simulators did nothing to help my dying disk space:\n\nhowever, cd ~/Library/Developer/Xcode/iOS\\ DeviceSupport and running du -sh * I got all of these guys:\n2.9G    10.0.1 (14A403)\n1.3G    10.1.1 (14B100)\n2.9G    10.3.2 (14F89)\n1.3G    10.3.3 (14G60)\n1.9G    11.0.1 (15A402)\n1.9G    11.0.3 (15A432)\n2.0G    11.1.2 (15B202)\n2.0G    11.2 (15C114)\n2.0G    11.2.1 (15C153)\n2.0G    11.2.2 (15C202)\n2.0G    11.2.6 (15D100)\n2.0G    11.4 (15F79)\n2.0G    11.4.1 (15G77)\n2.3G    12.0 (16A366)\n2.3G    12.0.1 (16A404)\n2.3G    12.1 (16B92)\n\nAll together that's 33 GB! \nA blood bath ensued\nsee more details here\n\n\nA42:\n\nBitcode is deprecated from XCode 14 Beta:\nhttps://developer.apple.com/documentation/xcode-release-notes/xcode-14-release-notes\n\nStarting with Xcode 14, bitcode is no longer required for watchOS and tvOS applications, and the App Store no longer accepts bitcode submissions from Xcode 14.\nXcode no longer builds bitcode by default and generates a warning message if a project explicitly enables bitcode: \u201cBuilding with bitcode is deprecated. Please update your project and/or target settings to disable bitcode.\u201d The capability to build with bitcode will be removed in a future Xcode release. IPAs that contain bitcode will have the bitcode stripped before being submitted to the App Store. Debug symbols for past bitcode submissions remain available for download. (86118779)\n\nTo anyone wondering about bitcode, it seems that it won't be necessary anymore.\n\n\nA43:\n\nXcode pre-v11. Embedded Binaries vs Linked Frameworks and Libraries\n\nHistory\nEmbedded Binaries vs Linked Frameworks and Libraries -> Frameworks, Libraries, and Embedded Content\n\n[Xcode v11. Frameworks, Libraries, and Embedded Content] replaced it from Xcode v11 section in General tab\nembedded binaries and Linked Frameworks are a part of Dependency management [About]\n[Xcode v11]\nLink Binary\nGeneral -> Linked Frameworks and Libraries is a mirror of Build Phases -> Link Binary With Libraries.\nStatic Library and Framework\nIf you add a Static Library or Static Framework to this section it will appear at Frameworks group[About](Project Navigator -> <workspace/project> -> Frameworks) and there will be a reference added to your project for it. Then it will be used by Static Linker. Static Linker at compile time will include/copy all code from the library into the executable object file. Static linker works in pair with Build Settings -> <Library/Framework> Search Paths\n\nStatic Library\n\nBuild Settings -> Library Search Paths[library not found] If you do not add a static library to this section you will get a linker error[ld: symbol(s) not found]\n\nStatic Framework\n\nBuild Settings -> Framework Search Paths. If you do not add a static framework to this section you will get a compile error[No such module]\n\nEmbed binary\nStatic Library and Static Framework\nEmbedding wouldn\u2019t make any sense for a Static Library and Static Framework because the symbols from them are compiled into the executable binary. Xcode won\u2019t let you drop a static library under the Embed section.\nDynamic Framework\nGeneral -> Embedded Binaries is a mirror of Build Phases -> Embed Frameworks.\nEmbedding actually adds a copy of the framework into your application bundle(not merging framework's and application's code into single executable binary)\nBy default the bundle's folder is Frameworks but you can change it using Destination field. Moreover you can specify a Subpath.\n\nDynamic linker :dyld at load or run time will try to find the embedded framework using @rpath[About] If it is not found the error will occur [dyld: Library not loaded]\n\nResult:\n\nStatic Library - Link\nStatic Framework - Link\nDynamic Framework - Embed\n\n[Static vs Dynamic linker]\n[When use Link and Embed]\n[Vocabulary]\n\n\nA44:\n\nIn Xcode 6 and above, you can find and delete the simulators from the path /Library/Developer/CoreSimulator/Profiles/Runtimes. Restart Xcode in order to take effect (may not be needed).\n\n\nA45:\n\nThe question you linked references the \"Link Binary With Libraries\" functionality, which is somewhat different than an embedded binary.\n\"Link Binary With Libraries\" means what you'd expect it to with respect to linkage: Regardless of whether the binary is a static library, dynamic library, or framework it will be linked to your object code at link time after compilation.\nWhen you think of linkage with a static library, what happens is pretty clear:  the linker copies the code from the library (e.g. libFoo.a) into your output binary.  Your output file grows in size but doesn't need to resolve any external dependencies at runtime.  Everything your program needs to run (with respect to the static library) is present after it is built.\nWith a dynamic library (.dylib, or system-supplied framework), the expectation is that the library you are linking against will be present somewhere in the system's dynamic-library loader path when you run your program.  This way you don't have the overhead of copying all the third party external libraries into your binary, and all the different programs on a computer that also link to that library will be able to find it, which saves minimally disk space, but also potentially memory space, depending on how and where the system caches libraries.\nA framework is much like a dynamic library, but can contain resources in its directory structure (images, audio, other frameworks, etc.).  In this case a simple static-library or .dylib file won't cut it so you might have to link to a framework just so it can find what it needs to run properly.\nWhen you link to a third-party framework (say something you downloaded from github and built yourself), it might not be present on the system you intend to run on.  In this case, you'd not only link to the framework, but embed it inside your application bundle as well using the \"Copy Frameworks\" phase.  When your program runs, the runtime-linker (aka the resolver) will look inside your bundle in addition to the system loader path, find the embedded framework, and link it so your app will have the code it needs in order to run.\nFinally, what is properly an \"embedded binary\" is an executable you both embed in your application bundle via a Copy-Files Phase, and that you execute yourself, perhaps with a call to popen() or similar.  The embedded binary may be called by your program, but it isn't linked with it.  It is a fully external entity (like programs in the /bin directory).\nIn practice, for system-supplied libraries and frameworks you will link against them and that's all you need to do.\nIf you need to link a library you built that doesn't need any embedded resources (i.e. doesn't require a framework to exist), then you can just link against a static library.  If you find you have multiple modules in your program that want to use the same library code, then converting it to a framework or dynamic library and linking against that can save space and may be convenient (particularly if memory usage is a concern).\nFinally, frameworks can include not only resources, but header and/or license files.  Using a framework to convey these files is actually a convenient distribution mechanism so often you may want to incorporate a framework just so these things can tag along with your binary (i.e. license requirements may make this mandatory).\n--- EDIT ---\nAdam Johns posted the following question as a comment:\n\nThis is a great answer. There is something I'm still a little confused on, however. What does it mean to execute the binary yourself? Do you mean simply using the embedded framework's code? I know you mentioned popen(), but you're saying my app is calling popen()? I don't really know what that means.\n\nI'm saying an embedded binary is just another resource file in your bundle, like an audio file or image, although the file is instead an executable command-line tool. The popen() function (man popen from your terminal to read more about it) lets you execute arbitrary programs from another running program. The system() function is another way. There are others, and I'll give a historical example here that may make understanding use of an embedded binary a bit more clear:\nAs you're probably aware, when you launch an app on Mac OS X it is launched with a user id of the current user. Under most common installations that's the default user-at-the-Desktop admin user, who is given user id 501.\nOn Unix-based operating systems only the root user (user id 0) has full access to the entire filesystem. Sometimes it happens that an installer program launched by the Desktop user needs to install files in a privileged directory (drivers for example). In this case, the application program needs to escalate its privileges to the root user so it can write in these restricted directories.\nTo facilitate this in operating systems through OS X 10.7, Apple provided in its Authorization Services API the function AuthorizationExecuteWithPrivileges() (this is now deprecated, but is still a useful example).\nAuthorizationExecuteWithPrivileges() took as an argument a path to a command-line tool to execute as root. The command line tool was an executable shell script or compiled binary that you wrote to run your install logic. This tool was installed inside your application bundle just like any other resource file.\nWhen called, the OS put up an authorization dialog asking for the user's password (you've seen this before!) and when entered would execute the program as root on your app's behalf. This process is similar to just executing a program with popen() yourself, although popen() alone doesn't give you the benefit of privilege escalation.\n\n\nA46:\n\nFrom the docs\n\ncan I use the above method without any negative impact and without compromising a future appstore submission?\n\nBitcode will allow apple to optimise the app without you having to submit another build. But, you can only enable this feature if all frameworks and apps in the app bundle have this feature enabled.\nHaving it helps, but not having it should not have any negative impact.\n\nWhat does the ENABLE_BITCODE actually do, will it be a non-optional requirement in the future?\n\n\nFor iOS apps, bitcode is the default, but optional. If you provide\n  bitcode, all apps and frameworks in the app bundle need to include\n  bitcode. For watchOS apps, bitcode is required.\n\n\nAre there any performance impacts if I enable / disable it?\n\n\nThe App Store and operating system optimize the installation of iOS\n  and watchOS apps by tailoring app delivery to the capabilities of the\n  user\u2019s particular device, with minimal footprint. This optimization,\n  called app thinning, lets you create apps that use the most device\n  features, occupy minimum disk space, and accommodate future updates\n  that can be applied by Apple. Faster downloads and more space for\n  other apps and content provides a better user experience.\n\nThere should not be any performance impacts.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 7, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\n\nHow often does Python flush to a file?\nHow often does Python flush to stdout?\n\nI'm unsure about (1).\nAs for (2), I believe Python flushes to stdout after every new line.  But, if you overload stdout to be to a file, does it flush as often?\n\n\nCandidate answers:\nA1:\n\nBased on the answer described here, using subprocess is another option.\nSomething like this:\nsubprocess.call(\"mv %s %s\" % (source_files, destination_folder), shell=True)\n\nI am curious to know the pro's and con's of this method compared to shutil.  Since in my case I am already using subprocess for other reasons and it seems to work I am inclined to stick with it.\nThis is dependent on the shell you are running your script in. The mv command is for most Linux shells (bash, sh, etc.), but would also work in a terminal like Git Bash on Windows. For other terminals you would have to change mv to an alternate command.\n\n\nA2:\n\nAs lassevk suggests, write out the new file as you go, here is some example code:\nfin = open(\"a.txt\")\nfout = open(\"b.txt\", \"wt\")\nfor line in fin:\n    fout.write( line.replace('foo', 'bar') )\nfin.close()\nfout.close()\n\n\n\nA3:\n\nI first struggled to understand how the flush option was working. I wanted to do a 'loading display' and here is the solution I found:\nfor i in range(100000):\n    print('{:s}\\r'.format(''), end='', flush=True)\n    print('Loading index: {:d}/100000'.format(i+1), end='')\n\nThe first line flushes the previous print and the second line prints a new updated message. I don't know if an one-line syntax exists here.\n\n\nA4:\n\n\nHow to flush output of Python print?\n\nI suggest five ways of doing this:\n\nIn Python 3, call print(..., flush=True) (the flush argument is not available in Python 2's print function, and there is no analogue for the print statement).\nCall file.flush() on the output file (we can wrap python 2's print function to do this), for example, sys.stdout\napply this to every print function call in the module with a partial function,\nprint = partial(print, flush=True) applied to the module global.\napply this to the process with a flag (-u) passed to the interpreter command\napply this to every python process in your environment with PYTHONUNBUFFERED=TRUE (and unset the variable to undo this).\n\nPython 3.3+\nUsing Python 3.3 or higher, you can just provide flush=True as a keyword argument to the print function:\nprint('foo', flush=True) \n\nPython 2 (or < 3.3)\nThey did not backport the flush argument to Python 2.7 So if you're using Python 2 (or less than 3.3), and want code that's compatible with both 2 and 3, may I suggest the following compatibility code. (Note the __future__ import must be at/very \"near the top of your module\"):\nfrom __future__ import print_function\nimport sys\n\nif sys.version_info[:2] < (3, 3):\n    old_print = print\n    def print(*args, **kwargs):\n        flush = kwargs.pop('flush', False)\n        old_print(*args, **kwargs)\n        if flush:\n            file = kwargs.get('file', sys.stdout)\n            # Why might file=None? IDK, but it works for print(i, file=None)\n            file.flush() if file is not None else sys.stdout.flush()\n\nThe above compatibility code will cover most uses, but for a much more thorough treatment, see the six module.\nAlternatively, you can just call file.flush() after printing, for example, with the print statement in Python 2:\nimport sys\nprint 'delayed output'\nsys.stdout.flush()\n\nChanging the default in one module to flush=True\nYou can change the default for the print function by using functools.partial on the global scope of a module:\nimport functools\nprint = functools.partial(print, flush=True)\n\nif you look at our new partial function, at least in Python 3:\n>>> print = functools.partial(print, flush=True)\n>>> print\nfunctools.partial(<built-in function print>, flush=True)\n\nWe can see it works just like normal:\n>>> print('foo')\nfoo\n\nAnd we can actually override the new default:\n>>> print('foo', flush=False)\nfoo\n\nNote again, this only changes the current global scope, because the print name on the current global scope will overshadow the builtin print function (or unreference the compatibility function, if using one in Python 2, in that current global scope).\nIf you want to do this inside a function instead of on a module's global scope, you should give it a different name, e.g.:\ndef foo():\n    printf = functools.partial(print, flush=True)\n    printf('print stuff like this')\n\nIf you declare it a global in a function, you're changing it on the module's global namespace, so you should just put it in the global namespace, unless that specific behavior is exactly what you want.\nChanging the default for the process\nI think the best option here is to use the -u flag to get unbuffered output.\n$ python -u script.py\n\nor\n$ python -um package.module\n\nFrom the docs:\n\nForce stdin, stdout and stderr to be totally unbuffered. On systems where it matters, also put stdin, stdout and stderr in binary mode.\nNote that there is internal buffering in file.readlines() and File Objects (for line in sys.stdin) which is not influenced by this option. To work around this, you will want to use file.readline() inside a while 1: loop.\n\nChanging the default for the shell operating environment\nYou can get this behavior for all python processes in the environment or environments that inherit from the environment if you set the environment variable to a nonempty string:\ne.g., in Linux or OSX:\n$ export PYTHONUNBUFFERED=TRUE\n\nor Windows:\nC:\\SET PYTHONUNBUFFERED=TRUE\n\nfrom the docs:\n\nPYTHONUNBUFFERED\nIf this is set to a non-empty string it is equivalent to specifying the -u option.\n\n\nAddendum\nHere's the help on the print function from Python 2.7.12 - note that there is no flush argument:\n>>> from __future__ import print_function\n>>> help(print)\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file: a file-like object (stream); defaults to the current sys.stdout.\n    sep:  string inserted between values, default a space.\n    end:  string appended after the last value, default a newline.\n\n\n\nA5:\n\nUsing the -u command-line switch works, but it is a little bit clumsy. It would mean that the program would potentially behave incorrectly if the user invoked the script without the -u option.  I usually use a custom stdout, like this:\nclass flushfile:\n  def __init__(self, f):\n    self.f = f\n\n  def write(self, x):\n    self.f.write(x)\n    self.f.flush()\n\nimport sys\nsys.stdout = flushfile(sys.stdout)\n\n... Now all your print calls (which use sys.stdout implicitly), will be automatically flushed.\n\n\nA6:\n\nfileinput is quite straightforward as mentioned on previous answers:\nimport fileinput\n\ndef replace_in_file(file_path, search_text, new_text):\n    with fileinput.input(file_path, inplace=True) as file:\n        for line in file:\n            new_line = line.replace(search_text, new_text)\n            print(new_line, end='')\n\nExplanation:\n\nfileinput can accept multiple files, but I prefer to close each single file as soon as it is being processed. So placed single file_path in with statement.\nprint statement does not print anything when inplace=True, because STDOUT is being forwarded to the original file.\nend='' in print statement is to eliminate intermediate blank new lines.\n\nYou can used it as follows:\nfile_path = '/path/to/my/file'\nreplace_in_file(file_path, 'old-text', 'new-text')\n\n\n\nA7:\n\nSince you don't care about the return value, you can do\nimport os\nos.system(\"mv src/* dest/\")\n\n\n\nA8:\n\nI did it like this in Python 3.4:\n'''To write to screen in real-time'''\nmessage = lambda x: print(x, flush=True, end=\"\")\nmessage('I am flushing out now...')\n\n\n\nA9:\n\nRunning python -h, I see a command line option:\n\n-u     : unbuffered binary stdout and stderr; also PYTHONUNBUFFERED=x\nsee man page for details on internal buffering relating to '-u'\n\nHere is the relevant documentation.\n\n\nA10:\n\nThe shortest way would probably be to use the fileinput module. For example, the following adds line numbers to a file, in-place:\nimport fileinput\n\nfor line in fileinput.input(\"test.txt\", inplace=True):\n    print('{} {}'.format(fileinput.filelineno(), line), end='') # for Python 3\n    # print \"%d: %s\" % (fileinput.filelineno(), line), # for Python 2\n\nWhat happens here is:\n\nThe original file is moved to a backup file\nThe standard output is redirected to the original file within the loop\nThus any print statements write back into the original file\n\nfileinput has more bells and whistles. For example, it can be used to automatically operate on all files in sys.args[1:], without your having to iterate over them explicitly. Starting with Python 3.2 it also provides a convenient context manager for use in a with statement.\n\nWhile fileinput is great for throwaway scripts, I would be wary of using it in real code because admittedly it's not very readable or familiar. In real (production) code it's worthwhile to spend just a few more lines of code to make the process explicit and thus make the code readable.\nThere are two options:\n\nThe file is not overly large, and you can just read it wholly to memory. Then close the file, reopen it in writing mode and write the modified contents back.\nThe file is too large to be stored in memory; you can move it over to a temporary file and open that, reading it line by line, writing back into the original file. Note that this requires twice the storage.\n\n\n\nA11:\n\nIf you're wanting a generic function that replaces any text with some other text, this is likely the best way to go, particularly if you're a fan of regex's:\nimport re\ndef replace( filePath, text, subs, flags=0 ):\n    with open( filePath, \"r+\" ) as file:\n        fileContents = file.read()\n        textPattern = re.compile( re.escape( text ), flags )\n        fileContents = textPattern.sub( subs, fileContents )\n        file.seek( 0 )\n        file.truncate()\n        file.write( fileContents )\n\n\n\nA12:\n\nAlthough os.rename() and shutil.move() will both rename files, the command that is closest to the Unix mv command is shutil.move().  The difference is that os.rename() doesn't work if the source and destination are on different disks, while shutil.move() is files disk agnostic.\n\n\nA13:\n\nWith Python 3.x the print() function has been extended:\nprint(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False)\n\nSo, you can just do:\nprint(\"Visiting toilet\", flush=True)\n\n\nPython Docs Entry\n\n\nA14:\n\nThis is solution, which does not enables shell using mv.\nfrom subprocess import Popen, PIPE, STDOUT\n\nsource = \"path/to/current/file.foo\", \ndestination = \"path/to/new/destination/for/file.foo\"\n\np = Popen([\"mv\", \"-v\", source, destination], stdout=PIPE, stderr=STDOUT)\noutput, _ = p.communicate()\noutput = output.strip().decode(\"utf-8\")\nif p.returncode:\n    print(f\"E: {output}\")\nelse:\n    print(output)\n\n\n\nA15:\n\nExpanding on @Kiran's answer, which I agree is more succinct and Pythonic, this adds codecs to support the reading and writing of UTF-8:\nimport codecs \n\nfrom tempfile import mkstemp\nfrom shutil import move\nfrom os import remove\n\n\ndef replace(source_file_path, pattern, substring):\n    fh, target_file_path = mkstemp()\n\n    with codecs.open(target_file_path, 'w', 'utf-8') as target_file:\n        with codecs.open(source_file_path, 'r', 'utf-8') as source_file:\n            for line in source_file:\n                target_file.write(line.replace(pattern, substring))\n    remove(source_file_path)\n    move(target_file_path, source_file_path)\n\n\n\nA16:\n\nThis should work: (inplace editing)\nimport fileinput\n\n# Does a list of files, and\n# redirects STDOUT to the file in question\nfor line in fileinput.input(files, inplace = 1): \n      print line.replace(\"foo\", \"bar\"),\n\n\n\nA17:\n\nHere is my version, which provides writelines() and fileno(), too:\nclass FlushFile(object):\n    def __init__(self, fd):\n        self.fd = fd\n\n    def write(self, x):\n        ret = self.fd.write(x)\n        self.fd.flush()\n        return ret\n\n    def writelines(self, lines):\n        ret = self.writelines(lines)\n        self.fd.flush()\n        return ret\n\n    def flush(self):\n        return self.fd.flush\n\n    def close(self):\n        return self.fd.close()\n\n    def fileno(self):\n        return self.fd.fileno()\n\n\n\nA18:\n\nBased on the answer by Thomas Watnedal. \nHowever, this does not answer the line-to-line part of the original question exactly. The function can still replace on a line-to-line basis \nThis implementation replaces the file contents without using temporary files, as a consequence file permissions remain unchanged.\nAlso re.sub instead of replace, allows regex replacement instead of plain text replacement only.\nReading the file as a single string instead of line by line allows for multiline match and replacement.\nimport re\n\ndef replace(file, pattern, subst):\n    # Read contents from file as a single string\n    file_handle = open(file, 'r')\n    file_string = file_handle.read()\n    file_handle.close()\n\n    # Use RE package to allow for replacement (also allowing for (multiline) REGEX)\n    file_string = (re.sub(pattern, subst, file_string))\n\n    # Write contents to file.\n    # Using mode 'w' truncates the file.\n    file_handle = open(file, 'w')\n    file_handle.write(file_string)\n    file_handle.close()\n\n\n\nA19:\n\nThe accepted answer is not the right one, because the question is not about renaming a file into a file, but moving many files into a directory. shutil.move will do the work, but for this purpose os.rename is useless (as stated on comments) because destination must have an explicit file name.\n\n\nA20:\n\nUsing hamishmcn's answer as a template I was able to search for a line in a file that match my regex and replacing it with empty string.\nimport re \n\nfin = open(\"in.txt\", 'r') # in file\nfout = open(\"out.txt\", 'w') # out file\nfor line in fin:\n    p = re.compile('[-][0-9]*[.][0-9]*[,]|[-][0-9]*[,]') # pattern\n    newline = p.sub('',line) # replace matching strings with empty string\n    print newline\n    fout.write(newline)\nfin.close()\nfout.close()\n\n\n\nA21:\n\nlogging.basicConfig() can take a keyword argument handlers since Python 3.3, which simplifies logging setup a lot, especially when setting up multiple handlers with the same formatter:\n\nhandlers \u2013 If specified, this should be an iterable of already created handlers to add to the root logger. Any handlers which don\u2019t already have a formatter set will be assigned the default formatter created in this function.\n\nThe whole setup can therefore be done with a single call like this:\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.FileHandler(\"debug.log\"),\n        logging.StreamHandler()\n    ]\n)\n\n(Or with import sys + StreamHandler(sys.stdout) per original question's requirements \u2013 the default for StreamHandler is to write to stderr. Look at LogRecord attributes if you want to customize the log format and add things like filename/line, thread info etc.)\nThe setup above needs to be done only once near the beginning of the script. You can use the logging from all other places in the codebase later like this:\nlogging.info('Useful message')\nlogging.error('Something bad happened')\n...\n\nNote: If it doesn't work, someone else has probably already initialized the logging system differently. Comments suggest doing logging.root.handlers = [] before the call to basicConfig().\n\n\nA22:\n\nIn Python 3, print can take an optional flush argument:\nprint(\"Hello, World!\", flush=True)\n\nIn Python 2, after calling print, do:\nimport sys\nsys.stdout.flush()\n\nBy default, print prints to sys.stdout (see the documentation for more about file objects).\n\n\nA23:\n\nThis is what I'm using at the moment:\nimport os, shutil\npath = \"/volume1/Users/Transfer/\"\nmoveto = \"/volume1/Users/Drive_Transfer/\"\nfiles = os.listdir(path)\nfiles.sort()\nfor f in files:\n    src = path+f\n    dst = moveto+f\n    shutil.move(src,dst)\n\n\nYou can also turn this into a function, that accepts a source and destination directory, making the destination folder if it doesn't exist, and moves the files.  Also allows for filtering of the src files, for example if you only want to move images, then you use the pattern '*.jpg', by default, it moves everything in the directory\nimport os, shutil, pathlib, fnmatch\n\ndef move_dir(src: str, dst: str, pattern: str = '*'):\n    if not os.path.isdir(dst):\n        pathlib.Path(dst).mkdir(parents=True, exist_ok=True)\n    for f in fnmatch.filter(os.listdir(src), pattern):\n        shutil.move(os.path.join(src, f), os.path.join(dst, f))\n\n\n\nA24:\n\nAlso, as suggested in this blog post, one can reopen sys.stdout in unbuffered mode:\nsys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n\nEach stdout.write and print operation will be automatically flushed afterwards.\n\n\nA25:\n\nSince Python 3.3, you can force the normal print() function to flush without the need to use sys.stdout.flush(); just set the \"flush\" keyword argument to true.  From the documentation:\n\nprint(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False)\nPrint objects to the stream file, separated by sep and followed by end. sep, end and file, if present, must be given as keyword arguments.\nAll non-keyword arguments are converted to strings like str() does and written to the stream, separated by sep and followed by end. Both sep and end must be strings; they can also be None, which means to use the default values. If no objects are given, print() will just write end.\nThe file argument must be an object with a write(string) method; if it is not present or None, sys.stdout will be used. Whether output is buffered is usually determined by file, but if the flush keyword argument is true, the stream is forcibly flushed.\n\n\n\nA26:\n\nAlso possible with using subprocess.run() method.\npython:\n>>> import subprocess\n>>> new = \"/path/to/destination\"\n>>> old = \"/path/to/new/destination\"\n>>> process = \"mv ..{} ..{}\".format(old,new)\n>>> subprocess.run(process, shell=True) # do not remember, assign shell value to True.\n\nThis will work fine when working on Linux. Windows probably gives error since there is no mv Command.\n\n\nA27:\n\nIn Python 3 you can overwrite the print function with the default set to flush = True\ndef print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=True):\n    __builtins__.print(*objects, sep=sep, end=end, file=file, flush=flush)\n\n\n\nA28:\n\nAfter Python 3.4, you can also use pathlib's class Path to move file.\nfrom pathlib import Path\n\nPath(\"path/to/current/file.foo\").rename(\"path/to/new/destination/for/file.foo\")\n\nhttps://docs.python.org/3.4/library/pathlib.html#pathlib.Path.rename\n\n\nA29:\n\nHere's another example that was tested, and will match search & replace patterns:\nimport fileinput\nimport sys\n\ndef replaceAll(file,searchExp,replaceExp):\n    for line in fileinput.input(file, inplace=1):\n        if searchExp in line:\n            line = line.replace(searchExp,replaceExp)\n        sys.stdout.write(line)\n\nExample use:\nreplaceAll(\"/fooBar.txt\",\"Hello\\sWorld!$\",\"Goodbye\\sWorld.\")\n\n\n\nA30:\n\nDan's idea doesn't quite work:\n#!/usr/bin/env python\nclass flushfile(file):\n    def __init__(self, f):\n        self.f = f\n    def write(self, x):\n        self.f.write(x)\n        self.f.flush()\n\nimport sys\nsys.stdout = flushfile(sys.stdout)\n\nprint \"foo\"\n\nThe result:\nTraceback (most recent call last):\n  File \"./passpersist.py\", line 12, in <module>\n    print \"foo\"\nValueError: I/O operation on closed file\n\nI believe the problem is that it inherits from the file class, which actually isn't necessary. According to the documentation for sys.stdout:\n\nstdout and stderr needn\u2019t be built-in\nfile objects: any object is acceptable\nas long as it has a write() method\nthat takes a string argument.\n\nso changing\nclass flushfile(file):\n\nto\nclass flushfile(object):\n\nmakes it work just fine.\n\n\nA31:\n\nI guess something like this should do it. It basically writes the content to a new file and replaces the old file with the new file:\nfrom tempfile import mkstemp\nfrom shutil import move, copymode\nfrom os import fdopen, remove\n\ndef replace(file_path, pattern, subst):\n    #Create temp file\n    fh, abs_path = mkstemp()\n    with fdopen(fh,'w') as new_file:\n        with open(file_path) as old_file:\n            for line in old_file:\n                new_file.write(line.replace(pattern, subst))\n    #Copy the file permissions from the old file to the new file\n    copymode(file_path, abs_path)\n    #Remove original file\n    remove(file_path)\n    #Move new file\n    move(abs_path, file_path)\n\n\n\nA32:\n\nJust get a handle to the root logger and add the StreamHandler. The StreamHandler writes to stderr. Not sure if you really need stdout over stderr, but this is what I use when I setup the Python logger and I also add the FileHandler as well. Then all my logs go to both places (which is what it sounds like you want).\nimport logging\nlogging.getLogger().addHandler(logging.StreamHandler())\n\nIf you want to output to stdout instead of stderr, you just need to specify it to the StreamHandler constructor.\nimport sys\n# ...\nlogging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n\nYou could also add a Formatter to it so all your log lines have a common header.\nie:\nimport logging\nlogFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\nrootLogger = logging.getLogger()\n\nfileHandler = logging.FileHandler(\"{0}/{1}.log\".format(logPath, fileName))\nfileHandler.setFormatter(logFormatter)\nrootLogger.addHandler(fileHandler)\n\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\nPrints to the format of:\n2012-12-05 16:58:26,618 [MainThread  ] [INFO ]  my message\n\n\n\nA33:\n\nA more pythonic way would be to use context managers like the code below:\nfrom tempfile import mkstemp\nfrom shutil import move\nfrom os import remove\n\ndef replace(source_file_path, pattern, substring):\n    fh, target_file_path = mkstemp()\n    with open(target_file_path, 'w') as target_file:\n        with open(source_file_path, 'r') as source_file:\n            for line in source_file:\n                target_file.write(line.replace(pattern, substring))\n    remove(source_file_path)\n    move(target_file_path, source_file_path)\n\nYou can find the full snippet here.\n\n\nA34:\n\nos.rename(), os.replace(), or shutil.move()\nAll employ the same syntax:\nimport os\nimport shutil\n\nos.rename(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\nos.replace(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\nshutil.move(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\n\n\nThe filename (\"file.foo\") must be included in both the source and destination arguments. If it differs between the two, the file will be renamed as well as moved.\nThe directory within which the new file is being created must already exist.\nOn Windows, a file with that name must not exist or an exception will be raised, but os.replace() will silently replace a file even in that occurrence.\nshutil.move simply calls os.rename in most cases. However, if the destination is on a different disk than the source, it will instead copy and then delete the source file.\n\n\n\nA35:\n\nCreate a new file, copy lines from the old to the new, and do the replacing before you write the lines to the new file.\n\n\nA36:\n\nif you remove the indent at the like below, it will search and replace in multiple line.\nSee below for example.\ndef replace(file, pattern, subst):\n    #Create temp file\n    fh, abs_path = mkstemp()\n    print fh, abs_path\n    new_file = open(abs_path,'w')\n    old_file = open(file)\n    for line in old_file:\n        new_file.write(line.replace(pattern, subst))\n    #close temp file\n    new_file.close()\n    close(fh)\n    old_file.close()\n    #Remove original file\n    remove(file)\n    #Move new file\n    move(abs_path, file)\n\n\n\nA37:\n\nI don't know if this applies to python as well, but I think it depends on the operating system that you are running.\nOn Linux for example, output to terminal flushes the buffer on a newline, whereas for output to files it only flushes when the buffer is full (by default).  This is because it is more efficient to flush the buffer fewer times, and the user is less likely to notice if the output is not flushed on a newline in a file.  \nYou might be able to auto-flush the output if that is what you need.\nEDIT:  I think you would auto-flush in python this way (based \nfrom here)\n#0 means there is no buffer, so all output\n#will be auto-flushed\nfsock = open('out.log', 'w', 0)\nsys.stdout = fsock\n#do whatever\nfsock.close()\n\n\n\nA38:\n\nFor file operations, Python uses the operating system's default buffering unless you configure it do otherwise.  You can specify a buffer size, unbuffered, or line buffered.\nFor example, the open function takes a buffer size argument.\nhttp://docs.python.org/library/functions.html#open\n\"The optional buffering argument specifies the file\u2019s desired buffer size:\" \n\n0 means unbuffered, \n1 means line buffered, \nany other positive value means use a buffer of (approximately) that size. \nA negative buffering means to use the system default, which is usually line buffered for tty devices and fully buffered for other files. \nIf omitted, the system default is used. \n\ncode:\nbufsize = 0\nf = open('file.txt', 'w', buffering=bufsize)\n\n\n\nA39:\n\n  import os,shutil\n\n  current_path = \"\" ## source path\n\n  new_path = \"\" ## destination path\n\n  os.chdir(current_path)\n\n  for files in os.listdir():\n\n        os.rename(files, new_path+'{}'.format(f))\n        shutil.move(files, new_path+'{}'.format(f)) ## to move files from \n\ndifferent disk ex. C: --> D:\n\n\nA40:\n\nFor either the os.rename or shutil.move you will need to import the module.\nNo * character is necessary to get all the files moved.\nWe have a folder at /opt/awesome called source with one file named awesome.txt.\nin /opt/awesome\n\u25cb \u2192 ls\nsource\n\u25cb \u2192 ls source\nawesome.txt\n\npython \n>>> source = '/opt/awesome/source'\n>>> destination = '/opt/awesome/destination'\n>>> import os\n>>> os.rename(source, destination)\n>>> os.listdir('/opt/awesome')\n['destination']\n\nWe used os.listdir to see that the folder name in fact changed.\nHere's the shutil moving the destination back to source.\n>>> import shutil\n>>> source = '/opt/awesome/destination' \n>>> destination = '/opt/awesome/source'\n>>> shutil.move(source, destination)\n>>> os.listdir('/opt/awesome/source')\n['awesome.txt']\n\nThis time I checked inside the source folder to be sure the awesome.txt file I created exists. It is there\nNow we have moved a folder and its files from a source to a destination and back again.\n\n\nA41:\n\nAdding a StreamHandler without arguments goes to stderr instead of stdout. If some other process has a dependency on the stdout dump (i.e. when writing an NRPE plugin), then make sure to specify stdout explicitly or you might run into some unexpected troubles.\nHere's a quick example reusing the assumed values and LOGFILE from the question:\nimport logging\nfrom logging.handlers import RotatingFileHandler\nfrom logging import handlers\nimport sys\n\nlog = logging.getLogger('')\nlog.setLevel(logging.DEBUG)\nformat = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\nch = logging.StreamHandler(sys.stdout)\nch.setFormatter(format)\nlog.addHandler(ch)\n\nfh = handlers.RotatingFileHandler(LOGFILE, maxBytes=(1048576*5), backupCount=7)\nfh.setFormatter(format)\nlog.addHandler(fh)\n\n\n\nA42:\n\nUse an unbuffered file:\nf = open('xyz.log', 'a', 0)\n\nOr\nsys.stdout = open('out.log', 'a', 0)\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " so, (vs.  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  "}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 8, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIs there a difference between isset and !empty. If I do this double boolean check, is it correct this way or redundant? and is there a shorter way to do the same thing?\nisset($vars[1]) AND !empty($vars[1])\n\n\n\nCandidate answers:\nA1:\n\nAfter reading the other advices, I'm inclined to say that:\nAs a GENERIC rule, you will not always define setters for ALL properties, specially \"internal\" ones (semaphores, internal flags...). Read-only properties will not have setters, obviously, so some properties will only have getters; that's where __get() comes to shrink the code:\n\ndefine a __get() (magical global getters) for all those properties which are alike,\ngroup them in arrays so:\n\nthey'll share common characteristics: monetary values will/may come up properly formatted, dates in an specific layout (ISO, US, Intl.), etc.\nthe code itself can verify that only existing & allowed properties are being read using this magical method.\nwhenever you need to create a new similar property, just declare it and add its name to the proper array and it's done. That's way FASTER than defining a new getter, perhaps with some lines of code REPEATED again and again all over the class code.\n\n\nYes! we could write a private method to do that, also, but then again, we'll have MANY methods declared (++memory) that end up calling another, always the same, method. Why just not write a SINGLE method to rule them all...? [yep! pun absolutely intended! :)]\nMagic setters can also respond ONLY to specific properties, so all date type properties can be screened against invalid values in one method alone. If date type properties were listed in an array, their setters can be defined easily. Just an example, of course. there are way too many situations.\nAbout readability... Well... That's another debate: I don't like to be bound to the uses of an IDE (in fact, I don't use them, they tend to tell me (and force me) how to write... and I have my likes about coding \"beauty\"). I tend to be consistent about naming, so using ctags and a couple of other aids is sufficient to me... Anyway: once all this magic setters and  getters are done, I write the other setters that are too specific or \"special\" to be generalized in a __set() method. And that covers all I need about getting and setting properties. Of course: there's not always a common ground, or there are such a few properties that is not worth the trouble of coding a magical method, and then there's still the old good traditional setter/getter pair.\nProgramming languages are just that: human artificial languages. So, each of them has its own intonation or accent, syntax and flavor, so I won't pretend to write a Ruby or Python code using the same \"accent\" than Java or C#, nor I would write a JavaScript or PHP to resemble Perl or SQL... Use them the way they're meant to be used.\n\n\nA2:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA3:\n\nclass MyClass {\n    private $firstField;\n    private $secondField;\n    private $thirdField;\n\n    public function __get( $name ) {\n        if( method_exists( $this , $method = ( 'get' . ucfirst( $name  ) ) ) )\n            return $this->$method();\n        else\n            throw new Exception( 'Can\\'t get property ' . $name );\n    }\n\n    public function __set( $name , $value ) {\n        if( method_exists( $this , $method = ( 'set' . ucfirst( $name  ) ) ) )\n            return $this->$method( $value );\n        else\n            throw new Exception( 'Can\\'t set property ' . $name );\n    }\n\n    public function __isset( $name )\n    {\n        return method_exists( $this , 'get' . ucfirst( $name  ) ) \n            || method_exists( $this , 'set' . ucfirst( $name  ) );\n    }\n\n    public function getFirstField() {\n        return $this->firstField;\n    }\n\n    protected function setFirstField($x) {\n        $this->firstField = $x;\n    }\n\n    private function getSecondField() {\n        return $this->secondField;\n    }\n}\n\n$obj = new MyClass();\n\necho $obj->firstField; // works\n$obj->firstField = 'value'; // works\n\necho $obj->getFirstField(); // works\n$obj->setFirstField( 'value' ); // not works, method is protected\n\necho $obj->secondField; // works\necho $obj->getSecondField(); // not works, method is private\n\n$obj->secondField = 'value'; // not works, setter not exists\n\necho $obj->thirdField; // not works, property not exists\n\nisset( $obj->firstField ); // returns true\nisset( $obj->secondField ); // returns true\nisset( $obj->thirdField ); // returns false\n\nReady!\n\n\nA4:\n\nThis is completely redundant. empty is more or less shorthand for !isset($foo) || !$foo, and !empty is analogous to isset($foo) && $foo. I.e. empty does the reverse thing of isset plus an additional check for the truthiness of a value.  \nOr in other words, empty is the same as !$foo, but doesn't throw warnings if the variable doesn't exist. That's the main point of this function: do a boolean comparison without worrying about the variable being set.\nThe manual puts it like this:\n\nempty() is the opposite of (boolean) var, except that no warning is generated when the variable is not set.\n\nYou can simply use !empty($vars[1]) here.\n\n\nA5:\n\nIn addition to the already great and respected answers in here, I would like to expand on PHP having no setters/getters.\nPHP does not have getter and setter syntax. It provides subclassed or magic methods to allow \"hooking\" and overriding the property lookup process, as pointed out by Dave.\nMagic allows us lazy programmers to do more with less code at a time at which we are actively engaged in a project and know it intimately, but usually at the expense of readability.\nPerformance Every unnecessary function, that results from forcing a getter/setter-like code-architecture in PHP, involves its own memory stack-frame upon invocation and is wasting CPU cycles. \nReadability: The codebase incurs bloating code-lines, which impacts code-navigation as more LOC mean more scrolling,.\nPreference: Personally, as my rule of thumb, I take the failure of static code analysis \nas a sign to avoid going down the magical road as long as obvious long-term benefits elude me at that time.\nFallacies:\nA common argument is readability. For instance that $someobject->width is easier to read than $someobject->width(). However unlike a planet's circumference or width, which can be assumed to be  static, an object's instance such as $someobject, which requires a width function, likely takes a measurement of the object's instance width.\nTherefore readability increases mainly because of assertive naming-schemes and not by hiding the function away that outputs a given property-value.\n__get / __set uses:\n\npre-validation and pre-sanitation of property values\nstrings e.g.\n\"\nsome {mathsobj1->generatelatex} multi\nline text {mathsobj1->latexoutput}\nwith lots of variables for {mathsobj1->generatelatex}\n some reason\n\"\n\nIn this case generatelatex would adhere to a naming scheme of actionname + methodname\nspecial, obvious cases\n$dnastringobj->homeobox($one_rememberable_parameter)->gattaca->findrelated()\n$dnastringobj->homeobox($one_rememberable_parameter)->gttccaatttga->findrelated()\n\n\nNote: PHP chose not to implement getter/setter syntax. I am not claiming that getters/setter are generally bad.\n\n\nA6:\n\nisset() tests if a variable is set and not null:\nhttp://us.php.net/manual/en/function.isset.php\nempty() can return true when the variable is set to certain values:\nhttp://us.php.net/manual/en/function.empty.php\nTo demonstrate this, try the following code with $the_var unassigned, set to 0, and set to 1.\n<?php\n\n#$the_var = 0;\n\nif (isset($the_var)) {\n  echo \"set\";\n} else {\n  echo \"not set\";\n}\n\necho \"\\n\";\n\nif (empty($the_var)) {\n  echo \"empty\";\n} else {\n  echo \"not empty\";\n}\n?>\n\n\n\nA7:\n\nIf you preffer to use the __call function, you can use this method. It works with\n\nGET => $this->property()\nSET => $this->property($value)\nGET => $this->getProperty()\nSET => $this->setProperty($value)\n\nkalsdas\npublic function __call($name, $arguments) {\n\n    //Getting and setting with $this->property($optional);\n\n    if (property_exists(get_class($this), $name)) {\n\n\n        //Always set the value if a parameter is passed\n        if (count($arguments) == 1) {\n            /* set */\n            $this->$name = $arguments[0];\n        } else if (count($arguments) > 1) {\n            throw new \\Exception(\"Setter for $name only accepts one parameter.\");\n        }\n\n        //Always return the value (Even on the set)\n        return $this->$name;\n    }\n\n    //If it doesn't chech if its a normal old type setter ot getter\n    //Getting and setting with $this->getProperty($optional);\n    //Getting and setting with $this->setProperty($optional);\n    $prefix = substr($name, 0, 3);\n    $property = strtolower($name[3]) . substr($name, 4);\n    switch ($prefix) {\n        case 'get':\n            return $this->$property;\n            break;\n        case 'set':\n            //Always set the value if a parameter is passed\n            if (count($arguments) != 1) {\n                throw new \\Exception(\"Setter for $name requires exactly one parameter.\");\n            }\n            $this->$property = $arguments[0];\n            //Always return the value (Even on the set)\n            return $this->$name;\n        default:\n            throw new \\Exception(\"Property $name doesn't exist.\");\n            break;\n    }\n}\n\n\n\nA8:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA9:\n\nThere are many ways to create sourcecode in a netbeans-convention. This is nice. It makes thinks such easyer === FALSE. Just use the traditionel, specially if you are not sure which one of the properties should be encapsuled and which one not. I know, it is a boi.... pla... code, but for debugging-works and many other thinks it is the better, clear way. Dont spend to much time with thousend of arts how to make simple getters and setters. You cannot implement too some design patterns like the demeter-rule and so on, if you use magics. In specific situation you can use magic_calls or for small, fast and clear solutions. Sure you could make solutions for design-patters in this way too, but why to make you live more difficult. \n\n\nA10:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA11:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA12:\n\nWhy use getters and setters?\n\nScalability: It's easier refactor a getter than search all the var assignments in a project code.\nDebugging: You can put breakpoints at setters and getters.\nCleaner: Magic functions are not good solution for writting less, your IDE will not suggest the code. Better use templates for fast-writting getters.\n\n\n\n\nA13:\n\nWell, PHP does have magic methods __get, __set, __isset & __unset, which is always a start. Alas proper (get it?) OO properties is more than magic methods. The main problem with PHP's implementation is that magic methods are called for all inaccessible properties. Which means you have to Repeat Yourself (eg. by calling property_exists()) in the magic methods when determining if name is actually a property of your object. And you can't really solve this general problem with a base class unless all your classes inherit from ie. ClassWithProperties, since PHP lacks multiple inheritance.\nIn contrast, Python new style classes gives you property(), which lets you explicitly define all your properties. C# has special syntax. \nhttp://en.wikipedia.org/wiki/Property_(programming)\n\n\nA14:\n\n\"Empty\": only works on variables. Empty can mean different things for different variable types\n(check manual: http://php.net/manual/en/function.empty.php).\n\"isset\": checks if the variable exists and checks for a true NULL or false value.\nCan be unset by calling \"unset\". Once again, check the manual.\nUse of either one depends of the variable type you are using. \nI would say, it's safer to check for both, because you are checking first of all if the variable exists, and if it isn't really NULL or empty.\n\n\nA15:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA16:\n\nEncapsulation is important in any OO language, popularity has nothing to do with it. In dynamically typed languages, like PHP, it is especially useful because there is little ways to ensure a property is of a specific type without using setters.\nIn PHP, this works:\nclass Foo {\n   public $bar; // should be an integer\n}\n$foo = new Foo;\n$foo->bar = \"string\";\n\nIn Java, it doesn't:\nclass Foo {\n   public int bar;\n}\nFoo myFoo = new Foo();\nmyFoo.bar = \"string\"; // error\n\nUsing magic methods (__get and __set) also works, but only when accessing a property that has lower visibility than the current scope can access. It can easily give you headaches when trying to debug, if it is not used properly.\n\n\nA17:\n\nGenerally speaking, the first way is more popular overall because those with prior programming knowledge can easily transition to PHP and get work done in an object-oriented fashion. The first way is more universal. My advice would be to stick with what is tried and true across many languages. Then, when and if you use another language, you'll be ready to get something accomplished (instead of spending time reinventing the wheel).\n\n\nA18:\n\nValidating + Formatting/Deriving Values\nSetters let you to validate data and getters let you format or derive data. Objects allow you to encapsulate data and its validation and formatting code into a neat package that encourages DRY.\nFor example, consider the following simple class that contains a birth date.\nclass BirthDate {\n\n    private $birth_date;\n\n    public function getBirthDate($format='Y-m-d') {\n        //format $birth_date ...\n        //$birth_date = ...\n        return $birth_date;\n    }\n\n    public function setBirthDate($birth_date) {                   \n        //if($birth_date is not valid) throw an exception ...          \n        $this->birth_date = $birth_date;\n    }\n\n    public function getAge() {\n        //calculate age ...\n        return $age;\n    }\n\n    public function getDaysUntilBirthday() {\n        //calculate days until birth days\n        return $days;\n    }\n}\n\nYou'll want to validate that the value being set is\n\nA valid date\nNot in the future\n\nAnd you don't want to do this validation all over your application (or over multiple applications for that matter). Instead, it's easier to make the member variable protected or private (in order to make the setter the only access point) and to validate in the setter because then you'll know that the object contains a valid birth date no matter which part of the application the object came from and if you want to add more validation then you can add it in a single place.\nYou might want to add multiple formatters that operate on the same member variable i.e. getAge() and getDaysUntilBirthday() and you might want to enforce a configurable format in getBirthDate() depending on locale. Therefore I prefer consistently accessing values via getters as opposed to mixing $date->getAge() with $date->birth_date.\ngetters and setters are also useful when you extend objects. For example, suppose your application needed to allow 150+ year birth dates in some places but not in others. One way to solve the problem without repeating any code would be to extend the BirthDate object and put the additional validation in the setter.\nclass LivingBirthDate extends BirthDate {\n\n    public function setBirthDate($birth_date) {\n        //if $birth_date is greater than 150 years throw an exception\n        //else pass to parent's setter\n        return parent::setBirthDate($birth_date);\n    }\n}\n\n\n\nA19:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA20:\n\nI made an experiment using the magic method __call.\nNot sure if I should post it (because of all the \"DO NOT USE MAGIC METHODS\" warnings in the other answers and comments) but i'll leave it here.. just in case someone find it useful.\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = substr($_name, 4);\n\n    if (isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nJust add that method above in your class, now you can type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_foo(); // return \"bar\"\n$C->get_bom(); // return \"bim\"\n\n// as setter\n$C->set_foo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_bom(\"zam\"); // set \"zam\" as new value of bom\n\n\n\nThis way you can get/set everything in your class if it exist so, if you need it for only a few specific elements, you could use a \"whitelist\" as filter.  \nExample:\nprivate $callWhiteList = array(\n    \"foo\" => \"foo\",\n    \"fee\" => \"fee\",\n    // ...\n);\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = $this->callWhiteList[substr($_name, 4)];\n\n    if (!is_null($varName) && isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nNow you can only get/set \"foo\" and \"fee\".\nYou can also use that \"whitelist\" to assign custom names to access to your vars.\nFor example,\nprivate $callWhiteList = array(\n    \"myfoo\" => \"foo\",\n    \"zim\" => \"bom\",\n    // ...\n);\n\nWith that list you can now type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // private $callWhiteList = array( ... )\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_myfoo(); // return \"bar\"\n$C->get_zim(); // return \"bim\"\n\n// as setter\n$C->set_myfoo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_zim(\"zam\"); // set \"zam\" as new value of bom\n\n\n.\n.\n.\nThat's all.\n\nDoc:\n__call() is triggered when invoking inaccessible methods in an object context. \n\n\nA21:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA22:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA23:\n\nEmpty just check is the refered variable/array has an value if you check the php doc(empty) you'll see this things are considered emtpy \n\n* \"\" (an empty string)\n* 0 (0 as an integer)\n* \"0\" (0 as a string)\n* NULL\n* FALSE\n* array() (an empty array)\n* var $var; (a variable declared, but without a value in a class)\n\n\nwhile isset check if the variable isset and not null which can also be found in the php doc(isset) \n\n\nA24:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\nA25:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\nA26:\n\nYou can use php magic methods __get and __set.\n<?php\nclass MyClass {\n  private $firstField;\n  private $secondField;\n\n  public function __get($property) {\n    if (property_exists($this, $property)) {\n      return $this->$property;\n    }\n  }\n\n  public function __set($property, $value) {\n    if (property_exists($this, $property)) {\n      $this->$property = $value;\n    }\n\n    return $this;\n  }\n}\n?>\n\n\n\nA27:\n\nGoogle already published a guide on optimization of PHP and the conclusion was:\nNo getter and setter Optimizing PHP\nAnd no, you must not use magic methods.  For PHP, Magic Methods are evil. Why?\n\nThey are hard to debug.\nThere is a negative performance impact.\nThey require writing more code.\n\nPHP is not Java, C++, or C#. PHP is different and plays with different rules. \n\n\nA28:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA29:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 9, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nConsider the table creation script below:\ncreate_table :foo do |t|\n  t.datetime :starts_at, :null => false\nend\n\nIs it's possible to set the default value as the current time?\nI am trying to find a DB independent equivalent in rails for the SQL column definitions given below:\nOracle Syntax\nstart_at DATE DEFAULT SYSDATE() \n\nMySQL Syntax\nstart_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n\nOR\nstart_at DATETIME DEFAULT NOW()\n\n\n\nCandidate answers:\nA1:\n\nI was searching for a similar solutions but I ended using https://github.com/FooBarWidget/default_value_for.\nThe default_value_for plugin allows one to define default values for ActiveRecord models in a declarative manner. For example:\nclass User < ActiveRecord::Base\n  default_value_for :name, \"(no name)\"\n  default_value_for :last_seen do\n    Time.now\n  end\nend\n\nu = User.new\nu.name       # => \"(no name)\"\nu.last_seen  # => Mon Sep 22 17:28:38 +0200 2008\n\n\n\nA2:\n\nI favoured the Learning route. It seems the problem stems from \nIO.popen(command, options) { |f| output = f.read }\n\nreturning an empty string in execjs\\external_runtine.rb (line 173 in version 1.4.0). This is why the error message contains no text. The changes suggested did not work for me. I changed UTF-16LE to UTF-8, but it still returned an empty string. I removed \\\\U from the command - this at least returned text, but it was in the wrong encoding - in the browser it displayed as Chinese characters.\nAccording to this MSDN blog post, using the //U flag and redirecting to a file causes cscript to return the result using UTF-16.\nAnd then, magically, it worked (@#%$&^@$%!!!?!?!) using command as \"cscript //E:jscript //Nologo\" and encoding as \"UTF-8\". Oh well.\n\n\nA3:\n\nThough people have given long answers I suggest very simple and concise answer\nJust go to \n\nconfig/initializers/assets.rb\n  and\n\nAdd the following line\n\nRails.application.config.assets.precompile += %w( style.css )\n\nwhere style.css can be replaced by your any file name for css\n\n\nA4:\n\nWe didn't have a myapp/bin directory in our rails 4 app, so we created one and then copied in the my app/script/rails file, plus the bundle and rake files from under rvm/ruby/bin and then added these to the repo for git and pushed it up to heroku and all was well.\n\n\nA5:\n\nYou can define them in app/helpers/ as you tried but you need to include them trough the active admin's initializer like this:\n# in config/initializers/active_admin.rb\nActiveAdmin.setup do |config|\n    ....\nend\n\nmodule ActiveAdmin::ViewHelpers\n  include ApplicationHelper\nend\n\n\n\nA6:\n\nActiveRecord::Base.connection will provide you with everything you ever wanted to know about the database connection established by boot.rb and environment.rb\nActiveRecord::Base.connection returns a lot of information. So you've got to know exactly what you're looking for.\nAs Marcel points out:\nActiveRecord::Base.connection.instance_of? \n  ActiveRecord::ConnectionAdapters::MysqlAdapter \n\nis probably the best method of determining if your database MySQL.\nDespite relying on internal information that could change between ActiveRecord release, I prefer doing it this way:\nActiveRecord::Base.connection.instance_values[\"config\"][:adapter] == \"mysql\"\n\n\n\nA7:\n\nUsing activeadmin 1.0.0.pre1 from git://github.com/activeadmin/activeadmin.git\nRails 4.2.1\nThis worked for me...\nmy_app/app/helpers/active_admin/resources_helper.rb\nmodule ActiveAdmin\n  module ResourcesHelper\n    def resource_form_for(_resource, _params, _options = {}, &_block)\n      url = if _resource.new_record?\n              UrlBuilder.resources_path(_resource.class, _params)\n            else\n              UrlBuilder.resource_path(_resource.class, _params)\n            end\n\n      method = _resource.new_record? ? :post : :put\n\n      options = { url: url, method: method, builder: ActiveAdmin::FormBuilder }\n      options.merge!(_options)\n\n      semantic_form_for([:admin, _resource], options) do |f|\n        _block.call(f)\n      end\n    end\n  end\nend\n\nmy_app/app/admin/balance_sheets.rb\nActiveAdmin.register BalanceSheet do\n  form partial: 'form'\nend\n\nmy_app/app/views/admin/balance_sheets/_form.html.erb\n<%= resource_form_for(resource, params) do |f| %>\n  <%= f.inputs \"Fields\" do %>\n    <%= f.input :progress_status %>\n    <%= f.input :crew %>\n    <%= f.input :shift %>\n    <%= f.input :expected_progress %>\n    <%= f.input :real_progress %>\n    <%= f.input :analyst, collection: User.analysts %>\n    <%= f.input :activity_ids, as: :check_boxes, collection: Activity.balance_sheet_activities %>\n    <%= f.input :worker_ids, as: :check_boxes, collection: Worker.all %>\n  <% end %>\n  <%= f.actions %>\n<% end %>\n\n\n\nA8:\n\nDid you change the location of your code from C:\\Users\\this-user\\yo-app?\nWhen I was young in rails I have created an app and the default location of my app was C:\\Users\\Duncan\\my-app and then, when I changed my-app and placed it in D:\\All-my-Apps-folder i had that error....\nI scratched my head, tried 1,2,3 and more .....nothing! Until I returned all code to default folder location and to my amazement, I was rolling again :)\nIn case someone may find this useful (I can't explain why that happened, maybe someone may without speculation)\n\n\nA9:\n\nWith Ubuntu 16 (from a fresh Docker install):\napt install -y libxrender1 libxext6\n\n\n\nA10:\n\nMy friend was attempting a Rails tutorial on Win 8 RTM a few months ago and ran into this error.  Not sure if this issue exists in Windows 7 as well, but this may help.\nOptions:\n1) Removing  //= require_tree . / Ignoring the issue - As ColinR stated above, this line should not be causing an issue in the first place.  There is an actual problem with ExecJS working properly with the JavaScript runtime on your system and removing this line is just ignoring that fact.\n2) Installing Node.js / Running away -  Many people seem to just end up installing Node.js and using that instead of the JavaScript runtime already on their system.  While that is a valid option, it also requires additional software and only avoids the original issue, which is that ExecJS is not working properly with the JavaScript runtime already on your system.  If the existing JavaScript runtime on your system is supposed to work, why not make it work instead of installing more software?  According to the ExecJS creator, the runtime already built into Windows is in fact supported...\n\nExecJS lets you run JavaScript code from Ruby. It automatically picks the best runtime available to evaluate your JavaScript program, then returns the result to you as a Ruby object.\nExecJS supports these runtimes:\n\ntherubyracer - Google V8 embedded within Ruby\ntherubyrhino - Mozilla Rhino embedded within JRuby\nNode.js\nApple JavaScriptCore - Included with Mac OS X\nMicrosoft Windows Script Host (JScript)\n\n(from github.com/sstephenson/execjs#execjs )\n\n3) Actually fixing the issue / Learning - Use the knowledge of options 1 and 2 to search for other solutions.  I can't tell you how many webpages I closed upon seeing options 1 or 2 was the accepted solution before actually finding information about the root issue we were having.  The only reason we kept looking was that we couldn't believe the Rails team would (1) insert a line of code in every scaffold generated project that caused an issue, or (2) require that we install additional software just to run that default line of code.  And so we eventually arrived at a fix for our root issue (your miles may vary).\nThe Fix that worked for us:\nOn the system having issues, find ExecJS's runtimes.rb file.  It looks like this. Make a copy of the found file for backup. Open the original runtimes.rb for editing.  Find the section that starts with the line JScript = ExternalRuntime.new(. In that section, on the line containing :command     => \"cscript //E:jscript //Nologo //U\", - remove the //U only.  Then on the line containing :encoding    => 'UTF-16LE' # CScript with //U returns UTF-16LE - change UTF-16LE to UTF-8 .  Save the changes to the file.  This section of the file should now read:\nJScript = ExternalRuntime.new(\n    :name        => \"JScript\",\n    :command     => \"cscript //E:jscript //Nologo\",\n    :runner_path => ExecJS.root + \"/support/jscript_runner.js\",\n    :encoding    => 'UTF-8' # CScript with //U returns UTF-16LE\n)\n\nNext, stop then restart your Rails server and refresh the page in your browser that produced the original error. Hopefully the page loads without error now.  Here's the ExecJS issue thread where we originally posted our results: https://github.com/sstephenson/execjs/issues/81#issuecomment-9892952\nIf this did not fix the issue, you can always overwrite the modified runtimes.rb with the backup copy you (hopefully) made and everything will be back to square one.  In that case, consider option 3 and keep searching.  Let us know what eventually works for you.. unless it's removing the require_tree or installing node.js, there's plenty of that going around already. :)\n\n\nA11:\n\nOn rails 5.2.6, rake app:update:bin worked for me. Now deployment to heroku is working.\n\n\nA12:\n\nOne thing you can do to make it more readable is: \n((Time.zone.now - myActiveRecord.visit_date) / 1.day).to_i\n\nEdit:\nActually you can get rid of one set of the brackets with:\n(Time.zone.now - myActiveRecord.visit_date).to_i / 1.day\n\n\n\nA13:\n\nQuick and dirty solution: remove //= require_tree . from application.js.\nAs I explain in the comments for the question, this doesn't actually solve the underlying issue that is causing the error, but merely sidesteps it.\n\n\nA14:\n\nI got a similar error.\nI did not modify assets.rb or anything, just restart my server and no error anymore.\n\nActionView::Template::Error (Asset was not declared to be precompiled in production.\nAdd Rails.application.config.assets.precompile += %w( rails.png ) to config/initializers/assets.rb and restart your server):\n    10:   <%= link_to \"Sign up now!\", '#', class: \"btn btn-lg btn-primary\" %>\n    11: \n    12: \n    13: <%= link_to image_tag(\"rails.png\", alt: \"Rails logo\"),\n    14:             'http://rubyonrails.org/' %>\n  app/views/static_pages/home.html.erb:13:in `_app_views_static_pages_home_html_erb___1806898863626708249_70312070486240'\n\n\nA15:\n\napp/helpers/active_admin/view_helpers.rb\n\ndidn't help me\nEDITED: i changed it to views_helper.rb & ViewsHelper accordingly and it worked\n*but if you want to define it only for certain resource, you can do it in my way\n\ni had to define\n#app/helpers/active_admin/categories_helper.rb\n\nmodule ActiveAdmin::CategoriesHelper\n\n  def helper_method\n\n  end\n\nend\n\nfor my active_admin resource app/admin/categories.rb\n\n\nA16:\n\nAnother way to do this is to make the specific ActiveAdmin controller generated behind-the-scenes include the helper.  This method will allow making the inclusion of the helpers explicit per file rather than global.\nActiveAdmin.register MyModel do\n  controller do\n    include MyHelper\n  end\nend\n\n\n\nA17:\n\nRan into this today on a RHEL7 machine running the static version of wkhtmltopdf.  Solution was to install a few dependencies: \nyum install libXrender libXext fontconfig \n\n\n\nA18:\n\nLong Answer + Explanation\nI think the correct fix is to add the file to the precompiled assets, as recommended by the error message. Maybe that isn't fixing the issue for you because you've got an erb file that needs to be rendered at run time. I imagine if the file was a static json file then you would not still experience the issue after adding it to the precompiled assets.\nWhen you use the image_path helper, Sprockets is assuming that you've got a static asset. The fact that your app didn't raise errors before sprockets-rails 3.0 is somewhat surprising. This new version is doing a better job, apparently, at enforcing the standards. (it also looks like there are other problems with 3.0 that might be updated shortly)\nIf you need to have erb inside the manifest, then it would be best practice to use a route path helper rather than image_path or asset_path to get the url. This would require you to add a manifest route to your config/routes.rb file and render the json file through a controller action. The view file would be your .erb manifest.\n\nShort Answer\nThis started happening to me after doing a bundler update that changed my sprockets-rails version from 2.3.3 to 3.0.0. A simple fix is to revert sprockets-rails back to version 2.3.3 in your Gemfile and running bundle install again:\ngem 'sprockets-rails', '2.3.3'\nAs an aside: I was experiencing this issue in development environment and was able to fix it there by running rake assets:precompile. Unfortunately, it didn't get my tests passing.\n\n\nA19:\n\nI can confirm running rake rails:update:bin works, as said by @Ryan Taylor.\nI think I had this problem because I originally created this Rails app on Windows. Running the command above on Linux solved for me.\nAlso, on changing from Windows to Linux for development, it is a good idea to delete Gemfile.lock file and run bundle install to generate it again without Windows specific gems listed there.\n\n\nA20:\n\nI know this is a very late answer for this issue, but I got on something similar and went down the full path to understand what was really causing the issue.\nTurned out that the default windows jscript engine is still on es3, and many gems are taking advantage of es5 or es6 features.\nUnfortunately if this happen (you are using a gem or a piece of code that leverage es5 or es6 features), there is no way to let it work on windows with the native js engine.\nThis is the reason why installing node.js solves the problem (node is at least es5).\nHope this can help some folks struggling with a runtime error of jsexec.\nMy 2 cents advise is to install node(very easy) or install v8, and not removing the //=require_tree.\nNote execjs will automatically use node if detected. Otherwise force its use, adding in boot something like: \nENV['EXECJS_RUNTIME'] = 'Node'\n\nTo set the env to node.\n\n\nA21:\n\nRunning Win 8 64 bit\nrails 4.2.5\nruby 2.1.7\nThis one worked for me\n\n\nA22:\n\nIf your system is 64 bits, and your software requires 32 bits libs, you need to add the flag \"i386\", \nfor example on ubuntu:\nsudo apt-get install libxrender1:i386\n\n\n\nA23:\n\nThere is an adapter_name in AbstractAdapter and that is there since Rails2.\nSo it's easier to use in the migration like this:\nadapter_type = connection.adapter_name.downcase.to_sym\ncase adapter_type\nwhen :mysql, :mysql2\n  # do the MySQL part\nwhen :sqlite\n  # do the SQLite3 part\nwhen :postgresql\n  # etc.\nelse\n  raise NotImplementedError, \"Unknown adapter type '#{adapter_type}'\"\nend\n\n\n\nA24:\n\nRails 5 has a left_outer_joins method. So you can do\nUser.left_outer_joins(:user_points)\n\nor use the alias\nUser.left_joins(:user_points)\n\n\n\nA25:\n\nIn Rails 3, (maybe earlier, but I'm using Rails 3 currently) using ActiveRecord::ConnectionAdapters::MysqlAdapter is a poor way to go about it, as it's only initialized if the database adapter in use is MySQL.  Even if you have the MySQL gem installed, if it's not your connection type, that call wil fail:\nLoading development environment (Rails 3.0.3)\n>> ActiveRecord::Base.connection.instance_of? ActiveRecord::ConnectionAdapters::MysqlAdapter\nNameError: uninitialized constant ActiveRecord::ConnectionAdapters::MysqlAdapter\nfrom (irb):1\n\nSo, I'd recommend stasl's answer and use the adapter_name property of the connection.\n\n\nA26:\n\nI had this problem and was scowering the internet I am running Windows 8 with this rails gem file\nsource 'https://rubygems.org'\n\ngem 'rails', '3.2.9'\n\n# Bundle edge Rails instead:\n# gem 'rails', :git => 'git://github.com/rails/rails.git'\n\ngroup :development do gem 'sqlite3', '1.3.5' \nend\n\n# Gems used only for assets and not required # in production environments by default. \n\ngroup :assets do \n    gem 'sass-rails', '3.2.5' \n    gem 'coffee-rails', '3.2.2'\n\ngem 'uglifier', '1.2.3' \nend\n\ngem 'jquery-rails', '2.0.2'\n\ngroup :production do \n    gem 'pg', '0.12.2' \nend\n\nWent to http://nodejs.org/download/  installed - restarted the machine and everything worked.\n\n\nA27:\n\nThis is supported now in Rails 5. \nHere is a sample migration:\nclass CreatePosts < ActiveRecord::Migration[5.0]\n  def change\n    create_table :posts do |t|\n      t.datetime :modified_at, default: -> { 'CURRENT_TIMESTAMP' }\n      t.timestamps\n    end\n  end \nend\n\nSee discussion at https://github.com/rails/rails/issues/27077 and answer there by prathamesh-sonpatki \n\n\nA28:\n\nI know this question is a bit dated but I came across it while Googling for a similar problem. In my case I needed to know the difference in whole days on a macro and micro scale.\nFor example, I needed my code to be able to tell me that Dec 31, 2010 is 366 days before Jan 1, 2012 and that Dec 31, 2010 23:59 is 1 day away from Jan 1, 2011 00:00. The method above works in the former but in the case of the latter, it says they are 0 days apart.\nWhat I ended up doing was using Ruby's Date class to do the math for me. Using the code above my method looks like this:\n(Time.zone.now.to_date - myActiveRecord.visit_date.to_date).to_i\nThis will work with inputs in Time or DateTime due to the conversion. Another possible solution would be to first call beginning_of_day on each of the Times or DateTimes but in my case, the minutes were important.\n\n\nA29:\n\nJust for future reference, adding :all gives a deprecated message. In later versions of rails you can simply chain the methods like this:\nUser.joins(\"LEFT JOIN `user_points` ON user_points.user_id = users.id\").select(\"users.*, count(user_points.id)\").group(\"user_points.user_id\")\n\nOR use a scope like this:\nscope :my_scope_name_here, -> { \n        joins(\"LEFT JOIN `user_points` ON user_points.user_id = users.id\")\n        .select(\"users.*, count(user_points.id)\")\n        .group(\"user_points.user_id\")\n}\n\nYou can also chain .where between the .join and the .select.\nHope this helps someone in the future.\n\n\nA30:\n\nWhat worked for me with Rails 3.2.11 and and gem activeadmin (0.5.1) was not adding the app/active_admin/view_helpers.rb file, or declaring any modules in config/initializers/active_admin.rb\nI put my helpers logically, by model, into the app/*_helpers.rb files. Then inside the app/admin/model.rb file I used:\n# app/admin/[resource].rb\nActiveAdmin.register [Resource] do\n  ...\n  filter :gender, as: :select, collection: proc{genders}\n  ...\nend\n\nTo use the helper in filters, to display a drop down list of genders to filter on, in the list view. For the corresponding create form fields, I used:\n# app/admin/[resource].rb\nActiveAdmin.register [Resource] do\n  form do |f|\n    f.inputs \"Case Manager\" do\n      ...\n      f.input :gender, as: :radio, collection: genders\n      ...\n      f.buttons\n    end\n  end\nend\n\nTo display radio buttons for the input form.\nNot sure why the proc{} is required outside of the form do |f| block, but if anyone can explain why it's a bad idea, I'll find a different way.\n\n\nA31:\n\nI usually do:\ndef change\n  execute(\"\n    ALTER TABLE your_table\n    ALTER COLUMN your_column\n    SET DEFAULT CURRENT_TIMESTAMP\n  \")\nend\n\nSo, your schema.rb is going to have something like:\ncreate_table \"your_table\", force: :cascade do |t|\n  t.datetime \"your_column\", default: \"now()\"\nend\n\n\n\nA32:\n\nYou need to put your helper functions in app/helpers/active_admin/views_helper.rb file \nExample: \nmodule ActiveAdmin::ViewsHelper #camelized file name\n  def my_helper \n       # do something \n  end \nend \n\n\n\nA33:\n\nYou can also use ActiveAdmin partials :\nrender partial: 'admin/my_partial', locals: { var: my_var }\nAnd inside app/views/admin/_my_partial.html.arb your active_admin ruby code.\n\n\nA34:\n\nIn the answer given by @szymon-lipi\u0144ski (Szymon Lipi\u0144ski), the execute method didn't work for me. It was throwing a MySQL syntax error.\nThe MySQL syntax which worked for me is this.\nexecute \"ALTER TABLE mytable CHANGE `column_name` `column_name` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\"\n\nSo to set the default value for a datetime column in migration script can be done as follows:\ndef up\n  create_table :foo do |t|\n    t.datetime :starts_at, :null => false\n  end\n\n  execute \"ALTER TABLE `foo` CHANGE `starts_at` `starts_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\"\nend\n\n\n\nA35:\n\nI had a similar error and had to edit my manifest.js file in order to get it to work.\nEdit /assets/config.manifest.js and then\n// manifest.js\n//= link_tree ../images\n//= link_tree ../stylesheets .css\n\nThen do a bundle exec rake assets:precompile\n\n\nA36:\n\nFor windows users, this may work. There is a problem with coffee-script-source >1.9.0 running on windows.\nIt seems you have to add this to your gemfile:\ngem 'coffee-script-source', '1.8.0'\nthen do\nbundle update coffee-script-source\nI tried all the above options, and also mixed up a few combinations of them, till I found this Rails-4, ExecJS::ProgramError in Pages#welcome and had done multiple system gem updates and bundle installs and updates.\nI reverted all my trials and downgraded my coffee-script-source and it works. Posting here to help out anyone else, who may have a similar issue.\nUpdating files in vendor/cache\ncoffee-script-source-1.8.0.gem Removing outdated .gem files from vendor/cache\ncoffee-script-source-1.9.1.1.gem Bundle updated!\n\n\nA37:\n\nI had this problem also since I upgraded to rails 4.0.0\nRun this command\nrake rails:update:bin\n\nYou can go here for more info\nhttps://devcenter.heroku.com/articles/rails4\n\n\nA38:\n\nI had to add my nodejs folder to my Windows Path environment variable.  In Windows 8 open the Control Panel, go to System, Advanced system settings (on the left), click Environment Variables on the left, and edit the Path variable to include the directory to your nodejs folder (probably in Program Files).\nOf course you have to have Node.js installed (use the Windows installer) and have installed CoffeeScript through NPM.\n\n\nA39:\n\nWhat I have found using ActiveAdmin 0.6.1 is that ActiveAdmin will look for helpers in app/helpers/active_admin/*_helper.rb, but the name doesn't really matter.\nWhat does matter is:\n\nthe filename must end in \"_helper.rb\"\nthe module name must be the camel-case of the file name\nthe file must be in app/helpers/active_admin/ directory. \n\nIf anyone knows where this is officially documented, that would be awesome.\nHere is an example: https://gist.github.com/afred/7035a657e8ec5ec08d3b\n\n\nA40:\n\nI had the very same problem that you did. The issue lied in the fact that the bin folder was never pushed to the heroku repository. \nI looked, I looked, and then I looked again, there was no rule in the .gitignore file for the bin/ folder...\nThen, after a lot of pain and anguish, I realized that a couple of months before I had created a global .gitignore that would ignore all bin folders of all my repositories (why lord, why??).\nI deleted the global .gitignore, and everything worked fine.\n\n\nA41:\n\nYou can try this\nUser.find(:all, limit: 10,\n            joins:  \"LEFT JOIN `user_points` ON user_points.user_id = users.id\" ,\n            select: \"users.*, count(user_points.id)\", \n            group:  \"user_points.user_id\")\n\n\n\nA42:\n\nI had the same, issue on Debian Squeeze, amd64. Installing libxrender1 solved the issue for me.\nsudo apt-get install libxrender1\n\n\n\nA43:\n\nThe Unix packages x11-libs/libXext and x11-libs/libXrender are dependencies\nGentoo:\nsudo emerge libXext libXrender\n\nPDFkit wiki also explains in more detail how to get wkhtmltopdf working on engine yard.\nPDFkit wiki\nDebian or Ubuntu:\nsudo apt-get install libxrender1\n\n\n\nA44:\n\nI need also do after:\nsudo apt-get install libfontconfig1\n\n\n\nA45:\n\nHad the same issue\nOS- Windows 8\nError- 'ExecJS::RuntimeError...'\nSolution- missing Node.js\n\ninstall Node.js from http://www.nodejs.org/download/\nRestart the computer\n\n\n\nA46:\n\nI used the solution number 2 because previously i had have this mistake, but in this ocation didn't work, then I added the \ngem 'coffee-script-source', '1.8.0'\nand run \nbundle install\n\nand my problem was fixed\n\n\nA47:\n\n\nActive Record automatically timestamps create and update operations if the table has fields named created_at/created_on or updated_at/updated_on. Source - api.rubyonrails.org\n\nYou don't need to do anything else except to have that column.\n\n\nA48:\n\nSteps : \n\n\nbundle config --delete bin         # Turn off Bundler's stub generator\nrake rails:update:bin               # Use the new Rails 4 executables\ngit add bin or git add bin -f      # Add bin/ to source control\ngit commit -a -m \"you commit message\"\ngit push heroku master\nheroku open\n\n\n\n\nA49:\n\nI can make it work in ActiveAdmin 0.6.1 (finally!). The solution is to create a helper module as following:\n# app/helpers/active_admin_helpers.rb\nmodule ActiveAdminHelpers\n  # make this method public (compulsory)\n  def self.included(dsl)\n    # nothing ...\n  end\n\n  # define helper methods here ...\n  def helper_method\n    ...\n  end\nend\n\nthen include this module this way:\n# app/admin/[resource].rb\ninclude ActiveAdminHelpers\n\nActiveAdmin.register [Resource] do\n  ...\n\nend\n\nActually, it's not a nice solution but it's DRY and working good. I have already read and tried a lot of methods and solutions such as ViewHelpers module (put under 'app/helpers' or 'app/admin/active_admin'), ActiveAdmin::DSL monkey patching, ... but those never worked in version 0.6.1 (I don't have any ideas about other versions) :(\n\n\nA50:\n\nI was having the exact same problem! The solution, to me, was to install the 64bit version of wkhtmltoimage.\n\n\nA51:\n\nDefining ActiveAdmin::ViewHelpers in app/admin/active_admin/view_helpers.rb works for me with activeadmin 0.3.4 and 0.5.0.\n\n\nA52:\n\nHere's a less complicated solution, for beginners:\nIf you are just working through the tutorial, you are probably working with the default Gemfile (or very nearly). You can open it up in your text editor, and remove the pound sign from the front of this line:\n# gem 'therubyracer', :platforms => :ruby\n\nYou will need to re-run bundle install, which will likely download a few things. But once it does, you should be able to start the server without any problem.\nAt least, that worked for me.\nThis also works on Ubuntu 12.04, by the way.\n\n\nA53:\n\nAfter struggling with this for a bit, I noticed that my Rails 4 project had a /bin directory, unlike some older Rails 3 projects I had cloned.  /bin contains 3 files, bundle, rails, and rake, but these weren't making it to Heroku because I had bin in my global .gitignore file.  \nThis is a pretty common ignore rule if you work with Git and other languages (Java, etc.), so to fix this:\n\nRemove bin from ~/.gitignore\nRun bundle install\nCommit your\nchanges with git add . and git commit -m \"Add bin back\"\nPush your changes to Heroku with git push heroku master\n\n\n\nA54:\n\nEven more shorter call\nActiveRecord::Base.connection.adapter_name == 'MySQL'\n\n\n\nA55:\n\nI had this issue because the permissions on my ~/bin directory were 644 instead of 755.  Running rake rails:update:bin locally (on Mac/*nix) and then pushing the changes fixed the problem.\n\n\nA56:\n\nFor beginners like me:\n\nNavigate to \\app\\views\\layouts\\application.html.erb\nChange line 6 from:\n'<%= javascript_include_tag 'application', 'data-turbolinks-track' => true %>'\n\nto\n<%= javascript_include_tag 'defaults', 'data-turbolinks-track' => true %>\n\nSource from tutorial to fix here\n\n\nA57:\n\nRails actually has a method built in for just this sort of thing.\ncheckout #time_ago_in_words\nSo, for the original case...\n((Time.zone.now - myActiveRecord.visit_date)/86400).to_i\nvs\ntime_ago_in_words(myActiveRecord.visit_date)\n\n\nA58:\n\nYou can add a function in a model like this:\n  before_create :set_foo_to_now\n  def set_foo_to_now\n    self.foo = Time.now\n  end\n\nSo that the model will set the current time in the model.\nYou can also place some sql code in the migration for setting the default value at the database level, something like:\nexecute 'alter table foo alter column starts_at set default now()'\n\nSetting something like this:\ncreate_table :foo do |t|\n  t.datetime :starts_at, :null => false, :default => Time.now\nend\n\ncauses executing the Time.now function during migrating so then the table in database is created like this:\ncreate table foo ( starts_at timestamp not null default '2009-01-01 00:00:00');\n\nbut I think that it is not what you want.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " so, (vs. \"Yes, you are right, the problem is that the code is not clear.\n  so, let us make it a single"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 10, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nHow can I match a space character in a PHP regular expression?\nI mean like \"gavin schulz\", the space in between the two words. I am using a regular expression to make sure that I only allow letters, number and a space. But I'm not sure how to find the space. This is what I have right now:\n$newtag = preg_replace(\"/[^a-zA-Z0-9s|]/\", \"\", $tag);\n\n\n\nCandidate answers:\nA1:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA2:\n\nAfter reading the other advices, I'm inclined to say that:\nAs a GENERIC rule, you will not always define setters for ALL properties, specially \"internal\" ones (semaphores, internal flags...). Read-only properties will not have setters, obviously, so some properties will only have getters; that's where __get() comes to shrink the code:\n\ndefine a __get() (magical global getters) for all those properties which are alike,\ngroup them in arrays so:\n\nthey'll share common characteristics: monetary values will/may come up properly formatted, dates in an specific layout (ISO, US, Intl.), etc.\nthe code itself can verify that only existing & allowed properties are being read using this magical method.\nwhenever you need to create a new similar property, just declare it and add its name to the proper array and it's done. That's way FASTER than defining a new getter, perhaps with some lines of code REPEATED again and again all over the class code.\n\n\nYes! we could write a private method to do that, also, but then again, we'll have MANY methods declared (++memory) that end up calling another, always the same, method. Why just not write a SINGLE method to rule them all...? [yep! pun absolutely intended! :)]\nMagic setters can also respond ONLY to specific properties, so all date type properties can be screened against invalid values in one method alone. If date type properties were listed in an array, their setters can be defined easily. Just an example, of course. there are way too many situations.\nAbout readability... Well... That's another debate: I don't like to be bound to the uses of an IDE (in fact, I don't use them, they tend to tell me (and force me) how to write... and I have my likes about coding \"beauty\"). I tend to be consistent about naming, so using ctags and a couple of other aids is sufficient to me... Anyway: once all this magic setters and  getters are done, I write the other setters that are too specific or \"special\" to be generalized in a __set() method. And that covers all I need about getting and setting properties. Of course: there's not always a common ground, or there are such a few properties that is not worth the trouble of coding a magical method, and then there's still the old good traditional setter/getter pair.\nProgramming languages are just that: human artificial languages. So, each of them has its own intonation or accent, syntax and flavor, so I won't pretend to write a Ruby or Python code using the same \"accent\" than Java or C#, nor I would write a JavaScript or PHP to resemble Perl or SQL... Use them the way they're meant to be used.\n\n\nA3:\n\nValidating + Formatting/Deriving Values\nSetters let you to validate data and getters let you format or derive data. Objects allow you to encapsulate data and its validation and formatting code into a neat package that encourages DRY.\nFor example, consider the following simple class that contains a birth date.\nclass BirthDate {\n\n    private $birth_date;\n\n    public function getBirthDate($format='Y-m-d') {\n        //format $birth_date ...\n        //$birth_date = ...\n        return $birth_date;\n    }\n\n    public function setBirthDate($birth_date) {                   \n        //if($birth_date is not valid) throw an exception ...          \n        $this->birth_date = $birth_date;\n    }\n\n    public function getAge() {\n        //calculate age ...\n        return $age;\n    }\n\n    public function getDaysUntilBirthday() {\n        //calculate days until birth days\n        return $days;\n    }\n}\n\nYou'll want to validate that the value being set is\n\nA valid date\nNot in the future\n\nAnd you don't want to do this validation all over your application (or over multiple applications for that matter). Instead, it's easier to make the member variable protected or private (in order to make the setter the only access point) and to validate in the setter because then you'll know that the object contains a valid birth date no matter which part of the application the object came from and if you want to add more validation then you can add it in a single place.\nYou might want to add multiple formatters that operate on the same member variable i.e. getAge() and getDaysUntilBirthday() and you might want to enforce a configurable format in getBirthDate() depending on locale. Therefore I prefer consistently accessing values via getters as opposed to mixing $date->getAge() with $date->birth_date.\ngetters and setters are also useful when you extend objects. For example, suppose your application needed to allow 150+ year birth dates in some places but not in others. One way to solve the problem without repeating any code would be to extend the BirthDate object and put the additional validation in the setter.\nclass LivingBirthDate extends BirthDate {\n\n    public function setBirthDate($birth_date) {\n        //if $birth_date is greater than 150 years throw an exception\n        //else pass to parent's setter\n        return parent::setBirthDate($birth_date);\n    }\n}\n\n\n\nA4:\n\nThere are many ways to create sourcecode in a netbeans-convention. This is nice. It makes thinks such easyer === FALSE. Just use the traditionel, specially if you are not sure which one of the properties should be encapsuled and which one not. I know, it is a boi.... pla... code, but for debugging-works and many other thinks it is the better, clear way. Dont spend to much time with thousend of arts how to make simple getters and setters. You cannot implement too some design patterns like the demeter-rule and so on, if you use magics. In specific situation you can use magic_calls or for small, fast and clear solutions. Sure you could make solutions for design-patters in this way too, but why to make you live more difficult. \n\n\nA5:\n\nIf you're looking for a space, that would be \" \" (one space).\nIf you're looking for one or more, it's \"  *\" (that's two spaces and an asterisk) or \" +\" (one space and a plus).\nIf you're looking for common spacing, use \"[ X]\" or \"[ X][ X]*\" or \"[ X]+\" where X is the physical tab character (and each is preceded by a single space in all those examples).\nThese will work in every* regex engine I've ever seen (some of which don't even have the one-or-more \"+\" character, ugh).\nIf you know you'll be using one of the more modern regex engines, \"\\s\" and its variations are the way to go. In addition, I believe word boundaries match start and end of lines as well, important when you're looking for words that may appear without preceding or following spaces.\nFor PHP specifically, this page may help.\nFrom your edit, it appears you want to remove all non valid characters The start of this is (note the space inside the regex):\n$newtag = preg_replace (\"/[^a-zA-Z0-9 ]/\", \"\", $tag);\n#                                    ^ space here\n\nIf you also want trickery to ensure there's only one space between each word and none at the start or end, that's a little more complicated (and probably another question) but the basic idea would be:\n$newtag = preg_replace (\"/ +/\", \" \", $tag); # convert all multispaces to space\n$newtag = preg_replace (\"/^ /\", \"\", $tag);  # remove space from start\n$newtag = preg_replace (\"/ $/\", \"\", $tag);  # and end\n\n\n\nA6:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\nA7:\n\nYou can also use the \\b for a word boundary.  For the name I would use something like this:\n[^\\b]+\\b[^\\b]+(\\b|$)\n\nEDIT Modifying this to be a regex in Perl example\nif( $fullname =~ /([^\\b]+)\\b[^\\b]+([^\\b]+)(\\b|$)/ ) {\n $first_name = $1;\n $last_name = $2;\n}\n\nEDIT AGAIN Based on what you want:\n$new_tag = preg_replace(\"/[\\s\\t]/\",\"\",$tag);\n\n\n\nA8:\n\nIf you preffer to use the __call function, you can use this method. It works with\n\nGET => $this->property()\nSET => $this->property($value)\nGET => $this->getProperty()\nSET => $this->setProperty($value)\n\nkalsdas\npublic function __call($name, $arguments) {\n\n    //Getting and setting with $this->property($optional);\n\n    if (property_exists(get_class($this), $name)) {\n\n\n        //Always set the value if a parameter is passed\n        if (count($arguments) == 1) {\n            /* set */\n            $this->$name = $arguments[0];\n        } else if (count($arguments) > 1) {\n            throw new \\Exception(\"Setter for $name only accepts one parameter.\");\n        }\n\n        //Always return the value (Even on the set)\n        return $this->$name;\n    }\n\n    //If it doesn't chech if its a normal old type setter ot getter\n    //Getting and setting with $this->getProperty($optional);\n    //Getting and setting with $this->setProperty($optional);\n    $prefix = substr($name, 0, 3);\n    $property = strtolower($name[3]) . substr($name, 4);\n    switch ($prefix) {\n        case 'get':\n            return $this->$property;\n            break;\n        case 'set':\n            //Always set the value if a parameter is passed\n            if (count($arguments) != 1) {\n                throw new \\Exception(\"Setter for $name requires exactly one parameter.\");\n            }\n            $this->$property = $arguments[0];\n            //Always return the value (Even on the set)\n            return $this->$name;\n        default:\n            throw new \\Exception(\"Property $name doesn't exist.\");\n            break;\n    }\n}\n\n\n\nA9:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA10:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\nA11:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA12:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA13:\n\nIn Perl the switch is \\s (whitespace).\n\n\nA14:\n\nGoogle already published a guide on optimization of PHP and the conclusion was:\nNo getter and setter Optimizing PHP\nAnd no, you must not use magic methods.  For PHP, Magic Methods are evil. Why?\n\nThey are hard to debug.\nThere is a negative performance impact.\nThey require writing more code.\n\nPHP is not Java, C++, or C#. PHP is different and plays with different rules. \n\n\nA15:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA16:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\nA17:\n\nIn addition to the already great and respected answers in here, I would like to expand on PHP having no setters/getters.\nPHP does not have getter and setter syntax. It provides subclassed or magic methods to allow \"hooking\" and overriding the property lookup process, as pointed out by Dave.\nMagic allows us lazy programmers to do more with less code at a time at which we are actively engaged in a project and know it intimately, but usually at the expense of readability.\nPerformance Every unnecessary function, that results from forcing a getter/setter-like code-architecture in PHP, involves its own memory stack-frame upon invocation and is wasting CPU cycles. \nReadability: The codebase incurs bloating code-lines, which impacts code-navigation as more LOC mean more scrolling,.\nPreference: Personally, as my rule of thumb, I take the failure of static code analysis \nas a sign to avoid going down the magical road as long as obvious long-term benefits elude me at that time.\nFallacies:\nA common argument is readability. For instance that $someobject->width is easier to read than $someobject->width(). However unlike a planet's circumference or width, which can be assumed to be  static, an object's instance such as $someobject, which requires a width function, likely takes a measurement of the object's instance width.\nTherefore readability increases mainly because of assertive naming-schemes and not by hiding the function away that outputs a given property-value.\n__get / __set uses:\n\npre-validation and pre-sanitation of property values\nstrings e.g.\n\"\nsome {mathsobj1->generatelatex} multi\nline text {mathsobj1->latexoutput}\nwith lots of variables for {mathsobj1->generatelatex}\n some reason\n\"\n\nIn this case generatelatex would adhere to a naming scheme of actionname + methodname\nspecial, obvious cases\n$dnastringobj->homeobox($one_rememberable_parameter)->gattaca->findrelated()\n$dnastringobj->homeobox($one_rememberable_parameter)->gttccaatttga->findrelated()\n\n\nNote: PHP chose not to implement getter/setter syntax. I am not claiming that getters/setter are generally bad.\n\n\nA18:\n\nIt seems to me like using a REGEX in this case would just be overkill.  Why not just just strpos to find the space character.  Also, there's nothing special about the space character in regular expressions, you should be able to search for it the same as you would search for any other character.  That is, unless you disabled pattern whitespace, which would hardly be necessary in this case.\n\n\nA19:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA20:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA21:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA22:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA23:\n\nI made an experiment using the magic method __call.\nNot sure if I should post it (because of all the \"DO NOT USE MAGIC METHODS\" warnings in the other answers and comments) but i'll leave it here.. just in case someone find it useful.\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = substr($_name, 4);\n\n    if (isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nJust add that method above in your class, now you can type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_foo(); // return \"bar\"\n$C->get_bom(); // return \"bim\"\n\n// as setter\n$C->set_foo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_bom(\"zam\"); // set \"zam\" as new value of bom\n\n\n\nThis way you can get/set everything in your class if it exist so, if you need it for only a few specific elements, you could use a \"whitelist\" as filter.  \nExample:\nprivate $callWhiteList = array(\n    \"foo\" => \"foo\",\n    \"fee\" => \"fee\",\n    // ...\n);\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = $this->callWhiteList[substr($_name, 4)];\n\n    if (!is_null($varName) && isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nNow you can only get/set \"foo\" and \"fee\".\nYou can also use that \"whitelist\" to assign custom names to access to your vars.\nFor example,\nprivate $callWhiteList = array(\n    \"myfoo\" => \"foo\",\n    \"zim\" => \"bom\",\n    // ...\n);\n\nWith that list you can now type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // private $callWhiteList = array( ... )\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_myfoo(); // return \"bar\"\n$C->get_zim(); // return \"bim\"\n\n// as setter\n$C->set_myfoo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_zim(\"zam\"); // set \"zam\" as new value of bom\n\n\n.\n.\n.\nThat's all.\n\nDoc:\n__call() is triggered when invoking inaccessible methods in an object context. \n\n\nA24:\n\nWhy use getters and setters?\n\nScalability: It's easier refactor a getter than search all the var assignments in a project code.\nDebugging: You can put breakpoints at setters and getters.\nCleaner: Magic functions are not good solution for writting less, your IDE will not suggest the code. Better use templates for fast-writting getters.\n\n\n\n\nA25:\n\nclass MyClass {\n    private $firstField;\n    private $secondField;\n    private $thirdField;\n\n    public function __get( $name ) {\n        if( method_exists( $this , $method = ( 'get' . ucfirst( $name  ) ) ) )\n            return $this->$method();\n        else\n            throw new Exception( 'Can\\'t get property ' . $name );\n    }\n\n    public function __set( $name , $value ) {\n        if( method_exists( $this , $method = ( 'set' . ucfirst( $name  ) ) ) )\n            return $this->$method( $value );\n        else\n            throw new Exception( 'Can\\'t set property ' . $name );\n    }\n\n    public function __isset( $name )\n    {\n        return method_exists( $this , 'get' . ucfirst( $name  ) ) \n            || method_exists( $this , 'set' . ucfirst( $name  ) );\n    }\n\n    public function getFirstField() {\n        return $this->firstField;\n    }\n\n    protected function setFirstField($x) {\n        $this->firstField = $x;\n    }\n\n    private function getSecondField() {\n        return $this->secondField;\n    }\n}\n\n$obj = new MyClass();\n\necho $obj->firstField; // works\n$obj->firstField = 'value'; // works\n\necho $obj->getFirstField(); // works\n$obj->setFirstField( 'value' ); // not works, method is protected\n\necho $obj->secondField; // works\necho $obj->getSecondField(); // not works, method is private\n\n$obj->secondField = 'value'; // not works, setter not exists\n\necho $obj->thirdField; // not works, property not exists\n\nisset( $obj->firstField ); // returns true\nisset( $obj->secondField ); // returns true\nisset( $obj->thirdField ); // returns false\n\nReady!\n\n\nA26:\n\nWell, PHP does have magic methods __get, __set, __isset & __unset, which is always a start. Alas proper (get it?) OO properties is more than magic methods. The main problem with PHP's implementation is that magic methods are called for all inaccessible properties. Which means you have to Repeat Yourself (eg. by calling property_exists()) in the magic methods when determining if name is actually a property of your object. And you can't really solve this general problem with a base class unless all your classes inherit from ie. ClassWithProperties, since PHP lacks multiple inheritance.\nIn contrast, Python new style classes gives you property(), which lets you explicitly define all your properties. C# has special syntax. \nhttp://en.wikipedia.org/wiki/Property_(programming)\n\n\nA27:\n\nYou can use php magic methods __get and __set.\n<?php\nclass MyClass {\n  private $firstField;\n  private $secondField;\n\n  public function __get($property) {\n    if (property_exists($this, $property)) {\n      return $this->$property;\n    }\n  }\n\n  public function __set($property, $value) {\n    if (property_exists($this, $property)) {\n      $this->$property = $value;\n    }\n\n    return $this;\n  }\n}\n?>\n\n\n\nA28:\n\nEncapsulation is important in any OO language, popularity has nothing to do with it. In dynamically typed languages, like PHP, it is especially useful because there is little ways to ensure a property is of a specific type without using setters.\nIn PHP, this works:\nclass Foo {\n   public $bar; // should be an integer\n}\n$foo = new Foo;\n$foo->bar = \"string\";\n\nIn Java, it doesn't:\nclass Foo {\n   public int bar;\n}\nFoo myFoo = new Foo();\nmyFoo.bar = \"string\"; // error\n\nUsing magic methods (__get and __set) also works, but only when accessing a property that has lower visibility than the current scope can access. It can easily give you headaches when trying to debug, if it is not used properly.\n\n\nA29:\n\nGenerally speaking, the first way is more popular overall because those with prior programming knowledge can easily transition to PHP and get work done in an object-oriented fashion. The first way is more universal. My advice would be to stick with what is tried and true across many languages. Then, when and if you use another language, you'll be ready to get something accomplished (instead of spending time reinventing the wheel).\n\n\nA30:\n\nTo match exactly the space character, you can use the octal value \\040 (Unicode characters displayed as octal) or the hexadecimal value \\x20 (Unicode characters displayed as hex).\nHere is the regex syntax reference: https://www.regular-expressions.info/nonprint.html.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "The, you can use the \"X\" to indicate that theuser is using the default.\n(vs. the \"X\" for the \"X\""}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 11, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nHow can I simply and directly find the index within an array of objects meeting some condition?\nFor example, given this input:\nvar hello = { hello: 'world', foo: 'bar'};\nvar qaz = { hello: 'stevie', foo: 'baz'}\n\nvar myArray = [];\nmyArray.push(hello, qaz);\n\nHow can I search myArray to find the index of the element whose hello property equals 'stevie' (in this case, the result should be 1)?\n\n\nCandidate answers:\nA1:\n\nNPM stands for Node Package Manager.\nIt comes pre-installed with Node.js. NPM helps to manage packages in your projects as dependencies.\nWhen using NPM, there are two ways to install a package into your local computer.\n\nLocally: When a package is installed locally, it is installed in\n./node_modules/.bin/ of the local project directory.\n\nGlobally: A global package is installed in the user environment\npath. /usr/local/bin for Linux and AppData%/npm for Windows.\n\n\nTo execute a locally installed package, it should be specified in the package.json scripts block as shown below.\n\"scripts\": {\n    \"your-package\":  \"your-package-name\"\n}\n\nThen, you can execute the package with:\nnpm run your-package-name\n\nNPX is an NPM package executor.\nCurrently, NPX is bundled with NPM when you install the NPM version 5.2.0 or higher.\nWhy NPX over NPM?\n\nNo need to edit the package.json file with node_modules paths.\n\nYou can directly execute the tool from the command line.\n\n\n\n\nA2:\n\nIntroducing npx: an npm package runner\nNPM - Manages packages but doesn't make life easy executing any.NPX - A tool for executing Node packages.\n\nNPX comes bundled with NPM version 5.2+ \n\nNPM by itself does not simply run any package. It doesn't run any package as a matter of fact. If you want to run a package using NPM, you must specify that package in your package.json file.\nWhen executables are installed via NPM packages, NPM links to them:\n\nlocal installs have \"links\" created at ./node_modules/.bin/ directory.\nglobal installs have \"links\" created from the global bin/ directory (e.g. /usr/local/bin) on Linux or at %AppData%/npm on Windows.\n\nDocumentation you should read\n\nNPM:\nOne might install a package locally on a certain project:\nnpm install some-package\n\nNow let's say you want NodeJS to execute that package from the command line:\n$ some-package\n\nThe above will fail. Only globally installed packages can be executed by typing their name only.\nTo fix this, and have it run, you must type the local path:\n$ ./node_modules/.bin/some-package\n\nYou can technically run a locally installed package by editing your packages.json file and adding that package in the scripts section:\n{\n  \"name\": \"whatever\",\n  \"version\": \"1.0.0\",\n  \"scripts\": {\n    \"some-package\": \"some-package\"\n  }\n}\n\nThen run the script using npm run-script (or npm run):\nnpm run some-package\n\n\nNPX:\nnpx will check whether <command> exists in $PATH, or in the local project binaries, and execute it. So, for the above example, if you wish to execute the locally-installed package some-package all you need to do is type:\nnpx some-package\n\nAnother major advantage of npx is the ability to execute a package which wasn't previously installed:\n$ npx create-react-app my-app\n\nThe above example will generate a react app boilerplate within the path the command had run in, and ensures that you always use the latest version of a generator or build tool without having to upgrade each time you\u2019re about to use it.\n\nUse-Case Example:\nnpx command may be helpful in the script section of a package.json file,\nwhen it is unwanted to define a dependency which might not be commonly used or any other reason:\n\"scripts\": {\n    \"start\": \"npx gulp@3.9.1\",\n    \"serve\": \"npx http-server\"\n}\n\nCall with: npm run serve\n\nRelated questions:\n\nHow to use package installed locally in node_modules?\nNPM: how to source ./node_modules/.bin folder?\nHow do you run a js file using npm scripts?\n\n\n\nA3:\n\nI normally use the circular-json npm package to solve this.\n// Felix Kling's example\nvar a = {};\na.b = a;\n// load circular-json module\nvar CircularJSON = require('circular-json');\nconsole.log(CircularJSON.stringify(a));\n//result\n{\"b\":\"~\"}\n\nNote: circular-json has been deprecated, I now use flatted (from the creator of CircularJSON):\n// ESM\nimport {parse, stringify} from 'flatted/esm';\n\n// CJS\nconst {parse, stringify} = require('flatted/cjs');\n\nconst a = [{}];\na[0].a = a;\na.push(a);\n\nstringify(a); // [[\"1\",\"0\"],{\"a\":\"0\"}]\n\nfrom: https://www.npmjs.com/package/flatted\n\n\nA4:\n\nnpm - package manager\nnpx - Execute npm package\nThis is a difference with it.\n\n\nA5:\n\nHere's an example of NPX in action: npx cowsay hello\nIf you type that into your bash terminal you'll see the result. The benefit of this is that npx has temporarily installed cowsay. There is no package pollution since cowsay is not permanently installed. This is great for one off packages where you want to avoid package pollution. \nAs mentioned in other answers, npx is also very useful in cases where (with npm) the package needs to be installed then configured before running. E.g. instead of using npm to install and then configure the json.package file and then call the configured run command just use npx instead. A real example:\nnpx create-react-app my-app\n\n\nA6:\n\nNPM => Is a JS package manager.\nNPX => Is a tool for executing Node packages and execute npm package binaries.\nIt is easy to remember:\n-npm stands for MANAGER\n-npx stands for EXECUTE\n\n\nA7:\n\nBased on zainengineer's answer... Another approach is to make a deep copy of the object and strip circular references and stringify the result.\n\n\nfunction cleanStringify(object) {\r\n    if (object && typeof object === 'object') {\r\n        object = copyWithoutCircularReferences([object], object);\r\n    }\r\n    return JSON.stringify(object);\r\n\r\n    function copyWithoutCircularReferences(references, object) {\r\n        var cleanObject = {};\r\n        Object.keys(object).forEach(function(key) {\r\n            var value = object[key];\r\n            if (value && typeof value === 'object') {\r\n                if (references.indexOf(value) < 0) {\r\n                    references.push(value);\r\n                    cleanObject[key] = copyWithoutCircularReferences(references, value);\r\n                    references.pop();\r\n                } else {\r\n                    cleanObject[key] = '###_Circular_###';\r\n                }\r\n            } else if (typeof value !== 'function') {\r\n                cleanObject[key] = value;\r\n            }\r\n        });\r\n        return cleanObject;\r\n    }\r\n}\r\n\r\n// Example\r\n\r\nvar a = {\r\n    name: \"a\"\r\n};\r\n\r\nvar b = {\r\n    name: \"b\"\r\n};\r\n\r\nb.a = a;\r\na.b = b;\r\n\r\nconsole.log(cleanStringify(a));\r\nconsole.log(cleanStringify(b));\n\n\n\n\n\nA8:\n\nSee this example: http://jsfiddle.net/89C54/\nfor (i = 0; i < myArray.length; i++) {\n    if (myArray[i].hello === 'stevie') {\n        alert('position: ' + i);\n        return;\n    }\n}\n\nIt starts to count with zero.\n\n\nA9:\n\nOr prototype it :\nArray.prototype.indexOfObject = function arrayObjectIndexOf(property, value) {\n    for (var i = 0, len = this.length; i < len; i++) {\n        if (this[i][property] === value) return i;\n    }\n    return -1;\n}\n\nmyArr.indexOfObject(\"name\", \"stevie\");\n\n\n\nA10:\n\nSimplest Definition:\nNPX\n\nThe npx stands for Node Package Execute and it comes with the npm,\nwhen you installed npm above 5.2.0 version then automatically npx will\ninstalled. It is an npm package runner that can execute any package\nthat you want from the npm registry without even installing that\npackage.\n\nNPM\n\nnpm is a package manager for the JavaScript programming language\nmaintained by npm, Inc. npm is the default package manager for the\nJavaScript runtime environment Node.js. It consists of a command line\nclient, also called npm, and an online database of public and paid-for\nprivate packages\n\n\n\nA11:\n\nNPX is a tool for creating and executing some features in a new project\nNPM is the package manager that contains all of libraries\n\n\nA12:\n\nNPM - NODE PACKAGE MANAGER\n\nit's a package manager.\n\nnode.js  package can be installed using NPM\n\n\nNPX - NODE PACKAGE EXECUTE\n\nit execute the  node.js  package\n\ndoesn't matter whether you installed  node.js  package locally or\nglobally.\n\nNPX will temporarily install package and run it.\n\nNPX is used when you want to execute package for single time.\n\n\n.\n\n\nA13:\n\nSimply npm is the Node Package Manager and\nnpx is the executeable version that run npm packages\n\n\nA14:\n\nIn my case I simply forgot to use async/await thing while building the route:\napp.get('/products', async (req, res) => {\n    const products = await Product.find();\n    res.send(products );\n});\n\n\n\nA15:\n\nI resolve this problem on NodeJS like this:\nvar util = require('util');\n\n// Our circular object\nvar obj = {foo: {bar: null}, a:{a:{a:{a:{a:{a:{a:{hi: 'Yo!'}}}}}}}};\nobj.foo.bar = obj;\n\n// Generate almost valid JS object definition code (typeof string)\nvar str = util.inspect(b, {depth: null});\n\n// Fix code to the valid state (in this example it is not required, but my object was huge and complex, and I needed this for my case)\nstr = str\n    .replace(/<Buffer[ \\w\\.]+>/ig, '\"buffer\"')\n    .replace(/\\[Function]/ig, 'function(){}')\n    .replace(/\\[Circular]/ig, '\"Circular\"')\n    .replace(/\\{ \\[Function: ([\\w]+)]/ig, '{ $1: function $1 () {},')\n    .replace(/\\[Function: ([\\w]+)]/ig, 'function $1(){}')\n    .replace(/(\\w+): ([\\w :]+GMT\\+[\\w \\(\\)]+),/ig, '$1: new Date(\"$2\"),')\n    .replace(/(\\S+): ,/ig, '$1: null,');\n\n// Create function to eval stringifyed code\nvar foo = new Function('return ' + str + ';');\n\n// And have fun\nconsole.log(JSON.stringify(foo(), null, 4));\n\n\n\nA16:\n\nnpm - Package manager for JavaScript, just like: pip (Python), Maven (Java), NuGet (.NET), Composer (PHP), RubyGems (Ruby), ...\nnpx - runs a command of a package without installing it explicitly.\nUse cases:\n\nYou don't want to install packages neither globally nor locally.\nYou don't have permission to install it globally.\nJust want to test some commands.\nSometime, you want to have a script command (generate, convert something, ...) in package.json to execute something without installing these packages as project's dependencies.\n\nSyntax:\nnpx [options] [-p|--package <package>] <command> [command-arg]...\n\nPackage is optional:\nnpx   -p uglify-js         uglifyjs --output app.min.js app.js common.js\n      +----------------+   +--------------------------------------------+\n      package (optional)   command, followed by arguments\n\nFor example:\nStart a HTTP Server      : npx http-server\nLint code                : npx eslint ./src\n                         # Run uglifyjs command in the package uglify-js\nMinify JS                : npx -p uglify-js uglifyjs -o app.min.js app.js common.js\nMinify CSS               : npx clean-css-cli -o style.min.css css/bootstrap.css style.css\nMinify HTML              : npx html-minifier index-2.html -o index.html --remove-comments --collapse-whitespace\nScan for open ports      : npx evilscan 192.168.1.10 --port=10-9999\nCast video to Chromecast : npx castnow http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\n\nMore about command:\n\nhttps://docs.npmjs.com/files/package.json#bin\nhttps://github.com/mishoo/UglifyJS2/blob/master/package.json#L17\n\n\n\nA17:\n\nnpm is package manager or installer on the other hand Packages used by npx are not installed globally so you have to carefree for the pollution for the long term.\n\n\nA18:\n\nIt means that the object you pass in the request (I guess it is pagedoc) has a circular reference, something like:\nvar a = {};\na.b = a;\n\nJSON.stringify cannot convert structures like this.\nN.B.: This would be the case with DOM nodes, which have circular references, even if they are not attached to the DOM tree. Each node has an ownerDocument which refers to document in most cases. document has a reference to the DOM tree at least through document.body and document.body.ownerDocument refers back to document again, which is only one of multiple circular references in the DOM tree.\n\n\nA19:\n\nUsing Ramda:\nimport {addIndex, map} from 'ramda';\n\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nconst mapIndexed = addIndex(map);\nmapIndexed((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return 'X';\n}, list);\n\n\n\nA20:\n\nOne approach is to strip object and functions from main object. And stringify the simpler form\nfunction simpleStringify (object){\n    // stringify an object, avoiding circular structures\n    // https://stackoverflow.com/a/31557814\n    var simpleObject = {};\n    for (var prop in object ){\n        if (!object.hasOwnProperty(prop)){\n            continue;\n        }\n        if (typeof(object[prop]) == 'object'){\n            continue;\n        }\n        if (typeof(object[prop]) == 'function'){\n            continue;\n        }\n        simpleObject[prop] = object[prop];\n    }\n    return JSON.stringify(simpleObject); // returns cleaned up JSON\n};\n\nif you are using node js use inspect()\nimport {inspect} from \"util\";\nconsole.log(inspect(object));\n\n\n\nA21:\n\nArray.prototype.findIndex is supported in all browsers other than IE (non-edge). But the polyfill provided is nice.\nvar indexOfStevie = myArray.findIndex(i => i.hello === \"stevie\");\n\nThe solution with map is okay. But you are iterating over the entire array every search. That is only the worst case for findIndex which stops iterating once a match is found.\n\nThere's not really a concise way (when devs had to worry about IE8), but here's a common solution:\nvar searchTerm = \"stevie\",\n    index = -1;\nfor(var i = 0, len = myArray.length; i < len; i++) {\n    if (myArray[i].hello === searchTerm) {\n        index = i;\n        break;\n    }\n}\n\nor as a function:\nfunction arrayObjectIndexOf(myArray, searchTerm, property) {\n    for(var i = 0, len = myArray.length; i < len; i++) {\n        if (myArray[i][property] === searchTerm) return i;\n    }\n    return -1;\n}\narrayObjectIndexOf(arr, \"stevie\", \"hello\"); // 1\n\nJust some notes:\n\nDon't use for...in loops on arrays\nBe sure to break out of the loop or return out of the function once you've found your \"needle\"\nBe careful with object equality\n\nFor example,\nvar a = {obj: 0};\nvar b = [a];\nb.indexOf({obj: 0}); // -1 not found\n\n\n\nA22:\n\nYou will be able to get the current iteration's index for the map method through its 2nd parameter.\nExample:\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nlist.map((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return currElement; //equivalent to list[index]\n});\n\nOutput:\nThe current iteration is: 0 <br>The current element is: h\n\nThe current iteration is: 1 <br>The current element is: e\n\nThe current iteration is: 2 <br>The current element is: l\n\nThe current iteration is: 3 <br>The current element is: l \n\nThe current iteration is: 4 <br>The current element is: o\n\nSee also: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array/map\n\nParameters\ncallback - \n    Function that produces an element of the new Array, taking three arguments:\n1) currentValue\n        The current element being processed in the array.\n2) index\n      The index of the current element being processed in the array.\n3) array\n      The array map was called upon.\n\n\n\nA23:\n\nNPX:\nFrom https://www.futurehosting.com/blog/npx-makes-life-easier-for-node-developers-plus-node-vulnerability-news/:\n\nWeb developers can have dozens of projects on their development\n  machines, and each project has its own particular set of npm-installed\n  dependencies. A few years back, the usual advice for dealing with CLI\n  applications like Grunt or Gulp was to install them locally in each\n  project and also globally so they could easily be run from the command\n  line.\nBut installing globally caused as many problems as it solved. Projects\n  may depend on different versions of command line tools, and polluting\n  the operating system with lots of development-specific CLI tools isn\u2019t\n  great either. Today, most developers prefer to install tools locally\n  and leave it at that.\nLocal versions of tools allow developers to pull projects from GitHub\n  without worrying about incompatibilities with globally installed\n  versions of tools. NPM can just install local versions and you\u2019re good\n  to go. But project specific installations aren\u2019t without their\n  problems: how do you run the right version of the tool without\n  specifying its exact location in the project or playing around with\n  aliases?\nThat\u2019s the problem npx solves. A new tool included in NPM 5.2, npx is\n  a small utility that\u2019s smart enough to run the right application when\n  it\u2019s called from within a project.\nIf you wanted to run the project-local version of mocha, for example,\n  you can run npx mocha inside the project and it will do what you\n  expect.\nA useful side benefit of npx is that it will automatically install npm\n  packages that aren\u2019t already installed. So, as the tool\u2019s creator Kat\n  March\u00e1n points out, you can run npx benny-hill without having to deal\n  with Benny Hill polluting the global environment.\nIf you want to take npx for a spin, update to the most recent version\n  of npm.\n\n\n\nA24:\n\nnpm is a tool that use to install packages and npx is a tool that use to execute packages.\nnpm-If you wish to run package through npm then you have to specify that package in your package.json and install it locally.\nnpx-A package can be executable without installing the package. It is an npm package runner so if any packages aren\u2019t already installed it will install them automatically.\n\n\nA25:\n\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages. \nIt doesn't matter whether you installed that package globally or locally. NPX will temporarily install it and run it. NPM also can run packages if you configure a package.json file and include it in the script section. \n\nSo remember this, if you want to check/run a node package  quickly without installing locally or globally use NPX.\n\nnpM -  Manager\nnpX -  Execute - easy to remember\n\n\nA26:\n\n\nsuppose you have an array like\n\n\n\n   const arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    \n    \n    arr.map((myArr, index) => {\n      console.log(`your index is -> ${index} AND value is ${myArr}`);\n    })\n\n\n\n> output will be\n index is -> 0 AND value is 1\n index is -> 1 AND value is 2\n index is -> 2 AND value is 3\n index is -> 3 AND value is 4\n index is -> 4 AND value is 5\n index is -> 5 AND value is 6\n index is -> 6 AND value is 7\n index is -> 7 AND value is 8\n index is -> 8 AND value is 9\n\n\n\nA27:\n\nvar idx = myArray.reduce( function( cur, val, index ){\n\n    if( val.hello === \"stevie\" && cur === -1 ) {\n        return index;\n    }\n    return cur;\n\n}, -1 );\n\n\n\nA28:\n\nwith npx:\nnpx create-react-app MyApp\n\nearlier:\nnpm install -g create-react-app\ncreate-react-app MyApp\n\nEarlier it was necessary to install a CLI tool locally before using it, with npx that is not required anymore.\n\n\nA29:\n\nFor my case I was getting that error when I was using async function on my server-side to fetch documents using mongoose. It turned out that the reason was I forgot to put await before calling find({}) method. Adding that part fixed my issue. \n\n\nA30:\n\nSimple Definition:\nnpm - Javascript package manager \nnpx - Execute npm package binaries\n\n\nA31:\n\nNPM stands for Node Package Manager. NPM is Node.JS's default package manager. It's written in Javascript. The role of NPM is to manage the package and modules of node.js.\nNPX stands for Node Package Execute. NPX comes with npm, when npm is installed above the 5.2.0 version, it gets installed automatically. NPX is an npm package runner and its role is to execute the package from the registry without even installing that package.\nNow, the differences between NPM and NPX are as below:\n\ni) NPM is used to install the packages while NPX is used to execute the packages.\n\n\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages which are used by npx don't need to be taken care of as they are not installed globally.\n\n\n\nA32:\n\nHere is the simple definition.\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages.\n\n\nA33:\n\nArray.prototype.map() index:\nOne can access the index Array.prototype.map() via the second argument of the callback function. Here is an example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst map = array.map((x, index) => {\n  console.log(index);\n  return x + index;\n});\n\nconsole.log(map);\n\n\n\nOther arguments of Array.prototype.map():\n\nThe third argument of the callback function exposes the array on which map was called upon\nThe second argument of Array.map() is a object which will be the this value for the callback function. Keep in mind that you have to use the regular function keyword in order to declare the callback since an arrow function doesn't have its own binding to the this keyword.\n\nFor example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst thisObj = { prop1: 1 }\n\nconst map = array.map((x, index, array) => {\n  console.log(array);\n  console.log(this)\n}, thisObj);\n\n\n\n\n\nA34:\n\nAs per the JSON docs at Mozilla, JSON.stringify has a second parameter replacer which can be used to filter/ignore children items while parsing the tree. However, perhaps you can avoid the circular references.\nIn Node.js we cannot. So we can do something like this:\nfunction censor(censor) {\n  var i = 0;\n  \n  return function(key, value) {\n    if(i !== 0 && typeof(censor) === 'object' && typeof(value) == 'object' && censor == value) \n      return '[Circular]'; \n    \n    if(i >= 29) // seems to be a harded maximum of 30 serialized objects?\n      return '[Unknown]';\n    \n    ++i; // so we know we aren't using the original object anymore\n    \n    return value;  \n  }\n}\n\nvar b = {foo: {bar: null}};\n\nb.foo.bar = b;\n\nconsole.log(\"Censoring: \", b);\n\nconsole.log(\"Result: \", JSON.stringify(b, censor(b)));\n\nThe result:\nCensoring:  { foo: { bar: [Circular] } }\nResult: {\"foo\":{\"bar\":\"[Circular]\"}}\n\nUnfortunately there seems to be a maximum of 30 iterations before it automatically assumes it's circular. Otherwise, this should work. I even used areEquivalent from here, but JSON.stringify still throws the exception after 30 iterations. Still, it's good enough to get a decent representation of the object at a top level, if you really need it. Perhaps somebody can improve upon this though? In Node.js for an HTTP request object, I'm getting:\n{\n\"limit\": null,\n\"size\": 0,\n\"chunks\": [],\n\"writable\": true,\n\"readable\": false,\n\"_events\": {\n    \"pipe\": [null, null],\n    \"error\": [null]\n},\n\"before\": [null],\n\"after\": [],\n\"response\": {\n    \"output\": [],\n    \"outputEncodings\": [],\n    \"writable\": true,\n    \"_last\": false,\n    \"chunkedEncoding\": false,\n    \"shouldKeepAlive\": true,\n    \"useChunkedEncodingByDefault\": true,\n    \"_hasBody\": true,\n    \"_trailer\": \"\",\n    \"finished\": false,\n    \"socket\": {\n        \"_handle\": {\n            \"writeQueueSize\": 0,\n            \"socket\": \"[Unknown]\",\n            \"onread\": \"[Unknown]\"\n        },\n        \"_pendingWriteReqs\": \"[Unknown]\",\n        \"_flags\": \"[Unknown]\",\n        \"_connectQueueSize\": \"[Unknown]\",\n        \"destroyed\": \"[Unknown]\",\n        \"bytesRead\": \"[Unknown]\",\n        \"bytesWritten\": \"[Unknown]\",\n        \"allowHalfOpen\": \"[Unknown]\",\n        \"writable\": \"[Unknown]\",\n        \"readable\": \"[Unknown]\",\n        \"server\": \"[Unknown]\",\n        \"ondrain\": \"[Unknown]\",\n        \"_idleTimeout\": \"[Unknown]\",\n        \"_idleNext\": \"[Unknown]\",\n        \"_idlePrev\": \"[Unknown]\",\n        \"_idleStart\": \"[Unknown]\",\n        \"_events\": \"[Unknown]\",\n        \"ondata\": \"[Unknown]\",\n        \"onend\": \"[Unknown]\",\n        \"_httpMessage\": \"[Unknown]\"\n    },\n    \"connection\": \"[Unknown]\",\n    \"_events\": \"[Unknown]\",\n    \"_headers\": \"[Unknown]\",\n    \"_headerNames\": \"[Unknown]\",\n    \"_pipeCount\": \"[Unknown]\"\n},\n\"headers\": \"[Unknown]\",\n\"target\": \"[Unknown]\",\n\"_pipeCount\": \"[Unknown]\",\n\"method\": \"[Unknown]\",\n\"url\": \"[Unknown]\",\n\"query\": \"[Unknown]\",\n\"ended\": \"[Unknown]\"\n}\n\nI created a small Node.js module to do this here: https://github.com/ericmuyser/stringy Feel free to improve/contribute!\n\n\nA35:\n\nNPM: NPM stands for Node Package Manager and is the default package manager for Node.js. It was developed by Isaac Z. Schlueter and was originally released on January 12, 2010. It is entirely written in JavaScript. It consists of a command-line client npm which manages all node.js packages and modules. When node.js is installed, it is included in the installation.\nnpm run your-package-name\n\nNPX is a tool that use to execute packages.\nNPX is an acronym for Node Package Execute The NPX package comes with npm, so when you install npm above 5.2.0, NPX will be installed automatically.\nIt is an npm package runner that can execute any package that you want from the npm registry without even installing that package. The npx is useful during a single time use package. If you have installed npm below 5.2.0 then npx is not installed in your system.\nRun the following command to determine if npx is installed:\nnpx -v\n\nThe following command can be run if npx is not installed.\nnpm install -g npx\n\nUse npx to execute the package:\nnpx your-package-name\n\n\n\n\nA36:\n\nHere's an example of what your app creation might look like using npx\n\nnpx create-react-app project-name --template all\n\n\n\nA37:\n\nI think you can solve it in one line using the map function:\nconst pos = myArray.map(e => e.hello).indexOf('stevie');\n\n\n\nA38:\n\nNPM vs. NPX\nNPM stands for the Node Package Manager. A text based program for Nodejs package management.\nWhile NPX is a Node Package Runner. Its function is to execute the Nodejs package\nNPX will execute binary files from the Nodejs package, both installed and not.\nEven NPX can also help us use certain versions of Nodejs without having to use nvm (node.js version management), nave (node.js virtual environment), and nvm (node.js version management).\n\n\nA39:\n\nnpx is a npm package runner (x probably stands for eXecute). One common way to use npx is to download and run a package temporarily or for trials.\ncreate-react-app is an npm package that is expected to be run only once in a project's lifecycle. Hence, it is preferred to use npx to install and run it in a single step.\nAs mentioned in the main page https://www.npmjs.com/package/npx, npx can run commands in the PATH or from node_modules/.bin by default.\nNote:\nWith some digging, we can find that create-react-app points to a Javascript file (possibly to /usr/lib/node_modules/create-react-app/index.js on Linux systems) that is executed within the node environment. This is simply a global tool that does some checks. The actual setup is done by react-scripts, whose latest version is installed in the project. Refer https://github.com/facebook/create-react-app for more info.\n\n\nA40:\n\nThe differences between NPM and NPX are as below:\ni) NPM is used to install the packages while NPX is used to execute the packages.\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages used by npx don't need to be taken care of as they are not installed globally.\n\n\nA41:\n\nSimple answer is like\nNPX: is used to execute any node package without installing the package on our machine.\nNPM: is used to install any node js package in our machine. We can use \"require(\"package-name')\" when we install any package using NPM. but we can not import the package when we use NPX.\nExample: You should run npm i axios\nin this case you are installing axios package in your local machine\nand npx create-react-app 'app-name'\nhere you are executing the create-react-app package directly on your machine without installing it's files.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "   ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 12, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nHow do I add a new key to an existing dictionary? It doesn't have an .add() method.\n\n\nCandidate answers:\nA1:\n\nWhat if you wanted a key which was a method, such as __eq__ or __getattr__?\nAnd you wouldn't be able to have an entry that didn't start with a letter, so using 0343853 as a key is out.\nAnd what if you didn't want to use a string?\n\n\nA2:\n\nCaveat emptor: For some reasons classes like this seem to break the multiprocessing package.  I just struggled with this bug for awhile before finding this SO: \nFinding exception in python multiprocessing\n\n\nA3:\n\nThis doesn't address the original question, but should be useful for people that, like me, end up here when looking for a lib that provides this functionality.\nAddict it's a great lib for this: https://github.com/mewwts/addict it takes care of many concerns mentioned in previous answers.\nAn example from the docs:\nbody = {\n    'query': {\n        'filtered': {\n            'query': {\n                'match': {'description': 'addictive'}\n            },\n            'filter': {\n                'term': {'created_by': 'Mats'}\n            }\n        }\n    }\n}\n\nWith addict:\nfrom addict import Dict\nbody = Dict()\nbody.query.filtered.query.match.description = 'addictive'\nbody.query.filtered.filter.term.created_by = 'Mats'\n\n\n\nA4:\n\nYou create a new key/value pair on a dictionary by assigning a value to that key\nd = {'key': 'value'}\nprint(d)  # {'key': 'value'}\n\nd['mynewkey'] = 'mynewvalue'\n\nprint(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\n\nIf the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten.\n\n\nA5:\n\nTo add onto @fortran's answer, if you want to iterate over a list of keys key_list as well as a list of values value_list:\nd = dict((key, value) for (key, value) in zip(key_list, value_list))\n\nor\nd = {key: value for (key, value) in zip(key_list, value_list)}\n\n\n\nA6:\n\nHere's a short example of immutable records using built-in collections.namedtuple:\ndef record(name, d):\n    return namedtuple(name, d.keys())(**d)\n\nand a usage example:\nrec = record('Model', {\n    'train_op': train_op,\n    'loss': loss,\n})\n\nprint rec.loss(..)\n\n\n\nA7:\n\n>>> {k: v**3 for (k, v) in zip(string.ascii_lowercase, range(26))}\n\nPython supports dict comprehensions, which allow you to express the creation of dictionaries at runtime using a similarly concise syntax.\nA dictionary comprehension takes the form {key: value for (key, value) in iterable}. This syntax was introduced in Python 3 and backported as far as Python 2.7, so you should be able to use it regardless of which version of Python you have installed.\nA canonical example is taking two lists and creating a dictionary where the item at each position in the first list becomes a key and the item at the corresponding position in the second list becomes the value.\nThe zip function used inside this comprehension returns an iterator of tuples, where each element in the tuple is taken from the same position in each of the input iterables. In the example above, the returned iterator contains the tuples (\u201ca\u201d, 1), (\u201cb\u201d, 2), etc.\nOutput:\n{'i': 512, 'e': 64, 'o': 2744, 'h': 343, 'l': 1331, 's': 5832, 'b': 1, 'w': 10648, 'c': 8, 'x': 12167, 'y': 13824, 't': 6859, 'p': 3375, 'd': 27, 'j': 729, 'a': 0, 'z': 15625, 'f': 125, 'q': 4096, 'u': 8000, 'n': 2197, 'm': 1728, 'r': 4913, 'k': 1000, 'g': 216, 'v': 9261}\n\n\n\nA8:\n\nJust to add some variety to the answer, sci-kit learn has this implemented as a Bunch:\nclass Bunch(dict):                                                              \n    \"\"\" Scikit Learn's container object                                         \n\n    Dictionary-like object that exposes its keys as attributes.                 \n    >>> b = Bunch(a=1, b=2)                                                     \n    >>> b['b']                                                                  \n    2                                                                           \n    >>> b.b                                                                     \n    2                                                                           \n    >>> b.c = 6                                                                 \n    >>> b['c']                                                                  \n    6                                                                           \n    \"\"\"                                                                         \n\n    def __init__(self, **kwargs):                                               \n        super(Bunch, self).__init__(kwargs)                                     \n\n    def __setattr__(self, key, value):                                          \n        self[key] = value                                                       \n\n    def __dir__(self):                                                          \n        return self.keys()                                                      \n\n    def __getattr__(self, key):                                                 \n        try:                                                                    \n            return self[key]                                                    \n        except KeyError:                                                        \n            raise AttributeError(key)                                           \n\n    def __setstate__(self, state):                                              \n        pass                       \n\nAll you need is to get the setattr and getattr methods - the getattr checks for dict keys and the moves on to checking for actual attributes. The setstaet is a fix for fix for pickling/unpickling \"bunches\" - if inerested check https://github.com/scikit-learn/scikit-learn/issues/6196\n\n\nA9:\n\nAfter not being satisfied with the existing options for the reasons below I developed MetaDict. It behaves exactly like dict but enables dot notation and IDE autocompletion without the shortcomings and potential namespace conflicts of other solutions. All features and usage examples can be found on GitHub (see link above).\nFull disclosure: I am the author of MetaDict.\nShortcomings/limitations I encountered when trying out other solutions:\n\nAddict\n\nNo key autocompletion in IDE\nNested key assignment cannot be turned off\nNewly assigned dict objects are not converted to support attribute-style key access\nShadows inbuilt type Dict\n\n\nProdict\n\nNo key autocompletion in IDE without defining a static schema (similar to dataclass)\nNo recursive conversion of dict objects when embedded in list or other inbuilt iterables\n\n\nAttrDict\n\nNo key autocompletion in IDE\nConverts list objects to tuple behind the scenes\n\n\nMunch\n\nInbuilt methods like items(), update(), etc. can be overwritten with obj.items = [1, 2, 3]\nNo recursive conversion of dict objects when embedded in list or other inbuilt iterables\n\n\nEasyDict\n\nOnly strings are valid keys, but dict accepts all hashable objects as keys\nInbuilt methods like items(), update(), etc. can be overwritten with obj.items = [1, 2, 3]\nInbuilt methods don't behave as expected: obj.pop('unknown_key', None) raises an AttributeError\n\n\n\n\n\nA10:\n\nthis approach uses iteration over the given date using a for loop.\nSyntax: {key: value for (key, value) in data}\n\nEg:\n# create a list comprehension with country and code:\n    Country_code = [('China', 86), ('USA', 1),\n            ('Ghana', 233), ('Uk', 44)]\n\n# use iterable method to show results\n{key: value for (key, value) in Country_code}\n\n\n\nA11:\n\nHere is another example of dictionary creation using dict comprehension:\nWhat i am tring to do here is to create a alphabet dictionary where each pair; is the english letter and its corresponding position in english alphabet\n>>> import string\n>>> dict1 = {value: (int(key) + 1) for key, value in \nenumerate(list(string.ascii_lowercase))}\n>>> dict1\n{'a': 1, 'c': 3, 'b': 2, 'e': 5, 'd': 4, 'g': 7, 'f': 6, 'i': 9, 'h': 8, \n'k': 11, 'j': 10, 'm': 13, 'l': 12, 'o': 15, 'n': 14, 'q': 17, 'p': 16, 's': \n19, 'r': 18, 'u': 21, 't': 20, 'w': 23, 'v': 22, 'y': 25, 'x': 24, 'z': 26}\n>>> \n\nNotice the use of enumerate here to get a list of alphabets and their indexes in the list and swapping the alphabets and indices to generate the key value pair for dictionary\nHope it gives a good idea of dictionary comp to you and encourages you to use it more often to make your code compact\n\n\nA12:\n\nFrom This other SO question there's a great implementation example that simplifies your existing code.  How about:\nclass AttributeDict(dict):\n    __slots__ = () \n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n\nMuch more concise and doesn't leave any room for extra cruft getting into your __getattr__ and __setattr__ functions in the future.\n\n\nA13:\n\nUpdate - 2020\nSince this question was asked almost ten years ago, quite a bit has changed in Python itself since then.\nWhile the approach in my original answer is still valid for some cases, (e.g. legacy projects stuck to older versions of Python and cases where you really need to handle dictionaries with very dynamic string keys), I think that in general the dataclasses introduced in Python 3.7 are the obvious/correct solution to vast majority of the use cases of AttrDict.\nOriginal answer\nThe best way to do this is:\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\nSome pros:\n\nIt actually works!\nNo dictionary class methods are shadowed (e.g. .keys() work just fine. Unless - of course - you assign some value to them, see below)\nAttributes and items are always in sync\nTrying to access non-existent key as an attribute correctly raises AttributeError instead of KeyError\nSupports [Tab] autocompletion (e.g. in jupyter & ipython)\n\nCons:\n\nMethods like .keys() will not work just fine if they get overwritten by incoming data\nCauses a memory leak in Python < 2.7.4 / Python3 < 3.2.3\nPylint goes bananas with E1123(unexpected-keyword-arg) and E1103(maybe-no-member)\nFor the uninitiated it seems like pure magic.\n\nA short explanation on how this works\n\nAll python objects internally store their attributes in a dictionary that is named __dict__.\nThere is no requirement that the internal dictionary __dict__ would need to be \"just a plain dict\", so we can assign any subclass of dict() to the internal dictionary.\nIn our case we simply assign the AttrDict() instance we are instantiating (as we are in __init__).\nBy calling super()'s __init__() method we made sure that it (already) behaves exactly like a dictionary, since that function calls all the dictionary instantiation code.\n\nOne reason why Python doesn't provide this functionality out of the box\nAs noted in the \"cons\" list, this combines the namespace of stored keys (which may come from arbitrary and/or untrusted data!) with the namespace of builtin dict method attributes. For example:\nd = AttrDict()\nd.update({'items':[\"jacket\", \"necktie\", \"trousers\"]})\nfor k, v in d.items():    # TypeError: 'list' object is not callable\n    print \"Never reached!\"\n\n\n\nA14:\n\nWherein I Answer the Question That Was Asked\nWhy doesn't Python offer it out of the box?\nI suspect that it has to do with the Zen of Python: \"There should be one -- and preferably only one -- obvious way to do it.\"  This would create two obvious ways to access values from dictionaries: obj['key'] and obj.key.\nCaveats and Pitfalls\nThese include possible lack of clarity and confusion in the code.  i.e., the following could be confusing to someone else who is going in to maintain your code at a later date, or even to you, if you're not going back into it for awhile.  Again, from Zen: \"Readability counts!\"\n>>> KEY = 'spam'\n>>> d[KEY] = 1\n>>> # Several lines of miscellaneous code here...\n... assert d.spam == 1\n\nIf d is instantiated or KEY is defined or d[KEY] is assigned far away from where d.spam is being used, it can easily lead to confusion about what's being done, since this isn't a commonly-used idiom.  I know it would have the potential to confuse me.\nAdditonally, if you change the value of KEY as follows (but miss changing d.spam), you now get:\n>>> KEY = 'foo'\n>>> d[KEY] = 1\n>>> # Several lines of miscellaneous code here...\n... assert d.spam == 1\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\nAttributeError: 'C' object has no attribute 'spam'\n\nIMO, not worth the effort.\nOther Items\nAs others have noted, you can use any hashable object (not just a string) as a dict key.  For example,\n>>> d = {(2, 3): True,}\n>>> assert d[(2, 3)] is True\n>>> \n\nis legal, but\n>>> C = type('C', (object,), {(2, 3): True})\n>>> d = C()\n>>> assert d.(2, 3) is True\n  File \"<stdin>\", line 1\n  d.(2, 3)\n    ^\nSyntaxError: invalid syntax\n>>> getattr(d, (2, 3))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: getattr(): attribute name must be string\n>>> \n\nis not.  This gives you access to the entire range of printable characters or other hashable objects for your dictionary keys, which you do not have when accessing an object attribute.  This makes possible such magic as a cached object metaclass, like the recipe from the Python Cookbook (Ch. 9).\nWherein I Editorialize\nI prefer the aesthetics of spam.eggs over spam['eggs'] (I think it looks cleaner), and I really started craving this functionality when I met the namedtuple.  But the convenience of being able to do the following trumps it.\n>>> KEYS = 'spam eggs ham'\n>>> VALS = [1, 2, 3]\n>>> d = {k: v for k, v in zip(KEYS.split(' '), VALS)}\n>>> assert d == {'spam': 1, 'eggs': 2, 'ham': 3}\n>>>\n\nThis is a simple example, but I frequently find myself using dicts in different situations than I'd use obj.key notation (i.e., when I need to read prefs in from an XML file).  In other cases, where I'm tempted to instantiate a dynamic class and slap some attributes on it for aesthetic reasons, I continue to use a dict for consistency in order to enhance readability.\nI'm sure the OP has long-since resolved this to his satisfaction, but if he still wants this functionality, then I suggest he download one of the packages from pypi that provides it:\n\nBunch is the one I'm more familiar with.  Subclass of dict, so you have all that functionality.\nAttrDict also looks like it's also pretty good, but I'm not as familiar with it and haven't looked through the source in as much detail as I have Bunch.\nAddict Is actively maintained and provides attr-like access and more.\nAs noted in the comments by Rotareti, Bunch has been deprecated, but there is an active fork called Munch.\n\nHowever, in order to improve readability of his code I strongly recommend that he not mix his notation styles.  If he prefers this notation then he should simply instantiate a dynamic object, add his desired attributes to it, and call it a day:\n>>> C = type('C', (object,), {})\n>>> d = C()\n>>> d.spam = 1\n>>> d.eggs = 2\n>>> d.ham = 3\n>>> assert d.__dict__ == {'spam': 1, 'eggs': 2, 'ham': 3}\n\n\nWherein I Update, to Answer a Follow-Up Question in the Comments\nIn the comments (below), Elmo asks:\n\nWhat if you want to go one deeper? ( referring to type(...) )\n\nWhile I've never used this use case (again, I tend to use nested dict, for\nconsistency), the following code works:\n>>> C = type('C', (object,), {})\n>>> d = C()\n>>> for x in 'spam eggs ham'.split():\n...     setattr(d, x, C())\n...     i = 1\n...     for y in 'one two three'.split():\n...         setattr(getattr(d, x), y, i)\n...         i += 1\n...\n>>> assert d.spam.__dict__ == {'one': 1, 'two': 2, 'three': 3}\n\n\n\nA15:\n\nYou can create a new dict for each pair and merge it with the previous dict:\nreduce(lambda p, q: {**p, **{q[0]: q[1]}}, bla bla bla, {})\n\nObviously this approaches requires reduce from functools.\n\n\nA16:\n\nYou can have all legal string characters as part of the key if you use array notation.\nFor example, obj['!#$%^&*()_']\n\n\nA17:\n\nIn Python 3 and Python 2.7+, dictionary comprehensions look like the below:\nd = {k:v for k, v in iterable}\n\nFor Python 2.6 or earlier, see fortran's answer.\n\n\nA18:\n\nApparently there is now a library for this - https://pypi.python.org/pypi/attrdict - which implements this exact functionality plus recursive merging and json loading. Might be worth a look.\n\n\nA19:\n\nUse a dict comprehension (Python 2.7 and later):\n{key: value for key, value in zip(keys, values)}\n\n\nAlternatively, use the dict constructor (for str keys only):\npairs = [('a', 1), ('b', 2)]\ndict(pairs)                          # \u2192 {'a': 1, 'b': 2}\ndict((k, v + 10) for k, v in pairs)  # \u2192 {'a': 11, 'b': 12}\n\nGiven separate lists of keys and values, use the dict constructor with zip:\nkeys = ['a', 'b']\nvalues = [1, 2]\ndict(zip(keys, values))              # \u2192 {'a': 1, 'b': 2}\n\n\n\nA20:\n\nPython version >= 2.7, do the below:\nd = {i: True for i in [1,2,3]}\n\nPython version < 2.7(RIP, 3 July 2010 - 31 December 2019), do the below:\nd = dict((i,True) for i in [1,2,3])\n\n\n\nA21:\n\n\nCreate a dictionary with list comprehension in Python\nI like the Python list comprehension syntax.\nCan it be used to create dictionaries too? For example, by iterating\nover pairs of keys and values:\nmydict = {(k,v) for (k,v) in blah blah blah}\n\n\nYou're looking for the phrase \"dict comprehension\" - it's actually:\nmydict = {k: v for k, v in iterable}\n\nAssuming blah blah blah is an iterable of two-tuples - you're so close. Let's create some \"blahs\" like that:\nblahs = [('blah0', 'blah'), ('blah1', 'blah'), ('blah2', 'blah'), ('blah3', 'blah')]\n\nDict comprehension syntax:\nNow the syntax here is the mapping part. What makes this a dict comprehension instead of a set comprehension (which is what your pseudo-code approximates) is the colon, : like below:\nmydict = {k: v for k, v in blahs}\n\nAnd we see that it worked, and should retain insertion order as-of Python 3.7:\n>>> mydict\n{'blah0': 'blah', 'blah1': 'blah', 'blah2': 'blah', 'blah3': 'blah'}\n\nIn Python 2 and up to 3.6, order was not guaranteed:\n>>> mydict\n{'blah0': 'blah', 'blah1': 'blah', 'blah3': 'blah', 'blah2': 'blah'}\n\nAdding a Filter:\nAll comprehensions feature a mapping component and a filtering component that you can provide with arbitrary expressions.\nSo you can add a filter part to the end:\n>>> mydict = {k: v for k, v in blahs if not int(k[-1]) % 2}\n>>> mydict\n{'blah0': 'blah', 'blah2': 'blah'}\n\nHere we are just testing for if the last character is divisible by 2 to filter out data before mapping the keys and values.\n\n\nA22:\n\ndictionary[key] = value\n\n\n\nA23:\n\nI found myself wondering what the current state of \"dict keys as attr\" in the python ecosystem. As several commenters have pointed out, this is probably not something you want to roll your own from scratch, as there are several pitfalls and footguns, some of them very subtle. Also, I would not recommend using Namespace as a base class, I've been down that road, it isn't pretty.\nFortunately, there are several open source packages providing this functionality, ready to pip install! Unfortunately, there are several packages. Here is a synopsis, as of Dec 2019.\nContenders (most recent commit to master|#commits|#contribs|coverage%):\n\naddict   (2021-01-05 | 229 | 22 | 100%)\nmunch (2021-01-22 | 166 | 17 | ?%)\neasydict (2021-02-28 | 54 | 7 | ?%)\nattrdict (2019-02-01 | 108 | 5 | 100%)\nprodict  (2021-03-06 | 100 | 2 | ?%)\n\nNo longer maintained or under-maintained:\n\ntreedict (2014-03-28 | 95 | 2 | ?%)\nbunch (2012-03-12 | 20 | 2 | ?%)\nNeoBunch\n\nI currently recommend munch or addict. They have the most commits, contributors, and releases, suggesting a healthy open-source codebase for each. They have the cleanest-looking readme.md, 100% coverage, and good looking set of tests.\nI do not have a dog in this race (for now!), besides having rolled my own dict/attr code and wasted a ton of time because I was not aware of all these options :). I may contribute to addict/munch in the future as I would rather see one solid package than a bunch of fragmented ones. If you like them, contribute! In particular, looks like munch could use a codecov badge and addict could use a python version badge.\naddict pros:\n\nrecursive initialization (foo.a.b.c = 'bar'), dict-like arguments become addict.Dict\n\naddict cons:\n\nshadows typing.Dict if you from addict import Dict\nNo key checking. Due to allowing recursive init, if you misspell a key, you just create a new attribute, rather than KeyError (thanks  AljoSt)\n\nmunch pros:\n\nunique naming\nbuilt-in ser/de functions for JSON and YAML\n\nmunch cons:\n\nno recursive init (you cannot construct foo.a.b.c = 'bar', you must set foo.a, then foo.a.b, etc.\n\nWherein I Editorialize\nMany moons ago, when I used text editors to write python, on projects with only myself or one other dev, I liked the style of dict-attrs, the ability to insert keys by just declaring foo.bar.spam = eggs. Now I work on teams, and use an IDE for everything, and I have drifted away from these sorts of data structures and dynamic typing in general, in favor of static analysis, functional techniques and type hints. I've started experimenting with this technique, subclassing Pstruct with objects of my own design:\nclass  BasePstruct(dict):\n    def __getattr__(self, name):\n        if name in self.__slots__:\n            return self[name]\n        return self.__getattribute__(name)\n\n    def __setattr__(self, key, value):\n        if key in self.__slots__:\n            self[key] = value\n            return\n        if key in type(self).__dict__:\n            self[key] = value\n            return\n        raise AttributeError(\n            \"type object '{}' has no attribute '{}'\".format(type(self).__name__, key))\n\n\nclass FooPstruct(BasePstruct):\n    __slots__ = ['foo', 'bar']\n\n\nThis gives you an object which still behaves like a dict, but also lets you access keys like attributes, in a much more rigid fashion. The advantage here is I (or the hapless consumers of your code) know exactly what fields can and can't exist, and the IDE can autocomplete fields. Also subclassing vanilla dict means json serialization is easy. I think the next evolution in this idea would be a custom protobuf generator which emits these interfaces, and a nice knock-on is you get cross-language data structures and IPC via gRPC for nearly free.\nIf you do decide to go with attr-dicts, it's essential to document what fields are expected, for your own (and your teammates') sanity.\nFeel free to edit/update this post to keep it recent!\n\n\nA24:\n\nIt doesn't work in generality.  Not all valid dict keys make addressable attributes (\"the key\").  So, you'll need to be careful.\nPython objects are all basically dictionaries.  So I doubt there is much performance or other penalty.\n\n\nA25:\n\nIn fact, you don't even need to iterate over the iterable if it already comprehends some kind of mapping, the dict constructor doing it graciously for you:\n>>> ts = [(1, 2), (3, 4), (5, 6)]\n>>> dict(ts)\n{1: 2, 3: 4, 5: 6}\n>>> gen = ((i, i+1) for i in range(1, 6, 2))\n>>> gen\n<generator object <genexpr> at 0xb7201c5c>\n>>> dict(gen)\n{1: 2, 3: 4, 5: 6}\n\n\n\nA26:\n\nYes, it's possible. In python, Comprehension can be used in List, Set, Dictionary, etc.\nYou can write it this way\nmydict = {k:v for (k,v) in blah}\n\nAnother detailed example of Dictionary Comprehension with the Conditional Statement and Loop:\nparents = [father, mother]\n            \nparents = {parent:1 - P[\"mutation\"] if parent in two_genes else 0.5 if parent in one_gene else P[\"mutation\"] for parent in parents}\n\n\n\nA27:\n\nAdding to @Ekhtiar answer, if you want to make look up dict from list, you can use this:\nnames = ['a', 'b', 'd', 'f', 'c']\nnames_to_id = {v:k for k, v in enumerate(names)}\n# {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'f': 4}\n\nOr in rare case that you want to filter duplicate, use set first (best in list of number):\nnames = ['a', 'b', 'd', 'f', 'd', 'c']\nsorted_list = list(set(names))\nsorted_list.sort()\nnames_to_id = {v:k for k, v in enumerate(sorted_list)}\n# {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'f': 4}\n\nnames = [1,2,5,5,6,2,1]\nnames_to_id = {v:k for k, v in enumerate(set(names))}\n# {1: 0, 2: 1, 5: 2, 6: 3}\n\n\n\nA28:\n\nIn Python 2.7, it goes like:\n>>> list1, list2 = ['a', 'b', 'c'], [1,2,3]\n>>> dict( zip( list1, list2))\n{'a': 1, 'c': 3, 'b': 2}\n\nZip them!\n\n\nA29:\n\nJust to throw in another example. Imagine you have the following list:\nnums = [4,2,2,1,3]\n\nand you want to turn it into a dict where the key is the index and value is the element in the list. You can do so with the following line of code:\n{index:nums[index] for index in range(0,len(nums))}\n\n\n\nA30:\n\nAssuming blah blah blah is a two-tuples list:\nLet's see two methods:\n# method 1\n>>> lst = [('a', 2), ('b', 4), ('c', 6)]\n>>> dict(lst)\n{'a': 2, 'b': 4, 'c': 6}\n\n# method 2\n>>> lst = [('a', 2), ('b', 4), ('c', 6)]\n>>> d = {k:v for k, v in lst}\n>>> d\n{'a': 2, 'b': 4, 'c': 6}\n\n\n\nA31:\n\nTry this,\ndef get_dic_from_two_lists(keys, values):\n    return { keys[i] : values[i] for i in range(len(keys)) }\n\nAssume we have two lists country and capital\ncountry = ['India', 'Pakistan', 'China']\ncapital = ['New Delhi', 'Islamabad', 'Beijing']\n\nThen create dictionary from the two lists:\nprint get_dic_from_two_lists(country, capital)\n\nThe output is like this,\n{'Pakistan': 'Islamabad', 'China': 'Beijing', 'India': 'New Delhi'}\n\n\n\nA32:\n\ntuples can be used dict keys. How would you access tuple in your construct?\nAlso, namedtuple is a convenient structure which can provide values via the attribute access.\n\n\nA33:\n\nHow about Prodict, the little Python class that I wrote to rule them all:)\nPlus, you get auto code completion, recursive object instantiations and auto type conversion!\nYou can do exactly what you asked for:\np = Prodict()\np.foo = 1\np.bar = \"baz\"\n\nExample 1: Type hinting\nclass Country(Prodict):\n    name: str\n    population: int\n\nturkey = Country()\nturkey.name = 'Turkey'\nturkey.population = 79814871\n\n\nExample 2: Auto type conversion\ngermany = Country(name='Germany', population='82175700', flag_colors=['black', 'red', 'yellow'])\n\nprint(germany.population)  # 82175700\nprint(type(germany.population))  # <class 'int'>\n\nprint(germany.flag_colors)  # ['black', 'red', 'yellow']\nprint(type(germany.flag_colors))  # <class 'list'>\n\n\n\nA34:\n\nThis code will create dictionary using list comprehension for multiple lists with different values that can be used for pd.DataFrame()\n#Multiple lists \nmodel=['A', 'B', 'C', 'D']\nlaunched=[1983,1984,1984,1984]\ndiscontinued=[1986, 1985, 1984, 1986]\n\n#Dictionary with list comprehension\nkeys=['model','launched','discontinued']\nvals=[model, launched,discontinued]\ndata = {key:vals[n] for n, key in enumerate(keys)}\n\n#Convert dict to dataframe\ndf=pd.DataFrame(data)\ndisplay(df)\n\nenumerate will pass n to vals to match each key with its list\n\n\nA35:\n\nYou can pull a convenient container class from the standard library:\nfrom argparse import Namespace\n\nto avoid having to copy around code bits. No standard dictionary access, but easy to get one back if you really want it. The code in argparse is simple,\nclass Namespace(_AttributeHolder):\n    \"\"\"Simple object for storing attributes.\n\n    Implements equality by attribute names and values, and provides a simple\n    string representation.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        for name in kwargs:\n            setattr(self, name, kwargs[name])\n\n    __hash__ = None\n\n    def __eq__(self, other):\n        return vars(self) == vars(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __contains__(self, key):\n        return key in self.__dict__\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n(2)"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 13, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nOn http://github.com developer keep the HTML, CSS, JavaScript and images files of the project. How can I see the HTML output in browser?\nFor example this: https://github.com/necolas/css3-social-signin-buttons/blob/master/index.html\nWhen I open this it doesn't show the rendered HTML of the code of author. It shows the page as a source code.\nIs it possible to see it as rendered HTML directly? Otherwise I always need to download the whole ZIP just to see the result.\n\n\nCandidate answers:\nA1:\n\nThis error mostly caused by WRONG URL, please check:\n\nhttp or https\nURL Name\nusername@git_url\nwrong git name\n\n\n\nA2:\n\nIn my case, this was happening with a submodule that had defaulted to https even though the main project was using ssh. I fixed it by changing the submodule's repo path in SourceTree's Repository Settings to use the ssh path instead of https.\n\n\nA3:\n\nA solution worked for me given by Andrew Magill, Copied from Source : link\nThe problem is that SourceTree's embedded Git client comes with git-credential-manager v1.12, which no longer works with Github since they disabled TLS 1.1 on their site.  You need to get git-credential-manager v1.14.  Easiest way to do that is to install a current copy of the Git client separately, and then switch SourceTree over to use that (\"system git\") instead of its embedded client.  Alternatively, you can update git-credential-manager in your embedded client by replacing its files with the newest version from Microsoft.\n\n\nA4:\n\nI got this issue when using GitStack. I looked into C:/GitStack/apache/logs/error.log and found that GitStack was looking for the repository in the wrong location although its settings said D:/Repositories (my custom location), GitStack looked for the repositories in its default location. So I simply went and save the settings again from GitStack admin page. It solved the problem.\n\n\nA5:\n\nIf you don't want to download an archive you can use GitHub Pages to render this.\n\nFork the repository to your account.\nClone it locally on your machine\nCreate a gh-pages branch (if one already exists, remove it and create a new one based off master).\nPush the branch back to GitHub.\nView the pages at http://username.github.io/repo`\n\nIn code:\ngit clone git@github.com:username/repo.git\ncd repo\ngit branch gh-pages\n# Might need to do this first: git branch -D gh-pages\ngit push -u origin gh-pages # Push the new branch back to github\nGo to http://username.github.io/repo\n\n\n\nA6:\n\nI think it is very preferable if you first pull the existing Github repo on the local and then add the new files to the Github repo\nThis link will help: https://stackoverflow.com/a/61285076/5840973\n\n\nA7:\n\nif you have more files in my case i have 7000 image files when i try to add them from project's route folder it hasn't added them but when i go to the image folder everything is ok. Go through the target folder and command like abows\ngit add .\ngit commit -am \"image uploading\"\ngit push origin master\n\n\n\ngit push origin master Enumerating objects: 6574, done. Counting objects: 100% (6574/6574), done. Delta compression using up to 4\n    threads Compressing objects: 100% (6347/6347), done. Writing objects: \n    28% (1850/6569), 142.17 MiB | 414.00 KiB/s\n\n\n\n\nA8:\n\nThe most comfortable way to preview HTML files on GitHub is to go to https://htmlpreview.github.io/ or just prepend it to the original URL, i.e.: https://htmlpreview.github.io/?https://github.com/bartaz/impress.js/blob/master/index.html\n\n\nA9:\n\nIf you are not using GitHub 2FA, another reason for asking you your password is:  \nYour GitHub repo url is an ssh one, not an https one.\nSince git does not find your ssh keys (or your ssh key is not registered to your GitHub account), it fails back to account/password mechanism. \nIf you are using 2FA (and an https url), then you need a personal token as a password.\n\n\nA10:\n\nI had a similar thing after an update; I tried re-adding my bitbucket credentials, changing the protocols, etc. with no luck, until...\n\nI opened my keychain and searched for bitbucket (probably the same for github, just search for github instead) under the Passwords category. \nI found that I had multiple Access Keys for my user, so I closed my repository, deleted all the keys of \"application password\" Kind, opened the SourceTree preferences > Accounts > removed my account and re-added it (login as usual - I used Basic with HTTPS). \nKeychain asks for permission to save the password, which I said yes and now I only see one key of the kind \"application password\". \n\nI opened my repository from the repo browser and hey presto, no more password popups!\nThis bugged me for a whole week! If this helped you, you can do a little dance with me now :)\n\n\nA11:\n\nMaybe an obvious thing, but...\nIf you have problem with the index, use git-gui. You get a very good view how the index (staging area) actually works.\nAnother source of information that helped me understand the index was Scott Chacons \"Getting Git\" page 259 and forward.\nI started off using the command line because most documentation only showed that...\nI think git-gui and gitk actually make me work faster, and I got rid of bad habits like \"git pull\" for example... Now I always fetch first... See what the new changes really are before I merge.\n\n\nA12:\n\nThe easiest way is to create an app password and use that password.\nHere you can see how you can create an app password on Bitbucket:\nhttps://docs.snyk.io/tutorials/getting-started/atlassian-integrations/atlassian-bitbucket-app-password\n\n\nA13:\n\nThis happened to me and at first it was not apparent what had gone wrong. The previous day I had renamed my repo on github.com only changing the first character of the repo name from a lowercase letter to uppercase. I was not aware that the repo name in the origin URL was case-sensitive unit I received the error which contained the text, \"did you run git update-server-info on the server.\" Once I edited the .git/config file to correct the case of the same letter in the origin URL, the problem was solved, the error no longer occurred and I was once again able to push my changes up to the repo on github.com. Another bit of advice: if you make changes to your repo on github, make sure to test sync'ing your local repo right away. In my case, I did not and it was the next day when I had tried to sync and couldn't because of the error and I forgot that I had renamed the repo, so I was scratching my head for a bit. Had I tested sync'ing right away, I would have caught this problem right away.\n\n\nA14:\n\nDid you create a new repository on the http://github.com with the same name? \nIf not, do it! And make sure each letter is correct and case sensitive.\n\n\nA15:\n\nIf you are using two-factor authentication with GitHub you will need to create a personal access token and use it with SourceTree:\n\nTo work with GitHub's two-factor authentication in SourceTree you can simply use your access token instead of your password. The steps to do this are as follows:\n\nGo to your Personal Access Tokens settings in GitHub.\nClick on the Generate new token button.\nName the token something descriptive.\nSelect which scopes you wish to grant this token.\nClick the Generate token button.\nCopy the token and use it as a password in your hosted repositories.\n\nYou can find more information about this on GitHub's help here.\n\nMac Users:\nIf your SourceTree keeps on asking for the password, go to the Terminal and type this:\ngit config --global credential.helper osxkeychain\n\n\n\nA16:\n\nSince this question is specifically about SourceTree and Github, the answer in 2021 is simple. Quoting bcr:\n\nEnsured I was using OAuth for my GitHub account stored in Tools ->\nOptions -> Authentication\n\n\nGo there. Select Add (new account).\nSelect OAuth.\nClick Refresh OAuth Token. A browser page will open (github.com) requesting renewal of your token.\nLogin to accept. SourceTree will immediately respond by acknowledging the token provided by github.com.\n\n\n\nA17:\n\nJust another stupid reason: you may have changed credentials on your laptop, but there is another development PC running but not frequently used, that has SourceTree started and uses the old credentials frequently to poll status.\nSounds stupid, but took me some time to solve :-)\n\n\nA18:\n\nYou didn't add the changes. Either specifically add them via\ngit add filename1 filename2\n\nor add all changes (from root path of the project)\ngit add .\n\nor use the shorthand -a while commiting:\ngit commit -a -m \"message\".\n\n\n\nA19:\n\nI got the same problem while using a github repository, and connecting to it via https, while using the OS X Keychain Credential helper.\nMy problem was that I had the wrong credentials stored in OS X's Keychain (I was using the email address that I used to sign up for github.com rather than the [username]@github.com address it provides you). I deleted the old account in the keychain and only left the @github.com one and it fixed the problem.\nNot sure if it is related, but when I checked the user.email local config:\ngit config -l\n\nit showed the incorrect email address as well, so I updated the local git user.email to use the correct account too:\ngit config user.email <username>@github.com\n\n\n\nA20:\n\n1. first create a git repostry.\n2. second open git bash in  existing or uploading project.\n3. perform git init \n4. git add .\n5. git commit -m \"print message\"\n6. git remote add github<repostry url>\n7. git remote -v\n8. git push github master\n\nOR\ngit push origin master\n\nif  you get any error, you may use it\ngit push -f origin master\n\n\n\nA21:\n\nIn my case I had old version of the git. Upgrade to latest version fixed this issue.\n\n\nA22:\n\nFor future references, I solved this problem by changing the GIT system used in Sourcetree, from Embedded to System.\nHow to change the git system:\n \n1. Find the Options submenu (located under Tools menu) and click on it to view the Options popup \n2. On the Options setting popup, find the Git tab, then click on it to view the Git setting \n3. Find Git version setting, then check the version used in it. In my case, I changed it to system because previously I'm using Git bash cmd, and it looks like that the git embedded in Sourcetree clashes with each other. \n4. Restart Sourcetree\n\n\nA23:\n\nHere is what worked for me.\nI tried almost all the solutions mentioned here and on the web a lot of times. I gave up multiple times before and was using it this way for more than a year I think. Finally, I was able to solve this annoying issue.\n\nAs suggested by user \"AEQ\" in his answer, I deleted all the passwords related to GitHub from my Keychain Access.\nTried deleting and re-adding the Github account. But still, the problem persisted. (generated a personal access token from \"https://github.com/settings/tokens\" and use it as the password)\n\nWhat made it work was\nMake sure to use HTTPS and not SSH when adding the new user account. If you have already added the account, just edit it. That's it.\nFinally, It feels good.\nAttaching screenshots for ease.\n\n\n\nA24:\n\nLets say you have remote repo with files and local repo with the same files.\nAnd you want to add Git to local files, and dont want to push.\nThen you can do such commands on local repo:\ngit init\ngit remote add origin <url>\ngit fetch --all\ngit reset --hard origin/master\n\nAfter that your repository is synced with remote repo.\nYou can change files, git add, push and so on.\n\n\nA25:\n\nMake sure that your user account is added to the repository as a collaborator.\nSetting --> Collaborators\n\n\nA26:\n\nIt looks like that's a private (or deleted) repository; if you visit the repository page while logged it'll give you the real URL, which'll probably be https://TeaCodie@github.com/TeaCodie/TeaCodie-Website.git , i.e. with a username specified?\n\n\nA27:\n\nThis error could also happen if the repository you are attempting to reach was deleted.\n\n\nA28:\n\nIn my case, the Sourcetree login screen repeatedly showed when pushing to GitHub. Logging in from that screen failed. I finally determined it was due to an expired OAuth token, which is trivial to fix.\n\nNavigate to the GitHub account config screen: Tools > Options > Authentication\nClick your GitHub account and then click edit\nOn the Edit Hosting Account dialog click the Refresh OAuth Token button\n\n\n\nA29:\n\nIn my repo's directory on my machine in the terminal I had to reset the origin url:\ngit remote set-url origin git@github.com:repoaccountname/repo-name.git\n\n\n\nA30:\n\nAs the message says:\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nGit has a \"staging area\" where files need to be added before being committed, you can read an explanation of it here.\n\nFor your specific example, you can use:\ngit commit -am \"save arezzo files\"\n\n(note the extra a in the flags, can also be written as git commit -a -m \"message\" - both do the same thing)\nAlternatively, if you want to be more selective about what you add to the commit, you use the git add command to add the appropriate files to the staging area, and git status to preview what is about to be added (remembering to pay attention to the wording used).\nYou can also find general documentation and tutorials for how to use git on the git documentation page which will give more detail about the concept of staging/adding files.\n\nOne other thing worth knowing about is interactive staging - this allows you to add parts of a file to the staging area, so if you've made three distinct code changes (for related but different functionality), you can use interactive mode to split the changes and add/commit each part in turn. Having smaller specific commits like this can be helpful.\n\n\nA31:\n\nI had more luck with navigating in my terminal to the directory I wanted to add to the repository, then (assuming you're working on a branch called master):\n    git init\n    git add .\n    git commit -m \"my commit\"\n    git remote add origin <remote repository URL>\n    git push origin master\n\nHere's a link to an article explaining how to do it in more detail: https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/\nNote that you won't be able to run the \"git add .\" line if the directory in question is open.\n\n\nA32:\n\nI find this problem appearing when I've done a git add . in a subdirectory below where my .gitignore file lives (the home directory of my repository, so to speak).  Try changing directories to your uppermost directory and running git add . followed by git commit -m \"my commit message\".\n\n\nA33:\n\nYou have to use -f when you are going to push on already existing repo.\ngit init\ngit add *\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin <repo url>\ngit push -f origin main\n\n\n\nA34:\n\nGo to the directory where you code is,\ngit init\ngit add .\ngit commit -m \"Your message\"\n\nNow add your address go to your git hub copy the clone address,\ngit remote add origin <remote repository URL>\n\nNow add push your code with,\ngit push -u -f origin master\n\nAnd you are done.\n\n\nA35:\n\nYou should do:\ngit commit . -m \"save arezzo files\"\n\n\n\nA36:\n\n\ngit init\ngit add .\ngit commit -m \"initial commit\"\ngit remote add origin [URL]\ngit push origin masteror\n\ngit push -f origin master\n\ngit push --force.\nIt does force the update\n\n\n\nA37:\n\nFollow to @Chris answer in case you've enabled 2FA, to use Personal Access Token in SourceTree, you need to add your acc with this token in SourceTree > Preferences with following information:\n\nAuth Type: Basic  \nUsername: {your-github-account} \nPassword: {your-personal-access-token} \nProtocol: HTTPS\n\nAfter that, you are able to clone your Github repo.\n\n\n\nA38:\n\nI copied a small sub project I had that was under Git source control into another project and forgot to delete the .git folder. When I went to commit I got the same message as above and couldn't clear it until I deleted the .git folder.\nIt is a bit silly, but it is worth checking you don't have a .git folder under the folder that doesn't commit.\n\n\nA39:\n\nIn my case, I cloned the repo using HTTPS based url. Then I configured my githib account with 2-factor authentication using SSH key. So whenever I was pulling/pushing changes, it was showing me dialog to provide my password.\nI fixed it by enabling SSH access by right clicking on project repo-menu on SourceTree.\n\n\n\nA40:\n\nopen terminal and clone your repo. repository url must have your usedid,\nFor ex:\n$ git clone https://<username>@github.com/<userId>/<reponame>.git\n$ git clone https://mark@github.com/mak123/<reponame>.git\n\nThen drag and drop the cloned folder into the sourceTree window.You can also do by clicking \"+New Repository\" button and from the menu by selecting \"Add Existing Local Repository\".\nIt may ask for password again but this time it will get added to your keychain.\n\n\nA41:\n\nI had an issue where I was doing commit --amend even after issuing a git add . and it still wasn't working.  Turns out I made some .vimrc customizations and my editor wasn't working correctly.  Fixing these errors so that vim returns the correct code resolved the issue.\n\n\nA42:\n\nif you have a subfolder, which was cloned from other git-Repository, first you have to remove the $.git$ file from the child-Repository:\nrm -rf .git\nafter that you can change to parent folder and use git add -A. \n\n\nA43:\n\nIn my case I needed to put my ssh password instead of my personal access token. Then it worked.\n\n\nA44:\n\nMy issue was that I used the clone https url widget provided by github. That URL doesn't work for private repositories as you need to add a username to the front of it.  \nExample: a private repo owned by john and named widget with collaborator sam the correct url would be: \nhttps://sam@github.com/john/widget.git\nThe github provided url: \nhttps://github.com/john/widget.git\nThe error message leaves much to be desired.\n\n\nA45:\n\nI had a very similar issue with the same error message. \"Changes not staged for commit\", yet when I do a diff it shows differences.  I finally figured out that a while back I had changed a directories case. ex. \"PostgeSQL\" to \"postgresql\".  As I remember now sometimes git will leave a file or two behind in the old case directory.  Then you will commit a new version to the new case.\nThus git doesn't know which one to rely on.  So to resolve it, I had to go onto the github's website. Then you're able to view both cases.  And you must delete all the files in the incorrect cased directory. Be sure that you have the correct version saved off or in the correct cased directory.\nOnce you have deleted all the files in the old case directory, that whole directory will disappear. Then do a commit.\nAt this point you should be able to do a Pull on your local computer and not see the conflicts any more.  Thus being able to commit again.  :)\n\n\nA46:\n\nThis is what I discovered (after searching for 'sourcetree' in my drive C:):\n\nUninstall SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\Atlassian\\SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\SourceTree\nReinstall SourceTree\n\n\n\nA47:\n\nIf you are using \"Username & Password\" authentication,you can try to change to use SSH keys for the authentication.\nI have met this kind of situation when I commit to GitLab everytime.When I changed the way of authentication:SSH keys,the problem has been solved.\n\n\nA48:\n\nAssume that I would like to add FreeRTOS repository, which URL is https://github.com/FreeRTOS/FreeRTOS-Kernel.git, into my repository, example URL is https://github.com/username/example as a submodule\ngit submodule add https://github.com/FreeRTOS/FreeRTOS-Kernel.git\ngit add .\ngit commit -m 'add a submodule'\ngit push\n\nTo clone using HTTPS:\ngit clone https://github.com/username/example.git --recurse-submodules\n\nUsing SSH:\ngit clone git@github.com:username/example.git --recurse-submodules\n\nIf you have downloaded the repo without using the --recurse-submodules argument, you need to run:\ngit submodule update --init --recursive\n\n\n\nA49:\n\nSourceTree version 2.4.8.0\nThe problem for me seemed to be the order of setup between SourceTree and GitHub. I setup SourceTree first and just clicked all the \"setup later\" options.\nAn easy fix - Uninstall and re-install SourceTree. The non-obvious bit was needing to delete the actual program files manually.\nFrom any \"SourceTree.exe\" shortcut, open file location. Mine forked at \"C:\\Users\\myusername\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Atlassian\" where the shortcut pointed to another shortcut. \nOpen that shortcut's file location. I eventually ended up at \"C:\\Users\\myusername\\AppData\\Local\\SourceTree\". Delete the whole \"SourceTree\" folder. Go up one level to \"...\\Local\" and delete the \"Atlassian\" folder as well.\nNow uninstall SourceTree. Make sure your GitHub is setup the way you desire (for me, I also created and saved an SSH key that I used for SourceTree setup). Re-install SourceTree using all the correct credentials, point to the right repository and such. \nAfter all this, SourceTree asked for my GitHub credentials twice, with two different dialog boxes, then stopped and worked! No more endless loop. Good luck!\n\n\nA50:\n\nSo i had this project that wasnt under source control i made some changes to and wanted to keep stuff i changed.\ngit init\ngit remote add origin <url>\ngit fetch\ngit branch master origin/master\ngit restore --staged .\n\n\n\nA51:\n\nprobably you were trying to clone like this: \ngit clone https://github.com/TeaCodie/TeaCodie-Website.git\nThen you got this error:\nfatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\nHere is what worked for me:\ngit clone https://github.com/TeaCodie/TeaCodie-Website.git/.git\nbecause the directory \"info/refs\" is in the .git directory.\n\n\nA52:\n\nIn my case my github account did not have permissions to the repo. Added the github account as a collaborator for the repo and that fixed it.\n\n\nA53:\n\nOpen your Terminal, access to this folder and write:\ngit init\ngit add .\ngit commit -m \"my commit\"\ngit remote set-url origin git@github.com:username/repo.git\ngit push origin master\n\n\n\nA54:\n\nFor windows user\n\nchange SSH Client from Putty/Plink to OpenSSH\n\nin Tools -> Option -> SSHClient\n\n\n\nA55:\n\nI finally was able to solve this after months of no luck. On a Mac :\n\nOpen 'Keychain Access' on a Mac by using Launchpad or Spotlight Search\nNarrow down your search by selecting \"login\" under Keychains and \"Passwords\" under the Category (both filters on the left side of the window)\nIn the top right hand corner of the window, do a search for \"git\"\nYou'll likely see a keychain there. I right-clicked and deleted the one entry I had.\nGo into sourcetree and try to do a pull/fetch/anything. You'll be prompted for a password. This should re-initiate your password with Sourcetree. To double check this, go into your keychain and repeat steps 3 & 4. Click on the keychain and look at the access control tab. SourcetreeLogin should be listed as always having access now.\n\n\n\n\nA56:\n\nI received this same error when I attempted to git clone something from Kiln what was actually a Mercurial repository.\n\n\nA57:\n\nIn my case I was using the wrong protocol in the repository URL (http instead of https)\n\n\nA58:\n\nYou could have done a:\ngit add -u -n\n\nTo check which files you modified and are going to be added (dry run: -n option), and then\ngit add -u\n\nTo add just modified files\n\n\nA59:\n\nReally struggled with this on Mac, couldn't get it to work with SSH. Ended up following this guide which worked: https://www.youtube.com/watch?v=otRHRPnHDeI.\nThere were a couple things which could have been the cause, it may not have been the SSH itself. For instance, Oauth apparently doesn't work on SourceTree despite nothing appearing wrong immediately when using it. Making sure you use a Personal Access Token and not your real password among others.\nA good rule of thumb is, if after connecting your account it only shows your public repos, it's already broken. https://github.com/settings/tokens\n\n\nA60:\n\nIn addition to setting up the Personal Access Token on GitHub, and doing what A_01 states above, this may be helpful (it's how I finally got it working)\n\nI made sure had the latest version of Sourcetree \nI upgraded to the latest version of and used System Git (Tools -> Options -> Git)\nUpgraded to the latest Git Credential Manager for Windows (this was via an installer I downloaded and double-clicked, very easy)\nEnsured I was using OAuth for my GitHub account stored in Tools -> Options -> Authentication\nMade sure I checked off the box \"Allow Sourcetree to manage my credentials using the Git Credential Manager\" (this was important!!) on Tools -> Options -> Git\nThen when I triggered the GitHub dialog (via pushing a new branch, just an example), I logged in there (this is where it used to fail).\nI next got the 2 factor auth dialog, and the code was sent to my phone as expected. I was able to enter the code from my phone into this dialog.\n\n\n\nA61:\n\nIn my case, the password that Sourcetree was asking for was for the ssh private key password. If I remove the password from the ssh key, this prompt doesn't come up.\nHowever since I wanted the password to be there, I kept investigating why it wasn't accepting my ssh key password.\nTurns out there is a wrinkle. If you have multiple ssh private keys which have different passwords, Sourcetree will give you a password prompt for each password.\nThink of it this way - if you have a fresh shell in your system, and you just started ssh-agent up, and ran ssh-add, what are the password prompts that you get? For every such password prompt, Sourcetree will give you this dialog box. In fact, if you add passwords to the dialog in the same order as you would in the shell to an ssh-add, this would work. In fact, I am guessing Sourcetree is doing exactly the same in the background.\nSo my final solution was 1) keep the passwords on the ssh keys 2) make all these passwords the same. So now Sourcetree will give me a single password prompt, and entering the ssh private key password will work.\n\n\nA62:\n\nI met up with the same problem.\nHow I solved this problem is:\nI use git bash to create a new repo, when I typed \"git push origin master\"\nIt reported \n\"fatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\"\nFinally, I found there was not a repo on the github at all.\nYou'd better create a new repo first on github. \nMaybe this experience can help somebody.\n\n\nA63:\n\nI encountered this message when using Jenkins 2.176.1 and git plugin 3.10.0 using a very old command line git version (1.7.1) as included on CentOS 6.\nThe message does not occur on newer versions of command line git (1.8 or later) with the Jenkins git plugin.\nUpgrading to a newer command line git version resolves the problem.\nOfficially, the Jenkins git plugin does not support command line git 1.7.1.  The minimum supported version is command line git 1.7.10. Many Jenkins git plugin use cases require at least command line git 1.9.0.\nCommand line git 1.7.1 behaves differently than later versions when a repository has been initialized (with git init) and then a git fetch is performed with a refspec which references 'origin'.  Later versions of command line git do not have the same problem.\n\n\nA64:\n\nAlso make sure the repo you've entered is cased correctly (it's case sensitive).\n\n\nA65:\n\nGo to Sourctree -> Tools -> Options -> Git Select \n\n\nA66:\n\ndelete each .git file in all projects\nyou can use this command\nrm -rf .git\n\n\n\nA67:\n\nI am using a SSH key, but SourceTree was asking me a password every time.\nFound on the community of Atlassian, this solution worked for me:\n\nOpen the terminal, get into your project directory\nType git config credential.helper store\nType git pull\nInput username/password (if asked)\n\nVoil\u00e0 !\n\n\nA68:\n\nMac:\nThis solution works for me.\nAfter clone code successfully, you open terminal and run this code\ngit config --global credential.helper osxkeychain\n\n\n\nA69:\n\nYou might have changed your repository name\nIn your local repository edit the file:\n.git/config\n\nThen check:\n[remote \"origin\"]\n   url = \n\nthat the URL matches your remote repository\n\n\nA70:\n\nAll the answers above seems to guide about creating a new repository in git but the question is about adding a folder to existing repo. To do that following steps can be followed.\n\nClone your existing repo using following command:\ngit clone https://github.com/company/our_repo.git \nManually take your project folder to the desired location i.e. trunk/bin\nNow commit and then push in the repo using the commands:\ngit commit -m \"message\" and git push origin master \n\n\n\nA71:\n\nIn my case, I was trying to clone a private repository in our group Github account to me laptop:\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone https://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nfatal: https://github.com/nyusngroup/velocity_CCSNe.git/info/refs?service=git-upload-pack not found: did you run git update-server-info on the server?\n\nI found two ways can solve this.\n(1) Use http instead of https. It asks me the name and password of our group Github account. After I entered the information, I can clone it.\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone http://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nUsername for 'http://github.com':nyusngroup\nPassword for 'http://nyusngroup@github.com': \n\n(2) Add my Github account to the collaborators of the private repository in our group Github account, as pointed by Monkey King's answer above.\n\n\nA72:\n\nThe reason why this is happening is because you have a folder that is already being tracked by Git inside another folder that is also tracked by Git. For example, I had a project and I added a subfolder to it. Both of them were being tracked by Git before I put one inside the other. In order to stop tracking the one inside, find it and remove the Git file with:\nrm -rf .git\n\nIn my case I had a WordPress application and the folder I added inside was a theme. So I had to go to the theme root, and remove the Git file, so that the whole project would now be tracked by the parent, the WordPress application.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 14, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nstatic uint8_t togglecode[256] = {\n    [0x3A] CAPSLOCK,\n    [0x45] NUMLOCK,\n    [0x46] SCROLLLOCK\n};\n\nWhat's the meaning of [0x3A] here? I have only learned statements like int a[2] = {1, 2};\n\n\nCandidate answers:\nA1:\n\nI think the main reason is that unsigned floats would have really limited uses compared to unsigned ints. I don't buy the argument that it's because the hardware doesn't support it. Older processors had no floating point capabilities at all, it was all emulated in software. If unsigned floats were useful they would have been implemented in software first and the hardware would have followed suit.\n\n\nA2:\n\nWhy C++ doesn't have support for unsigned floats is because there is no equivalent machine code operations for the CPU to execute.  So it would be very inefficient to support it.  \nIf C++ did support it, then you would be sometimes using an unsigned float and not realizing that your performance has just been killed.  If C++ supported it then every floating point operation would need to be checked to see if it is signed or not.  And for programs that do millions of floating point operations, this is not acceptable. \nSo the question would be why don't hardware implementers support it.  And I think the answer to that is that there was no unsigned float standard defined originally.  Since languages like to be backwards compatible, even if it were added languages couldn't make use of it.  To see the floating point spec you should look at the IEEE standard 754 Floating-Point.\nYou can get around not having an unsigned floating point type though by creating a unsigned float class that encapsulates a float or double and throws warnings if you try to pass in a negative number.   This is less efficient, but probably if you aren't using them intensely you won't care about that slight performance loss.\nI definitely see the usefulness of having an unsigned float.  But C/C++ tends to chose efficiency that works best for everyone over safety. \n\n\nA3:\n\nI think Treb is on the right track. It's more important for integers that you have an unsigned corresponding type. Those are the ones that are used in bit-shifting and used in bit-maps. A sign bit just gets into the way. For example, right-shifting a negative value, the resulting value is implementation defined in C++. Doing that with an unsigned integer or overflowing such one has perfectly defined semantics because there is no such bit in the way. \nSo for integers at least, the need for a separate unsigned type is stronger than just giving warnings. All the above points do not need to be considered for floats. So, there is, i think, no real need for hardware support for them, and C will already don't support them at that point. \n\n\nA4:\n\nIt can take time for the network close to be observed - the total time is nominally about 2 minutes (yes, minutes!) after a close before the packets destined for the port are all assumed to be dead.  The error condition is detected at some point.  With a small write, you are inside the MTU of the system, so the message is queued for sending.  With a big write, you are bigger than the MTU and the system spots the problem quicker.  If you ignore the SIGPIPE signal, then the functions will return EPIPE error on a broken pipe - at some point when the broken-ness of the connection is detected.\n\n\nA5:\n\nYou do not need to send code through a network to get this error. For example, this Python 3 code (now my favourite piece of code):\nwhile 1:print()\n\nwill print a lot of newlines, and eventually a BrokenPipeError will be raised. So you will get this error if you write too much data to an IO or other stream.\n\n\nA6:\n\nMaybe the 40 bytes fits into the pipe buffer, and the 40000 bytes doesn't?\nEdit:\nThe sending process is sent a SIGPIPE signal when you try to write to a closed pipe. I don't know exactly when the signal is sent, or what effect the pipe buffer has on this. You may be able to recover by trapping the signal with the sigaction call.\n\n\nA7:\n\nNeither GCC nor Clang are compilers -- they're actually toolchain driver programs.  That means they invoke the compiler, the assembler, and the linker.\nIf you compile your code with a C or a C++ compiler you will get the same assembly produced.  The Assembler will produce the same objects.  The difference is that the toolchain driver will provide different input to the linker for the two different languages:  different startups (C++ requires code for executing constructors and destructors for objects with static or thread-local storage duration at namespace level, and requires infrastructure for stack frames to support unwinding during exception processing, for example), the C++ standard library (which also has objects of static storage duration at namespace level), and probably additional runtime libraries (for example, libgcc with its stack-unwinding infrastructure).\nIn  short, it's not the compiler causing the increase in footprint, it's the linking in of stuff you've chose to use by choosing the C++ language.\nIt's true that C++ has the \"pay only for what you use\" philosophy, but by using the language, you pay for it.  You can disable parts of the language (RTTI, exception handling) but then you're not using C++ any more.  As mentioned in another answer, if you don't use the standard library at all you can instruct the driver to leave that out (--Wl,--as-needed) but if you're not going to use any of the features of C++ or its library, why are you even choosing C++ as a programming language?\n\n\nA8:\n\nFFCALL lets you build closures in C -- callback = alloc_callback(&function, data) returns a function pointer such that callback(arg1, ...) is equivalent to calling function(data, arg1, ...).  You will have to handle garbage collection manually, though.\nRelatedly, blocks have been added to Apple's fork of GCC; they're not function pointers, but they let you pass around lambdas while avoiding the need to build and free storage for captured variables by hand (effectively, some copying and reference counting happens, hidden behind some syntactic sugar and runtime libraries).\n\n\nA9:\n\n(As an aside, Perl 6 lets you write\nsubset Nonnegative::Float of Float where { $_ >= 0 };\n\nand then you can use Nonnegative::Float just like you would any other type.)\nThere's no hardware support for unsigned floating point operations, so C doesn't offer it.    C is mostly designed to be \"portable assembly\", that is, as close to the metal as you can be without being tied down to a specific platform.\n[edit]\nC is like assembly: what you see is exactly what you get.  An implicit \"I'll check that this float is nonnegative for you\" goes against its design philosophy.  If you really want it, you can add assert(x >= 0) or similar, but you have to do that explicitly.\n\n\nA10:\n\nSession timeout settings may be the reason of broken pipe.\n For example: Server session timeout 3 hours and load balancer has 1 hour.\n Load balancer blocks after 1 hour, but server keeps sending response. In this case, one end of the pipe is broken.\nBut it can be also a user behavior. User closes the page during download etc.\n\n\nA11:\n\nThe main thing that comes to mind is the use of code generators.  Would you be willing to program in a different language that provided the functional programming and then generate the C code from that?\nIf that's not an attractive option, then you could abuse CPP to get part of the way there.  The macro system should let you emulate some functional programming ideas.  I've heard tell that gcc is implemented this way but I've never checked.\nC can of course pass functions around using function pointers, the main problems are lack of closures and the type system tends to get in the way.  You could explore more powerful macro systems than CPP such as M4.  I guess ultimately, what I'm suggesting is that true C isn't up to the task without great effort but you could extend C to make it be up to the task.  That extension would look the most like C if you use CPP or you could go to the other end of the spectrum and generate C code from some other language.\n\n\nA12:\n\nPrerequisite for functional programming style is a first class function.\nIt could be simulated in portable C if you tolerate next:\n\nmanual management of lexical scope bindings, aka closures.\nmanual management of function variables lifetime.\nalternative syntax of function application/call.\n\n/* \n * with constraints desribed above we could have\n * good approximation of FP style in plain C\n */\n\nint increment_int(int x) {\n  return x + 1;\n}\n\nWRAP_PLAIN_FUNCTION_TO_FIRST_CLASS(increment, increment_int);\n\nmap(increment, list(number(0), number(1)); // --> list(1, 2)\n\n\n/* composition of first class function is also possible */\n\nfunction_t* computation = compose(\n  increment,\n  increment,\n  increment\n);\n\n*(int*) call(computation, number(1)) == 4;\n\nruntime for such code could be as small as one below\n\nstruct list_t {\n  void* head;\n  struct list_t* tail;\n};\n\nstruct function_t {\n   void* (*thunk)(list_t*);\n   struct list_t* arguments;\n}\n\nvoid* apply(struct function_t* fn, struct list_t* arguments) {\n  return fn->thunk(concat(fn->arguments, arguments));\n}\n\n/* expansion of WRAP_PLAIN_FUNCTION_TO_FIRST_CLASS */\nvoid* increment_thunk(struct list_t* arguments) {\n  int x_arg = *(int*) arguments->head;\n  int value = increment_int(x_arg);\n  int* number = malloc(sizeof *number);\n\n  return number ? (*number = value, number) : NULL;\n}\n\nstruct function_t* increment = &(struct function_t) {\n  increment_thunk,\n  NULL\n};\n\n/* call(increment, number(1)) expands to */\napply(increment, &(struct list_t) { number(1), NULL });\n\nIn essence we imitate first class function with closures represented as pair of function/arguments plus bunch of macroses. Complete code could be found here.\n\n\nA13:\n\nHartel & Muller's book, Functional C, can nowadays (2012-01-02) be found at: http://eprints.eemcs.utwente.nl/1077/ (there is a link to PDF version).\n\n\nA14:\n\nThat was introduced in C99 and it's called a designated initialiser.\nIt basically allows you to set specific values in an array with the rest left as defaults.\nIn this particular case, the array indexes are the keyboard scan codes. 0x3a is the scan code in set #1 (see section 10.6) for the CapsLock key, 0x45 is NumLock and 0x46 is ScrollLock.\nOn the first link above, it states that:\nint a[6] = { [4] = 29, [2] = 15 };\n\nis equivalent to:\nint a[6] = { 0, 0, 15, 0, 29, 0 };\n\nInterestingly enough, though the link states that = is necessary, that doesn't appear to be the case here. That's not part of the standard but is a hangover from a rather old version of gcc.\n\n\nA15:\n\nUnsigned integer types in C are defined in such a way as to obey the rules of an abstract algebraic ring.  For example, for any value X and Y, adding X-Y to Y will yield X.  Unsigned integer types are guaranteed to obey these rules in all cases which do not involve conversion to or from any other numeric type [or unsigned types of different sizes], and that guarantee is one of the most important feature of such types.  In some cases, it's worthwhile to give up the ability to represent negative numbers in exchange for the extra guarantees only unsigned types can provide.  Floating-point types, whether signed or not, cannot abide by all the rules of an algebraic ring [e.g. they cannot guarantee that X+Y-Y will equal X], and indeed IEEE doesn't even allow them to abide by the rules of an equivalence class [by requiring that certain values compare unequal to themselves].  I don't think an \"unsigned\" floating-point type could abide by any axioms which an ordinary floating-point type could not, so I'm not sure what advantages it would offer.\n\n\nA16:\n\nThe current state of a socket is determined by 'keep-alive' activity. In your case, this is possible that when you are issuing the send call, the keep-alive activity tells that the socket is active and so the send call will write the required data (40 bytes) in to the buffer and returns without giving any error.\nWhen you are sending a bigger chunk, the send call goes in to blocking state.\nThe send man page also confirms this:\n\nWhen the message does not fit into the send buffer of the socket, send() normally blocks, unless the socket has been placed in non-blocking I/O mode. In non-blocking mode it would return EAGAIN in this case\n\nSo, while blocking for the free available buffer, if the caller is notified (by keep-alive mechanism) that the other end is no more present, the send call will fail.\nPredicting the exact scenario is difficult with the mentioned info, but I believe, this should be the reason for you problem.\n\n\nA17:\n\nAccording to the GCC docs this is ISO C99 compliant. They refer to it as \"Designated Initializers\":\n\nTo specify an array index, write `[index] =' before the element value.\nFor example,\n int a[6] = { [4] = 29, [2] = 15 };\n\nis equivalent to\n int a[6] = { 0, 0, 15, 0, 29, 0 };\n\n\nI've never seen this syntax before, but I just compiled it with gcc 4.4.5, with -Wall. It compiled successfully and gave no warnings.\nAs you can see from that example, it allows you to initialize specific array elements, with the others being set to their default value (0).\n\n\nA18:\n\nBit fields vary widely from compiler to compiler, sorry.\nWith GCC, big endian machines lay out the bits big end first and little endian machines lay out the bits little end first.\nK&R says \"Adjacent [bit-]field members of structures are packed into implementation-dependent storage units in an implementation-dependent direction. When a field following another field will not fit ... it may be split between units or the unit may be padded. An unnamed field of width 0 forces this padding...\"\nTherefore, if you need machine independent binary layout you must do it yourself.\nThis last statement also applies to non-bitfields due to padding -- however all compilers seem to have some way of forcing byte packing of a structure, as I see you already discovered for GCC.\n\n\nA19:\n\n\nA square-root will definately never return a negative number. There are other places as well where a negative float value has no meaning. Perfect candidate for an unsigned float.\n\nC99 supports complex numbers, and a type generic form of sqrt, so sqrt( 1.0 * I) will be negative.\n\nThe commentors highlighted a slight gloss above, in that I was referring to the type-generic sqrt macro rather than the function, and it will return a scalar floating point value by truncation of the complex to its real component: \n#include <complex.h>\n#include <tgmath.h>\n\nint main () \n{\n    complex double a = 1.0 + 1.0 * I;\n\n    double f = sqrt(a);\n\n    return 0;\n}\n\nIt also contains a brain-fart, as the real part of the sqrt of any complex number is positive or zero, and sqrt(1.0*I) is sqrt(0.5) + sqrt(0.5)*I not -1.0.\n\n\nA20:\n\nLook at Hartel & Muller's book, Functional C\n\nhttps://ris.utwente.nl/ws/portalfiles/portal/5128727/book.pdf\nhttp://www.cs.bris.ac.uk/~henkm/f2c/index.html \u2014 missing since 2016\nhttp://www.cs.bris.ac.uk/~henkm/f2c/index.html \u2014 via Internet Archive aka WayBack Machine.\n\n\n\nA21:\n\nWell quite a few programming languages are written in C. And some of them support functions as first class citizens, languages in that area are ecl (embbedabble common lisp IIRC), Gnu Smalltalk (gst) (Smalltalk has blocks), then there are libraries for \"closures\" e.g in glib2 http://library.gnome.org/devel/gobject/unstable/chapter-signal.html#closure \nwhich at least got near functional programming. So maybe using some of those implementations to do functional programming may be an option. \nWell  or you can go learning Ocaml, Haskell, Mozart/Oz or the like ;-)\nRegards\n\n\nA22:\n\nThe way I went about doing functional programming in C was to write a functional language interpreter in C.  I named it Fexl, which is short for \"Function EXpression Language.\"\nThe interpreter is very small, compiling down to 68K on my system with -O3 enabled.  It's not a toy either - I'm using it for all the new production code I write for my business (web-based accounting for investment partnerships.)\nNow I write C code only to (1) add a built-in function that calls a system routine (e.g. fork, exec, setrlimit, etc.), or (2) optimize a function that could otherwise be written in Fexl (e.g. search for a substring).\nThe module mechanism is based on the concept of a \"context\".  A context is a function (written in Fexl) which maps a symbol to its definition.  When you read a Fexl file, you can resolve it with any context you like.  This allows you to create custom environments, or run code in a restricted \"sandbox.\"\nhttp://fexl.com\n\n\nA23:\n\nBitfields should be avoided - they aren't very portable between compilers even for the same platform. from the C99 standard 6.7.2.1/10 - \"Structure and union specifiers\" (there's similar wording in the C90 standard):\n\nAn implementation may allocate any addressable storage unit large enough to hold a bitfield. If enough space remains, a bit-field that immediately follows another bit-field in a structure shall be packed into adjacent bits of the same unit. If insufficient space remains, whether a bit-field that does not fit is put into the next unit or overlaps adjacent units is implementation-defined. The order of allocation of bit-fields within a unit (high-order to low-order or low-order to high-order) is implementation-defined. The alignment of the addressable storage unit is unspecified.\n\nYou cannot guarantee whether a bit field will 'span' an int boundary or not and you can't specify whether a bitfield starts at the low-end of the int or the high end of the int (this is independant of whether the processor is big-endian or little-endian).\nPrefer bitmasks.  Use inlines (or even macros) to set, clear and test the bits.\n\n\nA24:\n\nIf you want to implement closures, you'll have to get groady with assembly language and stack swapping/management. Not recommending against it, just saying that's what you'll have to do.\nNot sure how you'll handle anonymous functions in C. On a von Neumann machine, you could do anonymous functions in asm, though.\n\n\nA25:\n\nI believe the unsigned int was created because of the need for a larger value margin than the signed int could offer. \nA float has a much larger margin, so there was never a 'physical' need for an unsigned float. And as you point out yourself in your question, the additional 1 bit precision is nothing to kill for.\nEdit:\nAfter reading the answer by Brian R. Bondy, I have to modify my answer: \nHe is definitely right that the underlying CPUs did not have unsigned float operations.  However, I maintain my belief that this was a design decision based on the reasons I stated above ;-)\n\n\nA26:\n\nIt means initialise the n-th element of the array. The example you've given will mean that:\ntogglecode[0x3A] == CAPSLOCK\ntogglecode[0x45] == NUMLOCK\ntogglecode[0x46] == SCROLLLOCK\n\nThese are called \"designated initializers\", and are actually part of the C99 standard. However, the syntax without the = is not. From that page:\n\nAn alternative syntax for this which has been obsolete since GCC 2.5 but GCC still accepts is to write [index] before the element value, with no =. \n\n\n\nA27:\n\nThere is a significant difference between signed and unsigned integers in C/C++:\nvalue >> shift\n\nsigned values leave the top bit unchanged (sign extend), unsigned values clear the top bit.\nThe reason there is no unsigned float is that you quickly run into all sorts of problems if there are no negative values. Consider this:\nfloat a = 2.0f, b = 10.0f, c;\nc = a - b;\n\nWhat value does c have? -8. But what would that mean in a system without negative numbers. FLOAT_MAX - 8 perhaps? Actually, that doesn't work as FLOAT_MAX - 8 is FLOAT_MAX due to precision effects so things are even more screwy. What if it was part of a more complex expression:\nfloat a = 2.0f, b = 10.0f, c = 20.0f, d = 3.14159f, e;\ne = (a - b) / d + c;\n\nThis isn't a problem for integers due to the nature of the 2's complement system.\nAlso consider standard mathematical functions: sin, cos and tan would only work for half their input values, you couldn't find the log of values < 1, you couldn't solve quadratic equations: x = (-b +/- root (b.b - 4.a.c)) / 2.a, and so on. In fact, it probably wouldn't work for any complex function as these tend to be implemented as polynomial approximations which would use negative values somewhere.\nSo, unsigned floats are pretty useless.\nBut that doesn't mean to say that a class that range checks float values isn't useful, you may want to clamp values to a given range, for example RGB calculations.\n\n\nA28:\n\n\nI'm not aware of any programming language that supports unsigned\nfloating-point numbers. Any idea why they don't exist?\n\nUnsigned floats exist. See the unsigned float16 (11 fractions bit, 5 exponent bits, 0 sign bits) for GPU hardware, HDR format DXGI_FORMAT_BC6H. It's just that they're uncommon enough across most computing hardware that mainstream programming languages omit them. In this usage, the sign is omitted because colors darker than black make no sense anyway.\nEven the far more common IEEE half or signed float16_t, which is used quite frequently in the field of graphics and machine learning for HDR images and lower bandwidth tensors, hasn't received the honor of being incorporated into C/C++ (though, more domain-specific languages like CUDA/HLSL do have half/float16_t, and there have been C++ proposals too). So if even signed float16 can't make into C++ outside of compiler specific extensions (e.g. gcc __fp16), then an unsigned float16 has little hope :b, and not even CUDA or HLSL have the unsigned type in the language, just in the texture definition itself (found in a .DDS file or in GPU texture memory). Until then, we'll have to continue to implement more exotic types without compiler help via helper libraries.\nupdate 2022-10-09 C++23 includes signed float16_t from proposal https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p1467r9.html, albeit as optional extensions. I don't see anything for an unsigned float though.\n\n\nA29:\n\nFunctional programming is not about lambdas, it is all about pure functions.  So the following broadly promote functional style:\n\nOnly use function arguments, do not use global state.\nMinimise side effects i.e. printf, or any IO.  Return data describing IO which can be executed instead of causing the side effects directly in all functions.  \n\nThis can be achieved in plain c, no need for magic.\n\n\nA30:\n\nYou can use GCC's nested functions to simulate lambda expressions, in fact, I have a macro to do it for me:\n#define lambda(return_type, function_body) \\\n  ({ \\\n    return_type anon_func_name_ function_body \\\n    anon_func_name_; \\\n  })\n\nUse like this:\nint (*max)(int, int) = lambda (int, (int x, int y) { return x > y ? x : y; });\n\n\n\nA31:\n\nNo, it will not be fully-portable.  Packing options for structs are extensions, and are themselves not fully portable.  In addition to that, C99 \u00a76.7.2.1, paragraph 10 says: \"The order of allocation of bit-fields within a unit (high-order to low-order or low-order to high-order) is implementation-defined.\"\nEven a single compiler might lay the bit field out differently depending on the endianness of the target platform, for example.\n\n\nA32:\n\nendianness are talking about byte orders not bit orders. Nowadays , it is 99% sure that bit orders are fixed. However, when using bitfields, endianness should be taken in count. See the example below.\n#include <stdio.h>\n\ntypedef struct tagT{\n\n    int a:4;\n    int b:4;\n    int c:8;\n    int d:16;\n}T;\n\n\nint main()\n{\n    char data[]={0x12,0x34,0x56,0x78};\n    T *t = (T*)data;\n    printf(\"a =0x%x\\n\" ,t->a);\n    printf(\"b =0x%x\\n\" ,t->b);\n    printf(\"c =0x%x\\n\" ,t->c);\n    printf(\"d =0x%x\\n\" ,t->d);\n\n    return 0;\n}\n\n//- big endian :  mips24k-linux-gcc (GCC) 4.2.3 - big endian\na =0x1\nb =0x2\nc =0x34\nd =0x5678\n 1   2   3   4   5   6   7   8\n\\_/ \\_/ \\_____/ \\_____________/\n a   b     c           d\n\n// - little endian : gcc (Ubuntu 4.3.2-1ubuntu11) 4.3.2\na =0x2\nb =0x1\nc =0x34\nd =0x7856\n 7   8   5   6   3   4   1   2\n\\_____________/ \\_____/ \\_/ \\_/\n       d           c     b   a\n\n\n\nA33:\n\nThe Felix language compiles to C++. Maybe that could be a step stone, if you don't mind C++.\n\n\nA34:\n\nMost of the time, probably, but don't bet the farm on it, because if you're wrong, you'll lose big.\nIf you really, really need to have identical binary information, you'll need to create bitfields with bitmasks - e.g. you use an unsigned short (16 bit) for Message, and then make things like versionMask = 0xE000 to represent the three topmost bits.\nThere's a similar problem with alignment within structs. For instance, Sparc, PowerPC, and 680x0 CPUs are all big-endian, and the common default for Sparc and PowerPC compilers is to align struct members on 4-byte boundaries. However, one compiler I used for 680x0 only aligned on 2-byte boundaries - and there was no option to change the alignment!\nSo for some structs, the sizes on Sparc and PowerPC are identical, but smaller on 680x0, and some of the members are in different memory offsets within the struct.\nThis was a problem with one project I worked on, because a server process running on Sparc would query a client and find out it was big-endian, and assume it could just squirt binary structs out on the network and the client could cope. And that worked fine on PowerPC clients, and crashed big-time on 680x0 clients. I didn't write the code, and it took quite a while to find the problem. But it was easy to fix once I did.\n\n\nA35:\n\nThe heap usage comes from the C++ standard library. It allocates memory for internal library use on startup. If you don't link against it, there should be zero difference between the C and C++ version. With GCC and Clang, you can compile the file with:\n\ng++ -Wl,--as-needed main.cpp\n\nThis will instruct the linker to not link against unused libraries. In your example code, the C++ library is not used, so it should not link against the C++ standard library.\nYou can also test this with the C file. If you compile with:\n\ngcc main.c -lstdc++\n\nThe heap usage will reappear, even though you've built a C program.\nThe heap use is obviously dependant to the specific C++ library implementation you're using. In your case, that's the GNU C++ library, libstdc++. Other implementations might not allocate the same amount of memory, or they might not allocate any memory at all (at least not on startup.) The LLVM C++ library (libc++) for example does not do heap allocation on startup, at least on my Linux machine:\n\nclang++ -stdlib=libc++ main.cpp\n\nThe heap use is the same as not linking at all against it.\n(If compilation fails, then libc++ is probably not installed. The package name usually contains \"libc++\" or \"libcxx\".)\n\n\nA36:\n\nIt's (close to) the syntax of designated initializers, a C99 feature.\nBasically, it initializes parts of an array, for example;\nint aa[4] = { [2] = 3, [1] = 6 };\n\nIntializes the second value of the array to 6, and the third to 3.\nIn your case the array offsets happen to be in hex (0x3a) which initializes the 58'th element of the array to the value of CAPSLOCK which presumably is defined in the code above the code you're showing.\nThe version in your code without the = seems to be a gcc specific extension.\n\n\nA37:\n\nWhen peer close, you just do not know whether it just stop sending or both sending and receiving.Because TCP allows this, btw, you should know the difference between close and shutdown. \nIf peer both stop sending and receiving, first you send some bytes, it will succeed. But the peer kernel will send you RST. So subsequently you send some bytes, your kernel will send you SIGPIPE signal, if you catch or ignore this signal, when your send returns, you just get Broken pipe error, or if you don't , the default behavior of your program is crashing.\n\n\nA38:\n\nThanks @BenVoigt for your very useful comment starting \n\nNo, they were created to save memory.\n\nLinux source does use a bit field to match to an external structure: /usr/include/linux/ip.h has this code for the first byte of an IP datagram\nstruct iphdr {\n#if defined(__LITTLE_ENDIAN_BITFIELD)\n        __u8    ihl:4,\n                version:4;\n#elif defined (__BIG_ENDIAN_BITFIELD)\n        __u8    version:4,\n                ihl:4;\n#else\n#error  \"Please fix <asm/byteorder.h>\"\n#endif\n\nHowever in light of your comment I'm giving up trying to get this to work for the multi-byte bit field frag_off.\n\n\nA39:\n\nI guess it depends on that the IEEE floating-point specifications only are signed and that most programming languages use them.\nWikipedia article on IEEE-754 floating-point numbers\nEdit: Also, as noted by others, most hardware does not support non-negative floats, so the normal kind of floats are more efficient to do since there is hardware support.\n\n\nA40:\n\nWhat is it about C that you want to make functional, the syntax or the semantics? The semantics of functional programming could certainly be added to the C compiler, but by the time you were done, you'd essentially have the equivalent of one of the existing functional languages, such as Scheme, Haskell, etc.\nIt would be a better use of time to just learn the syntax of those languages which directly support those semantics.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n(Note: The \"1\" is a \"1\" in the \"1\" format.\nThe \"1\" is a \"1\" in the"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 15, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nWhere are the Windows binaries for the command-line version of SVN? Everywhere I search, it has to be some company's fancy client software with an installer.\nI'm working on a machine where I don't have administrator account. Just give me the plain command-line binaries, where the hell are they?\n\n\nCandidate answers:\nA1:\n\nKris Erickson is right, vote him up. In my case the problem was installing TFS powertools , which adds explorer shell integration just like TSVN and TCVS. It adds another 5 overlays. Since they are prefixed with Tfs*, they take priority over Tortoise*. I also just prefixed the Tfs stuff with z_ and the TSVN overlays came back. No need to reboot/logoff though, just kill explorer.exe and start it again.\n\n\nA2:\n\nTwo other possible solutions:\n\nKill TSVNCache.exe and let it re-launch.  This has caused my shell overlay icons to re-appear on several occasions.\nIf you've recently upgraded TortoiseSVN from a previous version, you may need to upgrade your working copy.  This can be done by right-clicking on your project folder and choose \"SVN Upgrade working copy\" from the context menu.\n\n\n\nA3:\n\nsvn propset svn:executable \"*\" someScript\nThe syntax is propset key value so svn:executable is the key and \"*\" is the value\nsomeScript is the filename\n\n\nA4:\n\nI resolved it by using Autoruns from Sysinternals.\n\nRun Autoruns\nIn the 'Explorer' tab, scroll down to the 'OverlayIconIdentifier' and 'ShellOverlayIconIdentifier' entries\nUn-tick unwanted overlays (such as most of any Dropbox, SkyDrive etc.)\nRestart Explorer.exe\n\n\n\n\nA5:\n\nAll of these solutions setup the standard Apache HTTPD server with Subversion integration. They do no real magic.\nUberSVN and VisualSVN provide a pretty front end interface, but behind the scenes, it's just standard Apache setup. If you are not a CM, and don't want that to be your job, that's not a bad alternative. However, do not ever try to tweak the configuration behind their backs. They will either revert the changes, or really mess things up.\nHowever, I use Apache httpd in order to access my Windows Active Directory. I setup my users, so their Windows account and password are their Subversion account and password. I setup a Active Directory Security Group or a Active Directory Exchange Group as my authorization database. If a user is a member of this group, they have access to the repository via their Windows login. If not, they don't have access. Clean and simple. I am no longer in the user authorization business which gives me more time to play Candy Crush ..I mean.. other CM work.\nOtherwise, I might as well just use svnserve: It's faster than Apache httpd, much easier to setup, and more stable too.\nIf you're not going to use Active Directory as your authorization backend, you are technical, and don't mind getting your hands a bit dirty, I would use either play svnserve (and set it up as a Windows Service. No need for any of these packages.\nIf you are going to use Active Directory as your authorization backend (and why not?), I would use CollabNet Subversion Edge to help setup Apache httpd and Subversion and do all of my configuration manually. Setting up Active Directory via LDAP via Apache httpd isn't really that difficult, and once done, you probably never have to touch again. The documentation on integrating Apache httpd/Subversion/LDAP/ActiveDirectory can easily be found.\nI suggest you look at the on line Subversion Documentation and go through it. If thinking about using the svn command line client makes you woozy, then get VisualSVN or UberSVN and let them take care of things for you. If you don't mind going all 1990 with your computer, then forget about the pretty front ends, and use CollabNet Edge.\nOne more thing, I am not a fan of ViewVC that comes with CollabNet Edge. It has to use file:// protocol and must run on the same server as your Subversion server software. Instead, I prefer Sventon. It can run on a separate server, and you can use the same credentials as your standard Subversion credentials. Plus, I like the interface a bit better.\n\n\nA6:\n\nAlso restarting your PC is not needed. Instead restart explorer.exe:\n\nPress Ctrl+Shift+Esc.\nClick the Processes tab.\nRight-click on explorer.exe.\nSelect End Process.\nClick End Process button.\nClick the Applications tab.\nRight-click.\nClick New Task (Run).\nSet Open to: explorer\nClick OK.\n\nWindows Explorer has restarted and the TortoiseSVN icons reappear.\n\n\nA7:\n\nhttp://subversion.tigris.org/\nThe client-side binaries ship with the source, as well as several other packaged installers.\n\n\nA8:\n\nSVN ignores UNIX file permissions when commiting. The way that you set exec permissions on a file is to change the svn properties for that file. If you are on Windows, the easy way to do this is to install Tortoise SVN. Then after you have committed, use Tortoise to open the repo-browser. Find a file that needs to be executable and right-click it to open properties.\nThe dialog that appears is the SVN properties dialog. Click New to add a new property, select svn:executable from the dropdown list, and set the property value to *.\nThat's it. When you click OK, it is committed to the SVN repo. The next time you, or the build machine, do an svn update, that file will have executable permissions on Unix/Linux.\n\n\nA9:\n\nI suggest complaining to Microsoft regarding the ridiculously low icon overlay limit.  If enough of us complain, perhaps they'll fix the true root cause of this problem:\nSee comments at bottom of page:\nhttp://msdn.microsoft.com/en-us/library/cc144123(VS.85).aspx\nGive Microsoft Windows 7 team feedback on this issue:\nhttp://mymfe.microsoft.com/Windows%207/Feedback.aspx?formID=195\n\n\nA10:\n\nMy icons had disappeared too. The registry fixes did not work for me either.\nThis is how I got them back:\n\ninstall the latest version of TortoiseOverlays \nrestart explorer.exe\n\n\n\nA11:\n\nSame problem for me. It turns out that the cause of the problem was the new JungleDisk 3.0, which rudely installs three overlays named \"1Sync...\" \"2Sync...\" and \"3Sync...\" pushing the Tortoise ones off the end. \nJust delete those JungleDisk keys in the reg hive listed at the top (or prefix them with z_) and re-start the system and Tortoise should work fine again. \nGiven that this overlay limit exists in Windows and is easily hit with current tools, tool vendors really should ask during advanced installation if the user wants to install them. I have no need nor desire for the new \"Sync\" feature and don't really care for the tactic of stuffing the icons at the top of the list with clever naming. Shame on JungleDisk.\n\n\nA12:\n\nActually, I had this problem same as you.\nMy windows is server 2008 and my subversion info is :\nTortoiseSVN 1.7.6, Build 22632 - 64 Bit , 2012/03/08 18:29:39\nSubversion 1.7.4, \napr 1.4.5\napr-utils 1.3.12\nneon 0.29.6\nOpenSSL 1.0.0g 18 Jan 2012\nzlib 1.2.5\nI used this way and I solved this problem.\nI used [group] option. this option makes problem. \nI rewrite authz file contents.\nI remove group option. and I set one by one.\nI use well.\nThanks for reading.\n\n\nA13:\n\nWindows can only show a limited number of Overlay Icons (15 total, 11 after what Windows uses).  Programs like Office Groove, Dropbox, Mozy, Carbonite, etc, will hijack a bunch of the 11 possible overlay icons (boy would it be nice if Microsoft upped the number of these as the number of applications that use them seem to increase and increase)...\nYou can see what overlays are set up, and change them (at your own risk) in the registry here:\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\explorer\\ShellIconOverlayIdentifiers\nIf you are using TortoiseCVS (and have nothing else using overlay icons), you will get a couple of TortoiseSVN Icons, and all of your TortoiseCVS icons.  This is because the overlay icons are used in alphabetical order. Again, at your own risk (editing the registry may blow up your computer, yada, yada, yada -- and if you are reading Stack Overflow and using Windows and haven't edited the registry, you are a rare beast indeed), feel free to rename them (I suggest putting numbers in front of the ones you want to use and \"z_\"'s prefixed to the ones you don't need).  The TortoiseSVN Shell extensions are nicely named so you know what they do, the TortoiseCVS extensions are not.  After looking through the source code, I found the pertinent information:\n\nTortoiseCVS0 - In CVS\nTortoiseCVS1 - Not in CVS\nTortoiseCVS3 - Conflicted\nTortoiseCVS4 - In CVS readonly\nTortoiseCVS5 - Ignored\nTortoiseCVS6 - Added\nTortoiseCVS7 - Deleted\nTortoiseCVS8 - Locked\n\n\n\nA14:\n\nAs a new user to these two software packages, I experienced the exact same problem.  As was also discovered above, my solution was to use the same case letters as is in the Repository path.\nHere's a tip that I find helpful:  In VisualSVN, you can right click on the path, then click \"Copy URL to Clipboard\" for pasting in Tortoise to be sure that the path is the identical case.\n\n\nA15:\n\nI had the same issue as the OP: Win 7 (x64), TortoiseSVN (x64), and DropBox (x86). The info from some of the other answers gave me all the info. I've only ever had the x64 version of TSVN installed on this machine. \nIn my case TSVN and DropBox were installed the same day I did the OS install and the overlays worked fine until a couple of days ago. I did nothing involving changing settings for either app to cause them to stop working.\nHere is what I had in the icon overlay registry section after the problem started (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\explorer\\ShellIconOverlayIdentifiers): \n\nDropboxExt1\nDropboxExt2\nDropboxExt3\nEnhancedStorageShell\nOffline Files\nSharingPrivate\nTortoiseAdded\nTortoiseConflict\nTortoiseDeleted\nTortoiseIgnored\nTortoiseLocked\nTortoiseModified\nTortoiseNormal\nTortoiseReadOnly\nTortoiseUnversioned\n\nI verified that only the overlays corresponding to the first 11 entries display in Explorer. When I modified the order of above entries by adding 'z' to the start of some of them, again only the first 11 overlays (under the updated order) would display. \nWith the above I had everything I needed to solve the problem (either rename or or delete entries so that the TSVN entries I want working are <= #11 on the list). Below deals with wondering why this suddenly happened.\nI know that based on the overlays that worked prior to a couple of days ago, keys 1-3, 7-9, 12-13 were all <= 11 in the list (not sure if overlay #14 ever worked since I never had files w/ read-only status. #15 never worked on this machine so i know it was never in the top 11). I also assume the block of TSVN keys move up/down in unison, therefore they were bumped down either two or three places (* see below). This implies that 2-3 items were added between the DropBox & TSVN blocks. The three that are there now are added by Windows and I would assume they'd be there as soon as the OS installed. \nIs the list of 15 overlays determined at run-time? Seems like the overlay handlers might sometimes tell the windows shell that there are no icons to add to the list. Possibly some settings I messed with a couple of days ago related to file sharing and file encryption caused some of those items at the 4-6 spots to become \"activated\" and push the SVN ones down.\nIn the end I deleted a couple of entries and moved some, so my final list looks like this: \n\nDropboxExt1\nDropboxExt2\nDropboxExt3\nSharingPrivate (i want this to show up)\nTortoiseAdded\nTortoiseConflict\nTortoiseDeleted\nTortoiseModified\nTortoiseNormal\nTortoiseReadOnly\nTortoiseUnversioned\nzOffline Files (i don't use Sync Center, or \"Offline Files\" so I don't care about this)\nzEnhancedStorageShell (don't really know what Enhanced Storage is, don't think I need this)\n\n\n\nA16:\n\nWe chose VisualSVN bec it makes it easy to use windows authentication instead of having a separate SVN user.\nOther benefits is an easy to use GUI for permission management and for managing SVN hooks.\nThe one thing I'm not a real fan of is the web GUI. As far as I know you cannot view web based diffs, so if that makes a difference.... \n\n\nA17:\n\nTry UberSVN http://www.wandisco.com/ubersvn/download#windows\nThe only downside is that is requires lot of server memory. Best free SVN server I ever tried. VisualSVN free version is OK as well.\n\n\nA18:\n\nI've just configured win32svn server according to this article by Jeff Atwood.\nIn contrast to VisualSVN win32svn is not so tightly integrated to Windows, orientated on terminal usage and open sourced under Apache License.\n\n\nA19:\n\nMy friends who run a Windows shop speak highly of VisualSVN\n\n\nA20:\n\nActually, I had this problem same as you.  You can get the \"Forbidden\" error if your commit includes different directories ; Like external items.\nAnd \u0131 solved in one step. Just commit external items in another case.\nAdditionally, I advise you to read articles on External Items in Subversion and VisualSVN Server:\nVisualSVN Team's article about Daily Use Guide External Items. It explains the principles of External Items in SVN.\nhttps://tortoisesvn.net/docs/release/TortoiseSVN_en/tsvn-dug-externals.html\n\n\nA21:\n\nThe Command Line Client from CollabNet, while it does have an installer for windows, all it asks you is where you want to put the files, and then finish.  Its just the command line client, like it says, nothing else.  Subversion doesn't distribute their own binaries.\n\nThe Subversion project does not\n  officially endorse or maintain any\n  binary packages of the Subversion\n  software. However, volunteers have\n  created binary packages for different\n  distributions and platforms, and as a\n  convenience, we maintain a list of\n  links to them here. If there are any\n  problems with or questions about the\n  different binary packages please send\n  email to the Subversion users mailing\n  list.\n\n\n\nA22:\n\nI, too, lost my overlays after upgrading to 1.6.8 on Windows 7 x64.  I only installed the x64 version and not the 32-bit version.  To fix, I ran a repair with the original installer and rebooted.  The overlays displayed normally thereafter.\n\n\nA23:\n\nSometimes you just need to go to TortoiseSVN \"settings\", turn the icons off, click \"apply\", turn them back on.\nWindows Start->All Programs->TortoiseSVN->Settings\n\n\n\nA24:\n\nIf you want to use Tortoise from within a 32 bit Application on Windows 7 64 bit, you need to install both the 64bit and the 32bit versions of Tortoise. According to Tortoise's makers, this works fine. (source)\n\n\nA25:\n\nHave you tried to change in Tortoise Settings the status cache to 'Default'? I had this problem with the overlay icon on folders because I had this option in 'Shell'.\nThe option is in Settings -> Icons overlay.\nMaybe this could help you http://tortoisesvn.net/node/97\n\n\nA26:\n\nYou can get the \"Forbidden\" error if your user account lacks access permissions to a repository or repository path; it makes sense to check authorization settings for your user account. Make sure your system administrator hasn't provided you with No Access permission to the repository path.\nIf you are sure that permissions are set correctly, then double-check the URL you use. URLs in Apache Subversion are case-sensitive.\nAdditionally, I advise you to read articles on authorization in Subversion and VisualSVN Server:\n\nSVNBook about path-based authorization.\nVisualSVN Team's article about path-based authorization. It explains the principles of SVN authorization by comparing it with Windows Access Control.\n\n\n\nA27:\n\nThe solution for me was to check the case sensitivity of the username. A lot of people are mentioning that the URL is case sensitive, but it seems the username is as well!\n\n\nA28:\n\nThe former direct links don't work, but the subversion project now provides several offsite links to Windows binaries:\nhttp://subversion.apache.org/packages.html#windows\n\n\nA29:\n\nI found that if you install the 32bit version first it some how mucks up the 64bit version. On reinstalling windows 7 i just installed the 64bit version and it seems to work fine now.\n\n\nA30:\n\nThey display fine here. Are you using the 64-bit version of Windows 7 along with the 32-bit version of TortoiseSVN? If so, then they will only show up in the 32-bit Explorer (or in the CFDs of 32-bit applications). You can install both 32-bit and 64-bit versions side-by-side, though.\n\n\nA31:\n\nI had a similar issue in Mac where svn was picking mac login as user name and I was getting error as\nsvn: E170013: Unable to connect to a repository at URL 'https://repo:8443/svn/proj/trunk'\nsvn: E175013: Access to '/svn/proj/trunk' forbidden\n\nI used the --username along with svn command to pass the correct username which helped me.\nAlternatively, you can delete ~/.subversion/auth file, after which svn will prompt you for username.\n\n\nA32:\n\nHalt ye!\nBefore doing anything to your registry or similar procedures listed in Kris Erickson's (excellent) answer or the ones below, there's something to consider...\nAre you on a network drive?\nIf so, go to Tortoise SVN settings (right click any folder > TortoiseSVN > Settings), then go to 'Icon Overlays'\nMake sure you've checked 'Network Drives' as pictured:\n\nBy default on a fresh Tortoise install, network drives don't have the icons added.\nThis solved the problem for us. If this fails for you then obviously you can go through the (slightly) more involved solutions listed here.\n\n\nA33:\n\nIf you have recently upgraded Tortoise SVN, it may be because version 1.7 seems to need a new format for storing the status of files - in which case, you can right click the root folder of your repository in question, and use the Tortoise menu option of 'SVN Upgrade Working Copy'.\nThis resolved the problem for me. \nCheers\nMatt\n\n\nA34:\n\nMy issue was my SVN permissions.\nI had the same problem \"Access to '/svn/[my path]/!svn/me' forbidden\" when trying to commit files to a project I had been working on daily for several months.  After trying the steps above, I could not resolve the issue.  I also tried pulling the project down from scratch, logging in/out of SVN, etc.  Finally I contacted my company's IT department and there was a permissions issue that spontaneously emerged which changed my access from read/write to read-only access.  The IT department refreshed my permissions and this solved the problem. \n\n\nA35:\n\nI just upgraded to TSVN 1.6.8.19260 on a WinXP and two Win7 machines. After upgrading, the overlay icons disappeared on all 3 machines. Under the above mentioned registry key, I only found the entry for offline files.\nI simply had to reinstall/repair the TSVN installation and the icons reappeared after a reboot. I've never detected that behaviour in any previous TSVN version and I certainly installed TSVN everytime the same way with the same permissions.\n\n\nA36:\n\nPossible problems and solutions\n\nLimitation of number of usable overlay icons by Windows, refer to Kris Erickson's answer.\nInappropriate setting of TortoiseSVN, refer to Ralph Cowling's answer.\nKnown bug for updating from earlier version (pre 1.6.10), see here. \nTortoiseSVN was not installed for all users, try to install with cmd like below, if all below three solutions don't work, I believe this method will save your life.\n\n\nrem Set your own path.\nset TOOL_ROOT=d:\\Tools\\TortoiseSVN-1.6.13.20954-win32-svn-1.6.16\\tortoisesvn_1.6.16.21511\nset PACKAGENAME=TortoiseSVN-1.6.16.21511-x64-svn-1.6.17.msi\nset TOOL_INSTALLROOT=\"c:\\Program Files (x86)\\TortoiseSVN\"\n\nmsiexec /i \"%TOOL_ROOT%\\%PACKAGENAME%\" INSTALLDIR=%TOOL_INSTALLROOT% ALLUSERS=1 ARPSYSTEMCOMPONENT=1 RebootYesNo=No REBOOT=ReallySupress /qb-\n\nset ret=%ERRORLEVEL%\n\nrem The installer seems to return same values as MSI\nrem These codes mean that the product is properly installed, even though the code is not zero.\nrem 1641 ERROR_SUCCESS_REBOOT_INITIATED\nrem 3010 ERROR_SUCCESS_REBOOT_REQUIRED\nif %ret% EQU 1641 set ret=0\nif %ret% EQU 3010 set ret=0\nif %ret% GTR 0 exit %ret%\n\n\n\nA37:\n\nTo complete Johannes's answer, you can check this thread, which mentions another cause (the first one being installing the 32bit client instead of the 64 one) (emphasis mine):\n\nOnly more recent versions of TSVN and TCVS are able to share overlay icons, and since Explorer has a limit of 14 overlay images in total with extraneous overlay icons\n  simply not showing up you might have too many... \nIf it's overlay icons are showing up you'd have to actually be using\n  the 32-bit Windows Explorer, which of course means you'll have to also\n  install a 32-bit version of TSVN.\n\nThe issue might come up when one install also first TortoiseVCS, because:\n\nLooking at what got installed in the filesystem and the registry, it looks like they tried to do what I do with my Shell extensions - install both 64 and 32 -bit versions, so that nobody says \"I don't see my overlays/menus\"! ;)\nUnfortunately, it looks like the registry entries for 32-bit got hosed, so the 32-bit extension DLL, even though present, does not have the correct registry entries. \nIf in doubt I suggest to deinstall TCVS and TSVN.\n  Then install TSVN.\n  Maybe after installation reboot twice, not only once.\n  And if they still don't show up go to software control panel and choose to repair the installation. That did the trick for me with when upgrading from the previous to the current version.\nYou should probably install the TortoiseXYZ variant you plan to use mainly first, to improve its chances to fall into one of those limited number of overlay icons slots before the TortoiseXYZ variant you use less.\nAlways keep these limited number of overlay icon slots in mind when trying to analyze malfunctions. \n\n\n\nA38:\n\nIf the problem lies client side, this could be one of the causes of the error.\nOn clients TortoiseSVN saves client credentials under\n\nTortoise settings / saved data / authentication data.\n\nI got the same error trying to commit my files, but my credentials were changed. Clearing this cache here will give you a popup on next commit attempt for re-entering your correct credentials.\n\n\nA39:\n\nWe use an Apache SSPI module with the Collab.Net subversion server.\nHas worked perfectly for us.\n\n\nA40:\n\nIn my case, Dropbox overlays were starting with a \" (quoted identifier) in the registry. I deleted all the \" prefixes and restarted explorer.exe. \nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\explorer\\ShellIconOverlayIdentifiers\n\nEdit: I installed Windows 10 and this solution didn't work for me. So I just went to the same registry location and deleted all Google and SkyDrive records and restarted explorer.exe. \nSecond edit: After installing TortoiseGit it fixed everything without any customisation. \n\n\nA41:\n\nPutting in a vote for CollabNet Subversion Edge, it's dead simple to set up and administer via a web frontend. I'm comfortable editing config files if I must, especially coming from a Linux environment, but it's nice not to be required to do so - CollabNet makes that easy.\n\n\nA42:\n\nI was unable to commit csharp-files (*.cs). In the end the problem was that at some point i installed mod_mono, which made the *.cs-files inaccessible, through its configuration. So it may well be an apache-configuration issue, if only some sort of files are not accessible.\ngrep \".cs\" /etc/apache2/mods-enabled/*\n...\nmod_mono_auto.conf:AddType application/x-asp-net .cs\n...\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "  \n   (Note: The \"?\" is a placeholder for the user's name, which is not shown in theKonstantin Brueckner"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 16, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIn my project there is a method which only returns a const char*, whereas I need a char* string, as the API doesn't accept const char*.\nAny idea how to convert between const char* to char*?\n\n\nCandidate answers:\nA1:\n\nPointers to Pointers\n\nSince we can have pointers to int, and pointers to char, and pointers to any structures we've defined, and in fact pointers to any type in C, it shouldn't come as too much of a surprise that we can have pointers to other pointers.\n\n\n\nA2:\n\nHow do pointers to pointers work in C?\nFirst a pointer is a variable, like any other variable, but that holds the address of a variable.\nA pointer to a pointer is a variable, like any other variable, but that holds the address of a variable.  That variable just happens to be a pointer.\nWhen would you use them?\nYou can use them when you need to return a pointer to some memory on the heap, but not using the return value. \nExample:\nint getValueOf5(int *p)\n{\n  *p = 5;\n  return 1;//success\n}\n\nint get1024HeapMemory(int **p)\n{\n  *p = malloc(1024);\n  if(*p == 0)\n    return -1;//error\n  else \n    return 0;//success\n}\n\nAnd you call it like this:\nint x;\ngetValueOf5(&x);//I want to fill the int varaible, so I pass it's address in\n//At this point x holds 5\n\nint *p;    \nget1024HeapMemory(&p);//I want to fill the int* variable, so I pass it's address in\n//At this point p holds a memory address where 1024 bytes of memory is allocated on the heap\n\nThere are other uses too, like the main() argument of every C program has a pointer to a pointer for argv, where each element holds an array of chars that are the command line options.  You must be careful though when you use pointers of pointers to point to 2 dimensional arrays, it's better to use a pointer to a 2 dimensional array instead.  \nWhy it's dangerous?\nvoid test()\n{\n  double **a;\n  int i1 = sizeof(a[0]);//i1 == 4 == sizeof(double*)\n\n  double matrix[ROWS][COLUMNS];\n  int i2 = sizeof(matrix[0]);//i2 == 240 == COLUMNS * sizeof(double)\n}\n\nHere is an example of a pointer to a 2 dimensional array done properly:\nint (*myPointerTo2DimArray)[ROWS][COLUMNS]\n\nYou can't use a pointer to a 2 dimensional array though if you want to support a variable number of elements for the ROWS and COLUMNS.  But when you know before hand you would use a 2 dimensional array.\n\n\nA3:\n\nhope  this piece of code can help.\n#include <iostream>\n#include <stdio.h>\nusing namespace std;\n\nint main()\n{\n    int i = 5, j = 6, k = 7;\n    int *ip1 = &i, *ip2 = &j;\n    int** ipp = &ip1;\n    printf(\"address of value i: %p\\n\", &i);\n    printf(\"address of value j: %p\\n\", &j);\n    printf(\"value ip1: %p\\n\", ip1);\n    printf(\"value ip2: %p\\n\", ip2);\n    printf(\"value ipp: %p\\n\", ipp);\n    printf(\"address value of ipp: %p\\n\", *ipp);\n    printf(\"value of address value of ipp: %d\\n\", **ipp);\n    *ipp = ip2;\n    printf(\"value ipp: %p\\n\", ipp);\n    printf(\"address value of ipp: %p\\n\", *ipp);\n    printf(\"value of address value of ipp: %d\\n\", **ipp);\n}\n\nit outputs: \n\n\n\nA4:\n\nBecause when you say \n*ipp = ip2\n\nyou're saying the 'object pointed by ipp' to point the direction of memory that ip2 is pointing.\nYou're not saying ipp to point ip2.\n\n\nA5:\n\nMy very personal opinion is that pictures with arrows pointing this way or that make pointers harder to understand. It does make them seem like some abstract, mysterious entities. They are not. \nLike everything else in your computer, pointers are numbers. The name \"pointer\" is just a fancy way of saying \"a variable containing an address\".\nTherefore, let me stir things around by explaining how a computer actually works.\nWe have an int, it has the name i and the value 5. This is stored in memory. Like everything stored in memory, it needs an address, or we wouldn't be able to find it. Lets say i ends up at address 0x12345678 and its buddy j with value 6 ends up just after it. Assuming a 32-bit CPU where int is 4 bytes and pointers are 4 bytes, then the variables are stored in physical memory like this:\nAddress     Data           Meaning\n0x12345678  00 00 00 05    // The variable i\n0x1234567C  00 00 00 06    // The variable j\n\nNow we want to point at these variables. We create one pointer to int, int* ip1, and one int* ip2. Like everything in the computer, these pointer variables get allocated somewhere in memory too. Lets assume they end up at the next adjacent addresses in memory, immediately after j. We set the pointers to contain the addresses of the variables previously allocated: ip1=&i; (\"copy the address of i into ip1\") and ip2=&j. What happens between the lines is:\nAddress     Data           Meaning\n0x12345680  12 34 56 78    // The variable ip1(equal to address of i)\n0x12345684  12 34 56 7C    // The variable ip2(equal to address of j)\n\nSo what we got were just yet some 4 byte chunks of memory containing numbers. There's no mystical or magical arrows anywhere in sight. \nIn fact, just by looking at a memory dump, we can't tell whether the address 0x12345680 contains an int or int*. The difference is how our program chooses to use the contents stored at this address. (The task of our program is actually just to tell the CPU what to do with these numbers.)\nThen we add yet another level of indirection with int** ipp = &ip1;. Again, we just get a chunk of memory:\nAddress     Data           Meaning\n0x12345688  12 34 56 80    // The variable ipp\n\nThe pattern does seem familiar. Yet another chunk of 4 bytes containing a number.\nNow, if we had a memory dump of the above fictional little RAM, we could manually check where these pointers point. We peek at what's stored at the address of the ipp variable and find the contents 0x12345680. Which is of course the address where ip1 is stored. We can go to that address, check the contents there, and find the address of i, and then finally we can go to that address and find the number 5.\nSo if we take the contents of ipp, *ipp, we will get the address of the pointer variable ip1. By writing *ipp=ip2 we copy ip2 into ip1, it is equivalent to ip1=ip2. In either case we would get\nAddress     Data           Meaning\n0x12345680  12 34 56 7C    // The variable ip1\n0x12345684  12 34 56 7C    // The variable ip2\n\n\n(These examples were given for a big endian CPU)\n\n\nA6:\n\nI like this \"real world\" code example of pointer to pointer usage, in Git 2.0, commit 7b1004b:\n\nLinus once said:\n\nI actually wish more people understood the really core low-level kind of coding. Not big, complex stuff like the lockless name lookup, but simply good use of pointers-to-pointers etc.\nFor example, I've seen too many people who delete a singly-linked list entry by keeping track of the \"prev\" entry, and then to delete the entry, doing something like:\n\n   if (prev)\n     prev->next = entry->next;\n   else\n     list_head = entry->next;\n\nand whenever I see code like that, I just go \"This person doesn't understand pointers\". And it's sadly quite common.\n\nPeople who understand pointers just use a \"pointer to the entry pointer\", and initialize that with the address of the list_head. And then as they traverse the list, they can remove the entry without using any conditionals, by just doing a\n*pp =  entry->next\n\n\nApplying that simplification lets us lose 7 lines from this function even while adding 2 lines of comment.\n- struct combine_diff_path *p, *pprev, *ptmp;\n+ struct combine_diff_path *p, **tail = &curr;\n\n\n\nChris points out in the comments to the 2016 video \"Linus Torvalds's Double Pointer Problem\".\n\nkumar points out in the comments the blog post \"Linus on Understanding Pointers\", where Grisha Trubetskoy explains:\n\nImagine you have a linked list defined as:\n   typedef struct list_entry {\n       int val;\n       struct list_entry *next;\n   } list_entry;\n\nYou need to iterate over it from the beginning to end and remove a specific element whose value equals the value of to_remove.\nThe more obvious way to do this would be:\n   list_entry *entry = head; /* assuming head exists and is the first entry of the list */\n   list_entry *prev = NULL;\n   \n   while (entry) { /* line 4 */\n       if (entry->val == to_remove)     /* this is the one to remove ; line 5 */\n           if (prev)\n              prev->next = entry->next; /* remove the entry ; line 7 */\n           else\n               head = entry->next;      /* special case - first entry ; line 9 */\n   \n       /* move on to the next entry */\n       prev = entry;\n       entry = entry->next;\n   }\n\nWhat we are doing above is:\n\niterating over the list until entry is NULL, which means we\u2019ve reached the end of the list (line 4).\nWhen we come across an entry we want removed (line 5),\nwe assign the value of current next pointer to the previous one,\nthus eliminating the current element (line 7).\n\nThere is a special case above - at the beginning of the iteration there is no previous entry (prev is NULL), and so to remove the first entry in the list you have to modify head itself (line 9).\nWhat Linus was saying is that the above code could be simplified by making the previous element a pointer to a pointer rather than just a pointer.\nThe code then looks like this:\n   list_entry **pp = &head; /* pointer to a pointer */\n   list_entry *entry = head;\n\n   while (entry) {\n       if (entry->val == to_remove)\n           *pp = entry->next;\n       else\n            pp = &entry->next;\n       entry = entry->next;\n   }\n\nThe above code is very similar to the previous variant, but notice how we no longer need to watch for the special case of the first element of the list, since pp is not NULL at the beginning. Simple and clever.\nAlso, someone in that thread commented that the reason this is better is because *pp = entry->next is atomic. It is most certainly NOT atomic.\nThe above expression contains two dereference operators (* and ->) and one assignment, and neither of those three things is atomic.\nThis is a common misconception, but alas pretty much nothing in C should ever be assumed to be atomic (including the ++ and -- operators)!\n\n\n\nA7:\n\nLet's assume an 8 bit computer with 8 bit addresses (and thus only 256 bytes of memory). This is part of that memory (the numbers at the top are the addresses):\n  54   55   56   57   58   59   60   61   62   63   64   65   66   67   68   69\n+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n|    | 58 |    |    | 63 |    | 55 |    |    | h  | e  | l  | l  | o  | \\0 |    |\n+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n\nWhat you can see here, is that at address 63 the string \"hello\" starts. So in this case, if this is the only occurrence of \"hello\" in memory then,\nconst char *c = \"hello\";\n\n... defines c to be a pointer to the (read-only) string \"hello\", and thus contains the value 63. c must itself be stored somewhere: in the example above at location 58. Of course we can not only point to characters, but also to other pointers. E.g.:\nconst char **cp = &c;\n\nNow cp points to c, that is, it contains the address of c (which is 58). We can go even further. Consider:\nconst char ***cpp = &cp;\n\nNow cpp stores the address of cp. So it has value 55 (based on the example above), and you guessed it: it is itself stored at address 60.\n\nAs to why one uses pointers to pointers:\n\nThe name of an array usually yields the address of its first element. So if the array contains elements of type t, a reference to the array has type t *. Now consider an array of arrays of type t: naturally a reference to this 2D array will have type (t *)* = t **, and is hence a pointer to a pointer.\nEven though an array of strings sounds one-dimensional, it is in fact two-dimensional, since strings are character arrays. Hence: char **.\nA function f will need to accept an argument of type t ** if it is to alter a variable of type t *.\nMany other reasons that are too numerous to list here.\n\n\n\nA8:\n\nThe difference is due to operator precedence.\nThe post-increment operator ++ has higher precedence than the dereference operator *. So *ptr++ is equivalent to *(ptr++). In other words, the post increment modifies the pointer, not what it points to.\nThe assignment operator += has lower precedence than the dereference operator *, so *ptr+=1 is equivalent to (*ptr)+=1. In other words, the assignment operator modifies the value that the pointer points to, and does not change the pointer itself. \n\n\nA9:\n\nWhen covering pointers on a programming course at university, we were given two hints as to how to begin learning about them. The first was to view Pointer Fun With Binky. The second was to think about the Haddocks' Eyes passage from Lewis Carroll's Through the Looking-Glass\n\n\u201cYou are sad,\u201d the Knight said in an anxious tone: \u201cLet me sing you a song to comfort you.\u201d\n\u201cIs it very long?\u201d Alice asked, for she had heard a good deal of poetry that day.\n\u201cIt's long,\u201d said the Knight, \u201cbut it's very, very beautiful. Everybody that hears me sing it - either it brings the tears to their eyes, or else -\u201d\n\u201cOr else what?\u201d said Alice, for the Knight had made a sudden pause.\n\u201cOr else it doesn't, you know. The name of the song is called \u2018Haddocks' Eyes.\u2019\u201d\n\u201cOh, that's the name of the song, is it?\" Alice said, trying to feel interested.\n\u201cNo, you don't understand,\u201d the Knight said, looking a little vexed. \u201cThat's what the name is called. The name really is \u2018The Aged Aged Man.\u2019\u201d\n\u201cThen I ought to have said \u2018That's what the song is called\u2019?\u201d Alice corrected herself.\n\u201cNo, you oughtn't: that's quite another thing! The song is called \u2018Ways And Means\u2019: but that's only what it's called, you know!\u201d\n\u201cWell, what is the song, then?\u201d said Alice, who was by this time completely bewildered.\n\u201cI was coming to that,\u201d the Knight said. \u201cThe song really is \u2018A-sitting On A Gate\u2019: and the tune's my own invention.\u201d\n\n\n\nA10:\n\nNotice the assignments:\nipp = &ip1;\n\nresults ipp to point to ip1.\nso for ipp to point to ip2, we should change in the similar manner,\nipp = &ip2;\n\nwhich we are clearly not doing. Instead we are changing the value at address pointed by ipp.\nBy doing the folowing\n*ipp = ip2;\n\nwe are just replacing the value stored in ip1.\nipp = &ip1 , means *ipp = ip1 = &i, \nNow, *ipp = ip2 = &j.\nSo, *ipp = ip2 is essentially same as ip1 = ip2.\n\n\nA11:\n\nVery beginning you set,\nipp = &ip1;\n\nNow dereference it as,\n*ipp = *&ip1 // Here *& becomes 1  \n*ipp = ip1   // Hence proved \n\n\n\nA12:\n\nBecause you are changing the pointer of *ipp. It means  \n\nipp (varaiable name)----go inside.\ninside ipp is address of ip1.\nnow *ipp so go to (adress of inside) ip1.  \n\nNow we are at ip1.\n*ipp(i.e.ip1) = ip2.\nip2 contain address of j.so ip1 content will be replace by contain of ip2(i.e. address of j),\nWE ARE NOT CHANGING ipp CONTENT.\nTHAT'S IT. \n\n\nA13:\n\nConsidere each variable represented like this:\ntype  : (name, adress, value)\n\nso your variables should be represented like this\nint   : ( i ,  &i , 5 ); ( j ,  &j ,  6); ( k ,  &k , 5 )\n\nint*  : (ip1, &ip1, &i); (ip1, &ip1, &j)\n\nint** : (ipp, &ipp, &ip1)\n\nAs the value of ipp is &ip1 so the inctruction:\n*ipp = ip2;\n\nchanges the value at the addess &ip1 to the value of ip2, which means ip1 is changed:\n(ip1, &ip1, &i) -> (ip1, &ip1, &j)\n\nBut ipp still:\n(ipp, &ipp, &ip1)\n\nSo the value of ipp still &ip1 which means it still points to ip1.\n\n\nA14:\n\nForget for a second about the pointing analogy.  What a pointer really contains is a memory address.  The & is the \"address of\" operator - i.e. it returns the address in memory of an object.  The * operator gives you the object a pointer refers to, i.e. given a pointer containing an address, it returns the object at that memory address.  So when you do *ipp = ip2, what you are doing is *ipp get the object at the address held in ipp which is   ip1 and then assign to ip1 the value stored in ip2, which is the address of j. \nSimply\n& --> Address of\n* --> Value at\n\n\nA15:\n\nConsider the below figure and program to understand this concept better.\n\nAs per the figure, ptr1 is a single pointer which is having address of variable num.\nptr1 = &num;\n\nSimilarly ptr2 is a pointer to pointer(double pointer) which is having the address of pointer ptr1. \nptr2 = &ptr1;\n\nA pointer which points to another pointer is known as double pointer. In this example ptr2 is a double pointer.\nValues from above diagram :\nAddress of variable num has : 1000\nAddress of Pointer ptr1 is: 2000\nAddress of Pointer ptr2 is: 3000\n\nExample:\n#include <stdio.h>\n\nint main ()\n{\n   int  num = 10;\n   int  *ptr1;\n   int  **ptr2;\n\n   // Take the address of var \n   ptr1 = &num;\n\n   // Take the address of ptr1 using address of operator &\n   ptr2 = &ptr1;\n\n   // Print the value\n   printf(\"Value of num = %d\\n\", num );\n   printf(\"Value available at *ptr1 = %d\\n\", *ptr1 );\n   printf(\"Value available at **ptr2 = %d\\n\", **ptr2);\n}\n\nOutput:\nValue of num = 10\nValue available at *ptr1 = 10\nValue available at **ptr2 = 10\n\n\n\nA16:\n\nipp = &ip1;\n\nNo later assignment has changed the value of ipp. This is why it still points to ip1.\nWhat you do with *ipp, i.e., with ip1, does not change the fact that ipp points to ip1.\n\n\nA17:\n\nVery good question.\nIn K&R \"C programming language\" \"5.1 Pointers and Addresses\", we can get an answer for this.\n\"The unary operators * and & bind more tightly than arithmetic operators\"\n*ptr += 1      //Increment what ptr points to.\n\n\"Unary operators like * and ++ associate right to left.\"\n*ptr++        //Increment prt instead of what ptr point to.\n\n//It works like *(ptr++).\nThe correct way is:\n(*ptr)++      //This will work.\n\n\n\nA18:\n\nipp can hold a value of (i.e point to) a pointer to pointer type object. When you do  \nipp = &ip2;  \n\nthen the ipp contains the address of the variable (pointer) ip2, which is (&ip2) of type pointer to pointer. Now the arrow of ipp in second pic will point to ip2.   \nWiki says:\nThe * operator is a dereference operator operates on pointer variable, and returns an l-value (variable) equivalent to the value at pointer address. This is called dereferencing the pointer.  \nApplying * operator on ipp derefrence it to a l-value of pointer to int type. The dereferenced l-value *ipp is of type pointer to int, it can hold the address of an int type data. After the statement   \nipp = &ip1;\n\nipp is holding the address of ip1 and *ipp is holding the address of (pointing to) i. You can say that *ipp is an alias of ip1. Both **ipp and *ip1 are alias for i.\nBy doing  \n *ipp = ip2;  \n\n*ipp and ip2 both points to same location but ipp is still pointing to ip1.   \nWhat *ipp = ip2; does actually is that it copies the contents of ip2 (the address of j) to ip1 (as *ipp is an alias for ip1), in effect making both pointers ip1 and ip2 pointing to the same object (j).\nSo, in the second figure, arrow of ip1 and ip2 is pointing to j while ipp is still pointing to ip1 as no modification is done to change the value of ipp.\n\n\nA19:\n\nA pointer to a pointer is also called a handle. One usage for it is often when an object can be moved in memory or removed. One is often responsible to lock and unlock the usage of the object so it will not be moved when accessing it.\nIt's often used in memory restricted environment, ie the Palm OS.\n\ncomputer.howstuffworks.com Link>>\nwww.flippinbits.com Link>>\n\n\n\nA20:\n\nA pointer-to-a-pointer is used when a reference to a pointer is required.  For example, when you wish to modify the value (address pointed to) of a pointer variable declared in a calling function's scope inside a called function.\nIf you pass a single pointer in as an argument, you will be modifying local copies of the pointer, not the original pointer in the calling scope.  With a pointer to a pointer, you modify the latter.\n\n\nA21:\n\nThe order of precedence for the 3 operators involved in your question is the following :\npost-increment ++ > dereference * > assignment +=\nYou can check this page for further details on the subject.\n\nWhen parsing an expression, an operator which is listed on some row will be bound tighter (as if by parentheses) to its arguments than any operator that is listed on a row further below it. For example, the expression *p++ is parsed as *(p++), and not as (*p)++.\n\nLong story short, in order to express this assignment *ptr+=1 using the post-increment operator you need to add parentheses to the dereference operator to give that operation precedence over ++ as in this (*ptr)++\n\n\nA22:\n\nLike most beginner questions in the C tag, this question can be answered by going back to first principles:\n\nA pointer is a kind of value.\nA variable contains a value.\nThe & operator turns a variable into a pointer.\nThe * operator turns a pointer into a variable.\n\n(Technically I should say \"lvalue\" instead of \"variable\", but I feel it is more clear to describe mutable storage locations as \"variables\".)\nSo we have variables:\nint i = 5, j = 6;\nint *ip1 = &i, *ip2 = &j;\n\nVariable ip1 contains a pointer. The & operator turns i into a pointer and that pointer value is assigned to ip1. So ip1 contains a pointer to i.\nVariable ip2 contains a pointer. The & operator turns j into a pointer and that pointer is assigned to ip2. So ip2 contains a pointer to j.\nint **ipp = &ip1;\n\nVariable ipp contains a pointer. The & operator turns variable ip1 into a pointer and that pointer value is assigned to ipp. So ipp contains a pointer to ip1.\nLet's sum up the story so far:\n\ni contains 5\nj contains 6\nip1 contains \"pointer to i\"\nip2 contains \"pointer to j\"\nipp contains \"pointer to ip1\"\n\nNow we say\n*ipp = ip2;\n\nThe * operator turns a pointer back into a variable. We fetch the value of ipp, which is \"pointer to ip1 and turn it into a variable. What variable? ip1 of course!\nTherefore this is simply another way of saying \nip1 = ip2;\n\nSo we fetch the value of ip2. What is it? \"pointer to j\". We assign that pointer value to ip1, so ip1 is now \"pointer to j\"\nWe only changed one thing: the value of ip1:\n\ni contains 5\nj contains 6\nip1 contains \"pointer to j\"\nip2 contains \"pointer to j\"\nipp contains \"pointer to ip1\"\n\n\nWhy does ipp still point to ip1 and not ip2?\n\nA variable changes when you assign to it. Count the assignments; there cannot be more changes to variables than there are assignments!  You start by assigning to i, j, ip1, ip2 and ipp.  You then assign to *ipp, which as we've seen means the same as \"assign to ip1\". Since you didn't assign to ipp a second time, it didn't change!\nIf you wanted to change ipp then you'll have to actually assign to ipp:\nipp = &ip2;\n\nfor instance.\n\n\nA23:\n\nTo make sure you don't break stuff, make a copy of the returned string.\nThe function returning const char* expects this string will never be changed. Therefore things can/will break if your code or the API you pass it make a change after all.\nEven worse, if a change is made, your program is likely to crash you in case the returned string was literal (e.g. \"hello I'm a literal string\") because they are (often) stored in memory that can't be written to.\nYou could use strdup() for this, but read the small print. Or you can of course create your own version if it's not there on your platform.\n\n\nA24:\n\nIf you'd want ipp to point to ip2, you'd have to say ipp = &ip2;. However, this would leave ip1 still pointing to i.\n\n\nA25:\n\n*ipp = ip2; implies:\nAssign ip2 to the variable pointed to by ipp. So this is equivalent to:\nip1 = ip2;\n\nIf you want the address of ip2 to be stored in ipp, simply do:\nipp = &ip2;\n\nNow ipp points to ip2.\n\n\nA26:\n\nYou have a variable that contains an address of something. That's a pointer.\nThen you have another variable that contains the address of the first variable. That's a pointer to pointer.\n\n\nA27:\n\nBecause you changed the value pointed to by ipp not the value of ipp. So, ipp still points to ip1 (the value of ipp), ip1's value is now the same as ip2's value, so they both point to j.\nThis:\n*ipp = ip2;\n\nis the same as:\nip1 = ip2;\n\n\n\nA28:\n\n\nMy question is: Why in the second picture, ipp is still point to ip1 but not ip2?\n\nyou placed nice pictures, I'm going to try to make nice ascii art:\nLike @Robert-S-Barnes said in his answer: forget about pointers, and what points to what, but think in terms of memory. Basically, an int* means that it contains the address of a variable and an int** contains the address of a variable that contains the address of a variable. Then you can use the pointer's algebra to access the values or the addresses: &foo means address of foo, and *foo means value of the address contained in foo.\nSo, as pointers is about dealing with memory, the best way to actually make that \"tangible\" is to show what the pointers algebra does to the memory.\nSo, here's your program's memory (simplified for the purpose of the example):\nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [   |   |   |   |   ]\n\nwhen you do your initial code:\nint i = 5, j = 6;\nint *ip1 = &i, *ip2 = &j;\n\nhere's how your memory looks like:\nname:    i   j ip1 ip2\naddr:    0   1   2   3\nmem : [  5|  6|  0|  1]\n\nthere you can see ip1 and ip2 gets the addresses of i and j and ipp still does not exists.\nDon't forget that addresses are simply integers stored with a special type.\nThen you declare and defined ipp such as:\nint **ipp = &ip1;\n\nso here's your memory:\nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [  5|  6|  0|  1|  2]\n\nand then, you're changing the value pointed by the address stored in ipp, which is\nthe address stored in ip1:\n*ipp = ip2;\n\nthe program's memory is \nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [  5|  6|  1|  1|  2]\n\nN.B.: as int* is a special type, I prefer to always avoid declaring multiple pointers on the same line, as I think the int *x; or int *x, *y; notation can be misleading. I prefer to write int* x; int* y;\nHTH\n\n\nA29:\n\n*ptr += 1 : Increment data that ptr points to.\n*ptr++ : Increment pointer that is point to next memory location instead of the data that pointer points to.\n\n\nA30:\n\nit's a pointer to the pointer's address value. (that's terrible I know)\nbasically, it lets you pass a pointer to the value of the address of another pointer, so you can modify where another pointer is pointing from a sub function, like:\nvoid changeptr(int** pp)\n{\n  *pp=&someval;\n}\n\n\n\nA31:\n\nLet's apply parentheses to show the order of operations\na + b / c\na + (b/c)\n\nLet's do it again with\n*ptr   += 1\n(*ptr) += 1\n\nAnd again with\n*ptr++\n*(ptr++)\n\n\nIn *ptr += 1, we increment the value of the variable our pointer points to.\nIn *ptr++, we increment the pointer after our entire statement (line of code) is done, and return a reference to the variable our pointer points to.\n\nThe latter allows you to do things like:\nfor(int i = 0; i < length; i++)\n{\n    // Copy value from *src and store it in *dest\n    *dest++ = *src++;\n\n    // Keep in mind that the above is equivalent to\n    *(dest++) = *(src++);\n}\n\nThis is a common method used to copy a src array into another dest array.\n\n\nA32:\n\nFirst of all you should do such things only if it is really necessary - e.g. to use some old-style API with char* arguments which are not modified. If an API function modifies the string which was const originally, then this is unspecified behaviour, very likely crash.\nUse cast:\n(char*)const_char_ptr\n\n\n\nA33:\n\nIf you add the dereference operator * to the pointer, you redirect from the pointer to the pointed-to object.\nExamples:\nint i = 0;\nint *p = &i; // <-- N.B. the pointer declaration also uses the `*`\n             //     it's not the dereference operator in this context\n*p;          // <-- this expression uses the pointed-to object, that is `i`\np;           // <-- this expression uses the pointer object itself, that is `p`\n\nTherefore:\n*ipp = ip2; // <-- you change the pointer `ipp` points to, not `ipp` itself\n            //     therefore, `ipp` still points to `ip1` afterwards.\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 17, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nThis is a complex question, please consider carefully before answering.\nConsider this situation.  Two threads (a reader and a writer) access a single global int.  Is this safe?  Normally, I would respond without thought, yes!\nHowever, it seems to me that Herb Sutter doesn't think so.  In his articles on effective concurrency he discusses a flawed lock-free queue and the corrected version.\nIn the end of the first article and the beginning of the second he discusses a rarely considered trait of variables, write ordering.  Int's are atomic, good, but ints aren't necessarily ordered which could destroy any lock-free algorithm, including my above scenario.  I fully agree that the only way to guarantee correct multithreaded behavior on all platforms present and future is to use atomics(AKA memory barriers) or mutexes.\nMy question; is write re-odering ever a problem on real hardware?  Or is the multithreaded paranoia just being pedantic?\nWhat about classic uniprocessor systems?\nWhat about simpler RISC processors like an embedded power-pc?\nClarification:  I'm more interested in what Mr. Sutter said about the hardware (processor/cache) reordering variable writes.  I can stop the optimizer from breaking code with compiler switches or hand inspection of the assembly post-compilation.  However, I'd like to know if the hardware can still mess up the code in practice.\n\n\nCandidate answers:\nA1:\n\nYup - use memory barriers to prevent instruction reordering where needed.  In some C++ compilers, the volatile keyword has been expanded to insert implicit memory barriers for every read and write - but this isn't a portable solution.  (Likewise with the Interlocked* win32 APIs).  Vista even adds some new finer-grained Interlocked APIs which let you specify read or write semantics.\nUnfortunately, C++ has such a loose memory model that any kind of code like this is going to be non-portable to some extent and you'll have to write different versions for different platforms.\n\n\nA2:\n\nThe basic rule is: the compiler must make the global state appear to be exactly as you coded it, but if it can prove that a given function doesn't use global variables then it can implement the algorithm any way it chooses.\nThe upshot is that traditional compilers always treated functions in another compilation unit as a memory barrier because they couldn't see inside those functions. Increasingly, modern compilers are growing \"whole program\" or \"link time\" optimization strategies which break down these barriers and will cause poorly written code to fail, even though it's been working fine for years.\nIf the function in question is in a shared library then it won't be able to see inside it, but if the function is one defined by the C standard then it doesn't need to -- it already knows what the function does -- so you have to be careful of those also. Note that a compiler will not recognise a kernel call for what it is, but the very act of inserting something that the compiler can't recognise (inline assembler, or a function call to an assembler file) will create a memory barrier in itself.\nIn your case, notify will either be a black box the compiler can't see inside (a library function) or else it will contain a recognisable memory barrier, so you are most likely safe.\nIn practice, you have to write very bad code to fall over this.\n\n\nA3:\n\nYour idea of inspecting the assembly is not good enough; the reordering can happen at the hardware level.\nTo answer your question \"is this ever a problem on read hardware:\" Yes!  In fact I've run into that problem myself.\nIs it OK to skirt the issue with uniprocessor systems or other special-case situations?  I would argue \"no\" because five years from now you might need to run on multi-core after all, and then finding all these locations will be tricky (impossible?).\nOne exception: Software designed for embedded hardware applications where indeed you have completely control over the hardware.  In fact I have \"cheated\" like this in those situations on e.g. an ARM processor.\n\n\nA4:\n\nI have also been looking for such a book, they are very hard to come by. This one will be released in May, if that's any help:\nhttp://www.manning.com/williams/\nI purchased this book:\nhttp://www.amazon.co.uk/gp/product/0123705916/ref=oss_product\nIt's very good, it's in java, but most of the principles apply to c/c++ anyway.\n\n\nA5:\n\nTo make this faster, use a thread pool pattern.\nThis will let you enqueue tasks in other threads without the overhead of creating a std::thread each time you want to use more than one thread.\nDon't count the overhead of setting up the queue in your performance metrics, just the time to enqueue and extract the results.\nCreate a set of threads and a queue of tasks (a structure containing a std::function<void()>) to feed them.  The threads wait on the queue for new tasks to do, do them, then wait on new tasks.\nThe tasks are responsible for communicating their \"done-ness\" back to the calling context, such as via a std::future<>.  The code that lets you enqueue functions into the task queue might do this wrapping for you, ie this signature:\ntemplate<typename R=void>\nstd::future<R> enqueue( std::function<R()> f ) {\n  std::packaged_task<R()> task(f);\n  std::future<R> retval = task.get_future();\n  this->add_to_queue( std::move( task ) ); // if we had move semantics, could be easier\n  return retval;\n}\n\nwhich turns a naked std::function returning R into a nullary packaged_task, then adds that to the tasks queue.  Note that the tasks queue needs be move-aware, because packaged_task is move-only.\nNote 1: I am not all that familiar with std::future, so the above could be in error.\nNote 2: If tasks put into the above described queue are dependent on each other for intermediate results, the queue could deadlock, because no provision to \"reclaim\" threads that are blocked and execute new code is described.  However, \"naked computation\" non-blocking tasks should work fine with the above model.\n\n\nA6:\n\n\nIntroduction to parallel computing: https://computing.llnl.gov/tutorials/parallel_comp/\nPOSIX threads programming: https://computing.llnl.gov/tutorials/pthreads/\n\n\n\nA7:\n\nA \"hardware thread\" is a physical CPU or core. So, a 4 core CPU can genuinely support 4 hardware threads at once - the CPU really is doing 4 things at the same time.\nOne hardware thread can run many software threads. In modern operating systems, this is often done by time-slicing - each thread gets a few milliseconds to execute before the OS schedules another thread to run on that CPU. Since the OS switches back and forth between the threads quickly, it appears as if one CPU is doing more than one thing at once, but in reality, a core is still running only one hardware thread, which switches between many software threads.\nModern JVMs map java threads directly to the native threads provided by the OS, so there is no inherent overhead introduced by java threads vs native threads. As to hardware threads, the OS tries to map threads to cores, if there are sufficient cores. So, if you have a java program that starts 4 threads, and have 4 or more cores, there's a good chance your 4 threads will run truly in parallel on 4 separate cores, if the cores are idle.\n\n\nA8:\n\nAlthough this is an old question, I recently asked the same question, and just answered it using QT 4.8 and some testing.\nAFAIK you cannot create objects with a parent from a QtConcurrent::run function.  I have tried the following two ways.  Let me define a code block then we will explore the behavior by selecting POINTER_TO_THREAD.\nSome psuedo code will show you my test\nClass MyClass : public QObject\n{\n  Q_OBJECT\npublic:\n  doWork(void)\n  {\n    QObject* myObj = new QObject(POINTER_TO_THREAD);\n    ....\n  }\n}\n\nvoid someEventHandler()\n{\n  MyClass* anInstance = new MyClass(this);\n  QtConcurrent::run(&anInstance, &MyClass::doWork)\n}\n\nIgnoring potential scoping issues...\nIf POINTER_TO_THREAD is set to this, then you will get an error because this will resolve to a pointer to the anInstance object which lives in the main thread, not the thread QtConcurrent has dispatched for it.  You will see something like...\nCannot create children for a parent in another thread.  Parent: anInstance, parents thread: QThread(xyz), currentThread(abc)\nIf POINTER_TO_THREAD is set to QObject::thread(), then you will get an error because because it will resolve to the QThread object in which anInstance lives, and not the thread QtConcurrent has dispatched for it. You will see something like...\nCannot create children for a parent in another thread.  Parent: QThread(xyz), parents thread: QThread(xyz), currentThread(abc)\nHope my testing is of use to someone else.  If anyone knows a way to get a pointer to the QThread which QtConcurrent runs the method in, I would be interested to hear it!\n\n\nA9:\n\nThis file is missing. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58016\nIt was fixed only in gcc 4.9, as its release notes says (http://gcc.gnu.org/gcc-4.9/changes.html)\n\n\nA10:\n\nSoftware threads are threads of execution managed by the operating system.\nHardware threads are a feature of some processors that allow better utilisation of the processor under some circumstances.  They may be exposed to/by the operating system as appearing to be additional cores (\"hyperthreading\").\nIn Java, the threads you create maintain the software thread abstraction, where the JVM is the \"operating system\".  Whether the JVM then maps Java threads to OS threads is the JVM's business (but it almost certainly does).  And then the OS will be using hardware threads if they are available.\n\n\nA11:\n\nYou cannot read the value of a std::atomic_flag without setting it to true. This is by design. It is not a boolean variable (we have std::atomic<bool> for that), but a minimal flag that is guaranteed lock free on all architectures that support C++11.\nOn some platforms the only atomic instructions are exchange instructions. On such platforms, std::atomic_flag::test_and_set() can be implemented with exchange var,1 and clear() with exchange var,0, but there is no atomic instruction for reading the value.\nSo, if you want to read the value without changing it, then you need std::atomic<bool>.\n\n\nA12:\n\nAlthough the Qt docs don't appear to specify the behaviour you could find out by keeping track of what QObject::thread() returns before and after the thread finishes.\n\n\nA13:\n\n\nHowever, once the worker thread terminates, the QObject thread affinity should no longer be valid.\n\nThe worker thread does NOT terminate after your function call. The whole point of using QtConcurrent::run is executing a large number of small tasks on the global thread pool (or some provided QThreadPool) while re-using threads to avoid the overhead of creating and destroying threads for each one of these small tasks. In addition to distributing computation across all available cores.\nYou can try looking at the source code for Qt to see how QtConcurrent::run is implemented. You will see that it ends up calling RunFunctionTaskBase::start, which essentially calls QThreadPool::start with a QRunnable that calls the function that was passed initially to QtConcurrent::run.\nNow the point that I want to get to is that, QThreadPool::start is implemented by adding the QRunnable to a queue, and then trying to wake up one of the threads from the thread pool (which are waiting for a new QRunnable to be added to the queue). The thing to note here, is that threads from the thread pool are not running an event loop (they are not designed to act this way), they are there just to execute QRunnables in the queue and nothing more (they are implemented this way for performance reasons obviously).\nThis means that, the moment you are creating a QObject in a function executed in QtConcurrent::run, you are just creating a QObject that lives in a thread with no event-loop, from the docs, restrictions include:\n\nIf no event loop is running, events won't be delivered to the object. For example, if you create a QTimer object in a thread but never call exec(), the QTimer will never emit its timeout() signal. Calling deleteLater() won't work either. (These restrictions apply to the main thread as well.)\n\n\nTL;DR: QtConcurrent::run runs functions in threads from the global QThreadPool (or a provided one). Those threads do not run an event loop, They just wait for QRunnables to run. So, a QObject living in a thread from these threads doesn't get any events delivered.\n\nIn the documentation, They have put using QThread (possibly, with an event loop and a worker object) and using QtConcurrent::run as two separate multi-threading technologies. They are not meant to be mixed together. So, no worker objects in thread pools, this is just asking for trouble.\n\nThe question: Does Qt automatically move the QObjects into the parent thread, or are we responsible in moving them to a valid thread before the worker thread terminates?\n\nI think that after looking at things this way, The answer is obvious that Qt does NOT move QObjects into any thread automatically. The documentation has warned about using a QObject in a QThread without an event loop, and that's it.\nYou are free to move them to whatever thread you like. But please keep in mind that moveToThread() can sometimes cause problems. For example, if moving your worker object involves moving a QTimer:\n\nNote that all active timers for the object will be reset. The timers are first stopped in the current thread and restarted (with the same interval) in the targetThread. As a result, constantly moving an object between threads can postpone timer events indefinitely.\n\n\nConclusion: I think that you should consider using your own QThread that runs its event loop, and create your worker QObjects there instead of using QtConcurrent. This way is far better than moving QObjects around, and can avoid many errors that can arise from using your current approach. Have a look at the comparison table of multi-threading technologies in Qt and choose the technology that best suits your use case. Only use QtConcurrent if you want to just execute a one-call function and get its return value. If you want permanent interaction with the thread, you should switch to using your own QThread with worker QObjects.\n\n\nA14:\n\nThe time needed to execute the program is very small (33msec). This means that the overhead to create and handle several threads may be more than the real benefit. Try using programs that need longer times for the execution (e.g., 10 sec).\n\n\nA15:\n\nThis is an old question, but since there is no answer given, why not use a class that has its own static registration?\n#include <mutex>\n#include <thread>\n#include <unordered_map>\n\nstruct foo;\n\nstatic std::unordered_map<std::thread::id, foo*> foos;\nstatic std::mutex foos_mutex;\n\nstruct foo\n{\n    foo()\n    {\n        std::lock_guard<std::mutex> lk(foos_mutex);\n        foos[std::this_thread::get_id()] = this;\n    }\n};\n\nstatic thread_local foo tls_foo;\n\n\nOf course you would need some kind of synchronization between the threads to ensure that the thread had registered the pointer, but you can then grab it from the map from any thread where you know the thread's id.\n\n\nA16:\n\nIf you want thread local variables that are not thread local, why don't you use global variables instead?\nImportant clarification!\nI am not suggesting that you use a single global to replace a thread-local variable. I 'm suggesting of using a single global array or other suitable collection of values to replace one thread-local variable.\nYou will have to provide synchronization of course, but since you want to expose a value modified in thread A to thread B there's no getting around that.\nUpdate:\nThe GCC documentation on __thread says:\n\nWhen the address-of operator is\n  applied to a thread-local variable, it\n  is evaluated at run-time and returns\n  the address of the current thread's\n  instance of that variable. An address\n  so obtained may be used by any thread.\n  When a thread terminates, any pointers\n  to thread-local variables in that\n  thread become invalid.\n\nTherefore, if you insist on going this way I imagine it's possible to get the address of a thread local variable from the thread it belongs to, just after the thread is spawned. You could then store a pointer to that memory location to a map (thread id => pointer), and let other threads access the variable this way. This assumes that you own the code for the spawned thread.\nIf you are really adventurous, you could try digging up information on ___tls_get_addr (start from this PDF which is linked to by the aforementioned GCC docs). But this approach is so highly compiler and platform specific and so lacking in documentation that it should be causing alarms to go off in anyone's head.\n\n\nA17:\n\nAs you discovered, rand is the culprit here.\nFor those who are curious, it's possible that this behavior comes from your implementation of rand using a mutex for thread safety.\nFor example, eglibc defines rand in terms of __random, which is defined as:\nlong int\n__random ()\n{\n  int32_t retval;\n\n  __libc_lock_lock (lock);\n\n  (void) __random_r (&unsafe_state, &retval);\n\n  __libc_lock_unlock (lock);\n\n  return retval;\n}\n\nThis kind of locking would force multiple threads to run serially, resulting in lower performance.\n\n\nA18:\n\nWith C++20 we got the test() method, which does exactly what OP wants.\n\n\nA19:\n\nLike you said, because of reordering done at cache or processor level, you actually do need some sort of memory barrier to ensure proper synchronisation, especially for multi-processors (and especially on non-x86 platforms). (I am given to believe that single-processor systems don't have these issues, but don't quote me on this---I'm certainly more inclined to play safe and do the synchronised access anyway.)\n\n\nA20:\n\nMemory barriers aren't just to prevent instruction reordering. Even if instructions aren't reordered it can still cause problems with cache coherence. As for the reordering - it depends on your compiler and settings. ICC is particularly agressive with reordering. MSVC w/ whole program optimization can be, too. \nIf your shared data variable is declared as volatile, even though it's not in the spec most compilers will generate a memory variable around reads and writes from the variable and prevent reordering. This is not the correct way of using volatile, nor what it was meant for.\n(If I had any votes left, I'd +1 your question for the narration.)\n\n\nA21:\n\nOn my system the behavior is same, but as Maxim mentioned, rand is not thread safe. When I change rand to rand_r, then the multi threaded code is faster as expected.\nvoid add_multi(int N, double& result) {\ndouble sum=0;\nunsigned int seed = time(NULL);\nfor (int i = 0; i < N; ++i){\n    sum+= sqrt(1.0*rand_r(&seed)/RAND_MAX);\n}\nresult = sum/N;\n}\n\n\n\nA22:\n\nIn practice, he's correct and a memory barrier is implied in this specific case. \nBut the point is that if its presence is \"debatable\", the code is already too complex and unclear.\nReally guys, use a mutex or other proper constructs. It's the only safe way to deal with threads and to write maintainable code.\nAnd maybe you'll see other errors, like that the code is unpredictable if send() is called more than one time.\n\n\nA23:\n\nIf you will be working with UNIX-like systems, then I recommend Programming With POSIX Threads by David R. Butenhof.\nIf you will be working with Microsoft Windows, then I recommend Writing Multithreaded Applications in Win32 by Jim Beveridge and Robert Wiener.\nIrrespective of which threading package(s) you will end up using, I recommend you look at two presentations I wrote: Generic Synchronization Policies and Multi-threaded Performance Pitfalls. Those short presentations contain useful information that, unfortunately, is not discussed in many other books and articles.\n\n\nA24:\n\nIt's a deliberate design choice/trade-off.\nFirst, it's not necessarily possible to find out whether the functionoid passed to async takes its arguments by reference or not. (If it's not a simple function but a function object, it could have an overloaded function call operator, for example.) So async cannot say, \"Hey, let me just check what the target function wants, and I'll do the right thing.\"\nSo the design question is, does it take all arguments by reference if possible (i.e. if they're lvalues), or does it always make copies? Making copies is the safe choice here: a copy cannot become dangling, and a copy cannot exhibit race conditions (unless it's really weird). So that's the choice that was made: all arguments are copied by default.\nBut then, the mechanism is written so that it actually fails to then pass the arguments to a non-const lvalue reference parameter. That's another choice for safety: otherwise, the function that you would expect to modify your original lvalue instead modifies the copy, leading to bugs that are very hard to track down.\nBut what if you really, really want the non-const lvalue reference parameter? What if you promise to watch out for dangling references and race conditions? That's what std::ref is for. It's an explicit opt-in to the dangerous reference semantics. It's your way of saying, \"I know what I'm doing here.\"\n\n\nA25:\n\nQThread is not documented to automatically move any QObjects when it finishes, so I think we can already conclude that it does no such thing. Such behavior would be very surprising, and at odds with the rest of the API.\nJust for completeness, I tested with Qt 5.6:\nQObject o;\n{\n    QThread t;\n    o.moveToThread(&t);\n    for (int i = 0; i < 2; ++i)\n    {\n        t.start();\n        QVERIFY(t.isRunning());\n        QVERIFY(o.thread() == &t);\n        t.quit();\n        t.wait();\n        QVERIFY(t.isFinished());\n        QVERIFY(o.thread() == &t);\n    }\n}\nQVERIFY(o.thread() == nullptr);\n\nRecall that a QThread is not a thread, it manages a thread.\nWhen a QThread finishes, it continues to exist, and the objects that live in it continue to live in it, but they no longer process events. The QThread can be restarted (not recommended), at which point event processing will resume (so the same QThread could then be managing a different thread).\nWhen a QThread is destroyed, the objects that lived in it cease to have any thread affinity. The documentation doesn't guarantee this, and in fact says \"You must ensure that all objects created in a thread are deleted before you delete the QThread.\"\n\n\nLet's say I call QtConcurrent::run() which runs a function in a worker thread, and in that function I dynamically allocate several QObjects (for later use). Since they were created in the worker thread, their thread affinity should be that of the worker thread. However, once the worker thread terminates, the QObject thread affinity should no longer be valid.\n\nThe QThread does not terminate in this scenario. When a task spawned by QtConcurrent::run finishes, the QThread it was running in is returned to the QThreadPool and may be reused by a subsequent call to QtConcurrent::run, and QObjects living in that QThread continue to live there.\nQThreadPool::globalInstance()->setMaxThreadCount(1);\nQObject *o = nullptr;\nQThread *t = nullptr;\nQFuture<void> f = QtConcurrent::run([&] {\n    o = new QObject;\n    t = o->thread();\n    QVERIFY(t == QThread::currentThread());\n});\nf.waitForFinished();\nQVERIFY(t == o->thread());\nQVERIFY(t->isRunning());\nf = QtConcurrent::run([=] {\n    QVERIFY(t == QThread::currentThread());\n});\nf.waitForFinished();\n\nYou might want to manually move an object out of a QThread before it is returned to the QThreadPool, or just don't use QtConcurrent::run. Having a QtConcurrent::run task construct QObjects which outlive the task is a questionable design, tasks should be self-contained. As noted by @Mike, the QThreads used by QtConcurrent::run do not have event loops.\n\n\nA26:\n\nI am searching for the same thing. \nAs I see nobody has answered your question after having searched the web in all ways I arrived to the subsequent information: supposing to compile for gcc on linux (ubuntu) and using -m64, the segment register gs holds the value 0. The hidden part of the segment (holding the linear address) \npoints to the thread specific local area.\nThat area contains at that address the address of that address ( 64 bits ). At lower addresses are stored all thread local variables.\nThat address is the native_handle().\nSo in order to access a threads local data you should do it via that pointer. \nIn other words: (char*)&variable-(char*)myThread.native_handle()+(char*)theOtherThread.native_handle()\nThe code that demonstrates the above supposing g++,linux,pthreads is:\n#include <iostream>\n#include <thread>\n#include <sstream>\n\nthread_local int B=0x11111111,A=0x22222222;\n\nbool shouldContinue=false;\n\nvoid code(){\n    while(!shouldContinue);\n    std::stringstream ss;\n    ss<<\" A:\"<<A<<\" B:\"<<B<<std::endl;\n    std::cout<<ss.str();\n}\n\n//#define ot(th,variable) \n//(*( (char*)&variable-(char*)(pthread_self())+(char*)(th.native_handle()) ))\n\nint& ot(std::thread& th,int& v){\n    auto p=pthread_self();\n    intptr_t d=(intptr_t)&v-(intptr_t)p;\n    return *(int*)((char*)th.native_handle()+d);\n}\n\nint main(int argc, char **argv)\n{       \n\n        std::thread th1(code),th2(code),th3(code),th4(code);\n\n        ot(th1,A)=100;ot(th1,B)=110;\n        ot(th2,A)=200;ot(th2,B)=210;\n        ot(th3,A)=300;ot(th3,B)=310;\n        ot(th4,A)=400;ot(th4,B)=410;\n\n        shouldContinue=true;\n\n        th1.join();\n        th2.join();\n        th3.join();\n        th4.join();\n\n    return 0;\n}\n\n\n\nA27:\n\nGCC Atomic Built-ins\n\n\nA28:\n\nIn practice, a function call is a compiler barrier, meaning that the compiler will not move global memory accesses past the call. A caveat to this is functions which the compiler knows something about, e.g. builtins, inlined functions (keep in mind IPO!) etc.\nSo a processor memory barrier (in addition to a compiler barrier) is in theory needed to make this work. However, since you're calling read and write which are syscalls that change the global state, I'm quite sure that the kernel issues memory barriers somewhere in the implementation of those. There is no such guarantee though, so in theory you need the barriers.\n\n\nA29:\n\nThe issue itself is only marginally related to std::async(): When defining the result of the operation, std::async() uses std::result_of<...>::type with all its arguments being std::decay<...>::type'ed. This is reasonable because std::async() takes arbitrary types and forwards them to store them in some location. To store them, values are needed for the function object as well as for the arguments. Thus, std::result_of<...> is used similar to this:\ntypedef std::result_of<void (*(int))(int&)>::type result_type;\n\n... and since int can't be bound to an int& (int isn't an lvalue type was is needed to be bound to int&), this fails. Failure in this case means that std::result_of<...> doesn't define a nested type.\nA follow-up question could be: What is this type used to instantiate std::result_of<...>? The idea is that the function call syntax consisting of ResultType(ArgumentTypes...) is abused: instead of a result type, a function type is passed and std::result_of<...> determines the type of the function called when that function type is called with the given list of arguments is called. For function pointer types it isn't really that interesting but the function type can also be a function object where overloading needs to be taken into account. So basically, std::result_of<...> is used like this:\ntypedef void (*function_type)(int&);\ntypedef std::result_of<function_type(int)>::type result_type; // fails\ntypedef std::result_of<function_type(std::reference_wrapper<int>)>::type result_type; //OK\n\n\n\nA30:\n\nIf you want to use atomic_flag to determine whether a thread should exit, you can do it like this:\nInitialization:\nstd::atomic_flag keep_running = ATOMIC_FLAG_INIT;\nkeep_running.test_and_set();\n\nThread loop:\nwhile (keep_running.test_and_set()) {\n    // do thread stuff\n}\n\nWhen you want the thread to exit:\nkeep_running.clear();\n\n\n\nA31:\n\nI am not sure if Qt automatically change the thread affinity. But even if it does, the only reasonable thread to move to is the main thread. I would push them at the end of the threaded function myself.\nmyObject->moveToThread(QApplication::instance()->thread());\n\nNow this only matters if the objects make use of event process like send and receive signals.\n\n\nA32:\n\n\nDoes Qt automatically move the QObjects into the parent thread, or are we responsible in moving them to a valid thread before the worker thread terminates?\n\nNo, Qt doesn't automatically move QObject into the parent thread. \nThis behavior doesn't explicitly documented, so I've done a small investigation of the Qt framework source code, master branch.\nQThread starts in QThreadPrivate::start:\nunsigned int __stdcall QT_ENSURE_STACK_ALIGNED_FOR_SSE QThreadPrivate::start(void *arg)\n{\n\n  ...\n\n  thr->run();\n\n  finish(arg);\n  return 0;\n}\n\nQThread::terminate() implementation:\nvoid QThread::terminate()\n{\n  Q_D(QThread);\n  QMutexLocker locker(&d->mutex);\n  if (!d->running)\n      return;\n  if (!d->terminationEnabled) {\n      d->terminatePending = true;\n      return;\n  }\n  TerminateThread(d->handle, 0);\n  d->terminated = true;\n  QThreadPrivate::finish(this, false);\n}\n\nIn both cases thread finalization is done in QThreadPrivate::finish:\nvoid QThreadPrivate::finish(void *arg, bool lockAnyway)\n{\n  QThread *thr = reinterpret_cast<QThread *>(arg);\n  QThreadPrivate *d = thr->d_func();\n\n  QMutexLocker locker(lockAnyway ? &d->mutex : 0);\n  d->isInFinish = true;\n  d->priority = QThread::InheritPriority;\n  bool terminated = d->terminated;\n  void **tls_data = reinterpret_cast<void **>(&d->data->tls);\n  locker.unlock();\n  if (terminated)\n      emit thr->terminated();\n  emit thr->finished();\n  QCoreApplication::sendPostedEvents(0, QEvent::DeferredDelete);\n  QThreadStorageData::finish(tls_data);\n  locker.relock();\n\n  d->terminated = false;\n\n  QAbstractEventDispatcher *eventDispatcher = d->data->eventDispatcher;\n  if (eventDispatcher) {\n      d->data->eventDispatcher = 0;\n      locker.unlock();\n      eventDispatcher->closingDown();\n      delete eventDispatcher;\n      locker.relock();\n  }\n\n  d->running = false;\n  d->finished = true;\n  d->isInFinish = false;\n\n  if (!d->waiters) {\n      CloseHandle(d->handle);\n      d->handle = 0;\n  }\n\n  d->id = 0;\n}\n\nIt posts QEvent::DeferredDelete event to cleanup QObject::deleteLater, than TLS data cleaned up with QThreadStorageData::finish(tls_data) and eventDispatcher deleted. After that QObject will receive no events from this thread, but QObject's thread affinity stays the same. It's interesting to see implementation of void QObject::moveToThread(QThread *targetThread) to understand how thread affinity changes.   \nImplementation of void QThreadPrivate::finish(void *arg, bool lockAnyway) makes clear that QObject's thread affinity is not changed by QThread.\n\n\nA33:\n\nI was unfortunately never able to find a way to do this.\nWithout some kind of thread init hook there just doesn't appear to be a way to get at that pointer (short of ASM hacks that would be platform dependent).\n\n\nA34:\n\nstd::async (and other functions that do perfect forwarding) look at the type of the argument that you pass to figure out what to do. They do not look at how that argument will eventually be used. So, to pass an object by reference you need to tell std::async that you're using a reference. However, simply passing a reference won't do that. You have to use std::ref(value) to pass value by reference.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 18, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nWhen I try to export my database through MySQL Workbench remotely from localserver,\nI am getting some below version error:\n\nmysqldump Version Mismatch  [Content] mysqldump.exe is version 5.5.16, but the MySQL Server to be dumped has version 5.6.10-log. Because the version of mysqldump is older than the server, some features may not be backed up properly. It is recommended you upgrade your local MySQL client programs, including mysqldump to a version equal to or newer than that of the target server. The path to the dump tool must then be set in Preferences -> Administrator -> Path to mysqldump Tool\n\nI am trying to find a solution - I searched for solution on google but couldn't find a good answer to solve the issue.\nDoes anyone know, how to fix this issue in MySQL Workbench?\n\n\nCandidate answers:\nA1:\n\nI spent a good deal of time researching this situation for my own needs. This and several other SO threads helped me a great deal, so I wanted to share what I came up with.\nWhile having access to the interpolated query string is a significant benefit while troubleshooting, we wanted to be able to maintain a log of only certain queries (therefore, using the database logs for this purpose was not ideal). We also wanted to be able to use the logs to recreate the condition of the tables at any given time, therefore, we needed to make certain the interpolated strings were escaped properly. Finally, we wanted to extend this functionality to our entire code base having to re-write as little of it as possible (deadlines, marketing, and such; you know how it is).\nMy solution was to extend the functionality of the default PDOStatement object to cache the parameterized values (or references), and when the statement is executed, use the functionality of the PDO object to properly escape the parameters when they are injected back in to the query string. We could then tie in to execute method of the statement object and log the actual query that was executed at that time (or at least as faithful of a reproduction as possible).\nAs I said, we didn't want to modify the entire code base to add this functionality, so we overwrite the default bindParam() and bindValue() methods of the PDOStatement object, do our caching of the bound data, then call parent::bindParam() or parent::bindValue(). This allowed our existing code base to continue to function as normal.\nFinally, when the execute() method is called, we perform our interpolation and provide the resultant string as a new property E_PDOStatement->fullQuery. This can be output to view the query or, for example, written to a log file.\nThe extension, along with installation and configuration instructions, are available on github:\nhttps://github.com/noahheck/E_PDOStatement\nDISCLAIMER:\nObviously, as I mentioned, I wrote this extension. Because it was developed with help from many threads here, I wanted to post my solution here in case anyone else comes across these threads, just as I did.\n\n\nA2:\n\nI was having the issue previously in XAMPP localhost with phpmyadmin version 4.2.11.\nIncreasing the timeout in php.ini didn't helped either. \nThen I edited xampp\\phpMyAdmin\\libraries\\config.default.php to change the value of $cfg['ExecTimeLimit'], which was 300 by default.\nThat solved my issue.\n\n\nA3:\n\nIn direct opposition to my other answer, this following function is probably safe, even with multi-byte characters.\n// replace any non-ascii character with its hex code.\nfunction escape($value) {\n    $return = '';\n    for($i = 0; $i < strlen($value); ++$i) {\n        $char = $value[$i];\n        $ord = ord($char);\n        if($char !== \"'\" && $char !== \"\\\"\" && $char !== '\\\\' && $ord >= 32 && $ord <= 126)\n            $return .= $char;\n        else\n            $return .= '\\\\x' . dechex($ord);\n    }\n    return $return;\n}\n\nI'm hoping someone more knowledgeable than myself can tell me why the code above won't work ...\n\n\nA4:\n\nI'm using version 4.0.3 of MAMP along with phpmyadmin. The top of /Applications/MAMP/bin/phpMyAdmin/libraries/config.default.php reads:\n\nDO NOT EDIT THIS FILE, EDIT config.inc.php INSTEAD !!!\n\nChanging the following line in /Applications/MAMP/bin/phpMyAdmin/config.inc.php and restarting MAMP worked for me.\n$cfg['ExecTimeLimit'] = 0;\n\n\n\nA5:\n\nWell, according to the mysql_real_escape_string function reference page:  \"mysql_real_escape_string() calls MySQL's library function mysql_real_escape_string, which escapes the following characters: \\x00, \\n, \\r, \\, ', \" and \\x1a.\"\nWith that in mind, then the function given in the second link you posted should do exactly what you need:\nfunction mres($value)\n{\n    $search = array(\"\\\\\",  \"\\x00\", \"\\n\",  \"\\r\",  \"'\",  '\"', \"\\x1a\");\n    $replace = array(\"\\\\\\\\\",\"\\\\0\",\"\\\\n\", \"\\\\r\", \"\\'\", '\\\"', \"\\\\Z\");\n\n    return str_replace($search, $replace, $value);\n}\n\n\n\nA6:\n\nPDOStatement has a public property $queryString. It should be what you want.\nI've just notice that PDOStatement has an undocumented method debugDumpParams() which you may also want to look at.\n\n\nA7:\n\nNone of the existing answers seemed complete or safe, so I came up with this function, which has the following improvements:\n\nworks with both unnamed (?) and named (:foo) parameters.\n\nusing PDO::quote() to properly escape values which are not NULL, int, float or bool.\n\nproperly handles string values containing \"?\" and \":foo\" without mistaking them for placeholders.\n\n\n    function interpolateSQL(PDO $pdo, string $query, array $params) : string {\n        $s = chr(2); // Escape sequence for start of placeholder\n        $e = chr(3); // Escape sequence for end of placeholder\n        $keys = [];\n        $values = [];\n\n        // Make sure we use escape sequences that are not present in any value\n        // to escape the placeholders.\n        foreach ($params as $key => $value) {\n            while( mb_stripos($value, $s) !== false ) $s .= $s;\n            while( mb_stripos($value, $e) !== false ) $e .= $e;\n        }\n        \n        \n        foreach ($params as $key => $value) {\n            // Build a regular expression for each parameter\n            $keys[] = is_string($key) ? \"/$s:$key$e/\" : \"/$s\\?$e/\";\n\n            // Treat each value depending on what type it is. \n            // While PDO::quote() has a second parameter for type hinting, \n            // it doesn't seem reliable (at least for the SQLite driver).\n            if( is_null($value) ){\n                $values[$key] = 'NULL';\n            }\n            elseif( is_int($value) || is_float($value) ){\n                $values[$key] = $value;\n            }\n            elseif( is_bool($value) ){\n                $values[$key] = $value ? 'true' : 'false';\n            }\n            else{\n                $value = str_replace('\\\\', '\\\\\\\\', $value);\n                $values[$key] = $pdo->quote($value);\n            }\n        }\n\n        // Surround placehodlers with escape sequence, so we don't accidentally match\n        // \"?\" or \":foo\" inside any of the values.\n        $query = preg_replace(['/\\?/', '/(:[a-zA-Z0-9_]+)/'], [\"$s?$e\", \"$s$1$e\"], $query);\n\n        // Replace placeholders with actual values\n        $query = preg_replace($keys, $values, $query, 1, $count);\n\n        // Verify that we replaced exactly as many placeholders as there are keys and values\n        if( $count !== count($keys) || $count !== count($values) ){\n            throw new \\Exception('Number of replacements not same as number of keys and/or values');\n        }\n\n        return $query;\n    }\n\nI'm sure it can be improved further.\nIn my case, I eventually ended up just logging the actual \"unprepared query\" (i.e. SQL containing placeholders) along with JSON-encoded parameters. However, this code might come in use for some use cases where you really need to interpolate the final SQL query.\n\n\nA8:\n\npreg_replace didn't work for me and when binding_ was over 9, binding_1 and binding_10 was replaced with str_replace (leaving the 0 behind), so I made the replacements backwards:\npublic function interpolateQuery($query, $params) {\n$keys = array();\n    $length = count($params)-1;\n    for ($i = $length; $i >=0; $i--) {\n            $query  = str_replace(':binding_'.(string)$i, '\\''.$params[$i]['val'].'\\'', $query);\n           }\n        // $query  = str_replace('SQL_CALC_FOUND_ROWS', '', $query, $count);\n        return $query;\n\n}\nHope someone finds it useful. \n\n\nA9:\n\nIt is impossible to safely escape a string without a DB connection. mysql_real_escape_string() and prepared statements need a connection to the database so that they can escape the string using the appropriate character set - otherwise SQL injection attacks are still possible using multi-byte characters.\nIf you are only testing, then you may as well use mysql_escape_string(), it's not 100% guaranteed against SQL injection attacks, but it's impossible to build anything safer without a DB connection.\n\n\nA10:\n\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic static function interpolateQuery($query, $params) {\n    $keys = array();\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n    }\n\n    $query = preg_replace($keys, $params, $query, 1, $count);\n\n    #trigger_error('replaced '.$count.' keys');\n\n    return $query;\n}\n\n\n\nA11:\n\nBut if you are using Plesk, change your settings in :\n/usr/local/psa/admin/htdocs/domains/databases/phpMyAdmin/libraries/config.default.php\nChange $cfg['ExecTimeLimit'] = 300; to $cfg['ExecTimeLimit'] = 0;\nAnd restart with Plesk UI or use:\n/etc/init.d/psa restart\n and\n/etc/init.d/httpd restart\n\n\nA12:\n\nYou can extend PDOStatement class to capture the bounded variables and store them for later use. Then 2 methods may be added, one for variable sanitizing ( debugBindedVariables ) and another to print the query with those variables ( debugQuery ):\nclass DebugPDOStatement extends \\PDOStatement{\n  private $bound_variables=array();\n  protected $pdo;\n\n  protected function __construct($pdo) {\n    $this->pdo = $pdo;\n  }\n\n  public function bindValue($parameter, $value, $data_type=\\PDO::PARAM_STR){\n    $this->bound_variables[$parameter] = (object) array('type'=>$data_type, 'value'=>$value);\n    return parent::bindValue($parameter, $value, $data_type);\n  }\n\n  public function bindParam($parameter, &$variable, $data_type=\\PDO::PARAM_STR, $length=NULL , $driver_options=NULL){\n    $this->bound_variables[$parameter] = (object) array('type'=>$data_type, 'value'=>&$variable);\n    return parent::bindParam($parameter, $variable, $data_type, $length, $driver_options);\n  }\n\n  public function debugBindedVariables(){\n    $vars=array();\n\n    foreach($this->bound_variables as $key=>$val){\n      $vars[$key] = $val->value;\n\n      if($vars[$key]===NULL)\n        continue;\n\n      switch($val->type){\n        case \\PDO::PARAM_STR: $type = 'string'; break;\n        case \\PDO::PARAM_BOOL: $type = 'boolean'; break;\n        case \\PDO::PARAM_INT: $type = 'integer'; break;\n        case \\PDO::PARAM_NULL: $type = 'null'; break;\n        default: $type = FALSE;\n      }\n\n      if($type !== FALSE)\n        settype($vars[$key], $type);\n    }\n\n    if(is_numeric(key($vars)))\n      ksort($vars);\n\n    return $vars;\n  }\n\n  public function debugQuery(){\n    $queryString = $this->queryString;\n\n    $vars=$this->debugBindedVariables();\n    $params_are_numeric=is_numeric(key($vars));\n\n    foreach($vars as $key=>&$var){\n      switch(gettype($var)){\n        case 'string': $var = \"'{$var}'\"; break;\n        case 'integer': $var = \"{$var}\"; break;\n        case 'boolean': $var = $var ? 'TRUE' : 'FALSE'; break;\n        case 'NULL': $var = 'NULL';\n        default:\n      }\n    }\n\n    if($params_are_numeric){\n      $queryString = preg_replace_callback( '/\\?/', function($match) use( &$vars) { return array_shift($vars); }, $queryString);\n    }else{\n      $queryString = strtr($queryString, $vars);\n    }\n\n    echo $queryString.PHP_EOL;\n  }\n}\n\n\nclass DebugPDO extends \\PDO{\n  public function __construct($dsn, $username=\"\", $password=\"\", $driver_options=array()) {\n    $driver_options[\\PDO::ATTR_STATEMENT_CLASS] = array('DebugPDOStatement', array($this));\n    $driver_options[\\PDO::ATTR_PERSISTENT] = FALSE;\n    parent::__construct($dsn,$username,$password, $driver_options);\n  }\n}\n\nAnd then you can use this inherited class for debugging purpouses.\n$dbh = new DebugPDO('mysql:host=localhost;dbname=test;','user','pass');\n\n$var='user_test';\n$sql=$dbh->prepare(\"SELECT user FROM users WHERE user = :test\");\n$sql->bindValue(':test', $var, PDO::PARAM_STR);\n$sql->execute();\n\n$sql->debugQuery();\nprint_r($sql->debugBindedVariables());\n\nResulting in\n\nSELECT user FROM users WHERE user = 'user_test' \nArray (\n      [:test] => user_test \n  )\n\n\n\nA13:\n\nI know this question is a bit old, but, I'm using this code since lot time ago (I've used response from @chris-go), and now, these code are obsolete with PHP 7.2\nI'll post an updated version of these code (Credit for the main code are from @bigwebguy, @mike and @chris-go, all of them answers of this question):\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_array($value))\n            $values[$key] = implode(',', $value);\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n    // Walk the array to see if we can add single-quotes to strings\n    array_walk($values, function(&$v, $k) { if (!is_numeric($v) && $v != \"NULL\") $v = \"\\'\" . $v . \"\\'\"; });\n\n    $query = preg_replace($keys, $values, $query, 1, $count);\n\n    return $query;\n}\n\nNote the change on the code are on array_walk() function, replacing create_function by an anonymous function. This make these good piece of code functional and compatible with PHP 7.2 (and hope future versions too).\n\n\nA14:\n\nThe $queryString property mentioned will probably only return the query passed in, without the parameters replaced with their values.  In .Net, I have the catch part of my query executer do a simple search replace on the parameters with their values which was supplied so that the error log can show actual values that were being used for the query.  You should be able to enumerate the parameters in PHP, and replace the parameters with their assigned value.\n\n\nA15:\n\nTo increase the phpMyAdmin Session Timeout, open config.inc.php in the root phpMyAdmin directory and add this setting (anywhere).\n$cfg['LoginCookieValidity'] = <your_new_timeout>;\n\nWhere  is some number larger than 1800.\nNote:\nAlways keep on mind that a short cookie lifetime is all well and good for the development server. So do not do this on your production server.\nReference\n\n\nA16:\n\nGo to: Edit -> Preferences -> Administrator -> Path to Mysqldumptool:\nLook for file mysqldump.exe in your MySQL Server installation folder (it could be: mysql/bin/).\nThen click it, and OK. After that try to do the backup.\n\n\nA17:\n\nA solution is to voluntarily put an error in the query and to print the error's message:\n//Connection to the database\n$co = new PDO('mysql:dbname=myDB;host=localhost','root','');\n//We allow to print the errors whenever there is one\n$co->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n//We create our prepared statement\n$stmt = $co->prepare(\"ELECT * FROM Person WHERE age=:age\"); //I removed the 'S' of 'SELECT'\n$stmt->bindValue(':age','18',PDO::PARAM_STR);\ntry {\n    $stmt->execute();\n} catch (PDOException $e) {\n    echo $e->getMessage();\n}\n\nStandard output:\n\nSQLSTATE[42000]: Syntax error or access violation: [...] near 'ELECT * FROM Person WHERE age=18' at line 1\n\nIt is important to note that it only prints the first 80 characters of the query.\n\n\nA18:\n\nI had the same issue and I used command line in order to import the SQL file. This method has 3 advantages:\n\nIt is a very easy way by running only 1 command line\nIt runs way faster\nIt does not have limitation\n\nIf you want to do this just follow this 3 steps:\n\nNavigate to this path (i use wamp):\nC:\\wamp\\bin\\mysql\\mysql5.6.17\\bin>\nCopy your sql file inside this path (ex file.sql)\nRun this command: \nmysql -u username -p database_name < file.sql\n\nNote: if you already have your msql enviroment variable path set, you don't need to move your file.sql in the bin directory and you should only navigate to the path of the file.\n\n\nA19:\n\nIf even after repeated upload you still get timeout error, pleasechange your settings in \n\\phpmyadmin\\libraries\\config.default.php \nfrom $cfg['ExecTimeLimit'] = 300; to $cfg['ExecTimeLimit'] = 0; and restart. Now there is no execution time limit (trust we are talking about local server). \nSource : Change Script time out in phpmyadmin \n\n\nA20:\n\ngo to  /phpMyAdmin/libraries/config.default.php\nLine Number 719\nChange\n$cfg['ExecTimeLimit'] = 300; \n\nto\n$cfg['ExecTimeLimit'] = 0;\n\n\n\nA21:\n\nI had this issue too and tried different memory expansion techniques I found on the web but had more troubles with it.\nI resolved to using the MySQL console source command, and of course you don't have to worry about phpMyAdmin or PHP maximum execution time and limits.\nSyntax: source c:\\path\\to\\dump_file.sql\nNote: It's better to specify an absolute path to the dump file since the mysql working directory might not be known.\n\n\nA22:\n\nA bit late probably but now there is PDOStatement::debugDumpParams\n\nDumps the informations contained by a prepared statement directly on\n  the output. It will provide the SQL query in use, the number of\n  parameters used (Params), the list of parameters, with their name,\n  type (paramtype) as an integer, their key name or position, and the\n  position in the query (if this is supported by the PDO driver,\n  otherwise, it will be -1).\n\nYou can find more on the official php docs\nExample:\n<?php\n/* Execute a prepared statement by binding PHP variables */\n$calories = 150;\n$colour = 'red';\n$sth = $dbh->prepare('SELECT name, colour, calories\n    FROM fruit\n    WHERE calories < :calories AND colour = :colour');\n$sth->bindParam(':calories', $calories, PDO::PARAM_INT);\n$sth->bindValue(':colour', $colour, PDO::PARAM_STR, 12);\n$sth->execute();\n\n$sth->debugDumpParams();\n\n?>\n\n\n\nA23:\n\nI need to log full query string after bind param so this is a piece in my code. Hope, it is useful for everyone hat has the same issue.\n/**\n * \n * @param string $str\n * @return string\n */\npublic function quote($str) {\n    if (!is_array($str)) {\n        return $this->pdo->quote($str);\n    } else {\n        $str = implode(',', array_map(function($v) {\n                    return $this->quote($v);\n                }, $str));\n\n        if (empty($str)) {\n            return 'NULL';\n        }\n\n        return $str;\n    }\n}\n\n/**\n * \n * @param string $query\n * @param array $params\n * @return string\n * @throws Exception\n */\npublic function interpolateQuery($query, $params) {\n    $ps = preg_split(\"/'/is\", $query);\n    $pieces = [];\n    $prev = null;\n    foreach ($ps as $p) {\n        $lastChar = substr($p, strlen($p) - 1);\n\n        if ($lastChar != \"\\\\\") {\n            if ($prev === null) {\n                $pieces[] = $p;\n            } else {\n                $pieces[] = $prev . \"'\" . $p;\n                $prev = null;\n            }\n        } else {\n            $prev .= ($prev === null ? '' : \"'\") . $p;\n        }\n    }\n\n    $arr = [];\n    $indexQuestionMark = -1;\n    $matches = [];\n\n    for ($i = 0; $i < count($pieces); $i++) {\n        if ($i % 2 !== 0) {\n            $arr[] = \"'\" . $pieces[$i] . \"'\";\n        } else {\n            $st = '';\n            $s = $pieces[$i];\n            while (!empty($s)) {\n                if (preg_match(\"/(\\?|:[A-Z0-9_\\-]+)/is\", $s, $matches, PREG_OFFSET_CAPTURE)) {\n                    $index = $matches[0][1];\n                    $st .= substr($s, 0, $index);\n                    $key = $matches[0][0];\n                    $s = substr($s, $index + strlen($key));\n\n                    if ($key == '?') {\n                        $indexQuestionMark++;\n                        if (array_key_exists($indexQuestionMark, $params)) {\n                            $st .= $this->quote($params[$indexQuestionMark]);\n                        } else {\n                            throw new Exception('Wrong params in query at ' . $index);\n                        }\n                    } else {\n                        if (array_key_exists($key, $params)) {\n                            $st .= $this->quote($params[$key]);\n                        } else {\n                            throw new Exception('Wrong params in query with key ' . $key);\n                        }\n                    }\n                } else {\n                    $st .= $s;\n                    $s = null;\n                }\n            }\n            $arr[] = $st;\n        }\n    }\n\n    return implode('', $arr);\n}\n\n\n\nA24:\n\nIf using Cpanel/WHM the location of file config.default.php is under\n/usr/local/cpanel/base/3rdparty/phpMyAdmin/libraries\nand you should change the $cfg['ExecTimeLimit'] = 300; to $cfg['ExecTimeLimit'] = 0;\n\n\nA25:\n\nThe message says you need a newer mysqldump tool. One that matches the server you want to dump from. So depending on the platform you are running get a copy of the mysqldump tool from a server installation that has a recent version. Each server comes with a mysqldump tool. So it should be easy to get a copy.\nPut the tool in a location where it has a persistent home, but does not conflict with other instances, and point MySQL Workbench at it (as the message says).\n\n\nA26:\n\nYou can use sprintf(str_replace('?', '\"%s\"', $sql), ...$params); \nHere is an example:\nfunction mysqli_prepared_query($link, $sql, $types='', $params=array()) {\n    echo sprintf(str_replace('?', '\"%s\"', $sql), ...$params);\n    //prepare, bind, execute\n}\n\n$link = new mysqli($server, $dbusername, $dbpassword, $database);\n$sql = \"SELECT firstname, lastname FROM users WHERE userage >= ? AND favecolor = ?\";\n$types = \"is\"; //integer and string\n$params = array(20, \"Brown\");\n\nif(!$qry = mysqli_prepared_query($link, $sql, $types, $params)){\n    echo \"Failed\";\n} else {\n    echo \"Success\";\n}\n\nNote this only works for PHP >= 5.6\n\n\nA27:\n\nI modified the method to include handling output of arrays for statements like WHERE IN (?). \nUPDATE: Just added check for NULL value and duplicated $params so actual $param values are not modified. \nGreat work bigwebguy and thanks!\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_string($value))\n            $values[$key] = \"'\" . $value . \"'\";\n\n        if (is_array($value))\n            $values[$key] = \"'\" . implode(\"','\", $value) . \"'\";\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n\n    $query = preg_replace($keys, $values, $query);\n\n    return $query;\n}\n\n\n\nA28:\n\nI assume you mean that you want the final SQL query, with parameter values interpolated into it.  I understand that this would be useful for debugging, but it is not the way prepared statements work.  Parameters are not combined with a prepared statement on the client-side, so PDO should never have access to the query string combined with its parameters.\nThe SQL statement is sent to the database server when you do prepare(), and the parameters are sent separately when you do execute().  MySQL's general query log does show the final SQL with values interpolated after you execute().  Below is an excerpt from my general query log.  I ran the queries from the mysql CLI, not from PDO, but the principle is the same.\n081016 16:51:28 2 Query       prepare s1 from 'select * from foo where i = ?'\n                2 Prepare     [2] select * from foo where i = ?\n081016 16:51:39 2 Query       set @a =1\n081016 16:51:47 2 Query       execute s1 using @a\n                2 Execute     [2] select * from foo where i = 1\n\nYou can also get what you want if you set the PDO attribute PDO::ATTR_EMULATE_PREPARES.  In this mode, PDO interpolate parameters into the SQL query and sends the whole query when you execute().  This is not a true prepared query.  You will circumvent the benefits of prepared queries by interpolating variables into the SQL string before execute().\n\nRe comment from @afilina:\nNo, the textual SQL query is not combined with the parameters during execution. So there's nothing for PDO to show you.\nInternally, if you use PDO::ATTR_EMULATE_PREPARES, PDO makes a copy of the SQL query and interpolates parameter values into it before doing the prepare and execute. But PDO does not expose this modified SQL query. \nThe PDOStatement object has a property $queryString, but this is set only in the constructor for the PDOStatement, and it's not updated when the query is rewritten with parameters.\nIt would be a reasonable feature request for PDO to ask them to expose the rewritten query. But even that wouldn't give you the \"complete\" query unless you use PDO::ATTR_EMULATE_PREPARES.\nThis is why I show the workaround above of using the MySQL server's general query log, because in this case even a prepared query with parameter placeholders is rewritten on the server, with parameter values backfilled into the query string. But this is only done during logging, not during query execution.\n\n\nA29:\n\nNone of the above answers solved it for me.\nI cant even find the 'libraries' folder in my xampp - ubuntu also.\nSo, I simply restarted using the following commands:\nsudo service apache2 restart\n\nand \nsudo service mysql restart\n\n\nJust restarted apache and mysql. Logged in phpmyadmin again and it worked as usual.\n\nThanks me..!!\n\n\nA30:\n\nIf any of you happen to use WAMP then at least in the current version (3.0.6 x64) there's a file located in <your-wamp-dir>\\alias\\phpmyadmin.conf which overrides some of your php.ini options.\nEdit this part:\n# To import big file you can increase values\n php_admin_value upload_max_filesize 512M\n php_admin_value post_max_size 512M\n php_admin_value max_execution_time 600\n php_admin_value max_input_time 600\n\n\n\nA31:\n\nAdded a little bit more to the code by Mike - walk the values to add single quotes \n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_array($value))\n            $values[$key] = implode(',', $value);\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n    // Walk the array to see if we can add single-quotes to strings\n    array_walk($values, create_function('&$v, $k', 'if (!is_numeric($v) && $v!=\"NULL\") $v = \"\\'\".$v.\"\\'\";'));\n\n    $query = preg_replace($keys, $values, $query, 1, $count);\n\n    return $query;\n}\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " V, (v, ( (v, ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 19, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nWhy does this line give the error Error: incomplete type is not allowed?\nstringstream ss;\n\n\n\nCandidate answers:\nA1:\n\n\nint may be as small as 16 bits on some platforms. It may not be sufficient for your application.\nuint32_t is not guaranteed to exist. It's an optional typedef that the implementation must provide iff it has an unsigned integer type of exactly 32-bits. Some have a 9-bit bytes for example, so they don't have a uint32_t.\nuint_fast32_t states your intent clearly: it's a type of at least 32 bits which is the best from a performance point-of-view. uint_fast32_t may be in fact 64 bits long. It's up to the implementation.\nThere's also uint_least32_t in the mix. It designates the smallest type that's at least 32 bits long, thus it can be smaller than uint_fast32_t. It's an alternative to uint32_t if the later isn't supported by the platform.\n\n\n... there is uint_fast32_t which has the same typedef as uint32_t ...\n\nWhat you are looking at is not the standard. It's a particular implementation (BlackBerry). So you can't deduce from there that uint_fast32_t is always the same as uint32_t.\nSee also:\n\nExotic architectures the standards committees care about.\n\nMy pragmatic opinion about integer types in C and C++.\n\n\n\n\nA2:\n\nUse overloaded functions. Does not require dynamic_cast or even RTTI support:\nclass A {};\nclass B : public A {};\n\nclass Foo {\npublic:\n    void Bar(A& a) {\n        // do something\n    }\n    void Bar(B& b) {\n        Bar(static_cast<A&>(b));\n        // do B specific stuff\n    }\n};\n\n\n\nA3:\n\nThe size_t type is meant to specify the size of something so it's natural to use it, for example, getting the length of a string and then processing each character:\nfor (size_t i = 0, max = strlen (str); i < max; i++)\n    doSomethingWith (str[i]);\n\nYou do have to watch out for boundary conditions of course, since it's an unsigned type. The boundary at the top end is not usually that important since the maximum is usually large (though it is possible to get there). Most people just use an int for that sort of thing because they rarely have structures or arrays that get big enough to exceed the capacity of that int.\nBut watch out for things like:\nfor (size_t i = strlen (str) - 1; i >= 0; i--)\n\nwhich will cause an infinite loop due to the wrapping behaviour of unsigned values (although I've seen compilers warn against this). This can also be alleviated by the (slightly harder to understand but at least immune to wrapping problems):\nfor (size_t i = strlen (str); i-- > 0; )\n\nBy shifting the decrement into a post-check side-effect of the continuation condition, this does the check for continuation on the value before decrement, but still uses the decremented value inside the loop (which is why the loop runs from len .. 1 rather than len-1 .. 0).\n\n\nA4:\n\nUse std::size_t for indexing/counting C-style arrays.\nFor STL containers, you'll have (for example) vector<int>::size_type, which should be used for indexing and counting vector elements.\nIn practice, they are usually both unsigned ints, but it isn't guaranteed, especially when using custom allocators.\n\n\nA5:\n\nIn Short, 2147483648 overflows to -2147483648, and (-(-2147483648) > 0) is true.\nThis is how 2147483648 looks like in binary.\nIn addition, in the case of signed binary calculations, the most significant bit (\"MSB\") is the sign bit. This question may help explain why.\n\n\nA6:\n\nNote that the fast version could be larger than 32 bits. While the fast int will fit nicely in a register and be aligned and the like: but, it will use more memory. If you have large arrays of these your program will be slower due to more memory cache hits and bandwidth.\nI don't think modern CPUS will benefit from fast_int32, since generally the sign extending of  32 to 64 bit can happen during the load instruction and the idea that there is a 'native' integer format that is faster is old fashioned.\n\n\nA7:\n\nThis is called RTTI, but you almost surely want to reconsider your design here, because finding the type and doing special things based on it makes your code more brittle.\n\n\nA8:\n\nAs others indicated you can use dynamic_cast. But generally using dynamic_cast for finding out the type of the derived class you are working upon indicates the bad design. If you are overriding a function that takes pointer of A as the parameter then it should be able to work with the methods/data of class A itself and should not depend on the the data of class B. In your case instead of overriding if you are sure that the method you are writing will work with only class B, then you should write a new method in class B.\n\n\nA9:\n\nJust to be complete, I'll build off of Robocide and point out that typeid can be used alone without using name():\n#include <typeinfo>\n#include <iostream>\n\nusing namespace std;\n\nclass A {\npublic:\n    virtual ~A() = default; // We're not polymorphic unless we\n                            // have a virtual function.\n};\nclass B : public A { } ;\nclass C : public A { } ;\n\nint\nmain(int argc, char* argv[])\n{\n    B b;\n    A& a = b;\n\n    cout << \"a is B: \" << boolalpha << (typeid(a) == typeid(B)) << endl;\n    cout << \"a is C: \" << boolalpha << (typeid(a) == typeid(C)) << endl;\n    cout << \"b is B: \" << boolalpha << (typeid(b) == typeid(B)) << endl;\n    cout << \"b is A: \" << boolalpha << (typeid(b) == typeid(A)) << endl;\n    cout << \"b is C: \" << boolalpha << (typeid(b) == typeid(C)) << endl;\n}\n\nOutput:\na is B: true\na is C: false\nb is B: true\nb is A: false\nb is C: false\n\n\n\nA10:\n\nIt is often better not to use size_t in a loop. For example,\nvector<int> a = {1,2,3,4};\nfor (size_t i=0; i<a.size(); i++) {\n    std::cout << a[i] << std::endl;\n}\nsize_t n = a.size();\nfor (size_t i=n-1; i>=0; i--) {\n    std::cout << a[i] << std::endl;\n}\n\nThe first loop is ok. But for the second loop:\nWhen i=0, the result of i-- will be ULLONG_MAX (assuming size_t = unsigned long long), which is not what you want in a loop.\nMoreover, if a is empty then n=0 and n-1=ULLONG_MAX which is not good either.\n\n\nA11:\n\nBecause your class is not polymorphic. Try:\nstruct BaseClas { int base; virtual ~BaseClas(){} };\nclass Derived1 : public BaseClas { int derived1; };\n\nNow BaseClas is polymorphic. I changed class to struct because the members of a struct are public by default.\n\n\nA12:\n\nIf you can access boost library, maybe type_id_with_cvr() function is what you need, which can provide data type without removing const, volatile, & and && modifiers. Here is an simple example in C++11:\n#include <iostream>\n#include <boost/type_index.hpp>\n\nint a;\nint& ff() \n{\n    return a;\n}\n\nint main() {\n    ff() = 10;\n    using boost::typeindex::type_id_with_cvr;\n    std::cout << type_id_with_cvr<int&>().pretty_name() << std::endl;\n    std::cout << type_id_with_cvr<decltype(ff())>().pretty_name() << std::endl;\n    std::cout << typeid(ff()).name() << std::endl;\n}\n\nHope this is useful.\n\n\nA13:\n\nFor myself, I would assume unsigned int is platform specific. Integers could be 16 bits, 32 bits or even 64 bits.\nOn the other hand, DWORD specifies its own size, which is Double Word. Words are 16 bits, so DWORD will be known as 32 bits across all platforms.\n\n\nA14:\n\nSDK developers prefer to define their own types using typedef. This allows changing underlying types only in one place, without changing all client code. It is important to follow this convention. DWORD is unlikely to be changed, but types like DWORD_PTR are different on different platforms, like Win32 and x64. So, if some function has DWORD parameter, use DWORD and not unsigned int, and your code will be compiled in all future windows headers versions.\n\n\nA15:\n\nWhen you #include inttypes.h in your program, you get access to a bunch of different ways for representing integers.\nThe uint_fast*_t type simply defines the fastest type for representing a given number of bits. \nThink about it this way: you define a variable of type short and use it several times in the program, which is totally valid. However, the system you're working on might work more quickly with values of type int. By defining a variable as type uint_fast*t, the computer simply chooses the most efficient representation that it can work with. \nIf there is no difference between these representations, then the system chooses whichever one it wants, and uses it consistently throughout.\n\n\nA16:\n\nsize_t is the result type of the sizeof operator.\nUse size_t for variables that model size or index in an array. size_t conveys semantics: you immediately know it represents a size in bytes or an index, rather than just another integer.\nAlso, using size_t to represent a size in bytes helps making the code portable.\n\n\nA17:\n\nThe compiler (VC2012) promote to the \"minimum\" integers that can hold the values. In the first case, signed int (and long int) cannot (before the sign is applied), but unsigned int can: 2147483648 has unsigned int ???? type. \nIn the second you force int from the unsigned. \nconst bool i= (-2147483648 > 0) ;  //   --> true\n\n\nwarning C4146: unary minus operator applied to unsigned type, result still unsigned\n\nHere are  related \"curiosities\":\nconst bool b= (-2147483647      > 0) ; //  false\nconst bool i= (-2147483648      > 0) ; //  true : result still unsigned\nconst bool c= ( INT_MIN-1       > 0) ; //  true :'-' int constant overflow\nconst bool f= ( 2147483647      > 0) ; //  true\nconst bool g= ( 2147483648      > 0) ; //  true\nconst bool d= ( INT_MAX+1       > 0) ; //  false:'+' int constant overflow\nconst bool j= ( int(-2147483648)> 0) ; //  false : \nconst bool h= ( int(2147483648) > 0) ; //  false\nconst bool m= (-2147483648L     > 0) ; //  true \nconst bool o= (-2147483648LL    > 0) ; //  false\n\nC++11 standard:\n\n2.14.2 Integer literals [lex.icon]\n\n\u2026  \n\nAn integer literal is a sequence of digits that has no period or\n  exponent part. An integer literal may have a prefix that specifies its\n  base and a suffix that specifies its type.\n\n\u2026 \n\nThe type of an integer literal is the first of the corresponding list\n  in which its value can be represented.\n\n \n\nIf an integer literal cannot be represented by any type in its list\n  and an extended integer type (3.9.1) can represent its value, it may\n  have that extended integer type. If all of the types in the list for\n  the literal are signed, the extended integer type shall be signed. If\n  all of the types in the list for the literal are unsigned, the\n  extended integer type shall be unsigned. If the list contains both\n  signed and unsigned types, the extended integer type may be signed or\n  unsigned. A program is ill-formed if one of its translation units\n  contains an integer literal that cannot be represented by any of the\n  allowed types.\n\nAnd these are the promotions rules for integers in the standard.\n\n4.5 Integral promotions [conv.prom] \nA prvalue of an integer type other than bool, char16_t, char32_t, or\n  wchar_t whose integer conversion rank (4.13) is less than the rank of\n  int can be converted to a prvalue of type int if int can represent all\n  the values of the source type; otherwise, the source prvalue can be\n  converted to a prvalue of type unsigned int.\n\n\n\nA18:\n\nsize_t is returned by various libraries to indicate that the size of that container is non-zero. You use it when you get once back :0\nHowever, in the your example above looping on a size_t is a potential bug. Consider the following:\nfor (size_t i = thing.size(); i >= 0; --i) {\n  // this will never terminate because size_t is a typedef for\n  // unsigned int which can not be negative by definition\n  // therefore i will always be >= 0\n  printf(\"the never ending story. la la la la\");\n}\n\nthe use of unsigned integers has the potential to create these types of subtle issues. Therefore imho I prefer to use size_t only when I interact with containers/types that require it.\n\n\nA19:\n\nA good rule of thumb is for anything that you need to compare in the loop condition against something that is naturally a std::size_t itself.\nstd::size_t is the type of any sizeof expression and as is guaranteed to be able to express the maximum size of any object (including any array) in C++. By extension it is also guaranteed to be big enough for any array index so it is a natural type for a loop by index over an array.\nIf you are just counting up to a number then it may be more natural to use either the type of the variable that holds that number or an int or unsigned int (if large enough) as these should be a natural size for the machine.\n\n\nA20:\n\nYour description is a little confusing. \nGenerally speaking, though some C++ implementations have mechanisms for it, you're not supposed to ask about the type. Instead, you are supposed to do a dynamic_cast on the pointer to A. What this will do is that at runtime, the actual contents of the pointer to A will be checked. If you have a B, you'll get your pointer to B. Otherwise, you'll get an exception or null.\n\n\nA21:\n\ndynamic_cast should do the trick\nTYPE& dynamic_cast<TYPE&> (object);\nTYPE* dynamic_cast<TYPE*> (object);\n\nThe dynamic_cast keyword casts a datum from one pointer or reference type to another, performing a runtime check to ensure the validity of the cast.\nIf you attempt to cast to pointer to a type that is not a type of actual object, the result of the cast will be NULL. If you attempt to cast to reference to a type that is not a type of actual object, the cast will throw a bad_cast exception.\nMake sure there is at least one virtual function in Base class to make dynamic_cast work.\nWikipedia topic Run-time type information\n\nRTTI is available only for classes that are polymorphic, which means\nthey have at least one virtual method. In practice, this is not a\nlimitation because base classes must have a virtual destructor to\nallow objects of derived classes to perform proper cleanup if they are\ndeleted from a base pointer.\n\n\n\nA22:\n\nshort answer:\nAlmost never. Use signed version ptrdiff_t or non-standard ssize_t. Use function std::ssize instead of std::size.\nlong answer:\nWhenever you need to have a vector of char bigger that 2gb on a 32 bit system. In every other use case, using a signed type is much safer than using an unsigned type.\nexample:\nstd::vector<A> data;\n[...]\n// calculate the index that should be used;\nsize_t i = calc_index(param1, param2);\n// doing calculations close to the underflow of an integer is already dangerous\n\n// do some bounds checking\nif( i - 1 < 0 ) {\n    // always false, because 0-1 on unsigned creates an underflow\n    return LEFT_BORDER;\n} else if( i >= data.size() - 1 ) {\n    // if i already had an underflow, this becomes true\n    return RIGHT_BORDER;\n}\n\n// now you have a bug that is very hard to track, because you never \n// get an exception or anything anymore, to detect that you actually \n// return the false border case.\n\nreturn calc_something(data[i-1], data[i], data[i+1]);\n\nThe signed equivalent of size_t is ptrdiff_t, not int. But using int is still much better in most cases than size_t. ptrdiff_t is long on 32 and 64 bit systems.\nThis means that you always have to convert to and from size_t whenever you interact with a std::containers, which not very beautiful. But on a going native conference the authors of c++ mentioned that designing std::vector with an unsigned size_t was a mistake.\nIf your compiler gives you warnings on implicit conversions from ptrdiff_t to size_t, you can make it explicit with constructor syntax:\ncalc_something(data[size_t(i-1)], data[size_t(i)], data[size_t(i+1)]);\n\nif just want to iterate a collection, without bounds cheking, use range based for:\nfor(const auto& d : data) {\n    [...]\n}\n\nhere some words from Bjarne Stroustrup (C++ author) at going native\nFor some people this signed/unsigned design error in the STL is reason enough, to not use the std::vector, but instead an own implementation.\n\n\nA23:\n\nI have been struggling myself with understanding what and when to use it. But size_t is just an unsigned integral data type which is defined in various header files such as  <stddef.h>, <stdio.h>, <stdlib.h>, <string.h>, <time.h>, <wchar.h> etc.\nIt is used to represent the size of objects in bytes hence it's used as the return type by the sizeof operator. The maximum permissible size is dependent on the compiler; if the compiler is 32 bit then it is simply a typedef (alias) for unsigned int but if the compiler is 64 bit then it would be a typedef for unsigned long long. The size_t data type is never negative(excluding ssize_t)\nTherefore many C library functions like malloc, memcpy and strlen declare their arguments and return type as size_t.\n/ Declaration of various standard library functions.\n  \n// Here argument of 'n' refers to maximum blocks that can be\n// allocated which is guaranteed to be non-negative.\nvoid *malloc(size_t n);\n  \n// While copying 'n' bytes from 's2' to 's1'\n// n must be non-negative integer.\nvoid *memcpy(void *s1, void const *s2, size_t n);\n  \n// the size of any string or `std::vector<char> st;` will always be at least 0.\nsize_t strlen(char const *s);\n\nsize_t or any unsigned type might be seen used as loop variable as loop variables are typically greater than or equal to 0.\n\n\nA24:\n\nYou are looking for dynamic_cast<B*>(pointer)\n\n\nA25:\n\nsize_t is an unsigned type that can hold maximum integer value for your architecture, so it is protected from integer overflows due to sign (signed int 0x7FFFFFFF incremented by 1 will give you -1)  or short size (unsigned short int 0xFFFF incremented by 1 will give you 0).\nIt is mainly used in array indexing/loops/address arithmetic and so on. Functions like memset() and alike accept size_t only, because theoretically you may have a block of memory of size 2^32-1 (on 32bit platform).\nFor such simple loops don't bother and use just int.\n\n\nA26:\n\nWhen MS-DOS and Windows 3.1 operated in 16-bit mode, an Intel 8086 word was 16 bits, a Microsoft WORD was 16 bits, a Microsoft DWORD was 32 bits, and a typical compiler's unsigned int was 16 bits.\nWhen Windows NT operated in 32-bit mode, an Intel 80386 word was 32 bits, a Microsoft WORD was 16 bits, a Microsoft DWORD was 32 bits, and a typical compiler's unsigned int was 32 bits.  The names WORD and DWORD were no longer self-descriptive but they preserved the functionality of Microsoft programs.\nWhen Windows operates in 64-bit mode, an Intel word is 64 bits, a Microsoft WORD is 16 bits, a Microsoft DWORD is 32 bits, and a typical compiler's unsigned int is 32 bits.  The names WORD and DWORD are no longer self-descriptive, AND an unsigned int no longer conforms to the principle of least surprises, but they preserve the functionality of lots of programs.\nI don't think WORD or DWORD will ever change.\n\n\nA27:\n\nDWORD is not a C++ type, it's defined in <windows.h>.\nThe reason is that DWORD has a specific range and format Windows functions rely on, so if you require that specific range use that type. (Or as they say \"When in Rome, do as the Romans do.\") For you, that happens to correspond to unsigned int, but that might not always be the case. To be safe, use DWORD when a DWORD is expected, regardless of what it may actually be.\nFor example, if they ever changed the range or format of unsigned int they could use a different type to underly DWORD to keep the same requirements, and all code using DWORD would be none-the-wiser. (Likewise, they could decide DWORD needs to be unsigned long long, change it, and all code using DWORD would be none-the-wiser.)\n\nAlso note unsigned int does not necessary have the range 0 to 4,294,967,295. See here.\n\n\nA28:\n\nBecause -2147483648 is actually 2147483648 with negation (-) applied to it, the number isn't what you'd expect. It is actually the equivalent of this pseudocode: operator -(2147483648)\nNow, assuming your compiler has sizeof(int) equal to 4 and CHAR_BIT is defined as 8, that would make 2147483648 overflow the maximum signed value of an integer (2147483647). So what is the maximum plus one? Lets work that out with a 4 bit, 2s compliment integer.\nWait! 8 overflows the integer! What do we do? Use its unsigned representation of 1000 and interpret the bits as a signed integer. This representation leaves us with -8 being applied the 2s complement negation resulting in 8, which, as we all know, is greater than 0.\nThis is why <limits.h> (and <climits>) commonly define INT_MIN as ((-2147483647) - 1) - so that the maximum signed integer (0x7FFFFFFF) is negated (0x80000001), then decremented (0x80000000).\n\n\nA29:\n\nsize_t is a very readable way to specify the size dimension of an item - length of a string, amount of bytes a pointer takes, etc.\nIt's also portable across platforms - you'll find that 64bit and 32bit both behave nicely with system functions and size_t - something that unsigned int might not do (e.g. when should you use unsigned long\n\n\nA30:\n\nThe type of the ternary ?: expression is the common type of its second and third argument. If both types are the same, you get a reference back. If they are convertable to each other, one gets chosen and the other gets converted (promoted in this case). Since you can't return an lvalue reference to a temporary (the converted / promoted variable), its type is a value type.\n\n\nA31:\n\nThe difference lies in their exact-ness and availability.\nThe doc here says:\n\nunsigned integer type with width of exactly 8, 16, 32 and 64 bits respectively (provided only if the implementation directly supports the type):  \nuint8_t\nuint16_t\nuint32_t\nuint64_t\n\n\nAnd\n\nfastest unsigned unsigned integer type with width of at least 8, 16, 32 and 64 bits  respectively\nuint_fast8_t\nuint_fast16_t\nuint_fast32_t\nuint_fast64_t    \n\n\nSo the difference is pretty much clear that uint32_t is a type which has exactly 32 bits, and an implementation should provide it only if it has type with exactly 32 bits, and then it can typedef that type as uint32_t. This means, uint32_t may or may not be available.\nOn the other hand, uint_fast32_t is a type which has at least 32 bits, which also means, if an implementation may typedef uint32_t as uint_fast32_t if it provides uint32_t. If it doesn't provide uint32_t, then uint_fast32_t could be a typedef of any type which has at least 32 bits.\n\n\nA32:\n\nExpressions don't have return types, they have a type and - as it's known in the latest C++ standard - a value category.\nA conditional expression can be an lvalue or an rvalue. This is its value category. (This is somewhat of a simplification, in C++11 we have lvalues, xvalues and prvalues.)\nIn very broad and simple terms, an lvalue refers to an object in memory and an rvalue is just a value that may not necessarily be attached to an object in memory.\nAn assignment expression assigns a value to an object so the thing being assigned to must be an lvalue.\nFor a conditional expression (?:) to be an lvalue (again, in broad and simple terms), the second and third operands must be lvalues of the same type. This is because the type and value category of a conditional expression is determined at compile time and must be appropriate whether or not the condition is true. If one of the operands must be converted to a different type to match the other then the conditional expression cannot be an lvalue as the result of this conversion would not be an lvalue.\n\nISO/IEC 14882:2011 references:\n3.10 [basic.lval] Lvalues and rvalues (about value categories)\n5.15 [expr.cond] Conditional operator (rules for what type and value category a conditional expression has)\n5.17 [expr.ass] Assignment and compound assignment operators (requirement that the l.h.s. of an assignment must be a modifiable lvalue)\n\n\n\nA33:\n\nSoon most computers will be 64-bit architectures with 64-bit OS:es running programs operating on containers of billions of elements. Then you must use size_t instead of int as loop index, otherwise your index will wrap around at the 2^32:th element, on both 32- and 64-bit systems.\nPrepare for the future!\n\n\nA34:\n\nSome of the system headers provide a forward declaration of std::stringstream without the definition. This makes it an 'incomplete type'. To fix that you need to include the definition, which is provided in the <sstream> header:\n#include <sstream>\n\n\n\nA35:\n\n-2147483648 is not a \"number\". C++ language does not support negative literal values.\n-2147483648 is actually an expression: a positive literal value 2147483648 with unary - operator in front of it. Value 2147483648 is apparently too large for the positive side of int range on your platform. If type long int had greater range on your platform, the compiler would have to automatically assume that 2147483648 has long int type. (In C++11 the compiler would also have to consider long long int type.) This would make the compiler to evaluate -2147483648 in the domain of larger type and the result would be negative, as one would expect. \nHowever, apparently in your case the range of long int is the same as range of int, and in general there's no integer type with greater range than int on your platform. This formally means that positive constant 2147483648 overflows all available signed integer types, which in turn means that the behavior of your program is undefined. (It is a bit strange that the language specification opts for undefined behavior in such cases, instead of requiring a diagnostic message, but that's the way it is.)\nIn practice, taking into account that the behavior is undefined, 2147483648 might get interpreted as some implementation-dependent negative value which happens to turn positive after having unary - applied to it. Alternatively, some implementations might decide to attempt using unsigned types to represent the value (for example, in C89/90 compilers were required to use unsigned long int, but not in C99 or C++). Implementations are allowed to do anything, since the behavior is undefined anyway.\nAs a side note, this is the reason why constants like INT_MIN are typically defined as\n#define INT_MIN (-2147483647 - 1)\n\ninstead of the seemingly more straightforward \n#define INT_MIN -2147483648\n\nThe latter would not work as intended.\n\n\nA36:\n\n#include <sstream> and use the fully qualified name  i.e. std::stringstream ss;\n\n\nA37:\n\nBy definition, size_t is the result of the sizeof operator. size_t was created to refer to sizes.\nThe number of times you do something (10, in your example) is not about sizes, so why use size_t? int, or unsigned int, should be ok.\nOf course it is also relevant what you do with i inside the loop. If you pass it to a function which takes an unsigned int, for example, pick unsigned int.\nIn any case, I recommend to avoid implicit type conversions. Make all type conversions explicit.\n\n\nA38:\n\nDynamic cast is the best for your description of problem, \nbut I just want to add that you can find the class type with:\n#include <typeinfo>\n\n...\nstring s = typeid(YourClass).name()\n\n\n\nA39:\n\nProbably embed into your objects an ID \"tag\" and use it to distinguish between objects of class A and objects of class B.\nThis however shows a flaw in the design. Ideally those methods in B which A doesn't have, should be part of A but left empty, and B overwrites them. This does away with the class-specific code and is more in the spirit of OOP. \n\n\nA40:\n\nWhen using size_t be careful with the following expression\nsize_t i = containner.find(\"mytoken\");\nsize_t x = 99;\nif (i-x>-1 && i+x < containner.size()) {\n    cout << containner[i-x] << \" \" << containner[i+x] << endl;\n}\n\nYou will get false in the if expression regardless of what value you have for x.\nIt took me several days to realize this (the code is so simple that I did not do unit test), although it only take a few minutes to figure the source of the problem. Not sure it is better to do a cast or use zero.\nif ((int)(i-x) > -1 or (i-x) >= 0)\n\nBoth ways should work. Here is my test run\nsize_t i = 5;\ncerr << \"i-7=\" << i-7 << \" (int)(i-7)=\" << (int)(i-7) << endl;\n\nThe output: i-7=18446744073709551614 (int)(i-7)=-2\nI would like other's comments.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 20, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nMy program needs to generate many random integers in some range (int min, int max). Each call will have a different range. What is a good (preferably thread-safe) way to do this? The following is not thread-safe (and uses rand(), which people seem to discourage):\nint intRand(const int & min, const int & max)\n{\n    return (rand() % (max+1-min)) + min;\n}\n\nThis is much slower, but uses <random>:\nint intRand(const int & min, const int & max) {\n    std::default_random_engine generator;\n    std::uniform_int_distribution<int> distribution(min,max);\n    return distribution(generator);\n}\n\nSomething like this is what I'm going for (the changeParameters function doesn't exist though):\nint intRand(const int & min, const int & max) {\n    static std::default_random_engine generator;\n    static std::uniform_int_distribution<int> distribution(0, 10);\n    distribution.changeParameters(min, max);\n    return distribution(generator);\n}\n\nAnother option would be to make a wide range on the uniform_int_distribution and then use mod like in the first example. However, I'm doing statistical work, so I want the numbers to come from as unbiased of a distribution as possible (e.g., if the range of the distribution used is not a multiple of (max-min), the distribution will be slightly biased). This is an option, but again, I would like to avoid it.\nSOLUTION This solution comes from the answers by @konrad-rudolph @mark-ransom and @mathk . The seeding of the random number generator is done to suit my particular needs. A more common approach would be to use time(NULL). If you make many threads in the same second, they would then get the same seed though. Even with clock() this is an issue, so we include the thread id. A drawback - this leaks memory --- one generator per thread.\n#if defined (_MSC_VER)  // Visual studio\n    #define thread_local __declspec( thread )\n#elif defined (__GCC__) // GCC\n    #define thread_local __thread\n#endif\n\n#include <random>\n#include <time.h>\n#include <thread>\n\nusing namespace std;\n\n/* Thread-safe function that returns a random number between min and max (inclusive).\nThis function takes ~142% the time that calling rand() would take. For this extra\ncost you get a better uniform distribution and thread-safety. */\nint intRand(const int & min, const int & max) {\n    static thread_local mt19937* generator = nullptr;\n    if (!generator) generator = new mt19937(clock() + this_thread::get_id().hash());\n    uniform_int_distribution<int> distribution(min, max);\n    return distribution(*generator);\n}\n\n\n\nCandidate answers:\nA1:\n\nAlthough the Qt docs don't appear to specify the behaviour you could find out by keeping track of what QObject::thread() returns before and after the thread finishes.\n\n\nA2:\n\nOn my system the behavior is same, but as Maxim mentioned, rand is not thread safe. When I change rand to rand_r, then the multi threaded code is faster as expected.\nvoid add_multi(int N, double& result) {\ndouble sum=0;\nunsigned int seed = time(NULL);\nfor (int i = 0; i < N; ++i){\n    sum+= sqrt(1.0*rand_r(&seed)/RAND_MAX);\n}\nresult = sum/N;\n}\n\n\n\nA3:\n\nWhile I am no standard guru, it seems quite obvious that destructors should be called after the copy is made - otherwise the very object you are copying would be destroyed before it is copied... :)\n\n\nA4:\n\nIf you will be working with UNIX-like systems, then I recommend Programming With POSIX Threads by David R. Butenhof.\nIf you will be working with Microsoft Windows, then I recommend Writing Multithreaded Applications in Win32 by Jim Beveridge and Robert Wiener.\nIrrespective of which threading package(s) you will end up using, I recommend you look at two presentations I wrote: Generic Synchronization Policies and Multi-threaded Performance Pitfalls. Those short presentations contain useful information that, unfortunately, is not discussed in many other books and articles.\n\n\nA5:\n\nI am not sure if Qt automatically change the thread affinity. But even if it does, the only reasonable thread to move to is the main thread. I would push them at the end of the threaded function myself.\nmyObject->moveToThread(QApplication::instance()->thread());\n\nNow this only matters if the objects make use of event process like send and receive signals.\n\n\nA6:\n\nTo make this faster, use a thread pool pattern.\nThis will let you enqueue tasks in other threads without the overhead of creating a std::thread each time you want to use more than one thread.\nDon't count the overhead of setting up the queue in your performance metrics, just the time to enqueue and extract the results.\nCreate a set of threads and a queue of tasks (a structure containing a std::function<void()>) to feed them.  The threads wait on the queue for new tasks to do, do them, then wait on new tasks.\nThe tasks are responsible for communicating their \"done-ness\" back to the calling context, such as via a std::future<>.  The code that lets you enqueue functions into the task queue might do this wrapping for you, ie this signature:\ntemplate<typename R=void>\nstd::future<R> enqueue( std::function<R()> f ) {\n  std::packaged_task<R()> task(f);\n  std::future<R> retval = task.get_future();\n  this->add_to_queue( std::move( task ) ); // if we had move semantics, could be easier\n  return retval;\n}\n\nwhich turns a naked std::function returning R into a nullary packaged_task, then adds that to the tasks queue.  Note that the tasks queue needs be move-aware, because packaged_task is move-only.\nNote 1: I am not all that familiar with std::future, so the above could be in error.\nNote 2: If tasks put into the above described queue are dependent on each other for intermediate results, the queue could deadlock, because no provision to \"reclaim\" threads that are blocked and execute new code is described.  However, \"naked computation\" non-blocking tasks should work fine with the above model.\n\n\nA7:\n\nAlthough this is an old question, I recently asked the same question, and just answered it using QT 4.8 and some testing.\nAFAIK you cannot create objects with a parent from a QtConcurrent::run function.  I have tried the following two ways.  Let me define a code block then we will explore the behavior by selecting POINTER_TO_THREAD.\nSome psuedo code will show you my test\nClass MyClass : public QObject\n{\n  Q_OBJECT\npublic:\n  doWork(void)\n  {\n    QObject* myObj = new QObject(POINTER_TO_THREAD);\n    ....\n  }\n}\n\nvoid someEventHandler()\n{\n  MyClass* anInstance = new MyClass(this);\n  QtConcurrent::run(&anInstance, &MyClass::doWork)\n}\n\nIgnoring potential scoping issues...\nIf POINTER_TO_THREAD is set to this, then you will get an error because this will resolve to a pointer to the anInstance object which lives in the main thread, not the thread QtConcurrent has dispatched for it.  You will see something like...\nCannot create children for a parent in another thread.  Parent: anInstance, parents thread: QThread(xyz), currentThread(abc)\nIf POINTER_TO_THREAD is set to QObject::thread(), then you will get an error because because it will resolve to the QThread object in which anInstance lives, and not the thread QtConcurrent has dispatched for it. You will see something like...\nCannot create children for a parent in another thread.  Parent: QThread(xyz), parents thread: QThread(xyz), currentThread(abc)\nHope my testing is of use to someone else.  If anyone knows a way to get a pointer to the QThread which QtConcurrent runs the method in, I would be interested to hear it!\n\n\nA8:\n\nThe time needed to execute the program is very small (33msec). This means that the overhead to create and handle several threads may be more than the real benefit. Try using programs that need longer times for the execution (e.g., 10 sec).\n\n\nA9:\n\n\nDoes Qt automatically move the QObjects into the parent thread, or are we responsible in moving them to a valid thread before the worker thread terminates?\n\nNo, Qt doesn't automatically move QObject into the parent thread. \nThis behavior doesn't explicitly documented, so I've done a small investigation of the Qt framework source code, master branch.\nQThread starts in QThreadPrivate::start:\nunsigned int __stdcall QT_ENSURE_STACK_ALIGNED_FOR_SSE QThreadPrivate::start(void *arg)\n{\n\n  ...\n\n  thr->run();\n\n  finish(arg);\n  return 0;\n}\n\nQThread::terminate() implementation:\nvoid QThread::terminate()\n{\n  Q_D(QThread);\n  QMutexLocker locker(&d->mutex);\n  if (!d->running)\n      return;\n  if (!d->terminationEnabled) {\n      d->terminatePending = true;\n      return;\n  }\n  TerminateThread(d->handle, 0);\n  d->terminated = true;\n  QThreadPrivate::finish(this, false);\n}\n\nIn both cases thread finalization is done in QThreadPrivate::finish:\nvoid QThreadPrivate::finish(void *arg, bool lockAnyway)\n{\n  QThread *thr = reinterpret_cast<QThread *>(arg);\n  QThreadPrivate *d = thr->d_func();\n\n  QMutexLocker locker(lockAnyway ? &d->mutex : 0);\n  d->isInFinish = true;\n  d->priority = QThread::InheritPriority;\n  bool terminated = d->terminated;\n  void **tls_data = reinterpret_cast<void **>(&d->data->tls);\n  locker.unlock();\n  if (terminated)\n      emit thr->terminated();\n  emit thr->finished();\n  QCoreApplication::sendPostedEvents(0, QEvent::DeferredDelete);\n  QThreadStorageData::finish(tls_data);\n  locker.relock();\n\n  d->terminated = false;\n\n  QAbstractEventDispatcher *eventDispatcher = d->data->eventDispatcher;\n  if (eventDispatcher) {\n      d->data->eventDispatcher = 0;\n      locker.unlock();\n      eventDispatcher->closingDown();\n      delete eventDispatcher;\n      locker.relock();\n  }\n\n  d->running = false;\n  d->finished = true;\n  d->isInFinish = false;\n\n  if (!d->waiters) {\n      CloseHandle(d->handle);\n      d->handle = 0;\n  }\n\n  d->id = 0;\n}\n\nIt posts QEvent::DeferredDelete event to cleanup QObject::deleteLater, than TLS data cleaned up with QThreadStorageData::finish(tls_data) and eventDispatcher deleted. After that QObject will receive no events from this thread, but QObject's thread affinity stays the same. It's interesting to see implementation of void QObject::moveToThread(QThread *targetThread) to understand how thread affinity changes.   \nImplementation of void QThreadPrivate::finish(void *arg, bool lockAnyway) makes clear that QObject's thread affinity is not changed by QThread.\n\n\nA10:\n\nQThread is not documented to automatically move any QObjects when it finishes, so I think we can already conclude that it does no such thing. Such behavior would be very surprising, and at odds with the rest of the API.\nJust for completeness, I tested with Qt 5.6:\nQObject o;\n{\n    QThread t;\n    o.moveToThread(&t);\n    for (int i = 0; i < 2; ++i)\n    {\n        t.start();\n        QVERIFY(t.isRunning());\n        QVERIFY(o.thread() == &t);\n        t.quit();\n        t.wait();\n        QVERIFY(t.isFinished());\n        QVERIFY(o.thread() == &t);\n    }\n}\nQVERIFY(o.thread() == nullptr);\n\nRecall that a QThread is not a thread, it manages a thread.\nWhen a QThread finishes, it continues to exist, and the objects that live in it continue to live in it, but they no longer process events. The QThread can be restarted (not recommended), at which point event processing will resume (so the same QThread could then be managing a different thread).\nWhen a QThread is destroyed, the objects that lived in it cease to have any thread affinity. The documentation doesn't guarantee this, and in fact says \"You must ensure that all objects created in a thread are deleted before you delete the QThread.\"\n\n\nLet's say I call QtConcurrent::run() which runs a function in a worker thread, and in that function I dynamically allocate several QObjects (for later use). Since they were created in the worker thread, their thread affinity should be that of the worker thread. However, once the worker thread terminates, the QObject thread affinity should no longer be valid.\n\nThe QThread does not terminate in this scenario. When a task spawned by QtConcurrent::run finishes, the QThread it was running in is returned to the QThreadPool and may be reused by a subsequent call to QtConcurrent::run, and QObjects living in that QThread continue to live there.\nQThreadPool::globalInstance()->setMaxThreadCount(1);\nQObject *o = nullptr;\nQThread *t = nullptr;\nQFuture<void> f = QtConcurrent::run([&] {\n    o = new QObject;\n    t = o->thread();\n    QVERIFY(t == QThread::currentThread());\n});\nf.waitForFinished();\nQVERIFY(t == o->thread());\nQVERIFY(t->isRunning());\nf = QtConcurrent::run([=] {\n    QVERIFY(t == QThread::currentThread());\n});\nf.waitForFinished();\n\nYou might want to manually move an object out of a QThread before it is returned to the QThreadPool, or just don't use QtConcurrent::run. Having a QtConcurrent::run task construct QObjects which outlive the task is a questionable design, tasks should be self-contained. As noted by @Mike, the QThreads used by QtConcurrent::run do not have event loops.\n\n\nA11:\n\nIt's a deliberate design choice/trade-off.\nFirst, it's not necessarily possible to find out whether the functionoid passed to async takes its arguments by reference or not. (If it's not a simple function but a function object, it could have an overloaded function call operator, for example.) So async cannot say, \"Hey, let me just check what the target function wants, and I'll do the right thing.\"\nSo the design question is, does it take all arguments by reference if possible (i.e. if they're lvalues), or does it always make copies? Making copies is the safe choice here: a copy cannot become dangling, and a copy cannot exhibit race conditions (unless it's really weird). So that's the choice that was made: all arguments are copied by default.\nBut then, the mechanism is written so that it actually fails to then pass the arguments to a non-const lvalue reference parameter. That's another choice for safety: otherwise, the function that you would expect to modify your original lvalue instead modifies the copy, leading to bugs that are very hard to track down.\nBut what if you really, really want the non-const lvalue reference parameter? What if you promise to watch out for dangling references and race conditions? That's what std::ref is for. It's an explicit opt-in to the dangerous reference semantics. It's your way of saying, \"I know what I'm doing here.\"\n\n\nA12:\n\nThe issue itself is only marginally related to std::async(): When defining the result of the operation, std::async() uses std::result_of<...>::type with all its arguments being std::decay<...>::type'ed. This is reasonable because std::async() takes arbitrary types and forwards them to store them in some location. To store them, values are needed for the function object as well as for the arguments. Thus, std::result_of<...> is used similar to this:\ntypedef std::result_of<void (*(int))(int&)>::type result_type;\n\n... and since int can't be bound to an int& (int isn't an lvalue type was is needed to be bound to int&), this fails. Failure in this case means that std::result_of<...> doesn't define a nested type.\nA follow-up question could be: What is this type used to instantiate std::result_of<...>? The idea is that the function call syntax consisting of ResultType(ArgumentTypes...) is abused: instead of a result type, a function type is passed and std::result_of<...> determines the type of the function called when that function type is called with the given list of arguments is called. For function pointer types it isn't really that interesting but the function type can also be a function object where overloading needs to be taken into account. So basically, std::result_of<...> is used like this:\ntypedef void (*function_type)(int&);\ntypedef std::result_of<function_type(int)>::type result_type; // fails\ntypedef std::result_of<function_type(std::reference_wrapper<int>)>::type result_type; //OK\n\n\n\nA13:\n\nWith C++20 we got the test() method, which does exactly what OP wants.\n\n\nA14:\n\nGCC Atomic Built-ins\n\n\nA15:\n\nYou can use one default_random_engine per thread using Thread Local Storage.\nI can not tell you how to correctly use TLS since it is OS dependent. The best source you can use is to search through the internet.\n\n\nA16:\n\nNo, it can not.  That functionality was proposed to the committee under the name upgrade_mutex and upgrade_lock, but the committee chose to reject that portion of the proposal.  There is currently no work under way to re-prepose that functionality.\nEdit\nIn response to the \"where to go from here\" edit in user3761401's question, I've created a partially crippled implementation of upgrade_mutex/upgrade_lock here:\nhttps://github.com/HowardHinnant/upgrade_mutex\nFeel free to use this.  It is in the public domain.  It is only lightly tested, and it does not have the full functionality described in N3427.  Specifically the following functionality is missing:\n\nOne can not convert a unique_lock to a shared_timed_lock.\nOne can not try- or timed-convert a shared_timed_lock to a unique_lock.\nOne can not try- or timed-convert a upgrade_lock to a unique_lock.\n\nThat being said, I've included this functionality in upgrade_mutex and it can be accessed at this low level in a very ugly manner (such examples are in main.cpp).\nThe other lock conversions mentioned in N3427 are available.\n\ntry- and timed-conversions from shared_timed_lock to upgrade_lock.\nconversion from upgrade_lock to shared_timed_lock.\nblocking conversion from upgrade_lock to unique_lock.\nconversion from unique_lock to upgrade_lock.\n\nIt has all been put in namespace acme.  Put it in whatever namespace you want.\nRequirements\nThe compiler needs to support \"rvalue-this\" qualifiers, and explicit conversion operators.\nDisclaimers\nThe code has been only lightly tested.  If you find bugs I would appreciate a pull request.\nIt is possible to optimize the upgrade_mutex through the use of std::atomic.  No effort has been done on that front (it is a difficult and error prone task, taking more time than I have at the moment).\n\n\nA17:\n\n\nIntroduction to parallel computing: https://computing.llnl.gov/tutorials/parallel_comp/\nPOSIX threads programming: https://computing.llnl.gov/tutorials/pthreads/\n\n\n\nA18:\n\nHave you tried this?\nint intRand(const int & min, const int & max) {\n    static thread_local std::mt19937 generator;\n    std::uniform_int_distribution<int> distribution(min,max);\n    return distribution(generator);\n}\n\nDistributions are extremely cheap (they will be completely inlined by the optimiser so that the only remaining overhead is the actual random number rescaling). Don\u2019t be afraid to regenerate them as often as you need \u2013\u00a0in fact, resetting them would conceptually be no cheaper (which is why that operation doesn\u2019t exist).\nThe actual random number generator, on the other hand, is a heavy-weight object carrying a lot of state and requiring quite some time to be constructed, so that should only be initialised once per thread (or even across threads, but then you\u2019d need to synchronise access which is more costly in the long run).\n\n\nA19:\n\nI was unfortunately never able to find a way to do this.\nWithout some kind of thread init hook there just doesn't appear to be a way to get at that pointer (short of ASM hacks that would be platform dependent).\n\n\nA20:\n\nYou cannot read the value of a std::atomic_flag without setting it to true. This is by design. It is not a boolean variable (we have std::atomic<bool> for that), but a minimal flag that is guaranteed lock free on all architectures that support C++11.\nOn some platforms the only atomic instructions are exchange instructions. On such platforms, std::atomic_flag::test_and_set() can be implemented with exchange var,1 and clear() with exchange var,0, but there is no atomic instruction for reading the value.\nSo, if you want to read the value without changing it, then you need std::atomic<bool>.\n\n\nA21:\n\nAs you discovered, rand is the culprit here.\nFor those who are curious, it's possible that this behavior comes from your implementation of rand using a mutex for thread safety.\nFor example, eglibc defines rand in terms of __random, which is defined as:\nlong int\n__random ()\n{\n  int32_t retval;\n\n  __libc_lock_lock (lock);\n\n  (void) __random_r (&unsafe_state, &retval);\n\n  __libc_lock_unlock (lock);\n\n  return retval;\n}\n\nThis kind of locking would force multiple threads to run serially, resulting in lower performance.\n\n\nA22:\n\nstd::async (and other functions that do perfect forwarding) look at the type of the argument that you pass to figure out what to do. They do not look at how that argument will eventually be used. So, to pass an object by reference you need to tell std::async that you're using a reference. However, simply passing a reference won't do that. You have to use std::ref(value) to pass value by reference.\n\n\nA23:\n\nFor previous standards (here I will use C++ 03), the closest the standard comes to declaring the sequence of operations in a return is from 6.6\n\n6.6 Jump statements\n\nOn exit from a scope (however accomplished), destructors (12.4) are called for all constructed objects with automatic storage duration (3.7.2) (named objects or temporaries) that are declared in that scope, in the\n  reverse order of their declaration. Transfer out of a loop, out of a block, or back past an initialized variable with automatic storage duration involves the destruction of variables with automatic storage duration that are in scope at the point transferred from...\n\n\nThe return statement must complete in order to exit the [function] scope, implying that the copy-initialization must also complete. This order is not explicit. Various other quotes from 3.7.2 and 12.8 concisely state the same as above without providing explicit order. Working revisions (after Nov. 2014) include the quote below to address that. The defect report clarifies the change.\n\nFrom the current working draft (N4527) of the standard as seen on the date of this question\n6.6.3 The Return Statement \n\nThe copy-initialization of the returned entity is sequenced before the destruction of temporaries at the end\n  of the full-expression established by the operand of the return statement, which, in turn, is sequenced before\n  the destruction of local variables (6.6) of the block enclosing the return statement.\n\n\nNotice that this quote refers directly to 6.6. So I think it is safe to assume that the Mutex object will always be destroyed after the return expression has copy-initialized the return value.\n\n\nA24:\n\nIf you want to use atomic_flag to determine whether a thread should exit, you can do it like this:\nInitialization:\nstd::atomic_flag keep_running = ATOMIC_FLAG_INIT;\nkeep_running.test_and_set();\n\nThread loop:\nwhile (keep_running.test_and_set()) {\n    // do thread stuff\n}\n\nWhen you want the thread to exit:\nkeep_running.clear();\n\n\n\nA25:\n\nI have also been looking for such a book, they are very hard to come by. This one will be released in May, if that's any help:\nhttp://www.manning.com/williams/\nI purchased this book:\nhttp://www.amazon.co.uk/gp/product/0123705916/ref=oss_product\nIt's very good, it's in java, but most of the principles apply to c/c++ anyway.\n\n\nA26:\n\n\nHowever, once the worker thread terminates, the QObject thread affinity should no longer be valid.\n\nThe worker thread does NOT terminate after your function call. The whole point of using QtConcurrent::run is executing a large number of small tasks on the global thread pool (or some provided QThreadPool) while re-using threads to avoid the overhead of creating and destroying threads for each one of these small tasks. In addition to distributing computation across all available cores.\nYou can try looking at the source code for Qt to see how QtConcurrent::run is implemented. You will see that it ends up calling RunFunctionTaskBase::start, which essentially calls QThreadPool::start with a QRunnable that calls the function that was passed initially to QtConcurrent::run.\nNow the point that I want to get to is that, QThreadPool::start is implemented by adding the QRunnable to a queue, and then trying to wake up one of the threads from the thread pool (which are waiting for a new QRunnable to be added to the queue). The thing to note here, is that threads from the thread pool are not running an event loop (they are not designed to act this way), they are there just to execute QRunnables in the queue and nothing more (they are implemented this way for performance reasons obviously).\nThis means that, the moment you are creating a QObject in a function executed in QtConcurrent::run, you are just creating a QObject that lives in a thread with no event-loop, from the docs, restrictions include:\n\nIf no event loop is running, events won't be delivered to the object. For example, if you create a QTimer object in a thread but never call exec(), the QTimer will never emit its timeout() signal. Calling deleteLater() won't work either. (These restrictions apply to the main thread as well.)\n\n\nTL;DR: QtConcurrent::run runs functions in threads from the global QThreadPool (or a provided one). Those threads do not run an event loop, They just wait for QRunnables to run. So, a QObject living in a thread from these threads doesn't get any events delivered.\n\nIn the documentation, They have put using QThread (possibly, with an event loop and a worker object) and using QtConcurrent::run as two separate multi-threading technologies. They are not meant to be mixed together. So, no worker objects in thread pools, this is just asking for trouble.\n\nThe question: Does Qt automatically move the QObjects into the parent thread, or are we responsible in moving them to a valid thread before the worker thread terminates?\n\nI think that after looking at things this way, The answer is obvious that Qt does NOT move QObjects into any thread automatically. The documentation has warned about using a QObject in a QThread without an event loop, and that's it.\nYou are free to move them to whatever thread you like. But please keep in mind that moveToThread() can sometimes cause problems. For example, if moving your worker object involves moving a QTimer:\n\nNote that all active timers for the object will be reset. The timers are first stopped in the current thread and restarted (with the same interval) in the targetThread. As a result, constantly moving an object between threads can postpone timer events indefinitely.\n\n\nConclusion: I think that you should consider using your own QThread that runs its event loop, and create your worker QObjects there instead of using QtConcurrent. This way is far better than moving QObjects around, and can avoid many errors that can arise from using your current approach. Have a look at the comparison table of multi-threading technologies in Qt and choose the technology that best suits your use case. Only use QtConcurrent if you want to just execute a one-call function and get its return value. If you want permanent interaction with the thread, you should switch to using your own QThread with worker QObjects.\n\n\nA27:\n\nI am searching for the same thing. \nAs I see nobody has answered your question after having searched the web in all ways I arrived to the subsequent information: supposing to compile for gcc on linux (ubuntu) and using -m64, the segment register gs holds the value 0. The hidden part of the segment (holding the linear address) \npoints to the thread specific local area.\nThat area contains at that address the address of that address ( 64 bits ). At lower addresses are stored all thread local variables.\nThat address is the native_handle().\nSo in order to access a threads local data you should do it via that pointer. \nIn other words: (char*)&variable-(char*)myThread.native_handle()+(char*)theOtherThread.native_handle()\nThe code that demonstrates the above supposing g++,linux,pthreads is:\n#include <iostream>\n#include <thread>\n#include <sstream>\n\nthread_local int B=0x11111111,A=0x22222222;\n\nbool shouldContinue=false;\n\nvoid code(){\n    while(!shouldContinue);\n    std::stringstream ss;\n    ss<<\" A:\"<<A<<\" B:\"<<B<<std::endl;\n    std::cout<<ss.str();\n}\n\n//#define ot(th,variable) \n//(*( (char*)&variable-(char*)(pthread_self())+(char*)(th.native_handle()) ))\n\nint& ot(std::thread& th,int& v){\n    auto p=pthread_self();\n    intptr_t d=(intptr_t)&v-(intptr_t)p;\n    return *(int*)((char*)th.native_handle()+d);\n}\n\nint main(int argc, char **argv)\n{       \n\n        std::thread th1(code),th2(code),th3(code),th4(code);\n\n        ot(th1,A)=100;ot(th1,B)=110;\n        ot(th2,A)=200;ot(th2,B)=210;\n        ot(th3,A)=300;ot(th3,B)=310;\n        ot(th4,A)=400;ot(th4,B)=410;\n\n        shouldContinue=true;\n\n        th1.join();\n        th2.join();\n        th3.join();\n        th4.join();\n\n    return 0;\n}\n\n\n\nA28:\n\nIf you want thread local variables that are not thread local, why don't you use global variables instead?\nImportant clarification!\nI am not suggesting that you use a single global to replace a thread-local variable. I 'm suggesting of using a single global array or other suitable collection of values to replace one thread-local variable.\nYou will have to provide synchronization of course, but since you want to expose a value modified in thread A to thread B there's no getting around that.\nUpdate:\nThe GCC documentation on __thread says:\n\nWhen the address-of operator is\n  applied to a thread-local variable, it\n  is evaluated at run-time and returns\n  the address of the current thread's\n  instance of that variable. An address\n  so obtained may be used by any thread.\n  When a thread terminates, any pointers\n  to thread-local variables in that\n  thread become invalid.\n\nTherefore, if you insist on going this way I imagine it's possible to get the address of a thread local variable from the thread it belongs to, just after the thread is spawned. You could then store a pointer to that memory location to a map (thread id => pointer), and let other threads access the variable this way. This assumes that you own the code for the spawned thread.\nIf you are really adventurous, you could try digging up information on ___tls_get_addr (start from this PDF which is linked to by the aforementioned GCC docs). But this approach is so highly compiler and platform specific and so lacking in documentation that it should be causing alarms to go off in anyone's head.\n\n\nA29:\n\nThis is an old question, but since there is no answer given, why not use a class that has its own static registration?\n#include <mutex>\n#include <thread>\n#include <unordered_map>\n\nstruct foo;\n\nstatic std::unordered_map<std::thread::id, foo*> foos;\nstatic std::mutex foos_mutex;\n\nstruct foo\n{\n    foo()\n    {\n        std::lock_guard<std::mutex> lk(foos_mutex);\n        foos[std::this_thread::get_id()] = this;\n    }\n};\n\nstatic thread_local foo tls_foo;\n\n\nOf course you would need some kind of synchronization between the threads to ensure that the thread had registered the pointer, but you can then grab it from the map from any thread where you know the thread's id.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 21, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nWhy is it taking more than an hour to simply update this table to add a column? This table has 15M rows.  It has 2 indexes and a single key primary key. The ALTER TABLE query has been in \"copy to tmp table\" state for 1 hour 15 minutes now.\nALTER TABLE `frugg`.`item_catalog_map` \nADD COLUMN `conversion_url` TEXT NULL DEFAULT NULL\n\nTable:\nmysql> describe item_catalog_map;\n+------------------------+---------------+------+-----+---------+-------+\n| Field                  | Type          | Null | Key | Default | Extra |\n+------------------------+---------------+------+-----+---------+-------+\n| catalog_unique_item_id | varchar(255)  | NO   | PRI | NULL    |       |\n| catalog_id             | int(11)       | YES  | MUL | NULL    |       |\n| item_id                | int(11)       | YES  | MUL | NULL    |       |\n| price                  | decimal(10,2) | YES  |     | 0.00    |       |\n+------------------------+---------------+------+-----+---------+-------+\n\nmysql> show index from item_catalog_map;\n+------------------+------------+----------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+\n| Table            | Non_unique | Key_name             | Seq_in_index | Column_name            | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment |\n+------------------+------------+----------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+\n| item_catalog_map |          0 | PRIMARY              |            1 | catalog_unique_item_id | A         |    15485115 |     NULL | NULL   |      | BTREE      |         |\n| item_catalog_map |          1 | IDX_ACD6184FCC3C66FC |            1 | catalog_id             | A         |          18 |     NULL | NULL   | YES  | BTREE      |         |\n| item_catalog_map |          1 | IDX_ACD6184F126F525E |            1 | item_id                | A         |    15485115 |     NULL | NULL   | YES  | BTREE      |         |\n+------------------+------------+----------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+\n\n\n\nCandidate answers:\nA1:\n\nUse MySQL FIND_IN_SET function:\n  SELECT * \n    FROM table_name \n   WHERE id IN (..,..,..,..) \nORDER BY FIND_IN_SET (coloumn_name, .., .., ..);\n\n\n\nA2:\n\nI think you should manage to store your data in a way that you will simply do a join and it will be perfect, so no hacks and complicated things going on.\nI have for instance a \"Recently played\" list of track ids, on SQLite i simply do:\nSELECT * FROM recently NATURAL JOIN tracks;\n\n\n\nA3:\n\nIf I remember correctly, in PostgreSQL you have to add every column you fetch from the table where the GROUP BY clause applies to the GROUP BY clause.\n\n\nA4:\n\nIf you want to do arbitrary sorting on a query using values inputted by the query in MS SQL Server 2008+, it can be done by creating a table on the fly and doing a join like so (using nomenclature from OP).\nSELECT table1.name, table1.description ... \nFROM (VALUES (id1,1), (id2,2), (id3,3) ...) AS orderTbl(orderKey, orderIdx) \nLEFT JOIN table1 ON orderTbl.orderKey=table1.id\nORDER BY orderTbl.orderIdx\n\nIf you replace the VALUES statement with something else that does the same thing, but in ANSI SQL, then this should work on any SQL database.\nNote: \nThe second column in the created table (orderTbl.orderIdx) is necessary when querying record sets larger than 100 or so. I originally didn't have an orderIdx column, but found that with result sets larger than 100 I had to explicitly sort by that column; in SQL Server Express 2014 anyways.\n\n\nA5:\n\nI think that .uniq [1] will solve your problem. \n[1] Available.select('...').uniq\n\nTake a look at http://guides.rubyonrails.org/active_record_querying.html#selecting-specific-fields\n\n\nA6:\n\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic static function interpolateQuery($query, $params) {\n    $keys = array();\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n    }\n\n    $query = preg_replace($keys, $params, $query, 1, $count);\n\n    #trigger_error('replaced '.$count.' keys');\n\n    return $query;\n}\n\n\n\nA7:\n\nTwo solutions that spring to mind:\n\norder by case id when 123 then 1 when 456 then 2 else null end asc\norder by instr(','||id||',',',123,456,') asc\n\n(instr() is from Oracle; maybe you have locate() or charindex() or something like that)\n\n\nA8:\n\nIf you're using MySQL, there is a method REGEXP that you can use...\nhttp://dev.mysql.com/doc/refman/5.1/en/regexp.html#operator_regexp\nSo then you would use:\nSELECT * FROM `shirts` WHERE `colors` REGEXP '\\b1\\b'\n\n\n\nA9:\n\nselect * from shirts where find_in_set('1',colors) <> 0\n\nWorks for me\n\n\nA10:\n\nYour table has 15 million rows, which is something. The ALTER TABLE involves copying over all the data from the table and recreating the indexes. As a first measurement try copying the data file (item_catalog_map.MYD if it's MyISAM) in your filesystem and see how long that takes. This is the time the ALTER TABLE will at least take.\n\n\nA11:\n\nThis is actually how your query works and is a normal behaviour. Using LIMIT you will not limit the count or sum but only the returned rows. So your query will return n rows as stated in your LIMIT clause. And since your query actually returns only one row, applying a (non-zero) limit has no effect on the results.\nHowever, your second query will work as expected and is an established way of solving this problem.\n\n\nA12:\n\nSELECT * FROM table WHERE field1 NOT LIKE '%$x%';   (Make sure you escape $x properly beforehand to avoid SQL injection)\nEdit: NOT IN does something a bit different - your question isn't totally clear so pick which one to use.   LIKE 'xxx%' can use an index.  LIKE '%xxx' or LIKE '%xxx%' can't.\n\n\nA13:\n\nYou can achieve this by following function.\nRun following query to create function.\nDELIMITER ||\nCREATE FUNCTION `TOTAL_OCCURANCE`(`commastring` TEXT, `findme`     VARCHAR(255)) RETURNS int(11)\nNO SQL\n-- SANI: First param is for comma separated string and 2nd for string to find.\nreturn ROUND (   \n    (\n        LENGTH(commastring)\n        - LENGTH( REPLACE ( commastring, findme, \"\") ) \n    ) / LENGTH(findme)        \n);\n\nAnd call this function like this\nmsyql> select TOTAL_OCCURANCE('A,B,C,A,D,X,B,AB', 'A');\n\n\n\nA14:\n\nMySQL's GROUP BY can be used without an aggregate function (which is contrary to the SQL standard), and returns the first row in the group (I don't know based on what criteria), while PostgreSQL must have an aggregate function (MAX, SUM, etc) on the column, on which the GROUP BY clause is issued.\n\n\nA15:\n\nMySQL's totally non standards compliant GROUP BY can be emulated by Postgres' DISTINCT ON. Consider this:\nMySQL:\nSELECT a,b,c,d,e FROM table GROUP BY a\n\nThis delivers 1 row per value of a (which one, you don't really know). Well actually you can guess, because MySQL doesn't know about hash aggregates, so it will probably use a sort... but it will only sort on a, so the order of the rows could be random. Unless it uses a multicolumn index instead of sorting. Well, anyway, it's not specified by the query.\nPostgres:\nSELECT DISTINCT ON (a) a,b,c,d,e FROM table ORDER BY a,b,c\n\nThis delivers 1 row per value of a, this row will be the first one in the sort according to the ORDER BY specified by the query. Simple.\nNote that here, it's not an aggregate I'm computing. So GROUP BY actually makes no sense. DISTINCT ON makes a lot more sense.\nRails is married to MySQL, so I'm not surprised that it generates SQL that doesn't work in Postgres.\n\n\nA16:\n\nNot the prettiest solution, but changing the group parameter to output every column in model works in PostgreSQL:\nexpiration = Available.find(:all,\n:joins => [ :room ],\n:conditions => [ \"rooms.hotel_id = ? AND availables.bookdate BETWEEN ? AND ?\", hostel_id, date.to_s, (date+days-1).to_s ],\n:group => Available.column_names.collect{|col| \"availables.#{col}\"},\n:order => 'availables.updated_at')\n\n\n\nA17:\n\nI assume you mean that you want the final SQL query, with parameter values interpolated into it.  I understand that this would be useful for debugging, but it is not the way prepared statements work.  Parameters are not combined with a prepared statement on the client-side, so PDO should never have access to the query string combined with its parameters.\nThe SQL statement is sent to the database server when you do prepare(), and the parameters are sent separately when you do execute().  MySQL's general query log does show the final SQL with values interpolated after you execute().  Below is an excerpt from my general query log.  I ran the queries from the mysql CLI, not from PDO, but the principle is the same.\n081016 16:51:28 2 Query       prepare s1 from 'select * from foo where i = ?'\n                2 Prepare     [2] select * from foo where i = ?\n081016 16:51:39 2 Query       set @a =1\n081016 16:51:47 2 Query       execute s1 using @a\n                2 Execute     [2] select * from foo where i = 1\n\nYou can also get what you want if you set the PDO attribute PDO::ATTR_EMULATE_PREPARES.  In this mode, PDO interpolate parameters into the SQL query and sends the whole query when you execute().  This is not a true prepared query.  You will circumvent the benefits of prepared queries by interpolating variables into the SQL string before execute().\n\nRe comment from @afilina:\nNo, the textual SQL query is not combined with the parameters during execution. So there's nothing for PDO to show you.\nInternally, if you use PDO::ATTR_EMULATE_PREPARES, PDO makes a copy of the SQL query and interpolates parameter values into it before doing the prepare and execute. But PDO does not expose this modified SQL query. \nThe PDOStatement object has a property $queryString, but this is set only in the constructor for the PDOStatement, and it's not updated when the query is rewritten with parameters.\nIt would be a reasonable feature request for PDO to ask them to expose the rewritten query. But even that wouldn't give you the \"complete\" query unless you use PDO::ATTR_EMULATE_PREPARES.\nThis is why I show the workaround above of using the MySQL server's general query log, because in this case even a prepared query with parameter placeholders is rewritten on the server, with parameter values backfilled into the query string. But this is only done during logging, not during query execution.\n\n\nA18:\n\nAlternative method:\nSELECT * FROM x WHERE CHARINDEX('$x', field1, 0)<1 \n\nExplanation: It searches for occurrences of '$x' starting from position 0. And only selects where count is 'less than one'.\n\n\nA19:\n\nIf you don't have an aggregate function in your where clause, another possible source of the 1111 - Invalid use of group function error is if you have nested aggregate functions:\nselect sum(avg(close)) from prices;\n(1111, 'Invalid use of group function')\n\nYou can get around this by breaking up the problem into two steps:\n\nSave the inner aggregation into a variable\n\nselect @avg:=avg(close) from prices;\n\n\nRun the outer aggregation against the variable\n\nselect sum(@avg) from prices;\n\n\n\nA20:\n\nIf the set of colors is more or less fixed, the most efficient and also most readable way would be to use string constants in your app and then use MySQL's SET type with FIND_IN_SET('red',colors) in your queries. When using the SET type with FIND_IN_SET, MySQL uses one integer to store all values and uses binary \"and\" operation to check for presence of values which is way more efficient than scanning a comma-separated string.\nIn SET('red','blue','green'), 'red' would be stored internally as 1, 'blue' would be stored internally as 2 and 'green' would be stored internally as 4. The value 'red,blue' would be stored as 3 (1|2) and 'red,green' as 5 (1|4).\n\n\nA21:\n\nCorrect, the solution to fixing this is to use :select and to select each field that you wish to decorate the resulting object with and group by them.\nNasty - but it is how group by should work as opposed to how MySQL works with it by guessing what you mean if you don't stick fields in your group by.\n\n\nA22:\n\nThe IN clause describes a set of values, and sets do not have order.\nYour solution with a join and then ordering on the display_order column is the most nearly correct solution; anything else is probably a DBMS-specific hack (or is doing some stuff with the OLAP functions in standard SQL).  Certainly, the join is the most nearly portable solution (though generating the data with the display_order values may be problematic).  Note that you may need to select the ordering columns; that used to be a requirement in standard SQL, though I believe it was relaxed as a rule a while ago (maybe as long ago as SQL-92).\n\n\nA23:\n\nUse MySQL's FIELD() function:\nSELECT name, description, ...\nFROM ...\nWHERE id IN([ids, any order])\nORDER BY FIELD(id, [ids in order])\n\nFIELD() will return the index of the first parameter that is equal to the first parameter (other than the first parameter itself).\nFIELD('a', 'a', 'b', 'c')\nwill return 1\nFIELD('a', 'c', 'b', 'a')\nwill return 3\nThis will do exactly what you want if you paste the ids into the IN() clause and the FIELD() function in the same order.\n\n\nA24:\n\nI modified the method to include handling output of arrays for statements like WHERE IN (?). \nUPDATE: Just added check for NULL value and duplicated $params so actual $param values are not modified. \nGreat work bigwebguy and thanks!\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_string($value))\n            $values[$key] = \"'\" . $value . \"'\";\n\n        if (is_array($value))\n            $values[$key] = \"'\" . implode(\"','\", $value) . \"'\";\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n\n    $query = preg_replace($keys, $values, $query);\n\n    return $query;\n}\n\n\n\nA25:\n\nAccording to MySQL's \"Debuking GROUP BY Myths\" http://dev.mysql.com/tech-resources/articles/debunking-group-by-myths.html. SQL (2003 version of the standard) doesn't requires columns referenced in the SELECT list of a query to also appear in the GROUP BY clause.\n\n\nA26:\n\nMySQL\u2019s ALTER TABLE performance can become a problem with very large tables. MySQL performs\nmost alterations by making an empty table with the desired new structure,  inserting all the data from the old table into the new one, and deleting the old table. This can take a very long time, especially if you\u2019re short on memory and the table is large and has lots of indexes. Many people have  experience with ALTER TABLE operations that have taken hours or days to complete.\nAnyway if you need to proceed with alter table, maybe the following resources could help you:\n\nhttps://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html\nhttps://github.com/soundcloud/lhm\nhttps://githubengineering.com/gh-ost-github-s-online-migration-tool-for-mysql/\n\n\n\nA27:\n\nAdded a little bit more to the code by Mike - walk the values to add single quotes \n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_array($value))\n            $values[$key] = implode(',', $value);\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n    // Walk the array to see if we can add single-quotes to strings\n    array_walk($values, create_function('&$v, $k', 'if (!is_numeric($v) && $v!=\"NULL\") $v = \"\\'\".$v.\"\\'\";'));\n\n    $query = preg_replace($keys, $values, $query, 1, $count);\n\n    return $query;\n}\n\n\n\nA28:\n\nThe classic way would be to add commas to the left and right:\nselect * from shirts where CONCAT(',', colors, ',') like '%,1,%'\n\nBut find_in_set also works:\nselect * from shirts where find_in_set('1',colors) <> 0\n\n\n\nA29:\n\nI spent a good deal of time researching this situation for my own needs. This and several other SO threads helped me a great deal, so I wanted to share what I came up with.\nWhile having access to the interpolated query string is a significant benefit while troubleshooting, we wanted to be able to maintain a log of only certain queries (therefore, using the database logs for this purpose was not ideal). We also wanted to be able to use the logs to recreate the condition of the tables at any given time, therefore, we needed to make certain the interpolated strings were escaped properly. Finally, we wanted to extend this functionality to our entire code base having to re-write as little of it as possible (deadlines, marketing, and such; you know how it is).\nMy solution was to extend the functionality of the default PDOStatement object to cache the parameterized values (or references), and when the statement is executed, use the functionality of the PDO object to properly escape the parameters when they are injected back in to the query string. We could then tie in to execute method of the statement object and log the actual query that was executed at that time (or at least as faithful of a reproduction as possible).\nAs I said, we didn't want to modify the entire code base to add this functionality, so we overwrite the default bindParam() and bindValue() methods of the PDOStatement object, do our caching of the bound data, then call parent::bindParam() or parent::bindValue(). This allowed our existing code base to continue to function as normal.\nFinally, when the execute() method is called, we perform our interpolation and provide the resultant string as a new property E_PDOStatement->fullQuery. This can be output to view the query or, for example, written to a log file.\nThe extension, along with installation and configuration instructions, are available on github:\nhttps://github.com/noahheck/E_PDOStatement\nDISCLAIMER:\nObviously, as I mentioned, I wrote this extension. Because it was developed with help from many threads here, I wanted to post my solution here in case anyone else comes across these threads, just as I did.\n\n\nA30:\n\nA bit late probably but now there is PDOStatement::debugDumpParams\n\nDumps the informations contained by a prepared statement directly on\n  the output. It will provide the SQL query in use, the number of\n  parameters used (Params), the list of parameters, with their name,\n  type (paramtype) as an integer, their key name or position, and the\n  position in the query (if this is supported by the PDO driver,\n  otherwise, it will be -1).\n\nYou can find more on the official php docs\nExample:\n<?php\n/* Execute a prepared statement by binding PHP variables */\n$calories = 150;\n$colour = 'red';\n$sth = $dbh->prepare('SELECT name, colour, calories\n    FROM fruit\n    WHERE calories < :calories AND colour = :colour');\n$sth->bindParam(':calories', $calories, PDO::PARAM_INT);\n$sth->bindValue(':colour', $colour, PDO::PARAM_STR, 12);\n$sth->execute();\n\n$sth->debugDumpParams();\n\n?>\n\n\n\nA31:\n\nFirst, the error you're getting is due to where you're using the COUNT function -- you can't use an aggregate (or group) function in the WHERE clause.\nSecond, instead of using a subquery, simply join the table to itself:\nSELECT a.pid \nFROM Catalog as a LEFT JOIN Catalog as b USING( pid )\nWHERE a.sid != b.sid\nGROUP BY a.pid\n\nWhich I believe should return only rows where at least two rows exist with the same pid but there is are at least 2 sids.  To make sure you get back only one row per pid I've applied a grouping clause.\n\n\nA32:\n\nYou should actually fix your database schema so that you have three tables:\nshirt: shirt_id, shirt_name\ncolor: color_id, color_name\nshirtcolor: shirt_id, color_id\n\nThen if you want to find all of the shirts that are red, you'd do a query like:\nSELECT *\nFROM shirt, color\nWHERE color.color_name = 'red'\n  AND shirt.shirt_id = shirtcolor.shirt_id\n  AND color.color_id = shirtcolor.color_id\n\n\n\nA33:\n\nWhat kind of field is this? The IN operator cannot be used with a single field, but is meant to be used in subqueries or with predefined lists:\n-- subquery\nSELECT a FROM x WHERE x.b NOT IN (SELECT b FROM y);\n-- predefined list\nSELECT a FROM x WHERE x.b NOT IN (1, 2, 3, 6);\n\nIf you are searching a string, go for the LIKE operator (but this will be slow):\n-- Finds all rows where a does not contain \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE '%text%';\n\nIf you restrict it so that the string you are searching for has to start with the given string, it can use indices (if there is an index on that field) and be reasonably fast:\n-- Finds all rows where a does not start with \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE 'text%';\n\n\n\nA34:\n\nSee following how to get sorted data.\nSELECT ...\n  FROM ...\n WHERE zip IN (91709,92886,92807,...,91356)\n   AND user.status=1\nORDER \n    BY provider.package_id DESC \n     , FIELD(zip,91709,92886,92807,...,91356)\nLIMIT 10\n\n\n\nA35:\n\nSELECT ORDER_NO, DELIVERY_ADDRESS \nfrom IFSAPP.PURCHASE_ORDER_TAB \nwhere ORDER_NO in ('52000077','52000079','52000167','52000297','52000204','52000409','52000126') \nORDER BY instr('52000077,52000079,52000167,52000297,52000204,52000409,52000126',ORDER_NO)\n\nworked really great\n\n\nA36:\n\nFIND_IN_SET is your friend in this case\nselect * from shirts where FIND_IN_SET(1,colors) \n\n\n\nA37:\n\nGive this a shot:\nSELECT name, description, ...\nWHERE id IN\n    (SELECT id FROM table1 WHERE...)\nORDER BY\n    (SELECT display_order FROM table1 WHERE...),\n    (SELECT name FROM table1 WHERE...)\n\nThe WHEREs will probably take a little tweaking to get the correlated subqueries working properly, but the basic principle should be sound.\n\n\nA38:\n\nPDOStatement has a public property $queryString. It should be what you want.\nI've just notice that PDOStatement has an undocumented method debugDumpParams() which you may also want to look at.\n\n\nA39:\n\nA solution is to voluntarily put an error in the query and to print the error's message:\n//Connection to the database\n$co = new PDO('mysql:dbname=myDB;host=localhost','root','');\n//We allow to print the errors whenever there is one\n$co->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n//We create our prepared statement\n$stmt = $co->prepare(\"ELECT * FROM Person WHERE age=:age\"); //I removed the 'S' of 'SELECT'\n$stmt->bindValue(':age','18',PDO::PARAM_STR);\ntry {\n    $stmt->execute();\n} catch (PDOException $e) {\n    echo $e->getMessage();\n}\n\nStandard output:\n\nSQLSTATE[42000]: Syntax error or access violation: [...] near 'ELECT * FROM Person WHERE age=18' at line 1\n\nIt is important to note that it only prints the first 80 characters of the query.\n\n\nA40:\n\n1. For MySQL:\nSELECT FIND_IN_SET(5, columnname) AS result \nFROM table\n\n2.For Postgres SQL :\nSELECT * \nFROM TABLENAME f\nWHERE 'searchvalue' = ANY (string_to_array(COLUMNNAME, ','))\n\nExample\nselect * \nfrom customer f\nwhere '11' = ANY (string_to_array(customerids, ','))\n\n\n\nA41:\n\nI just tried to do this is MS SQL Server where we do not have FIELD():\nSELECT table1.id\n... \nINNER JOIN\n    (VALUES (10,1),(3,2),(4,3),(5,4),(7,5),(8,6),(9,7),(2,8),(6,9),(5,10)\n    ) AS X(id,sortorder)\n        ON X.id = table1.id\n    ORDER BY X.sortorder\n\nNote that I am allowing duplication too.\n\n\nA42:\n\nAns to get sorted data.\nSELECT ...\nFROM ...\nORDER  BY FIELD(user_id,5,3,2,...,50)  LIMIT 10\n\n\n\nA43:\n\nThis will work for sure, and I actually tried it out:\nlwdba@localhost (DB test) :: DROP TABLE IF EXISTS shirts;\nQuery OK, 0 rows affected (0.08 sec)\n\nlwdba@localhost (DB test) :: CREATE TABLE shirts\n    -> (<BR>\n    -> id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    -> ticketnumber INT,\n    -> colors VARCHAR(30)\n    -> );<BR>\nQuery OK, 0 rows affected (0.19 sec)\n\nlwdba@localhost (DB test) :: INSERT INTO shirts (ticketnumber,colors) VALUES\n    -> (32423,'1,2,5,12,15'),\n    -> (32424,'1,5,12,15,30'),\n    -> (32425,'2,5,11,15,28'),\n    -> (32426,'1,2,7,12,15'),\n    -> (32427,'2,4,8,12,15');\nQuery OK, 5 rows affected (0.06 sec)\nRecords: 5  Duplicates: 0  Warnings: 0\n\nlwdba@localhost (DB test) :: SELECT * FROM shirts WHERE LOCATE(CONCAT(',', 1 ,','),CONCAT(',',colors,',')) > 0;\n+----+--------------+--------------+\n| id | ticketnumber | colors       |\n+----+--------------+--------------+\n|  1 |        32423 | 1,2,5,12,15  |\n|  2 |        32424 | 1,5,12,15,30 |\n|  4 |        32426 | 1,2,7,12,15  |\n+----+--------------+--------------+\n3 rows in set (0.00 sec)\n\nGive it a Try !!!\n\n\nA44:\n\nFor others looking for a way to order by any field, including joined field, in postgresql, use a subquery:\nSELECT * FROM(\nSELECT DISTINCT ON(availables.bookdate) `availables`.* \nFROM `availables` INNER JOIN `rooms` ON `rooms`.id = `availables`.room_id \nWHERE (rooms.hotel_id = 5056 \nAND availables.bookdate BETWEEN '2009-11-22' AND '2009-11-24')\n) AS distinct_selected\nORDER BY availables.updated_at\n\nor arel:\n\nsubquery = SomeRecord.select(\"distinct on(xx.id) xx.*, jointable.order_field\")\n      .where(\"\").joins(\")\nresult = SomeRecord.select(\"*\").from(\"(#{subquery.to_sql}) AS distinct_selected\").order(\" xx.order_field ASC, jointable.order_field ASC\")\n\n\n\nA45:\n\nFor Oracle, John's solution using instr() function works. Here's slightly different solution that worked - \nSELECT id \nFROM table1 \nWHERE id IN (1, 20, 45, 60) \nORDER BY instr('1, 20, 45, 60', id)\n\n\nA46:\n\nTake a look at the FIND_IN_SET function for MySQL.\nSELECT * \n    FROM shirts \n    WHERE FIND_IN_SET('1',colors) > 0\n\n\n\nA47:\n\nMy first thought was to write a single query, but you said that was not possible because one is run by the user and the other is run in the background.  How are you storing the list of ids to pass from the user to the background process?  Why not put them in a temporary table with a column to signify the order.\nSo how about this:\n\nThe user interface bit runs and inserts values into a new table you create.  It would insert the id, position and some sort of job number identifier)\nThe job number is passed to the background process (instead of all the ids)\nThe background process does a select from the table in step 1 and you join in to get the other information that you require.  It uses the job number in the WHERE clause and orders by the position column.\nThe background process, when finished, deletes from the table based on the job identifier.\n\n\n\nA48:\n\nPostgreSQL is more SQL compliant than MySQL. All fields - except computed field with aggregation function - in the output must be present in the GROUP BY clause.\n\n\nA49:\n\nYou need to use HAVING, not WHERE.\nThe difference is: the WHERE clause filters which rows MySQL selects. Then MySQL groups the rows together and aggregates the numbers for your COUNT function.\nHAVING is like WHERE, only it happens after the COUNT value has been computed, so it'll work as you expect. Rewrite your subquery as:\n(                  -- where that pid is in the set:\nSELECT c2.pid                  -- of pids\nFROM Catalog AS c2             -- from catalog\nWHERE c2.pid = c1.pid\nHAVING COUNT(c2.sid) >= 2)\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 22, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIf I throw a JavaScript exception myself (eg, throw \"AArrggg\"), how can I get the stack trace (in Firebug or otherwise)?  Right now I just get the message.\nedit: As many people below have posted, it is possible to get a stack trace for a JavaScript exception but I want to get a stack trace for my exceptions.  For example:\nfunction foo() {\n    bar(2);\n}\nfunction bar(n) {\n    if (n < 2)\n        throw \"Oh no! 'n' is too small!\"\n    bar(n-1);\n}\n\nWhen foo is called, I want to get a stack trace which includes the calls to foo, bar, bar.\n\n\nCandidate answers:\nA1:\n\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages. \nIt doesn't matter whether you installed that package globally or locally. NPX will temporarily install it and run it. NPM also can run packages if you configure a package.json file and include it in the script section. \n\nSo remember this, if you want to check/run a node package  quickly without installing locally or globally use NPX.\n\nnpM -  Manager\nnpX -  Execute - easy to remember\n\n\nA2:\n\nHere is the simple definition.\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages.\n\n\nA3:\n\nnpm is a tool that use to install packages and npx is a tool that use to execute packages.\nnpm-If you wish to run package through npm then you have to specify that package in your package.json and install it locally.\nnpx-A package can be executable without installing the package. It is an npm package runner so if any packages aren\u2019t already installed it will install them automatically.\n\n\nA4:\n\nSimply npm is the Node Package Manager and\nnpx is the executeable version that run npm packages\n\n\nA5:\n\nIt means that the object you pass in the request (I guess it is pagedoc) has a circular reference, something like:\nvar a = {};\na.b = a;\n\nJSON.stringify cannot convert structures like this.\nN.B.: This would be the case with DOM nodes, which have circular references, even if they are not attached to the DOM tree. Each node has an ownerDocument which refers to document in most cases. document has a reference to the DOM tree at least through document.body and document.body.ownerDocument refers back to document again, which is only one of multiple circular references in the DOM tree.\n\n\nA6:\n\nNPX:\nFrom https://www.futurehosting.com/blog/npx-makes-life-easier-for-node-developers-plus-node-vulnerability-news/:\n\nWeb developers can have dozens of projects on their development\n  machines, and each project has its own particular set of npm-installed\n  dependencies. A few years back, the usual advice for dealing with CLI\n  applications like Grunt or Gulp was to install them locally in each\n  project and also globally so they could easily be run from the command\n  line.\nBut installing globally caused as many problems as it solved. Projects\n  may depend on different versions of command line tools, and polluting\n  the operating system with lots of development-specific CLI tools isn\u2019t\n  great either. Today, most developers prefer to install tools locally\n  and leave it at that.\nLocal versions of tools allow developers to pull projects from GitHub\n  without worrying about incompatibilities with globally installed\n  versions of tools. NPM can just install local versions and you\u2019re good\n  to go. But project specific installations aren\u2019t without their\n  problems: how do you run the right version of the tool without\n  specifying its exact location in the project or playing around with\n  aliases?\nThat\u2019s the problem npx solves. A new tool included in NPM 5.2, npx is\n  a small utility that\u2019s smart enough to run the right application when\n  it\u2019s called from within a project.\nIf you wanted to run the project-local version of mocha, for example,\n  you can run npx mocha inside the project and it will do what you\n  expect.\nA useful side benefit of npx is that it will automatically install npm\n  packages that aren\u2019t already installed. So, as the tool\u2019s creator Kat\n  March\u00e1n points out, you can run npx benny-hill without having to deal\n  with Benny Hill polluting the global environment.\nIf you want to take npx for a spin, update to the most recent version\n  of npm.\n\n\n\nA7:\n\nHere's an example of NPX in action: npx cowsay hello\nIf you type that into your bash terminal you'll see the result. The benefit of this is that npx has temporarily installed cowsay. There is no package pollution since cowsay is not permanently installed. This is great for one off packages where you want to avoid package pollution. \nAs mentioned in other answers, npx is also very useful in cases where (with npm) the package needs to be installed then configured before running. E.g. instead of using npm to install and then configure the json.package file and then call the configured run command just use npx instead. A real example:\nnpx create-react-app my-app\n\n\nA8:\n\nOne approach is to strip object and functions from main object. And stringify the simpler form\nfunction simpleStringify (object){\n    // stringify an object, avoiding circular structures\n    // https://stackoverflow.com/a/31557814\n    var simpleObject = {};\n    for (var prop in object ){\n        if (!object.hasOwnProperty(prop)){\n            continue;\n        }\n        if (typeof(object[prop]) == 'object'){\n            continue;\n        }\n        if (typeof(object[prop]) == 'function'){\n            continue;\n        }\n        simpleObject[prop] = object[prop];\n    }\n    return JSON.stringify(simpleObject); // returns cleaned up JSON\n};\n\nif you are using node js use inspect()\nimport {inspect} from \"util\";\nconsole.log(inspect(object));\n\n\n\nA9:\n\nI normally use the circular-json npm package to solve this.\n// Felix Kling's example\nvar a = {};\na.b = a;\n// load circular-json module\nvar CircularJSON = require('circular-json');\nconsole.log(CircularJSON.stringify(a));\n//result\n{\"b\":\"~\"}\n\nNote: circular-json has been deprecated, I now use flatted (from the creator of CircularJSON):\n// ESM\nimport {parse, stringify} from 'flatted/esm';\n\n// CJS\nconst {parse, stringify} = require('flatted/cjs');\n\nconst a = [{}];\na[0].a = a;\na.push(a);\n\nstringify(a); // [[\"1\",\"0\"],{\"a\":\"0\"}]\n\nfrom: https://www.npmjs.com/package/flatted\n\n\nA10:\n\nNPM: NPM stands for Node Package Manager and is the default package manager for Node.js. It was developed by Isaac Z. Schlueter and was originally released on January 12, 2010. It is entirely written in JavaScript. It consists of a command-line client npm which manages all node.js packages and modules. When node.js is installed, it is included in the installation.\nnpm run your-package-name\n\nNPX is a tool that use to execute packages.\nNPX is an acronym for Node Package Execute The NPX package comes with npm, so when you install npm above 5.2.0, NPX will be installed automatically.\nIt is an npm package runner that can execute any package that you want from the npm registry without even installing that package. The npx is useful during a single time use package. If you have installed npm below 5.2.0 then npx is not installed in your system.\nRun the following command to determine if npx is installed:\nnpx -v\n\nThe following command can be run if npx is not installed.\nnpm install -g npx\n\nUse npx to execute the package:\nnpx your-package-name\n\n\n\n\nA11:\n\nSimple answer is like\nNPX: is used to execute any node package without installing the package on our machine.\nNPM: is used to install any node js package in our machine. We can use \"require(\"package-name')\" when we install any package using NPM. but we can not import the package when we use NPX.\nExample: You should run npm i axios\nin this case you are installing axios package in your local machine\nand npx create-react-app 'app-name'\nhere you are executing the create-react-app package directly on your machine without installing it's files.\n\n\nA12:\n\nFor my case I was getting that error when I was using async function on my server-side to fetch documents using mongoose. It turned out that the reason was I forgot to put await before calling find({}) method. Adding that part fixed my issue. \n\n\nA13:\n\nYou will be able to get the current iteration's index for the map method through its 2nd parameter.\nExample:\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nlist.map((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return currElement; //equivalent to list[index]\n});\n\nOutput:\nThe current iteration is: 0 <br>The current element is: h\n\nThe current iteration is: 1 <br>The current element is: e\n\nThe current iteration is: 2 <br>The current element is: l\n\nThe current iteration is: 3 <br>The current element is: l \n\nThe current iteration is: 4 <br>The current element is: o\n\nSee also: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array/map\n\nParameters\ncallback - \n    Function that produces an element of the new Array, taking three arguments:\n1) currentValue\n        The current element being processed in the array.\n2) index\n      The index of the current element being processed in the array.\n3) array\n      The array map was called upon.\n\n\n\nA14:\n\nArray.prototype.map() index:\nOne can access the index Array.prototype.map() via the second argument of the callback function. Here is an example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst map = array.map((x, index) => {\n  console.log(index);\n  return x + index;\n});\n\nconsole.log(map);\n\n\n\nOther arguments of Array.prototype.map():\n\nThe third argument of the callback function exposes the array on which map was called upon\nThe second argument of Array.map() is a object which will be the this value for the callback function. Keep in mind that you have to use the regular function keyword in order to declare the callback since an arrow function doesn't have its own binding to the this keyword.\n\nFor example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst thisObj = { prop1: 1 }\n\nconst map = array.map((x, index, array) => {\n  console.log(array);\n  console.log(this)\n}, thisObj);\n\n\n\n\n\nA15:\n\nwith npx:\nnpx create-react-app MyApp\n\nearlier:\nnpm install -g create-react-app\ncreate-react-app MyApp\n\nEarlier it was necessary to install a CLI tool locally before using it, with npx that is not required anymore.\n\n\nA16:\n\nIf you have firebug, there's a break on all errors option in the script tab. Once the script has hit your breakpoint, you can look at firebug's stack window:\n\n\n\nA17:\n\nNPX is a tool for creating and executing some features in a new project\nNPM is the package manager that contains all of libraries\n\n\nA18:\n\nThe differences between NPM and NPX are as below:\ni) NPM is used to install the packages while NPX is used to execute the packages.\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages used by npx don't need to be taken care of as they are not installed globally.\n\n\nA19:\n\nNPM - NODE PACKAGE MANAGER\n\nit's a package manager.\n\nnode.js  package can be installed using NPM\n\n\nNPX - NODE PACKAGE EXECUTE\n\nit execute the  node.js  package\n\ndoesn't matter whether you installed  node.js  package locally or\nglobally.\n\nNPX will temporarily install package and run it.\n\nNPX is used when you want to execute package for single time.\n\n\n.\n\n\nA20:\n\nI don't think there's anything built in that you can use however I did find lots of examples of people rolling their own.\n\nDIY javascript stack trace\nA Javascript stacktrace in any browser\n\n\n\nA21:\n\nNPM stands for Node Package Manager.\nIt comes pre-installed with Node.js. NPM helps to manage packages in your projects as dependencies.\nWhen using NPM, there are two ways to install a package into your local computer.\n\nLocally: When a package is installed locally, it is installed in\n./node_modules/.bin/ of the local project directory.\n\nGlobally: A global package is installed in the user environment\npath. /usr/local/bin for Linux and AppData%/npm for Windows.\n\n\nTo execute a locally installed package, it should be specified in the package.json scripts block as shown below.\n\"scripts\": {\n    \"your-package\":  \"your-package-name\"\n}\n\nThen, you can execute the package with:\nnpm run your-package-name\n\nNPX is an NPM package executor.\nCurrently, NPX is bundled with NPM when you install the NPM version 5.2.0 or higher.\nWhy NPX over NPM?\n\nNo need to edit the package.json file with node_modules paths.\n\nYou can directly execute the tool from the command line.\n\n\n\n\nA22:\n\nHere's an example of what your app creation might look like using npx\n\nnpx create-react-app project-name --template all\n\n\n\nA23:\n\nAs per the JSON docs at Mozilla, JSON.stringify has a second parameter replacer which can be used to filter/ignore children items while parsing the tree. However, perhaps you can avoid the circular references.\nIn Node.js we cannot. So we can do something like this:\nfunction censor(censor) {\n  var i = 0;\n  \n  return function(key, value) {\n    if(i !== 0 && typeof(censor) === 'object' && typeof(value) == 'object' && censor == value) \n      return '[Circular]'; \n    \n    if(i >= 29) // seems to be a harded maximum of 30 serialized objects?\n      return '[Unknown]';\n    \n    ++i; // so we know we aren't using the original object anymore\n    \n    return value;  \n  }\n}\n\nvar b = {foo: {bar: null}};\n\nb.foo.bar = b;\n\nconsole.log(\"Censoring: \", b);\n\nconsole.log(\"Result: \", JSON.stringify(b, censor(b)));\n\nThe result:\nCensoring:  { foo: { bar: [Circular] } }\nResult: {\"foo\":{\"bar\":\"[Circular]\"}}\n\nUnfortunately there seems to be a maximum of 30 iterations before it automatically assumes it's circular. Otherwise, this should work. I even used areEquivalent from here, but JSON.stringify still throws the exception after 30 iterations. Still, it's good enough to get a decent representation of the object at a top level, if you really need it. Perhaps somebody can improve upon this though? In Node.js for an HTTP request object, I'm getting:\n{\n\"limit\": null,\n\"size\": 0,\n\"chunks\": [],\n\"writable\": true,\n\"readable\": false,\n\"_events\": {\n    \"pipe\": [null, null],\n    \"error\": [null]\n},\n\"before\": [null],\n\"after\": [],\n\"response\": {\n    \"output\": [],\n    \"outputEncodings\": [],\n    \"writable\": true,\n    \"_last\": false,\n    \"chunkedEncoding\": false,\n    \"shouldKeepAlive\": true,\n    \"useChunkedEncodingByDefault\": true,\n    \"_hasBody\": true,\n    \"_trailer\": \"\",\n    \"finished\": false,\n    \"socket\": {\n        \"_handle\": {\n            \"writeQueueSize\": 0,\n            \"socket\": \"[Unknown]\",\n            \"onread\": \"[Unknown]\"\n        },\n        \"_pendingWriteReqs\": \"[Unknown]\",\n        \"_flags\": \"[Unknown]\",\n        \"_connectQueueSize\": \"[Unknown]\",\n        \"destroyed\": \"[Unknown]\",\n        \"bytesRead\": \"[Unknown]\",\n        \"bytesWritten\": \"[Unknown]\",\n        \"allowHalfOpen\": \"[Unknown]\",\n        \"writable\": \"[Unknown]\",\n        \"readable\": \"[Unknown]\",\n        \"server\": \"[Unknown]\",\n        \"ondrain\": \"[Unknown]\",\n        \"_idleTimeout\": \"[Unknown]\",\n        \"_idleNext\": \"[Unknown]\",\n        \"_idlePrev\": \"[Unknown]\",\n        \"_idleStart\": \"[Unknown]\",\n        \"_events\": \"[Unknown]\",\n        \"ondata\": \"[Unknown]\",\n        \"onend\": \"[Unknown]\",\n        \"_httpMessage\": \"[Unknown]\"\n    },\n    \"connection\": \"[Unknown]\",\n    \"_events\": \"[Unknown]\",\n    \"_headers\": \"[Unknown]\",\n    \"_headerNames\": \"[Unknown]\",\n    \"_pipeCount\": \"[Unknown]\"\n},\n\"headers\": \"[Unknown]\",\n\"target\": \"[Unknown]\",\n\"_pipeCount\": \"[Unknown]\",\n\"method\": \"[Unknown]\",\n\"url\": \"[Unknown]\",\n\"query\": \"[Unknown]\",\n\"ended\": \"[Unknown]\"\n}\n\nI created a small Node.js module to do this here: https://github.com/ericmuyser/stringy Feel free to improve/contribute!\n\n\nA24:\n\nNPM stands for Node Package Manager. NPM is Node.JS's default package manager. It's written in Javascript. The role of NPM is to manage the package and modules of node.js.\nNPX stands for Node Package Execute. NPX comes with npm, when npm is installed above the 5.2.0 version, it gets installed automatically. NPX is an npm package runner and its role is to execute the package from the registry without even installing that package.\nNow, the differences between NPM and NPX are as below:\n\ni) NPM is used to install the packages while NPX is used to execute the packages.\n\n\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages which are used by npx don't need to be taken care of as they are not installed globally.\n\n\n\nA25:\n\nNPM => Is a JS package manager.\nNPX => Is a tool for executing Node packages and execute npm package binaries.\nIt is easy to remember:\n-npm stands for MANAGER\n-npx stands for EXECUTE\n\n\nA26:\n\n\nsuppose you have an array like\n\n\n\n   const arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    \n    \n    arr.map((myArr, index) => {\n      console.log(`your index is -> ${index} AND value is ${myArr}`);\n    })\n\n\n\n> output will be\n index is -> 0 AND value is 1\n index is -> 1 AND value is 2\n index is -> 2 AND value is 3\n index is -> 3 AND value is 4\n index is -> 4 AND value is 5\n index is -> 5 AND value is 6\n index is -> 6 AND value is 7\n index is -> 7 AND value is 8\n index is -> 8 AND value is 9\n\n\n\nA27:\n\nSimplest Definition:\nNPX\n\nThe npx stands for Node Package Execute and it comes with the npm,\nwhen you installed npm above 5.2.0 version then automatically npx will\ninstalled. It is an npm package runner that can execute any package\nthat you want from the npm registry without even installing that\npackage.\n\nNPM\n\nnpm is a package manager for the JavaScript programming language\nmaintained by npm, Inc. npm is the default package manager for the\nJavaScript runtime environment Node.js. It consists of a command line\nclient, also called npm, and an online database of public and paid-for\nprivate packages\n\n\n\nA28:\n\nIn my case I simply forgot to use async/await thing while building the route:\napp.get('/products', async (req, res) => {\n    const products = await Product.find();\n    res.send(products );\n});\n\n\n\nA29:\n\nnpm is package manager or installer on the other hand Packages used by npx are not installed globally so you have to carefree for the pollution for the long term.\n\n\nA30:\n\nBased on zainengineer's answer... Another approach is to make a deep copy of the object and strip circular references and stringify the result.\n\n\nfunction cleanStringify(object) {\r\n    if (object && typeof object === 'object') {\r\n        object = copyWithoutCircularReferences([object], object);\r\n    }\r\n    return JSON.stringify(object);\r\n\r\n    function copyWithoutCircularReferences(references, object) {\r\n        var cleanObject = {};\r\n        Object.keys(object).forEach(function(key) {\r\n            var value = object[key];\r\n            if (value && typeof value === 'object') {\r\n                if (references.indexOf(value) < 0) {\r\n                    references.push(value);\r\n                    cleanObject[key] = copyWithoutCircularReferences(references, value);\r\n                    references.pop();\r\n                } else {\r\n                    cleanObject[key] = '###_Circular_###';\r\n                }\r\n            } else if (typeof value !== 'function') {\r\n                cleanObject[key] = value;\r\n            }\r\n        });\r\n        return cleanObject;\r\n    }\r\n}\r\n\r\n// Example\r\n\r\nvar a = {\r\n    name: \"a\"\r\n};\r\n\r\nvar b = {\r\n    name: \"b\"\r\n};\r\n\r\nb.a = a;\r\na.b = b;\r\n\r\nconsole.log(cleanStringify(a));\r\nconsole.log(cleanStringify(b));\n\n\n\n\n\nA31:\n\nNPM vs. NPX\nNPM stands for the Node Package Manager. A text based program for Nodejs package management.\nWhile NPX is a Node Package Runner. Its function is to execute the Nodejs package\nNPX will execute binary files from the Nodejs package, both installed and not.\nEven NPX can also help us use certain versions of Nodejs without having to use nvm (node.js version management), nave (node.js virtual environment), and nvm (node.js version management).\n\n\nA32:\n\nUsing Ramda:\nimport {addIndex, map} from 'ramda';\n\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nconst mapIndexed = addIndex(map);\nmapIndexed((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return 'X';\n}, list);\n\n\n\nA33:\n\nnpx is a npm package runner (x probably stands for eXecute). One common way to use npx is to download and run a package temporarily or for trials.\ncreate-react-app is an npm package that is expected to be run only once in a project's lifecycle. Hence, it is preferred to use npx to install and run it in a single step.\nAs mentioned in the main page https://www.npmjs.com/package/npx, npx can run commands in the PATH or from node_modules/.bin by default.\nNote:\nWith some digging, we can find that create-react-app points to a Javascript file (possibly to /usr/lib/node_modules/create-react-app/index.js on Linux systems) that is executed within the node environment. This is simply a global tool that does some checks. The actual setup is done by react-scripts, whose latest version is installed in the project. Refer https://github.com/facebook/create-react-app for more info.\n\n\nA34:\n\nnpm - Package manager for JavaScript, just like: pip (Python), Maven (Java), NuGet (.NET), Composer (PHP), RubyGems (Ruby), ...\nnpx - runs a command of a package without installing it explicitly.\nUse cases:\n\nYou don't want to install packages neither globally nor locally.\nYou don't have permission to install it globally.\nJust want to test some commands.\nSometime, you want to have a script command (generate, convert something, ...) in package.json to execute something without installing these packages as project's dependencies.\n\nSyntax:\nnpx [options] [-p|--package <package>] <command> [command-arg]...\n\nPackage is optional:\nnpx   -p uglify-js         uglifyjs --output app.min.js app.js common.js\n      +----------------+   +--------------------------------------------+\n      package (optional)   command, followed by arguments\n\nFor example:\nStart a HTTP Server      : npx http-server\nLint code                : npx eslint ./src\n                         # Run uglifyjs command in the package uglify-js\nMinify JS                : npx -p uglify-js uglifyjs -o app.min.js app.js common.js\nMinify CSS               : npx clean-css-cli -o style.min.css css/bootstrap.css style.css\nMinify HTML              : npx html-minifier index-2.html -o index.html --remove-comments --collapse-whitespace\nScan for open ports      : npx evilscan 192.168.1.10 --port=10-9999\nCast video to Chromecast : npx castnow http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\n\nMore about command:\n\nhttps://docs.npmjs.com/files/package.json#bin\nhttps://github.com/mishoo/UglifyJS2/blob/master/package.json#L17\n\n\n\nA35:\n\nSimple Definition:\nnpm - Javascript package manager \nnpx - Execute npm package binaries\n\n\nA36:\n\nIt is easier to get a stack trace on Firefox than it is on IE but fundamentally here is what you want to do:\nWrap the \"problematic\" piece of code in a try/catch block:\ntry {\n    // some code that doesn't work\n    var t = null;\n    var n = t.not_a_value;\n}\n    catch(e) {\n}\n\nIf you will examine the contents of the \"error\" object it contains the following fields:\ne.fileName : The source file / page where the issue came from\ne.lineNumber : The line number in the file/page where the issue arose \ne.message : A simple message describing what type of error took place\ne.name : The type of error that took place, in the example above it should be 'TypeError'\ne.stack : Contains the stack trace that caused the exception\nI hope this helps you out.\n\n\nA37:\n\nEdit 2 (2017):\nIn all modern browsers you can simply call: console.trace(); (MDN Reference)\nEdit 1 (2013):\nA better (and simpler) solution as pointed out in the comments on the original question is to use the stack property of an Error object like so:\nfunction stackTrace() {\n    var err = new Error();\n    return err.stack;\n}\n\nThis will generate output like this:\nDBX.Utils.stackTrace@http://localhost:49573/assets/js/scripts.js:44\nDBX.Console.Debug@http://localhost:49573/assets/js/scripts.js:9\n.success@http://localhost:49573/:462\nx.Callbacks/c@http://localhost:49573/assets/js/jquery-1.10.2.min.js:4\nx.Callbacks/p.fireWith@http://localhost:49573/assets/js/jquery-1.10.2.min.js:4\nk@http://localhost:49573/assets/js/jquery-1.10.2.min.js:6\n.send/r@http://localhost:49573/assets/js/jquery-1.10.2.min.js:6\n\nGiving the name of the calling function along with the URL, its calling function, and so on.\nOriginal (2009):\nA modified version of this snippet may somewhat help:\nfunction stacktrace() { \n  function st2(f) {\n    return !f ? [] : \n        st2(f.caller).concat([f.toString().split('(')[0].substring(9) + '(' + f.arguments.join(',') + ')']);\n  }\n  return st2(arguments.callee.caller);\n}\n\n\n\nA38:\n\nI resolve this problem on NodeJS like this:\nvar util = require('util');\n\n// Our circular object\nvar obj = {foo: {bar: null}, a:{a:{a:{a:{a:{a:{a:{hi: 'Yo!'}}}}}}}};\nobj.foo.bar = obj;\n\n// Generate almost valid JS object definition code (typeof string)\nvar str = util.inspect(b, {depth: null});\n\n// Fix code to the valid state (in this example it is not required, but my object was huge and complex, and I needed this for my case)\nstr = str\n    .replace(/<Buffer[ \\w\\.]+>/ig, '\"buffer\"')\n    .replace(/\\[Function]/ig, 'function(){}')\n    .replace(/\\[Circular]/ig, '\"Circular\"')\n    .replace(/\\{ \\[Function: ([\\w]+)]/ig, '{ $1: function $1 () {},')\n    .replace(/\\[Function: ([\\w]+)]/ig, 'function $1(){}')\n    .replace(/(\\w+): ([\\w :]+GMT\\+[\\w \\(\\)]+),/ig, '$1: new Date(\"$2\"),')\n    .replace(/(\\S+): ,/ig, '$1: null,');\n\n// Create function to eval stringifyed code\nvar foo = new Function('return ' + str + ';');\n\n// And have fun\nconsole.log(JSON.stringify(foo(), null, 4));\n\n\n\nA39:\n\nnpm - package manager\nnpx - Execute npm package\nThis is a difference with it.\n\n\nA40:\n\nIntroducing npx: an npm package runner\nNPM - Manages packages but doesn't make life easy executing any.NPX - A tool for executing Node packages.\n\nNPX comes bundled with NPM version 5.2+ \n\nNPM by itself does not simply run any package. It doesn't run any package as a matter of fact. If you want to run a package using NPM, you must specify that package in your package.json file.\nWhen executables are installed via NPM packages, NPM links to them:\n\nlocal installs have \"links\" created at ./node_modules/.bin/ directory.\nglobal installs have \"links\" created from the global bin/ directory (e.g. /usr/local/bin) on Linux or at %AppData%/npm on Windows.\n\nDocumentation you should read\n\nNPM:\nOne might install a package locally on a certain project:\nnpm install some-package\n\nNow let's say you want NodeJS to execute that package from the command line:\n$ some-package\n\nThe above will fail. Only globally installed packages can be executed by typing their name only.\nTo fix this, and have it run, you must type the local path:\n$ ./node_modules/.bin/some-package\n\nYou can technically run a locally installed package by editing your packages.json file and adding that package in the scripts section:\n{\n  \"name\": \"whatever\",\n  \"version\": \"1.0.0\",\n  \"scripts\": {\n    \"some-package\": \"some-package\"\n  }\n}\n\nThen run the script using npm run-script (or npm run):\nnpm run some-package\n\n\nNPX:\nnpx will check whether <command> exists in $PATH, or in the local project binaries, and execute it. So, for the above example, if you wish to execute the locally-installed package some-package all you need to do is type:\nnpx some-package\n\nAnother major advantage of npx is the ability to execute a package which wasn't previously installed:\n$ npx create-react-app my-app\n\nThe above example will generate a react app boilerplate within the path the command had run in, and ensures that you always use the latest version of a generator or build tool without having to upgrade each time you\u2019re about to use it.\n\nUse-Case Example:\nnpx command may be helpful in the script section of a package.json file,\nwhen it is unwanted to define a dependency which might not be commonly used or any other reason:\n\"scripts\": {\n    \"start\": \"npx gulp@3.9.1\",\n    \"serve\": \"npx http-server\"\n}\n\nCall with: npm run serve\n\nRelated questions:\n\nHow to use package installed locally in node_modules?\nNPM: how to source ./node_modules/.bin folder?\nHow do you run a js file using npm scripts?\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n(vs.  (v. 2.0.0.0)"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 23, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI was working with a new C++ developer a while back when he asked the question: \"Why can't variable names start with numbers?\"\nI couldn't come up with an answer except that some numbers can have text in them (123456L, 123456U) and that wouldn't be possible if the compilers were thinking everything with some amount of alpha characters was a variable name.\nWas that the right answer?  Are there any more reasons?\nstring 2BeOrNot2Be = \"that is the question\"; // Why won't this compile?\n\n\n\nCandidate answers:\nA1:\n\nLet extend @Deduplicator example:\n#include <iostream>\n\nstruct A {\n    int i;\n};\n\nstruct B : public A {\n    int i;\n    A A;\n};\n\nint main() {\n    B x {1, 2};\n    std::cout << x.i << '\\n';\n    std::cout << x.B::i << '\\n';  // The same as the line above.\n    std::cout << x.A.i << '\\n';\n    std::cout << x.A::i << '\\n';  // Not the same as the line above.\n}\n\nLive on Coliru Viewer\nNot having a possibility to differentiate with help of ::, which member we want to access, it is impossible to access members declared in a parent class with identical names.\n\n\nA2:\n\nThere are two ways to use void:\nvoid foo(void);\n\nor\nvoid *bar(void*);\n\nThe first indicates that no argument is being passed or that no argument is being returned.\nThe second tells the compiler that there is no type associated with the data effectively meaning that the you can't make use of the data pointed to until it is cast to a known type.\nFor example you will see void* used a lot when you have an interface which calls a function whose parameters can't be known ahead of time.\nFor example, in the Linux kernel, when deferring work you will set up a function to be run at a latter time by giving it a pointer to the function to be run and a pointer to the data to be passed to the function:\nstruct _deferred_work {\nsruct list_head mylist;\n.worker_func = bar;\n.data        = somedata;\n} deferred_work;\n\nThen a kernel thread goes over a list of deferred work and when it gets to this node it effectively executes:\nbar(somedata);\n\nThen in bar you have:\nvoid bar(void* mydata) {\n    int *data = mydata;\n    /* Do something with data */;\n}\n\n\n\nA3:\n\nIt indicates the absence of a return value in a function.\nSome languages have two sorts of subroutines: procedures and functions. Procedures are just a sequence of operations, whereas a function is a sequence of operations that return a result.\nIn C and its derivatives, the difference between the two is not explicit. Everything is basically a function. The void keyword indicates that it's not an \"actual\" function, since it doesn't return a value.\n\n\nA4:\n\nAn example where both are valid, but refer to different objects:\n#include <iostream>\n\nstruct A {\n    int i;\n};\n\nstruct B {\n    int i;\n    A B;\n};\n\nint main() {\n    B x {0, 1};\n    std::cout << x.B.i << '\\n';\n    std::cout << x.B::i << '\\n';\n}\n\nSee live on coliru.\n\n\nA5:\n\nC++ references can sometimes be forced to be 0 with some compilers (it's just a bad idea to do so*, and it violates the standard*).\nint &x = *((int*)0); // Illegal but some compilers accept it\n\nEDIT: according to various people who know the standard much better than myself, the above code produces \"undefined behavior\".  In at least some versions of GCC and Visual Studio, I've seen this do the expected thing: the equivalent of setting a pointer to NULL (and causes a NULL pointer exception when accessed).\n\n\nA6:\n\nBasically it means \"nothing\" or \"no type\"\nThere are 3 basic ways that void is used:\n\nFunction argument: int myFunc(void)\n-- the function takes nothing. \nFunction return value: void myFunc(int)\n-- the function returns nothing\nGeneric data pointer: void* data\n-- 'data' is a pointer to data of unknown type, and cannot be dereferenced\n\nNote: the void in a function argument is optional in C++, so int myFunc() is exactly the same as int myFunc(void), and it is left out completely in C#. It is always required for a return value.\n\n\nA7:\n\nThree usage cases for void:\n\nFunction signatures. void foo(int bar) does not return a value. int bar(void) does not take any parameters but this is usually expressed with empty argument list: int bar(). Usage of the void keyword here corresponds to its meaning in English.\nGeneric top-type pointer void * that points to unspecified data and cannot be dereferenced. Here the meaning of void is different from other meanings of void: universal type vs. no type.\nIn casts such as (void) new Foo(this) to signify that the return value is deliberately thrown away. Here the keyword usage also matches its meaning in English.\n\nCases 1 and 2 were already covered by @Gerald but case 3 has not been addressed yet.\n\n\nA8:\n\nEdit: New article by Dave Abrahams on cpp-next: Want speed? Pass by value.\n\nPass by value for structs where the copying is cheap has the additional advantage that the compiler may assume that the objects don't alias (are not the same objects). Using pass-by-reference the compiler cannot assume that always. Simple example:\nfoo * f;\n\nvoid bar(foo g) {\n    g.i = 10;\n    f->i = 2;\n    g.i += 5;\n}\n\nthe compiler can optimize it into\ng.i = 15;\nf->i = 2;\n\nsince it knows that f and g doesn't share the same location. if g was a reference (foo &), the compiler couldn't have assumed that. since g.i could then be aliased by f->i and have to have a value of 7. so the compiler would have to re-fetch the new value of g.i from memory.\nFor more pratical rules, here is a good set of rules found in Move Constructors article (highly recommended reading).\n\nIf the function intends to change the argument as a side effect, take it by non-const reference. \nIf the function doesn't modify its argument and the argument is of primitive type, take it by value.\nOtherwise take it by const reference, except in the following cases\n\n\nIf the function would then need to make a copy of the const reference anyway, take it by value.\n\n\n\"Primitive\" above means basically small data types that are a few bytes long and aren't polymorphic (iterators, function objects, etc...) or expensive to copy. In that paper, there is one other rule. The idea is that sometimes one wants to make a copy (in case the argument can't be modified), and sometimes one doesn't want (in case one wants to use the argument itself in the function if the argument was a temporary anyway, for example). The paper explains in detail how that can be done. In C++1x that technique can be used natively with language support. Until then, i would go with the above rules. \nExamples: To make a string uppercase and return the uppercase version, one should always pass by value: One has to take a copy of it anyway (one couldn't change the const reference directly) - so better make it as transparent as possible to the caller and make that copy early so that the caller can optimize as much as possible - as detailed in that paper:\nmy::string uppercase(my::string s) { /* change s and return it */ }\n\nHowever, if you don't need to change the parameter anyway, take it by reference to const:\nbool all_uppercase(my::string const& s) { \n    /* check to see whether any character is uppercase */\n}\n\nHowever, if you the purpose of the parameter is to write something into the argument, then pass it by non-const reference\nbool try_parse(T text, my::string &out) {\n    /* try to parse, write result into out */\n}\n\n\n\nA9:\n\nA reseatable reference would be functionally identical to a pointer.\nConcerning nullability: you cannot guarantee that such a \"reseatable reference\" is non-NULL at compile time, so any such test would have to take place at runtime.  You could achieve this yourself by writing a smart pointer-style class template that throws an exception when initialised or assigned NULL:\nstruct null_pointer_exception { ... };\n\ntemplate<typename T>\nstruct non_null_pointer {\n    // No default ctor as it could only sensibly produce a NULL pointer\n    non_null_pointer(T* p) : _p(p) { die_if_null(); }\n    non_null_pointer(non_null_pointer const& nnp) : _p(nnp._p) {}\n    non_null_pointer& operator=(T* p) { _p = p; die_if_null(); }\n    non_null_pointer& operator=(non_null_pointer const& nnp) { _p = nnp._p; }\n\n    T& operator*() { return *_p; }\n    T const& operator*() const { return *_p; }\n    T* operator->() { return _p; }\n\n    // Allow implicit conversion to T* for convenience\n    operator T*() const { return _p; }\n\n    // You also need to implement operators for +, -, +=, -=, ++, --\n\nprivate:\n    T* _p;\n    void die_if_null() const {\n        if (!_p) { throw null_pointer_exception(); }\n    }\n};\n\nThis might be useful on occasion -- a function taking a non_null_pointer<int> parameter certainly communicates more information to the caller than does a function taking int*.\n\n\nA10:\n\nVoid is used only in method signatures. For return types it means the method will not return anything to the calling code. For parameters it means no parameters are passed to the method.\nE.g.,\nvoid MethodThatReturnsAndTakesVoid(void)\n{\n    // Method body\n}\n\nIn C# we can omit the void for parameters and can write the above code as:\nvoid MethodThatReturnsAndTakesVoid()\n{\n    // Method body\n}\n\nVoid should not be confused with null. Null means for the variable whose address is on stack, and the value on the heap for that address is empty.\n\n\nA11:\n\nDepends on the type.  You are adding the small overhead of having to make a reference and dereference.  For types with a size equal or smaller than pointers that are using the default copy ctor, it would probably be faster to pass by value.\n\n\nA12:\n\nI would imagine that it is related to optimization. \nStatic optimization is much easier when you can know unambiguously what bit of memory  a variable means. Pointers break this condition and re-setable reference would too.\n\n\nA13:\n\nYou can't do this:\nint theInt = 0;\nint& refToTheInt = theInt;\n\nint otherInt = 42;\nrefToTheInt = otherInt;\n\n...for the same reason why secondInt and firstInt don't have the same value here:\nint firstInt = 1;\nint secondInt = 2;\nsecondInt = firstInt;\nfirstInt = 3;\n\nassert( firstInt != secondInt );\n\n\n\nA14:\n\nAs it has been pointed out, it depends on the type.  For built-in data types, it is best to pass by value.  Even some very small structures, such as a pair of ints can perform better by passing by value.\nHere is an example, assume you have an integer value and you want pass it to another routine.  If that value has been optimized to be stored in a register, then if you want to pass it be reference, it first must be stored in memory and then a pointer to that memory placed on the stack to perform the call.  If it was being passed by value, all that is required is the register pushed onto the stack.  (The details are a bit more complicated than that given different calling systems and CPUs).\nIf you are doing template programming, you are usually forced to always pass by const ref since you don't know the types being passed in.  Passing penalties for passing something bad by value are much worse than the penalties of passing a built-in type by const ref.\n\n\nA15:\n\nThis is not actually an answer, but a workaround for this limitation.\nBasically, when you try to \"rebind\" a reference you are actually trying to use the same name to refer to a new value in the following context. In C++, this can be achieve by introducing a block scope.\nIn jalf's example\nint i = 42;\nint k = 43;\nint& j = i;\n//change i, or change j?\nj = k;\n\nif you want to change i, write it as above. However, if you want to change the meaning of j to mean k, you can do this:\nint i = 42;\nint k = 43;\nint& j = i;\n//change i, or change j?\n//change j!\n{\n    int& j = k;\n    //do what ever with j's new meaning\n}\n\n\n\nA16:\n\nBecause then a string of digits would be a valid identifier as well as a valid number.\nint 17 = 497;\nint 42 = 6 * 9;\nString 1111 = \"Totally text\";\n\n\n\nA17:\n\nThe reason that C++ does not allow you to rebind references is given in Stroustrup's \"Design and Evolution of C++\" :\n\nIt is not possible to change what a reference refers to after initialization. That is, once a C++ reference is initialized it cannot be made to refer to a different object later; it cannot be re-bound. I had in the past been bitten by Algol68 references where r1=r2 can either assign through r1 to the object referred to or assign a new reference value to r1 (re-binding r1) depending on the type of r2. I wanted to avoid such problems in C++.\n\n\n\nA18:\n\nIt would probably have been less confusing to name C++ references \"aliases\"? As others have mentioned, references in C++ should be though of as the variable they refer to, not as a pointer/reference to the variable. As such, I can't think of a good reason they should be resettable.\nwhen dealing with pointers, it often makes sense allowing null as a value (and otherwise, you probably want a reference instead). If you specifically want to disallow holding null, you could always code your own smart pointer type ;)\n\n\nA19:\n\nThis is what i normally work by when designing the interface of a non-template function:\n\nPass by value if the function does not want to modify the parameter and the\nvalue is cheap to copy (int, double, float, char, bool, etc... Notice that std::string, std::vector, and the rest of the containers in the standard library are NOT)\nPass by const pointer if the value is expensive to copy and the function does\nnot want to modify the value pointed to and NULL is a value that the function handles.\nPass by non-const pointer if the value is expensive to copy and the function\nwants to modify the value pointed to and NULL is a value that the function handles.\nPass by const reference when the value is expensive to copy and the function does not want to modify the value referred to and NULL would not be a valid value if a pointer was used instead.\nPass by non-const reference when the value is expensive to copy and the function wants to modify the value referred to and NULL would not be a valid value if a pointer was used instead.\n\n\n\nA20:\n\nIf you're explaining the concept to a beginner, it might be helpful to use an analogy. The use of void in all these cases is analogous in meaning to a page in a book which has the following words, \"This page left intentionally blank.\" It is to differentiate to the compiler between something which should be flagged as an error, versus a type which is intentionally to be left blank because that is the behavior you want.\nIt always appears in code where normally you would expect to see a type appear, such as a return type or a pointer type. This is why in C#, void maps to an actual CLR type, System.Void because it is a type in itself.\nSome programming languages never developed the concept of void, just like some human cultures never invented the concept of the number zero. Void represents the same advancement in a programming language as the concept of zero represents to human language.\n\n\nA21:\n\nThere is difference between a::b and a.b where :: implies that a used as namespace, which means that it is namespace or typename. Provided that C++ supports non-virtual plural inheritance and that a variable can have same name as a type, this strips chances of referencing wrong object. It's necessary for template metaprogramming.\nAnother example would be &B::foo vs &B.foo in context of class B. \n\n\nA22:\n\nA reference is not a pointer, it may be implemented as a pointer in the background, but its core concept is not equivalent to a pointer.  A reference should be looked at like it *is* the object it is referring to.   Therefore you cannot change it, and it cannot be NULL.  \nA pointer is simply a variable that holds a memory address.  The pointer itself has a memory address of its own, and inside that memory address it holds another memory address that it is said to point to.  A reference is not the same, it does not have an address of its own, and hence it cannot be changed to \"hold\" another address.\nI think the parashift C++ FAQ on references says it best:\n\nImportant note: Even though a\n  reference is often implemented using\n  an address in the underlying assembly\n  language, please do not think of a\n  reference as a funny looking pointer\n  to an object. A reference is the\n  object. It is not a pointer to the\n  object, nor a copy of the object. It\n  is the object.\n\nand again in FAQ 8.5 :\n\nUnlike a pointer, once a reference is\n  bound to an object, it can not be\n  \"reseated\" to another object. The\n  reference itself isn't an object (it\n  has no identity; taking the address of\n  a reference gives you the address of\n  the referent; remember: the reference\n  is its referent).\n\n\n\nA23:\n\nVoid is the equivalent of Visual Basic's Sub.\n\n\nA24:\n\nDue to attempts to make C++ mostly compatible with the existing C code (which allows name collisions between object names and struct tags), C++ allows name collisions between class names and object names.\nWhich means that:\nstruct data {\n    static int member;\n};\n\nstruct data2 {\n    int member;\n};\n\nvoid f(data2& data) {\n    data.member = data::member;\n}\n\nis legit code.\n\n\nA25:\n\nWell think about this:\nint 2d = 42;\ndouble a = 2d;\n\nWhat is a?  2.0?  or 42?\nHint, if you don't get it, d after a number means the number before it is a double literal\n\n\nA26:\n\nVoid is an incomplete type which, by definition, can't be an lvalue. That means it can't get assigned a value.\nSo it also can't hold any value.\n\n\nA27:\n\nIt means \"no value\". You use void to indicate that a function doesn't return a value or that it has no parameters or both. Pretty much consistent with typical uses of word void in English.\n\n\nA28:\n\nAs a rule of thumb, value for non-class types and const reference for classes.\nIf a class is really small it's probably better to pass by value, but the difference is minimal. What you really want to avoid is passing some gigantic class by value and having it all duplicated - this will make a huge difference if you're passing, say, a std::vector with quite a few elements in it.\n\n\nA29:\n\nIntrestingly, many answers here are a bit fuzzy or even beside the point (e.g. it's not because references cannot be zero or similar, in fact, you can easily construct an example where a reference is zero).\nThe real reason why re-setting a reference is not possible is rather simple.\n\nPointers enable you to do two things: To change the value behind the pointer (either through the -> or the * operator), and to change the pointer itself (direct assign =). Example:\nint a;\nint * p = &a\n\nChanging the value requires dereferencing: *p = 42;\nChanging the pointer: p = 0;\n\nReferences allow you to only change the value. Why? Since there is no other syntax to express the re-set. Example:\nint a = 10;\nint b = 20;\nint & r = a;\nr = b; // re-set r to b, or set a to 20?\n\nIn other words, it would be ambiguous if you were allowed to re-set a reference. It makes even more sense when passing by reference:\nvoid foo(int & r)\n{\n    int b = 20;\n    r = b; // re-set r to a? or set a to 20?\n}\nvoid main()\n{\n    int a = 10;\n    foo(a);\n}\n\nHope that helps :-)\n\n\nA30:\n\nI was taught \"void\" means \"nothing\" in college - it's a faulty mental model and an anti-learning pattern.\nIn C/C++ void means \"untyped memory\".  void does not mean \"nothing\".  An undefined thing is different than no thing.\nFor example: MLT video framework returns a void * for newly allocated memory.\nIf a C/C++ program leaks void * memory, it's definitely leaking something.\nvoid *mlt_pool_alloc( int size );\nvoid *mlt_pool_realloc( void *ptr, int size );\nvoid mlt_pool_release( void *release );\n\nThe first function returns a void * type to a newly allocated \"raw\" memory.  Physical memory is just an array of 0/1's.  Raw physical memory is also practically meaningless for the compiler and programmer.\nThe programmer creates meaning when he casts void * to stuff * or a stuff array.\nNote that we can cast any * to a void *, and void * to any *.\nThis let's the programmer write any * code, but manage any's memory with void * code.  That's the essential benefit to the design.\n// Resize allocated memory\nvoid *mlt_pool_realloc( void *ptr, int size );\n\nThis function returns and accepts void * pointers.  This function can be used to expand the size of a previously allocated array.\nReallocating \"nothing\" to more \"nothing\" is confusing in a void=nothing mental model.\nvoid mlt_pool_release( void *release );\n\nThe final function accepts a void * and returns void.  The returned void can't be assigned, which leads to the assumption that void means nothing.\nReturning void is a language convention and an \"overloaded meaning\" of void.\nA void * is a pointer to raw untyped memory.\n\n\nA31:\n\nBecause then you'd have no reseatable type which can not be 0. Unless, you included 3 types of references/pointers. Which would just complicate the language for very little gain (And then why not add the 4th type too? Non-reseatable reference which can be 0?)\nA better question may be, why would you want references to be reseatable? If they were, that would make them less useful in a lot of situations. It would make it harder for the compiler to do alias analysis.\nIt seems that the main reason references in Java or C# are reseatable is because they do the work of pointers. They point to objects. They are not aliases for an object.\nWhat should the effect of the following be?\nint i = 42;\nint& j = i;\nj = 43;\n\nIn C++ today, with non-reseatable references, it is simple. j is an alias for i, and i ends up with the value 43.\nIf references had been reseatable, then the third line would bind the reference j to a different value. It would no longer alias i, but instead the integer literal 43 (which isn't valid, of course). Or perhaps a simpler (or at least syntactically valid) example:\nint i = 42;\nint k = 43;\nint& j = i;\nj = k;\n\nWith reseatable references. j would point to k after evaluating this code.\nWith C++'s non-reseatable references, j still points to i, and i is assigned the value 43.\nMaking references reseatable changes the semantics of the language. The reference can no longer be an alias for another variable. Instead it becomes a separate type of value, with its own assignment operator. And then one of the most common usages of references would be impossible. And nothing would be gained in exchange. The newly gained functionality for references already existed in the form of pointers. So now we'd have two ways to do the same thing, and no way to do what references in the current C++ language do.\n\n\nA32:\n\nvoid means that you won't be returning any value from the function or method.\n\n\nA33:\n\nAs a rule passing by const reference is better.\nBut if you need to modify you function argument locally you should better use passing by value.\nFor some basic types the performance in general the same both for passing by value and by reference. Actually reference internally represented by pointer, that is why you can expect for instance that for pointer both passing are the same in terms of performance, or even passing by value can be faster because of needless dereference.\n\n\nA34:\n\nIn C#, you'd use the void keyword to indicate that a method does not return a value:\npublic void DoSomeWork()\n{\n    // Some work\n}\n\n\n\nA35:\n\nI have always taken it to mean absent. Here are four cases in the C language that matches to this use of absent\n\nR f(void) - Function parameters are absent\nvoid f(P) - Return value is absent\nvoid *p - Type of what is pointed to is absent\n(void) p - Usage of value is absent\n\nOther C descendants use it for other things. The D programming language uses it for cases where an initializer is absent\n\nT t = void; - initializing value is absent\n\n\n\nA36:\n\nSounds like you got your answer.  Passing by value is expensive, but gives you a copy to work with if you need it.\n\n\nA37:\n\nThink of void as the \"empty structure\".  Let me explain.\nEvery function takes a sequence of parameters, where each parameter has a type.   In fact, we could package up the parameters into a structure, with the structure slots corresponding to the parameters.  This makes every function have exactly one argument.   Similarly, functions produce a result, which has a type.  It could be a boolean, or it could be float, or it could be a structure, containing an arbitrary set of other typed values.  If we want a languge that has multiple return values, it is easy to just insist they be packaged into a structure.  In fact, we could always insist that a function returned a structure.  Now every function takes exactly one argument, and produces exactly one value.\nNow, what happens when I need a function that produces \"no\" value?\nWell, consider what I get when I form a struct with 3 slots: it\nholds 3 values.  When I have 2 slots, it holds two values.  When it\nhas one slot, one value.  And when it has zero slots, it holds... uh,\nzero values, or \"no\" value\".  So, I can think of a function returning void\nas returning a struct containing no values.  You can even decide that \"void\"\nis just a synonym for the type represented by the empty structure,\nrather than a keyword in the language (maybe its just a predefined type :)\nSimilarly, I can think of a function requiring no values as accepting an empty structure, e.g., \"void\".\nI can even implement my programming language this way.  Passing a void value\ntakes up zero bytes, so passing void values is just a special case of passing\nother values of arbitrary size.  This makes it easy for the compiler to treat\nthe \"void\" result or argument.   You probably want a langauge feature\nthat can throw a function result away; in C, if you call the non-void result\nfunction foo in the following statement:\n      foo(...);\nthe compiler knows that foo produces a result and simply ignores it.\nIf void is a value, this works perfectly and now \"procedures\" (which are\njust an adjective for a function with void result) are just trivial special\ncases of general functions.\nVoid* is a bit funnier.   I don't think the C designers thought of void in the\nabove way; they just created a keyword.   That keyword was available when somebody\nneeded a point to an arbitrary type, thus void* as the idiom in C.\nIt actually works pretty well if you interpret void as an empty structure.\nA void* pointer is the address of a place where that empty structure has\nbeen put.  \nCasts from void* to T* for other types T, also work out with this perspective.\nPointer casts are a complete cheat that work on most common architectures to take advantage of the fact that if a compound type T has an element with subtype S placed physically at the beginning of T in its storage layout, then casting S* to T* and vice versa using the same physical machine address tends to work out, since most machine pointers have a single representation.   Replacing the type S by the type void gives exactly the same effect, and thus casting to/from void* works out.\nThe PARLANSE programming language implements the above ideas pretty closely.\nWe goofed in its design, and didn't pay close attention to \"void\" as a return\ntype and thus have langauge keywords for procedure.   Its mostly just a simple\nsyntax change but its one of things you don't get around to once you get\na large body working code in a language.   \n\n\nA38:\n\nIn C++, it is often said that \"the reference is the object\". In one sense, it is true: though references are handled as pointers when the source code is compiled, the reference is intended to signify an object that is not copied when a function is called. Since references are not directly addressable (for example, references have no address, & returns the address of the object), it would not semantically make sense to reassign them. Moreover, C++ already has pointers, which handles the semantics of re-setting.\n\n\nA39:\n\nVoid means no value is required in the return type from a function in all of the three languages.\n\n\nA40:\n\nIt used to be generally recommended best practice1 to use pass by const ref for all types, except for builtin types (char, int, double, etc.), for iterators and for function objects (lambdas, classes deriving from std::*_function).\nThis was especially true before the existence of move semantics. The reason is simple: if you passed by value, a copy of the object had to be made and, except for very small objects, this is always more expensive than passing a reference.\nWith C++11, we have gained move semantics. In a nutshell, move semantics permit that, in some cases, an object can be passed \u201cby value\u201d without copying it. In particular, this is the case when the object that you are passing is an rvalue.\nIn itself, moving an object is still at least as expensive as passing by reference. However, in many cases a function will internally copy an object anyway \u2014 i.e. it will take ownership of the argument.2\nIn these situations we have the following (simplified) trade-off:\n\nWe can pass the object by reference, then copy internally.\nWe can pass the object by value.\n\n\u201cPass by value\u201d still causes the object to be copied, unless the object is an rvalue. In the case of an rvalue, the object can be moved instead, so that the second case is suddenly no longer \u201ccopy, then move\u201d but \u201cmove, then (potentially) move again\u201d.\nFor large objects that implement proper move constructors (such as vectors, strings \u2026), the second case is then vastly more efficient than the first. Therefore, it is recommended to use pass by value if the function takes ownership of the argument, and if the object type supports efficient moving.\n\nA historical note:\nIn fact, any modern compiler should be able to figure out when passing by value is expensive, and implicitly convert the call to use a const ref if possible.\nIn theory. In practice, compilers can\u2019t always change this without breaking the function\u2019s binary interface. In some special cases (when the function is inlined) the copy will actually be elided if the compiler can figure out that the original object won\u2019t be changed through the actions in the function.\nBut in general the compiler can\u2019t determine this, and the advent of move semantics in C++ has made this optimisation much less relevant.\n\n1 E.g. in Scott Meyers, Effective C++.\n2 This is especially often true for object constructors, which may take arguments and store them internally to be part of the constructed object\u2019s state.\n\n\nA41:\n\nPass by value for small types.\nPass by const references for big types (the definition of big can vary between machines) BUT, in C++11, pass by value if you are going to consume the data, since you can exploit move semantics. For example:\nclass Person {\n public:\n  Person(std::string name) : name_(std::move(name)) {}\n private:\n  std::string name_;\n};\n\nNow the calling code would do:\nPerson p(std::string(\"Albert\"));\n\nAnd only one object would be created and moved directly into member name_ in class Person. If you pass by const reference, a copy will have to be made for putting it into name_.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 24, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIs there a Windows command that will output the size in bytes of a specified file like this?\n> filesize test.jpg\n65212\n\nI know that the dir command outputs this information, but it outputs other information also.\nI could easily write such a program, but I would prefer to use a native Windows command if possible, or only what is available in a fresh install of Windows\u00a0XP.\n\n\nCandidate answers:\nA1:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA2:\n\nBased on techtonik's explanation of the reason for the failing import of dulwich.errors, I found a solution which appears simpler to me than the already proposed ones:\nOn the Mercurial download page there is offered\n\nMercurial <x.y.z> for Python 2.7 on Windows x86 (source install)\n\nand\n\nMercurial <x.y.z> for Python 2.7 on Windows x64 (source install)\n\nas .EXE files which install Mercurial as a Python module into an existing Python 2.7 (x86 or x64) installation.\nIf hg-git and dulwich have been installed to this Python installation as well, the import errors should vanish.\nIf you are not sure what to do now, I hope this step-by-step explanation helps:\nPrerequisites\n\nPython 2.7 is installed\n<python 2.7 install dir> and <python 2.7 install dir>\\Scripts are in the PATH environment variable \nhg-git (and dulwich) have been installed into this Python installation via the instructions from here\n\nSteps\n\nDeinstall any existing Mercurial installation\nDownload and install one of the above-quoted EXE files, depending on whether your Python 2.7 installation is 32bit or 64bit (If you get a \"Python 2.7 cannot be found in registry\" error, you probably need the other one.)\n\nNow hg clone <some git repo> should work on the console.\n\n\nA3:\n\n\nThat makes me think dulwich is not\n  installed correctly, or not in the\n  path.\n\nYou're absolutely right. Mercurial binary distributions for Windows are 'frozen' - they use the Python code and interpreter bundled with them and therefore independent of packages installed in system PYTHONPATH. When you specify path to hggit extension in Mercurial.ini, hg tries to import it using direct path, but dulwich library is not imported explicitly by hg and doesn't bundled with its library, so the import fails.\nIt is possible to add both Dulwich and HgGit into library.zip that is installed along with hg.exe, but for me the best way is to install everything from source including Mercurial and execute commands using .bat files installed into \\Python\\Scripts. In this case you will need to:\n\nInstall Mercurial from source. This builds \"pure\" version, because Windows users usually don't have Visual Studio or alternative compiler for compiling C speedups.\nInstall Dulwich - I'd use latest trunk snapshot for both Git and Dulwich.\npython setup.py --pure install\nInstall latest HgGit snapshot\npython setup.py install\nEdit Mercurial.ini to enable hggit =\nLaunch Mercurial using your \\Python\\Scripts\\hg.bat\n\n\n\nA4:\n\nJust to supplement, I'm on a fairly highly controlled Windows workstation right now, and don't have access to much.  Downloading the \"executable installer\" that I usually use did not create the bat files nor, for some reason, vim.exe, though gvim.exe was installed in the vim74 dir for me and worked fine.  So though I also needed to set the PATH, that there was no bat file in C:\\WiNDOWS nor any command line executable in my VIm runtime folder to call meant that callingvim from the command line (or Powershell) didn't work.\nI'm guessing some portion of the install that's creating the command-line related stuff, apparently including vim.exe, isn't recovering gracefully when you don't have admin permissions.\nEither way, the \"right\" thing to do appears to be to set your PATH to your vim executable folder as usual (note that this might be a little more difficult than usual if you don't have admin privs), then download the \"Win32 console executable\" from the usual download page that matches the version of gvim that you've already installed, dig vim.exe out of that zip you just downloaded, and place that into the same folder as gvim.exe.\nLooking on another box where the executable installer did work \"fully\", there's some jive in the vim.bat file that wasn't installed for me about \"collect the arguments in VIMARGS for Win95\" and if .%OS%==.Windows_NT goto ntaction, etc etc, but not having any of that doesn't seem to be a problem on Win7, at least.  ;^)\n\n\nA5:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA6:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\nA7:\n\nTry following configuration (change to your path), which works for me:\n[extensions]\n; hg-git extention\nhgext.bookmarks =\nhggit = C:\\Python26\\Lib\\site-packages\\hg_git-0.2.1-py2.6.egg\\hggit\n\nIn my case when I have empty value for hggit =, I get the same error as you do in this case. But I can import dulwich without problem in python shell, so you should check your easy-install.pth (as pointed out by David) if it contains dulwich-0.5.0-py2.5.egg.\nI did install pure version of dulwich as well.\n\n\nA8:\n\nI got this error as well even after downloading the latest Tortoisehg and making sure the hggit plugin was installed as well as my .ini & hgrc files had the right entry to enable hggit.\nTurns out my problem was that I had both mercurial and tortoisehg in my path. So when I ran any hg commands, it was using the hg.exe in mercurial folder as opposed to the hg.exe in the torsoisehg directory.\nThis makes sense but my mercurial installation did not have the plug ins. My fix was to remove mercurial from my path so hg commands go through the tortoisehg directory since it has hg completely bundled. Note however, the recommended option might be to upgrade mercurual to a version that has the plugins that one needs but this is what worked for me. I tried replacing the library.zip in mercurial with the one in tortoisehg and this worked but it led to other errors as one would imagine.\n@techtonik's answer led me down this road for which I am grateful.\nRecap: verify which hg exe is running your hg commands by checking your path because that hg.exe does not find the plugins for whatever reason.\n\n\nA9:\n\nIf you are inside a batch script, you can use argument variable tricks to get the filesize:\nfilesize.bat:\n@echo off\necho %~z1\n\nThis gives results like the ones you suggest in your question.\nType\nhelp call\n\nat the command prompt for all of the crazy variable manipulation options. Also see this article for more information.\nEdit:\nThis only works in Windows 2000 and later\n\n\nA10:\n\nI found a simpler solution at http://candidcode.com/2010/01/12/a-guide-to-converting-from-mercurial-hg-to-git-on-a-windows-client/\nAnd then I found a yet simpler solution myself:\nTo use the hg-git Mercurial extension on Windows: \n\ninstall the official Mercurial binaries\nput dulwich folder\nfrom dulwich sources and hggit folder from hg-git sources to the\nroot of library.zip in Mercurial installation folder\nadd the following to %USERPROFILE%\\Mercurial.ini:\n\n\n[extensions]\nhgext.bookmarks=\nhggit=\n\nTo have SSH support you need plink.exe from PuTTY family. After that you should add the following to Mercurial.ini:\n[ui]\nusername = John Doe <[email\u00a0protected]>\nssh=d:/home/lib/dll/plink.exe -i \"d:/home2/ssh-private-key.ppk\"\n\nWhen connecting to a SSH server for the first time, you should start putty.exe and try to connect using it. It will add the server key fingerprint to the registry. Otherwise plink will ask you to accept the fingerprint but it doesn't accept any input.\nYou can use puttygen.exe to generate private keys. Either use keys without a passphrase or use Pageant.exe ssh authentication agent.\n\n\nA11:\n\nWhen you install gVim:\nPlease make sure [\u2713] Create .bat files for command line use is checked.\nIt'll create several .bat files in C:\\Windows\\:\nC:\\>cd %windir%\nC:\\WINDOWS>dir /b *.bat\nevim.bat\ngview.bat\ngvim.bat\ngvimdiff.bat\nview.bat\nvim.bat\nvimdiff.bat\nvimtutor.bat\n\nNotice that: C:\\WINDOWS is already in the PATH environment variable.\nWhen you type vim in command line, C:\\WINDOWS\\vim.bat will be launched.\nIf you leave the checkbox mentioned above unchecked, you need to modify PATH manually.\n\n\nA12:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA13:\n\nBecause of restrictions on my workstation, I had to install my instance of vim locally to the following folder: %LocalAppData%\\Vim\nBut just changing an environmental variable like others suggest is insufficient because the batch files aren't in the installation directory (such as vimdiff). So I figured out where these come from, and made my own batch files to add them to an environmental variable. To help you make these batch files, I've provided the list of commands below for you to run in the vim installation directory (if saving the commands to an install.bat file first, replace all % characters with %% to escape it):\necho @echo off > evim.bat\necho gvim.exe -y %* >> evim.bat\n\necho @echo off > view.bat\necho vim.exe -R %* >> view.bat\n\necho @echo off > gview.bat\necho gvim.exe -R %* >> gview.bat\n\necho @echo off > vimdiff.bat\necho vim.exe -d %* >> vimdiff.bat\n\necho @echo off > gvimdiff.bat\necho vim.exe -d %* >> gvimdiff.bat\n\nThen you have to go to Start > Search 'Edit environment variables for your account' > Find 'Path' > Edit > Add a new subentry for %LocalAppData%\\Vim\\vim82. Last you may need to close and open the command prompt, or kill explorer.exe and restart it.\n\n\nA14:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA15:\n\nHow to sign your app\nUse Microsoft's SignTool to sign your app.\nYou download it as part of the Windows SDK. Note that it's also possible to install SignTool without installing the entire SDK. Once installed you can use SignTool from the command line like so:\nsigntool sign /a /fd SHA256 /tr http://timestamp.digicert.com /td SHA256 MyFile.exe\n\nThis will sign MyFile.exe. Explanation of the used command line options:\n\n/a will automatically use the certificate that is valid for the longest time. If you have no certificate, SignTool will display an error.\n/fd SHA256 will use the SHA-256 digest algorithm for the file signature. Using SHA256 is recommended and considered to be more secure than the default SHA1 digest algorithm.\n/tr http://timestamp.digicert.com adds a timestamp to your signed apps. This is extremely important because this will allow the signature to remain valid even after the certificate itself has already expired. The argument for the /tr option is a timestamp URL. You can use any of the timestamp URL's from this list of free RFC 3161 timestamp servers.\n/td SHA256 will use the SHA-256 digest algorithm for the timestamp signature. As before, using SHA256 is recommended and considered to be more secure.\n\nHow and when to use self-signed certificates\nIf you'd like to get a hold of a certificate that you can use to test your process of signing the executable, you can use MakeCert to create a self-signed certificate.\nOnce you've created your own certificate and have used it to sign your executable, you'll need to manually add it as a Trusted Root CA for your machine in order for UAC to accept your self-signed certificate as a trusted source. Note that you can only do this on your own development machines. You usually can not do this on your user's computers, since most users will not accept to install a new Root CA for good reasons.\nHow to get rid of the \"unrecognized app\" warning\nEven if your app is signed, you might still see the following warning message when trying to run the app:\n\nMicrosoft Defender SmartScreen prevented an unrecognized app from\nstarting. Running this app might put your PC at risk.\n\nHow to avoid this warning is a somewhat complex topic. Please see this answer to get the whole picture about these Microsoft SmartScreen warnings and what you can do and should know about it.\n\n\nA16:\n\nUntil you get import dulwich to work, hggit won't work. Check that the dulwich egg file is in your easy-install.pth file under site-packages. \nFor further debugging you can try ask pkg_resources about it:\nimport pkg_resources\npkg_resources.require(\"dulwich\")\n\n\n\nA17:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA18:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA19:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\nA20:\n\nIf you can install TortoiseHg, it includes dulwich and other requirements.\n\n\nA21:\n\nInstall gVim on your window and enable \".bat\" when you install gvim and click next, done.\nYou can use vim on window.\n\n\nA22:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA23:\n\nIn PowerShell you can do:\n$imageObj = New-Object System.IO.FileInfo(\"C:\\test.jpg\")    \n$imageObj.Length\n\n\n\nA24:\n\nHad this problem today when installing the latest TortoiseHg.\nGet the latest python 2.7 (I used 2.7.16) setup on your system, you probably have this already.\nGet a command window and go to the c:\\Python27 folder\nTo run pip use Scripts\\pip or easy_install use Scripts\\easy_install\nTry pip -V to make sure you get the 2.7 version and not some 3.X version\nWrong:\nc:\\Python27>pip -V\npip 20.2.4 from c:\\python38\\lib\\site-packages\\pip (python 3.8)\n\nRight:\nc:\\Python27>Scripts\\pip -V\npip 20.2.4 from c:\\python27\\lib\\site-packages\\pip (python 2.7)\n\nIf dulwich or hg-git are installed already\nScripts\\pip uninstall dulwich\nScripts\\pip uninstall hg-git\n\nInstall hg-git\nScripts\\easy_install install hg-git\n\nYou should now have two folders\nC:\\Python27\\Lib\\site-packages\\dulwich-0.19.16-py2.7-win-amd64.egg\\dulwich\nC:\\Python27\\Lib\\site-packages\\hg_git-0.9.0-py2.7.egg\\hggit\n\nIt will only work if Dulwich is version 0.19.16 (less than 0.20.x)\nCopy these folders (dulwich and hggit) into the zip-file\nC:\\Program Files\\TortoiseHg\\lib\\library.zip\nfolder in zip file\n\n\nA25:\n\n\nsudo apt-get install python-dev # Dam you dependency!\nsudo easy_install dulwich\n\nSuccess!\n\n\nA26:\n\nWindows 10 has linux subsystem for windows. So you can install bash in windows and from bash you can use vim.I found it more convenient.\n\n\nA27:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA28:\n\nI ran into this problem too with dulwich.errors.  Instead of installing everything from scratch.  I just copied dulwich from my default site-packages to the mercurial site-packages.  worked with no problems.\n\n\nA29:\n\nAfter a long research I finally managed to setup hggit properly.\nWhat I have done:\n\nI have installed the newest version of TortoiseHg (in my case Version 5.9.1)\nThis official version (automatically) contains Python-2.7.18 (According to Help/About)\nDownload dulwich: cd C:\\dulwich, git clone https://github.com/dulwich/dulwich.git .\nTortoiseHg uses Python 2.7, dulwich doesn't support this (old) Python version anymore. We have to go back to an older version of dulwich (<0.20)\ngit reset --hard c6993451a745d313f61e5d080b9f9d8611a8d7f4\nThe subfolder C:\\dulwich\\dulwich has to be copied inside the zip file C:\\Program Files\\TortoiseHg\\lib\\library.zip so that there is a path C:\\Program Files\\TortoiseHg\\lib\\library.zip\\dulwich\nThis can be done by the following commands:\na) Extract the currenty content of TortoiseHg's library\nmd \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\ntar -xf \"C:\\Program Files\\TortoiseHg\\lib\\library.zip\" -C \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\nb) Copy dulwich files into directory\nxcopy C:\\dulwich\\dulwich \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\\dulwich\\\"\nc) Save old library.zip and create new library.zip\nren \"C:\\Program Files\\TortoiseHg\\lib\\library.zip\" library_old.zip\ncd \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\ntar -acf \"C:\\Program Files\\TortoiseHg\\lib\\library.zip\" *.*\nd) Cleanup\ncd ..\nrd /q /s \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\nrd /q /s C:\\dulwich\nDownload hg-git: cd C:\\hg-git, hg clone https://foss.heptapod.net/mercurial/hg-git .\nThe current version of hg-git also does not support Python 2.7 anymore. We have to go back to the last working version 0.9.0 (=revision 1320 in TortoiseHg):\nhg update 6d7d6f174df8\nCreate a new empty repository which will be \"connected\" to a remote git repository\ncd C:\\MyLocalHgRepoWhichIsConnectedToGit, hg init\nCreate a textfile C:\\MyLocalHgRepoWhichIsConnectedToGit\\.hg\\hgrc with the following content:\n\n\n[paths]\ndefault = C:\\ConnectedGitRepoForPushAndPull\n\n[extensions]\nhgext.bookmarks = \nhggit = C:\\hg-git\\hggit\n\n\nRemark: Alternatively you can also copy the folder C:\\hg-git\\hggit inside library.zip and leave the path in the hgrc file empty. Indeed this is my favourite way. Do it the same way as I have explained under step 4 with dulwich.\nThat's it. Now the Mercurial Repo is linked to the Git Repo\nand you can execute  hg pull or hg push inside the Hg Repo.\n\n\nIf you don't want to read all my explanations you can simply use this batch file and execute it:\nhggit.bat\n@echo off\nrem Installation von dulwich+hggit f\u00fcr TortoiseHg\nrem Michael Hutter, 22.09.2021\nrem https://stackoverflow.com/a/69279657/9134997\n\nif \"%1\"==\"install\" goto install\nif \"%1\"==\"clonegitrepo\" goto clonegitrepo\necho Syntax:\necho %0 install (installiert dulwich und hggit in TortoiseHg)\necho %0 clonegitrepo C:\\HgRepoToCreate C:\\GitRepoToClone (Klonen eines Git-Repos in ein Hg-Repo)\npause\ngoto ende\n\n\n:install\nset LibraryPath=C:\\Program Files\\TortoiseHg\\lib\nset TempPathDulwich=C:\\Temp\\dulwich\nset TempPathHggit=C:\\Temp\\hg-git\n\nif not exist \"%LibraryPath%\\library.zip\" (\necho Die Datei %LibraryPath%\\library.zip existiert nicht!\npause\ngoto Ende\n)\n\nrem Installation von dulwich\nmd %TempPathDulwich%\ncd %TempPathDulwich%\ngit clone https://github.com/dulwich/dulwich.git .\ngit reset --hard c6993451a745d313f61e5d080b9f9d8611a8d7f4\npause\n\nrem Installation von hg-git\nmd %TempPathHggit%\ncd %TempPathHggit%\nhg clone https://foss.heptapod.net/mercurial/hg-git .\nhg update 6d7d6f174df8\npause\n\nrem dulwich und hggit in library.zip aufnehmen\nmd \"%LibraryPath%\\ExtractedLibrary\"\ntar -xf \"%LibraryPath%\\library.zip\" -C \"%LibraryPath%\\ExtractedLibrary\"\nxcopy %TempPathDulwich%\\dulwich \"%LibraryPath%\\ExtractedLibrary\\dulwich\\\"\nxcopy %TempPathHggit%\\hggit \"%LibraryPath%\\ExtractedLibrary\\hggit\\\"\nren \"%LibraryPath%\\library.zip\" library_old.zip\nif exist \"%LibraryPath%\\library.zip\" del \"%LibraryPath%\\library.zip\"\ncd \"%LibraryPath%\\ExtractedLibrary\"\ntar -acf \"%LibraryPath%\\library.zip\" *.*\n\nrem Aufr\u00e4umen\ncd ..\nrd /q /s \"%LibraryPath%\\ExtractedLibrary\"\nrd /q /s %TempPathDulwich%\nrd /q /s %TempPathHggit%\npause\ngoto ende\n\n\n:clonegitrepo\nrem Klonen eines Git-Repos in ein lokales Hg-Repo\necho Erstelle neues HgRepo %2\nmd %2\ncd %2\nif not exist %2 (\necho \"Fehler: %2 existiert nicht!\"\npause\ngoto ende\n)\nhg init\necho [paths] > .hg\\hgrc\necho default = %3 >> .hg\\hgrc\necho. >> .hg\\hgrc\necho [extensions] >> .hg\\hgrc\necho hgext.bookmarks = >> .hg\\hgrc\necho hggit = >> .hg\\hgrc\nhg pull\nhg update\npause\n\n:ende\n\n\n\nA30:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA31:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( vs.  vs."}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 25, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIs there any way of online editing the commit message in GitHub.com, after submission?\nFrom the command line, one can do\ngit commit --amend -m \"New commit message\"\n\nas correctly suggested in the following question:\n\nHow to modify existing, unpushed commit messages?\n\nTrying git pull and then git push has worked (without any other commit having interfered in the mean time).\nBut can it be done via the GitHub website?\n\n\nCandidate answers:\nA1:\n\nSo i had this project that wasnt under source control i made some changes to and wanted to keep stuff i changed.\ngit init\ngit remote add origin <url>\ngit fetch\ngit branch master origin/master\ngit restore --staged .\n\n\n\nA2:\n\nI think it is very preferable if you first pull the existing Github repo on the local and then add the new files to the Github repo\nThis link will help: https://stackoverflow.com/a/61285076/5840973\n\n\nA3:\n\nHere is what worked for me.\nI tried almost all the solutions mentioned here and on the web a lot of times. I gave up multiple times before and was using it this way for more than a year I think. Finally, I was able to solve this annoying issue.\n\nAs suggested by user \"AEQ\" in his answer, I deleted all the passwords related to GitHub from my Keychain Access.\nTried deleting and re-adding the Github account. But still, the problem persisted. (generated a personal access token from \"https://github.com/settings/tokens\" and use it as the password)\n\nWhat made it work was\nMake sure to use HTTPS and not SSH when adding the new user account. If you have already added the account, just edit it. That's it.\nFinally, It feels good.\nAttaching screenshots for ease.\n\n\n\nA4:\n\nMake sure that your user account is added to the repository as a collaborator.\nSetting --> Collaborators\n\n\nA5:\n\nIn my case, I cloned the repo using HTTPS based url. Then I configured my githib account with 2-factor authentication using SSH key. So whenever I was pulling/pushing changes, it was showing me dialog to provide my password.\nI fixed it by enabling SSH access by right clicking on project repo-menu on SourceTree.\n\n\n\nA6:\n\nMac:\nThis solution works for me.\nAfter clone code successfully, you open terminal and run this code\ngit config --global credential.helper osxkeychain\n\n\n\nA7:\n\nThis error mostly caused by WRONG URL, please check:\n\nhttp or https\nURL Name\nusername@git_url\nwrong git name\n\n\n\nA8:\n\nSourceTree version 2.4.8.0\nThe problem for me seemed to be the order of setup between SourceTree and GitHub. I setup SourceTree first and just clicked all the \"setup later\" options.\nAn easy fix - Uninstall and re-install SourceTree. The non-obvious bit was needing to delete the actual program files manually.\nFrom any \"SourceTree.exe\" shortcut, open file location. Mine forked at \"C:\\Users\\myusername\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Atlassian\" where the shortcut pointed to another shortcut. \nOpen that shortcut's file location. I eventually ended up at \"C:\\Users\\myusername\\AppData\\Local\\SourceTree\". Delete the whole \"SourceTree\" folder. Go up one level to \"...\\Local\" and delete the \"Atlassian\" folder as well.\nNow uninstall SourceTree. Make sure your GitHub is setup the way you desire (for me, I also created and saved an SSH key that I used for SourceTree setup). Re-install SourceTree using all the correct credentials, point to the right repository and such. \nAfter all this, SourceTree asked for my GitHub credentials twice, with two different dialog boxes, then stopped and worked! No more endless loop. Good luck!\n\n\nA9:\n\nIn my case, I was trying to clone a private repository in our group Github account to me laptop:\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone https://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nfatal: https://github.com/nyusngroup/velocity_CCSNe.git/info/refs?service=git-upload-pack not found: did you run git update-server-info on the server?\n\nI found two ways can solve this.\n(1) Use http instead of https. It asks me the name and password of our group Github account. After I entered the information, I can clone it.\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone http://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nUsername for 'http://github.com':nyusngroup\nPassword for 'http://nyusngroup@github.com': \n\n(2) Add my Github account to the collaborators of the private repository in our group Github account, as pointed by Monkey King's answer above.\n\n\nA10:\n\nI met up with the same problem.\nHow I solved this problem is:\nI use git bash to create a new repo, when I typed \"git push origin master\"\nIt reported \n\"fatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\"\nFinally, I found there was not a repo on the github at all.\nYou'd better create a new repo first on github. \nMaybe this experience can help somebody.\n\n\nA11:\n\nAssume that I would like to add FreeRTOS repository, which URL is https://github.com/FreeRTOS/FreeRTOS-Kernel.git, into my repository, example URL is https://github.com/username/example as a submodule\ngit submodule add https://github.com/FreeRTOS/FreeRTOS-Kernel.git\ngit add .\ngit commit -m 'add a submodule'\ngit push\n\nTo clone using HTTPS:\ngit clone https://github.com/username/example.git --recurse-submodules\n\nUsing SSH:\ngit clone git@github.com:username/example.git --recurse-submodules\n\nIf you have downloaded the repo without using the --recurse-submodules argument, you need to run:\ngit submodule update --init --recursive\n\n\n\nA12:\n\nThis is what I discovered (after searching for 'sourcetree' in my drive C:):\n\nUninstall SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\Atlassian\\SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\SourceTree\nReinstall SourceTree\n\n\n\nA13:\n\nOpen your Terminal, access to this folder and write:\ngit init\ngit add .\ngit commit -m \"my commit\"\ngit remote set-url origin git@github.com:username/repo.git\ngit push origin master\n\n\n\nA14:\n\nThe easiest way is to create an app password and use that password.\nHere you can see how you can create an app password on Bitbucket:\nhttps://docs.snyk.io/tutorials/getting-started/atlassian-integrations/atlassian-bitbucket-app-password\n\n\nA15:\n\nThis error could also happen if the repository you are attempting to reach was deleted.\n\n\nA16:\n\nIf you are not using GitHub 2FA, another reason for asking you your password is:  \nYour GitHub repo url is an ssh one, not an https one.\nSince git does not find your ssh keys (or your ssh key is not registered to your GitHub account), it fails back to account/password mechanism. \nIf you are using 2FA (and an https url), then you need a personal token as a password.\n\n\nA17:\n\nAlso make sure the repo you've entered is cased correctly (it's case sensitive).\n\n\nA18:\n\nI got this issue when using GitStack. I looked into C:/GitStack/apache/logs/error.log and found that GitStack was looking for the repository in the wrong location although its settings said D:/Repositories (my custom location), GitStack looked for the repositories in its default location. So I simply went and save the settings again from GitStack admin page. It solved the problem.\n\n\nA19:\n\nI copied a small sub project I had that was under Git source control into another project and forgot to delete the .git folder. When I went to commit I got the same message as above and couldn't clear it until I deleted the .git folder.\nIt is a bit silly, but it is worth checking you don't have a .git folder under the folder that doesn't commit.\n\n\nA20:\n\nYou could have done a:\ngit add -u -n\n\nTo check which files you modified and are going to be added (dry run: -n option), and then\ngit add -u\n\nTo add just modified files\n\n\nA21:\n\n1. first create a git repostry.\n2. second open git bash in  existing or uploading project.\n3. perform git init \n4. git add .\n5. git commit -m \"print message\"\n6. git remote add github<repostry url>\n7. git remote -v\n8. git push github master\n\nOR\ngit push origin master\n\nif  you get any error, you may use it\ngit push -f origin master\n\n\n\nA22:\n\nI got the same problem while using a github repository, and connecting to it via https, while using the OS X Keychain Credential helper.\nMy problem was that I had the wrong credentials stored in OS X's Keychain (I was using the email address that I used to sign up for github.com rather than the [username]@github.com address it provides you). I deleted the old account in the keychain and only left the @github.com one and it fixed the problem.\nNot sure if it is related, but when I checked the user.email local config:\ngit config -l\n\nit showed the incorrect email address as well, so I updated the local git user.email to use the correct account too:\ngit config user.email <username>@github.com\n\n\n\nA23:\n\nAs the message says:\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nGit has a \"staging area\" where files need to be added before being committed, you can read an explanation of it here.\n\nFor your specific example, you can use:\ngit commit -am \"save arezzo files\"\n\n(note the extra a in the flags, can also be written as git commit -a -m \"message\" - both do the same thing)\nAlternatively, if you want to be more selective about what you add to the commit, you use the git add command to add the appropriate files to the staging area, and git status to preview what is about to be added (remembering to pay attention to the wording used).\nYou can also find general documentation and tutorials for how to use git on the git documentation page which will give more detail about the concept of staging/adding files.\n\nOne other thing worth knowing about is interactive staging - this allows you to add parts of a file to the staging area, so if you've made three distinct code changes (for related but different functionality), you can use interactive mode to split the changes and add/commit each part in turn. Having smaller specific commits like this can be helpful.\n\n\nA24:\n\nIn my case, the password that Sourcetree was asking for was for the ssh private key password. If I remove the password from the ssh key, this prompt doesn't come up.\nHowever since I wanted the password to be there, I kept investigating why it wasn't accepting my ssh key password.\nTurns out there is a wrinkle. If you have multiple ssh private keys which have different passwords, Sourcetree will give you a password prompt for each password.\nThink of it this way - if you have a fresh shell in your system, and you just started ssh-agent up, and ran ssh-add, what are the password prompts that you get? For every such password prompt, Sourcetree will give you this dialog box. In fact, if you add passwords to the dialog in the same order as you would in the shell to an ssh-add, this would work. In fact, I am guessing Sourcetree is doing exactly the same in the background.\nSo my final solution was 1) keep the passwords on the ssh keys 2) make all these passwords the same. So now Sourcetree will give me a single password prompt, and entering the ssh private key password will work.\n\n\nA25:\n\nIn my case I was using the wrong protocol in the repository URL (http instead of https)\n\n\nA26:\n\nFor windows user\n\nchange SSH Client from Putty/Plink to OpenSSH\n\nin Tools -> Option -> SSHClient\n\n\n\nA27:\n\nNo, this is not directly possible. The hash for every Git commit is also calculated based on the commit message. When you change the commit message, you change the commit hash. If you want to push that commit, you have to force that push (git push -f). But if already someone pulled your old commit and started a work based on that commit, they would have to rebase their work onto your new commit.\n\n\nA28:\n\nThis happened to me and at first it was not apparent what had gone wrong. The previous day I had renamed my repo on github.com only changing the first character of the repo name from a lowercase letter to uppercase. I was not aware that the repo name in the origin URL was case-sensitive unit I received the error which contained the text, \"did you run git update-server-info on the server.\" Once I edited the .git/config file to correct the case of the same letter in the origin URL, the problem was solved, the error no longer occurred and I was once again able to push my changes up to the repo on github.com. Another bit of advice: if you make changes to your repo on github, make sure to test sync'ing your local repo right away. In my case, I did not and it was the next day when I had tried to sync and couldn't because of the error and I forgot that I had renamed the repo, so I was scratching my head for a bit. Had I tested sync'ing right away, I would have caught this problem right away.\n\n\nA29:\n\nIn my case my github account did not have permissions to the repo. Added the github account as a collaborator for the repo and that fixed it.\n\n\nA30:\n\nYou might have changed your repository name\nIn your local repository edit the file:\n.git/config\n\nThen check:\n[remote \"origin\"]\n   url = \n\nthat the URL matches your remote repository\n\n\nA31:\n\nif you have more files in my case i have 7000 image files when i try to add them from project's route folder it hasn't added them but when i go to the image folder everything is ok. Go through the target folder and command like abows\ngit add .\ngit commit -am \"image uploading\"\ngit push origin master\n\n\n\ngit push origin master Enumerating objects: 6574, done. Counting objects: 100% (6574/6574), done. Delta compression using up to 4\n    threads Compressing objects: 100% (6347/6347), done. Writing objects: \n    28% (1850/6569), 142.17 MiB | 414.00 KiB/s\n\n\n\n\nA32:\n\nIn my repo's directory on my machine in the terminal I had to reset the origin url:\ngit remote set-url origin git@github.com:repoaccountname/repo-name.git\n\n\n\nA33:\n\nI encountered this message when using Jenkins 2.176.1 and git plugin 3.10.0 using a very old command line git version (1.7.1) as included on CentOS 6.\nThe message does not occur on newer versions of command line git (1.8 or later) with the Jenkins git plugin.\nUpgrading to a newer command line git version resolves the problem.\nOfficially, the Jenkins git plugin does not support command line git 1.7.1.  The minimum supported version is command line git 1.7.10. Many Jenkins git plugin use cases require at least command line git 1.9.0.\nCommand line git 1.7.1 behaves differently than later versions when a repository has been initialized (with git init) and then a git fetch is performed with a refspec which references 'origin'.  Later versions of command line git do not have the same problem.\n\n\nA34:\n\nJust another stupid reason: you may have changed credentials on your laptop, but there is another development PC running but not frequently used, that has SourceTree started and uses the old credentials frequently to poll status.\nSounds stupid, but took me some time to solve :-)\n\n\nA35:\n\nI had more luck with navigating in my terminal to the directory I wanted to add to the repository, then (assuming you're working on a branch called master):\n    git init\n    git add .\n    git commit -m \"my commit\"\n    git remote add origin <remote repository URL>\n    git push origin master\n\nHere's a link to an article explaining how to do it in more detail: https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/\nNote that you won't be able to run the \"git add .\" line if the directory in question is open.\n\n\nA36:\n\nA solution worked for me given by Andrew Magill, Copied from Source : link\nThe problem is that SourceTree's embedded Git client comes with git-credential-manager v1.12, which no longer works with Github since they disabled TLS 1.1 on their site.  You need to get git-credential-manager v1.14.  Easiest way to do that is to install a current copy of the Git client separately, and then switch SourceTree over to use that (\"system git\") instead of its embedded client.  Alternatively, you can update git-credential-manager in your embedded client by replacing its files with the newest version from Microsoft.\n\n\nA37:\n\nI had a similar thing after an update; I tried re-adding my bitbucket credentials, changing the protocols, etc. with no luck, until...\n\nI opened my keychain and searched for bitbucket (probably the same for github, just search for github instead) under the Passwords category. \nI found that I had multiple Access Keys for my user, so I closed my repository, deleted all the keys of \"application password\" Kind, opened the SourceTree preferences > Accounts > removed my account and re-added it (login as usual - I used Basic with HTTPS). \nKeychain asks for permission to save the password, which I said yes and now I only see one key of the kind \"application password\". \n\nI opened my repository from the repo browser and hey presto, no more password popups!\nThis bugged me for a whole week! If this helped you, you can do a little dance with me now :)\n\n\nA38:\n\nIf you are using two-factor authentication with GitHub you will need to create a personal access token and use it with SourceTree:\n\nTo work with GitHub's two-factor authentication in SourceTree you can simply use your access token instead of your password. The steps to do this are as follows:\n\nGo to your Personal Access Tokens settings in GitHub.\nClick on the Generate new token button.\nName the token something descriptive.\nSelect which scopes you wish to grant this token.\nClick the Generate token button.\nCopy the token and use it as a password in your hosted repositories.\n\nYou can find more information about this on GitHub's help here.\n\nMac Users:\nIf your SourceTree keeps on asking for the password, go to the Terminal and type this:\ngit config --global credential.helper osxkeychain\n\n\n\nA39:\n\nI had a very similar issue with the same error message. \"Changes not staged for commit\", yet when I do a diff it shows differences.  I finally figured out that a while back I had changed a directories case. ex. \"PostgeSQL\" to \"postgresql\".  As I remember now sometimes git will leave a file or two behind in the old case directory.  Then you will commit a new version to the new case.\nThus git doesn't know which one to rely on.  So to resolve it, I had to go onto the github's website. Then you're able to view both cases.  And you must delete all the files in the incorrect cased directory. Be sure that you have the correct version saved off or in the correct cased directory.\nOnce you have deleted all the files in the old case directory, that whole directory will disappear. Then do a commit.\nAt this point you should be able to do a Pull on your local computer and not see the conflicts any more.  Thus being able to commit again.  :)\n\n\nA40:\n\nAll the answers above seems to guide about creating a new repository in git but the question is about adding a folder to existing repo. To do that following steps can be followed.\n\nClone your existing repo using following command:\ngit clone https://github.com/company/our_repo.git \nManually take your project folder to the desired location i.e. trunk/bin\nNow commit and then push in the repo using the commands:\ngit commit -m \"message\" and git push origin master \n\n\n\nA41:\n\ndelete each .git file in all projects\nyou can use this command\nrm -rf .git\n\n\n\nA42:\n\nGo to Sourctree -> Tools -> Options -> Git Select \n\n\nA43:\n\nYou need to git push -f assuming that nobody has pulled the other commit before. Beware, you're changing history. \n\n\nA44:\n\nThe reason why this is happening is because you have a folder that is already being tracked by Git inside another folder that is also tracked by Git. For example, I had a project and I added a subfolder to it. Both of them were being tracked by Git before I put one inside the other. In order to stop tracking the one inside, find it and remove the Git file with:\nrm -rf .git\n\nIn my case I had a WordPress application and the folder I added inside was a theme. So I had to go to the theme root, and remove the Git file, so that the whole project would now be tracked by the parent, the WordPress application.\n\n\nA45:\n\n\ngit init\ngit add .\ngit commit -m \"initial commit\"\ngit remote add origin [URL]\ngit push origin masteror\n\ngit push -f origin master\n\ngit push --force.\nIt does force the update\n\n\n\nA46:\n\nIn my case I needed to put my ssh password instead of my personal access token. Then it worked.\n\n\nA47:\n\nSince this question is specifically about SourceTree and Github, the answer in 2021 is simple. Quoting bcr:\n\nEnsured I was using OAuth for my GitHub account stored in Tools ->\nOptions -> Authentication\n\n\nGo there. Select Add (new account).\nSelect OAuth.\nClick Refresh OAuth Token. A browser page will open (github.com) requesting renewal of your token.\nLogin to accept. SourceTree will immediately respond by acknowledging the token provided by github.com.\n\n\n\nA48:\n\nDid you create a new repository on the http://github.com with the same name? \nIf not, do it! And make sure each letter is correct and case sensitive.\n\n\nA49:\n\nI find this problem appearing when I've done a git add . in a subdirectory below where my .gitignore file lives (the home directory of my repository, so to speak).  Try changing directories to your uppermost directory and running git add . followed by git commit -m \"my commit message\".\n\n\nA50:\n\nLets say you have remote repo with files and local repo with the same files.\nAnd you want to add Git to local files, and dont want to push.\nThen you can do such commands on local repo:\ngit init\ngit remote add origin <url>\ngit fetch --all\ngit reset --hard origin/master\n\nAfter that your repository is synced with remote repo.\nYou can change files, git add, push and so on.\n\n\nA51:\n\nI am using a SSH key, but SourceTree was asking me a password every time.\nFound on the community of Atlassian, this solution worked for me:\n\nOpen the terminal, get into your project directory\nType git config credential.helper store\nType git pull\nInput username/password (if asked)\n\nVoil\u00e0 !\n\n\nA52:\n\nMaybe an obvious thing, but...\nIf you have problem with the index, use git-gui. You get a very good view how the index (staging area) actually works.\nAnother source of information that helped me understand the index was Scott Chacons \"Getting Git\" page 259 and forward.\nI started off using the command line because most documentation only showed that...\nI think git-gui and gitk actually make me work faster, and I got rid of bad habits like \"git pull\" for example... Now I always fetch first... See what the new changes really are before I merge.\n\n\nA53:\n\nI received this same error when I attempted to git clone something from Kiln what was actually a Mercurial repository.\n\n\nA54:\n\nYou didn't add the changes. Either specifically add them via\ngit add filename1 filename2\n\nor add all changes (from root path of the project)\ngit add .\n\nor use the shorthand -a while commiting:\ngit commit -a -m \"message\".\n\n\n\nA55:\n\nif you have a subfolder, which was cloned from other git-Repository, first you have to remove the $.git$ file from the child-Repository:\nrm -rf .git\nafter that you can change to parent folder and use git add -A. \n\n\nA56:\n\nIf you are using \"Username & Password\" authentication,you can try to change to use SSH keys for the authentication.\nI have met this kind of situation when I commit to GitLab everytime.When I changed the way of authentication:SSH keys,the problem has been solved.\n\n\nA57:\n\nI finally was able to solve this after months of no luck. On a Mac :\n\nOpen 'Keychain Access' on a Mac by using Launchpad or Spotlight Search\nNarrow down your search by selecting \"login\" under Keychains and \"Passwords\" under the Category (both filters on the left side of the window)\nIn the top right hand corner of the window, do a search for \"git\"\nYou'll likely see a keychain there. I right-clicked and deleted the one entry I had.\nGo into sourcetree and try to do a pull/fetch/anything. You'll be prompted for a password. This should re-initiate your password with Sourcetree. To double check this, go into your keychain and repeat steps 3 & 4. Click on the keychain and look at the access control tab. SourcetreeLogin should be listed as always having access now.\n\n\n\n\nA58:\n\nopen terminal and clone your repo. repository url must have your usedid,\nFor ex:\n$ git clone https://<username>@github.com/<userId>/<reponame>.git\n$ git clone https://mark@github.com/mak123/<reponame>.git\n\nThen drag and drop the cloned folder into the sourceTree window.You can also do by clicking \"+New Repository\" button and from the menu by selecting \"Add Existing Local Repository\".\nIt may ask for password again but this time it will get added to your keychain.\n\n\nA59:\n\nFollow to @Chris answer in case you've enabled 2FA, to use Personal Access Token in SourceTree, you need to add your acc with this token in SourceTree > Preferences with following information:\n\nAuth Type: Basic  \nUsername: {your-github-account} \nPassword: {your-personal-access-token} \nProtocol: HTTPS\n\nAfter that, you are able to clone your Github repo.\n\n\n\nA60:\n\nIn addition to setting up the Personal Access Token on GitHub, and doing what A_01 states above, this may be helpful (it's how I finally got it working)\n\nI made sure had the latest version of Sourcetree \nI upgraded to the latest version of and used System Git (Tools -> Options -> Git)\nUpgraded to the latest Git Credential Manager for Windows (this was via an installer I downloaded and double-clicked, very easy)\nEnsured I was using OAuth for my GitHub account stored in Tools -> Options -> Authentication\nMade sure I checked off the box \"Allow Sourcetree to manage my credentials using the Git Credential Manager\" (this was important!!) on Tools -> Options -> Git\nThen when I triggered the GitHub dialog (via pushing a new branch, just an example), I logged in there (this is where it used to fail).\nI next got the 2 factor auth dialog, and the code was sent to my phone as expected. I was able to enter the code from my phone into this dialog.\n\n\n\nA61:\n\nReally struggled with this on Mac, couldn't get it to work with SSH. Ended up following this guide which worked: https://www.youtube.com/watch?v=otRHRPnHDeI.\nThere were a couple things which could have been the cause, it may not have been the SSH itself. For instance, Oauth apparently doesn't work on SourceTree despite nothing appearing wrong immediately when using it. Making sure you use a Personal Access Token and not your real password among others.\nA good rule of thumb is, if after connecting your account it only shows your public repos, it's already broken. https://github.com/settings/tokens\n\n\nA62:\n\nIn my case, this was happening with a submodule that had defaulted to https even though the main project was using ssh. I fixed it by changing the submodule's repo path in SourceTree's Repository Settings to use the ssh path instead of https.\n\n\nA63:\n\nFor future references, I solved this problem by changing the GIT system used in Sourcetree, from Embedded to System.\nHow to change the git system:\n \n1. Find the Options submenu (located under Tools menu) and click on it to view the Options popup \n2. On the Options setting popup, find the Git tab, then click on it to view the Git setting \n3. Find Git version setting, then check the version used in it. In my case, I changed it to system because previously I'm using Git bash cmd, and it looks like that the git embedded in Sourcetree clashes with each other. \n4. Restart Sourcetree\n\n\nA64:\n\nMy issue was that I used the clone https url widget provided by github. That URL doesn't work for private repositories as you need to add a username to the front of it.  \nExample: a private repo owned by john and named widget with collaborator sam the correct url would be: \nhttps://sam@github.com/john/widget.git\nThe github provided url: \nhttps://github.com/john/widget.git\nThe error message leaves much to be desired.\n\n\nA65:\n\nIt looks like that's a private (or deleted) repository; if you visit the repository page while logged it'll give you the real URL, which'll probably be https://TeaCodie@github.com/TeaCodie/TeaCodie-Website.git , i.e. with a username specified?\n\n\nA66:\n\nYou have to use -f when you are going to push on already existing repo.\ngit init\ngit add *\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin <repo url>\ngit push -f origin main\n\n\n\nA67:\n\nprobably you were trying to clone like this: \ngit clone https://github.com/TeaCodie/TeaCodie-Website.git\nThen you got this error:\nfatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\nHere is what worked for me:\ngit clone https://github.com/TeaCodie/TeaCodie-Website.git/.git\nbecause the directory \"info/refs\" is in the .git directory.\n\n\nA68:\n\nIn my case I had old version of the git. Upgrade to latest version fixed this issue.\n\n\nA69:\n\nIn my case, the Sourcetree login screen repeatedly showed when pushing to GitHub. Logging in from that screen failed. I finally determined it was due to an expired OAuth token, which is trivial to fix.\n\nNavigate to the GitHub account config screen: Tools > Options > Authentication\nClick your GitHub account and then click edit\nOn the Edit Hosting Account dialog click the Refresh OAuth Token button\n\n\n\nA70:\n\nYou should do:\ngit commit . -m \"save arezzo files\"\n\n\n\nA71:\n\nGo to the directory where you code is,\ngit init\ngit add .\ngit commit -m \"Your message\"\n\nNow add your address go to your git hub copy the clone address,\ngit remote add origin <remote repository URL>\n\nNow add push your code with,\ngit push -u -f origin master\n\nAnd you are done.\n\n\nA72:\n\nI had an issue where I was doing commit --amend even after issuing a git add . and it still wasn't working.  Turns out I made some .vimrc customizations and my editor wasn't working correctly.  Fixing these errors so that vim returns the correct code resolved the issue.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " V3,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  "}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 26, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI know a common characteristic of the file names of a number of unwanted files on my Windows computer. How can I remove all of these files from a given folder or folder hierarchy with a single regular expression PowerShell command?\n\n\nCandidate answers:\nA1:\n\nThis is really weird, why nobody is mentioning the /IM switch ?! I've been using it for a long time in backup jobs. But I tried googling just now and I couldn't land on a single web page that says anything about it even on MS website !!! Also found so many user posts complaining about the same issue!!\nAnyway.. to use Robocopy to overwrite EVERYTHING what ever size or time in source or distination you must include these three switches in your command (/IS /IT /IM)\n/IS :: Include Same files. (Includes same size files)\n/IT :: Include Tweaked files. (Includes same files with different Attributes)\n/IM :: Include Modified files (Includes same files with different times).\n\nThis is the exact command I use to transfer few TeraBytes of mostly 1GB+ files (ISOs - Disk Images - 4K Videos):\nrobocopy B:\\Source D:\\Destination /E /J /COPYALL /MT:1 /DCOPY:DATE /IS /IT /IM /X /V /NP /LOG:A:\\ROBOCOPY.LOG\n\nI did a small test for you .. and here is the result:\n               Total    Copied   Skipped  Mismatch    FAILED    Extras\n    Dirs :      1028      1028         0         0         0       169\n   Files :      8053      8053         0         0         0         1\n   Bytes : 649.666 g 649.666 g         0         0         0   1.707 g\n   Times :   2:46:53   0:41:43                       0:00:00   0:41:44\n\n\n   Speed :           278653398 Bytes/sec.\n   Speed :           15944.675 MegaBytes/min.\n   Ended : Friday, August 21, 2020 7:34:33 AM\n\nDest, Disk: WD Gold 6TB (Compare the write speed with my result)\nEven with those \"Extras\", that's for reporting only because of the \"/X\" switch. As you can see nothing was Skipped and Total number and size of all files are equal to the Copied. Sometimes It will show small number of skipped files when I abuse it and cancel it multiple times during operation but even with that the values in the first 2 columns are always Equal. I also confirmed that once before by running a PowerShell script that scans all files in destination and generate a report of all time-stamps.\nSome performance tips from my history with it and so many tests & troubles!:\n. Despite of what most users online advise to use maximum threads \"/MT:128\" like it's a general trick to get the best performance ... PLEASE DON'T USE \"/MT:128\" WITH VERY LARGE FILES ... that's a big mistake and it will decrease your drive performance dramatically after several runs .. it will create very high fragmentation or even cause the files system to fail in some cases and you end up spending valuable time trying to recover a RAW partition and all that nonsense. And above all that, It will perform 4-6 times slower!!\nFor very large files:\n\nUse Only \"One\" thread \"/MT:1\" | Impact: BIG\nMust use \"/J\" to disable buffering. | Impact: High\nUse \"/NP\" with \"/LOG:file\" and Don't output to the console by \"/TEE\" | Impact: Medium.\nPut the \"/LOG:file\" on a separate drive from the source or destination | Impact: Low.\n\nFor regular big files:\n\nUse multi threads, I would not exceed \"/MT:4\" | Impact: BIG\nIF destination disk has low Cache specs use \"/J\" to disable buffering | Impact: High\n& 4 same as above.\n\nFor thousands of tiny files:\n\nGo nuts :) with Multi threads, at first I would start with 16 and multibly by 2 while monitoring the disk performance. Once it starts dropping I'll fall back to the prevouse value and stik with it | Impact: BIG\nDon't use \"/J\" | Impact: High\nUse \"/NP\" with \"/LOG:file\" and Don't output to the console by \"/TEE\" | Impact: HIGH.\nPut the \"/LOG:file\" on a separate drive from the source or destination | Impact: HIGH.\n\n\n\nA2:\n\nIf you need to query large (or small) log files on Windows, the best tool I have found is Microsoft's free Log Parser 2.2.  You can call it from PowerShell if you want and it will do all the heavy lifting for you, and very fast too.\n\n\nA3:\n\nInstead of unconditionally changing the working directory as mentioned in previous answers, you can write a simple function in the PowerShell profile to use Set-Location to quickly change the working directory whenever necessary.\nCheck Jeremy Danyow's answer to create/modify a PowerShell profile.\nAdd a function(s) to your PowerShell profile:\nfunction goto_this {set-location 'your\\path\\to\\some\\dir'}\nfunction goto_that {set-location 'your\\path to some\\dir with space'}\n\nJust change the function name and directory pointed to. Using quotes on the path is mandatory if it contains spaces. I try to keep the prefix goto_ as it helps in remembering the functions' names.\nYou can start typing goto_ then press TAB to cycle through all the added functions (remember to start a new PowerShell window after adding/modifying functions).\n\n\nA4:\n\n\"-TotalCount\" in this instance responds exactly like \"-head\". You have to use -TotalCount or -head to run the command like that. But -TotalCount is misleading - it does not work in ACTUALLY giving you ANY counts...\ngc -TotalCount 25 C:\\scripts\\logs\\robocopy_report.txt\n\nThe above script, tested in PS 5.1 is the SAME response as below...\ngc -head 25 C:\\scripts\\logs\\robocopy_report.txt\n\nSo then just use '-head 25\" already!\n\n\nA5:\n\nInvoke-Item will look up the default handler for the file type and tell it to run it.\nIt's basically the same as double-clicking the file in Explorer, or using start.exe.\n\n\nA6:\n\nFor PowerShell Core (Powershell Version 6.0+), you can use Automatic Variables: $IsLinux, $IsMacOS and $IsWindows.\nFor example,\nif ($IsLinux) {\n    Write-Host \"Linux\"\n}\nelseif ($IsMacOS) {\n    Write-Host \"macOS\"\n}\nelseif ($IsWindows) {\n    Write-Host \"Windows\"\n}\n\n\n\nA7:\n\nThe PowerShell answers are good, but the Rename-Item command doesn't work in the same target directory unless ALL of your files have the unwanted character in them (fails if it finds duplicates).\nIf you're like me and had a mix of good names and bad names, try this script instead (will replace spaces with an underscore):\nGet-ChildItem -recurse -name | ForEach-Object { Move-Item $_ $_.replace(\" \", \"_\") }\n\n\n\nA8:\n\nAn easier way to set the default directory is the following:\n\nRight click the Windows PowerShell icon and pin to Start\nRight click the Windows PowerShell icon in Start, and again right click Windows PowerShell and select Properties (not Run as Administrator and not Windows PowerShell ISE)\n\n\nIn the Shortcut tab -> 'Start in' field, change to the location you want PowerShell to start in.\n\n\n\n\n\nA9:\n\nIf what you want is to open powershell from windows terminal in the current directory, this worked for me:\n\nSelect defaults\nAdding . as starting directory\n\nNow, if I'm in a directory and hit:\n\nalt key + d (it selects the path in windows explorer)\ntype wt (it replaces the selected path with wt)\nhit enter\n\nIt opens powershell from windows terminal in the current directory\n\n\n\nA10:\n\n$Push_Pop = $ErrorActionPreference #Suppresses errors\n$ErrorActionPreference = \u201cSilentlyContinue\u201d #Suppresses errors\n#Script\n    #gc .\\output\\*.csv -ReadCount 5 | %{$_;throw \"pipeline end!\"} # head\n    #gc .\\output\\*.csv | %{$num=0;}{$num++;\"$num $_\"}             # cat -n\n    gc .\\output\\*.csv | %{$num=0;}{$num++; if($num -gt 2 -and $num -lt 7){\"$num $_\"}} # sed\n#End Script \n$ErrorActionPreference = $Push_Pop #Suppresses errors\n\nYou don't get all the errors with the pushpop code\nBTW, your code only works with the \"sed\" option. All the rest ignores anything but gc and path.\n\n\nA11:\n\nA one-liner command in Windows PowerShell to delete or rename certain characters will be as below. (here the whitespace is being replaced with underscore)\nDir | Rename-Item \u2013NewName { $_.name \u2013replace \" \",\"_\" }\n\n\n\nA12:\n\nI got some better solutions:\ngc log.txt -ReadCount 5 | %{$_;throw \"pipeline end!\"} # head\ngc log.txt | %{$num=0;}{$num++;\"$num $_\"}             # cat -n\ngc log.txt | %{$num=0;}{$num++; if($num -gt 2 -and $num -lt 7){\"$num $_\"}} # sed\n\n\n\nA13:\n\nYou can pipe a Get-ChildItem command through a Where-Object filter that accepts a RegEx pattern, and then pipe that into Remove-Item. I think that will get you a faster, and better result than using Select-String. With a command like:\nGet-ChildItem $Path | Where{$_.Name -Match \"<RegEx Pattern>\"} | Remove-Item\n\nThe Name attribute will only match the name of the file or folder, along with a file's extension. It will not match against other things along the path. This will pass a FileInfo object down the pipe, which Remove-Item takes as piped input and will remove the files in question.\nIf you want to include sub folders of your path you would add the -Recurse switch to your Get-ChildItem command, and it would look like this:\nGet-ChildItem $Path -Recurse | Where{$_.Name -Match \"<RegEx Pattern>\"} | Remove-Item\n\nIf you only want to delete files you can specify that in the Where statement by looking at the FileInfo object's PSIsContainer property and inverting it by prefixing the object with an exclamation point like such:\nGet-ChildItem $Path -Recurse | Where{$_.Name -Match \"<RegEx Pattern>\" -and !$_.PSIsContainer} | Remove-Item\n\n\n\nA14:\n\nYou could specify the directory to open when starting PowerShell:\npowershell.exe -NoExit -command \"& {Set-Location $env:systemroot}\"\n\nJust use it in your shortcut.\nOr use a profile to set a start directory.\n\n\nA15:\n\nUse PowerShell to do anything smarter for a DOS prompt. Here, I've shown how to batch rename all the files and directories in the current directory that contain spaces by replacing them with _ underscores.\nDir |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\nEDIT :\nOptionally, the Where-Object command can be used to filter out ineligible objects for the successive cmdlet (command-let). The following are some examples to illustrate the flexibility it can afford you:\n\nTo skip any document files\nDir |\nWhere-Object { $_.Name -notmatch \"\\.(doc|xls|ppt)x?$\" } |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\nTo process only directories (pre-3.0 version)\nDir |\nWhere-Object { $_.Mode -match \"^d\" } |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\nPowerShell v3.0 introduced new Dir flags. You can also use Dir -Directory there.\nTo skip any files already containing an underscore (or some other character)\nDir |\nWhere-Object { -not $_.Name.Contains(\"_\") } |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\n\n\n\nA16:\n\nThis batch file can help, but it has some limitations. The filename characters  = and % cannot be replaced (going from memory here) and an ^ in the filenames might be a problem too.\nIn this portion %newname: =_% on every line in the lower block it replaces the character after : with the character after = so as it stands the bunch of characters are going to be replaced with an underscore.\nRemove the echo to activate the ren command as it will merely print the commands to the console window until you do.\nIt will only process the current folder, unless you add /s to the DIR command portion and then it will process all folders under the current one too.\nTo delete a certain character, remove the character from after the = sign. In %newname:z=% an entry like this would remove all z characters (case insensitive).\n@echo off\nfor /f \"delims=\" %%a in ('dir /a:-d /o:n /b') do call :next \"%%a\"\npause\nGOTO:EOF\n:next\nset \"newname=%~nx1\"\n\nset \"newname=%newname: =_%\"\nset \"newname=%newname:)=_%\"\nset \"newname=%newname:(=_%\"\nset \"newname=%newname:&=_%\"\nset \"newname=%newname:^=_%\"\nset \"newname=%newname:$=_%\"\nset \"newname=%newname:#=_%\"\nset \"newname=%newname:@=_%\"\nset \"newname=%newname:!=_%\"\nset \"newname=%newname:-=_%\"\nset \"newname=%newname:+=_%\"\nset \"newname=%newname:}=_%\"\nset \"newname=%newname:{=_%\"\nset \"newname=%newname:]=_%\"\nset \"newname=%newname:[=_%\"\nset \"newname=%newname:;=_%\"\nset \"newname=%newname:'=_%\"\nset \"newname=%newname:`=_%\"\nset \"newname=%newname:,=_%\"\n\necho ren %1 \"%newname%\n\n\n\nA17:\n\nWrite-Output \"Set-Location C:\\\" >> $profile\n\n\n\nA18:\n\nYou can use the command,\nls -name | select-string -pattern \".*\\(\\d+\\).*\" | %{rm $_}\n\nWhere the content of the quotation marks is your regular expression. The regex in this example searches for files that have (#) in the file name, where # is any nonnegative integer. This is useful for deleting duplicates in a folder where the same set of files have been dumped many times, such as by a music manager.\nIf you add a -r after the -name\nls -name -r | select-string -pattern \".*\\(\\d+\\).*\" | %{rm $_}\n\nit will recurse through subfolders and delete matching files in all subfolders.\nThe structure of the command is as follows:\n\nls is an alias for the powershell command get-childitem. It lists all elements in the current folder. The -name argument specifies that only the names are to be produced; I don't want other information like file size.\nselect-string is mostly equivalent to UNIX grep, where it matches a pattern (regex) to a bunch of line-separated strings. The -pattern parameter sets the cmdlet up to take a regex.\n%{rm $} is a foreach loop. It is saying, \"for each line piped into me (from select-string in this case)\", do the following action, where $ is the given line. In this case we are rm-ing the item, where rm is an alias for Remove-Item.\n\n\n\nA19:\n\nSimplest way is to open Windows Powershell and click on the down arrow in the title bar to go to the Settings (you can use Ctrl+, as well). Make a window wider so you can see all the Profiles on the left side. Click on Windows Powershell profile and set your startup directory. Click Save at the bottom and you are done.\n\n\nA20:\n\nGet-Content (alias: gc) is your usual option for reading a text file. You can then filter further:\ngc log.txt | select -first 10 # head\ngc -TotalCount 10 log.txt     # also head\ngc log.txt | select -last 10  # tail\ngc -Tail 10 log.txt           # also tail (since PSv3), also much faster than above option\ngc log.txt | more             # or less if you have it installed\ngc log.txt | %{ $_ -replace '\\d+', '($0)' }         # sed\n\nThis works well enough for small files, larger ones (more than a few MiB) are probably a bit slow.\nThe PowerShell Community Extensions include some cmdlets for specialised file stuff (e.g. Get-FileTail).\n\n\nA21:\n\nFrom the documentation:\n\n/is Includes the same files.\n  /it Includes \"tweaked\" files.\n\n\"Same files\" means files that are identical (name, size, times, attributes). \"Tweaked files\" means files that have the same name, size, and times, but different attributes.\nrobocopy src dst sample.txt /is      # copy if attributes are equal\nrobocopy src dst sample.txt /it      # copy if attributes differ\nrobocopy src dst sample.txt /is /it  # copy irrespective of attributes\n\nThis answer on Super User has a good explanation of what kind of files the selection parameters match.\nWith that said, I could reproduce the behavior you describe, but from my understanding of the documentation and the output robocopy generated in my tests I would consider this a bug.\nPS C:\\temp> New-Item src -Type Directory >$null\nPS C:\\temp> New-Item dst -Type Directory >$null\nPS C:\\temp> New-Item src\\sample.txt -Type File -Value \"test001\" >$null\nPS C:\\temp> New-Item dst\\sample.txt -Type File -Value \"test002\" >$null\nPS C:\\temp> Set-ItemProperty src\\sample.txt -Name LastWriteTime -Value \"2016/1/1 15:00:00\"\nPS C:\\temp> Set-ItemProperty dst\\sample.txt -Name LastWriteTime -Value \"2016/1/1 15:00:00\"\nPS C:\\temp> robocopy src dst sample.txt /is /it /copyall /mir\n...\n  Options : /S /E /COPYALL /PURGE /MIR /IS /IT /R:1000000 /W:30\n\n------------------------------------------------------------------------------\n\n                           1    C:\\temp\\src\\\n            Modified                   7        sample.txt\n\n------------------------------------------------------------------------------\n\n               Total    Copied   Skipped  Mismatch    FAILED    Extras\n    Dirs :         1         0         0         0         0         0\n   Files :         1         1         0         0         0         0\n   Bytes :         7         7         0         0         0         0\n...\nPS C:\\temp> robocopy src dst sample.txt /is /it /copyall /mir\n...\n  Options : /S /E /COPYALL /PURGE /MIR /IS /IT /R:1000000 /W:30\n\n------------------------------------------------------------------------------\n\n                           1    C:\\temp\\src\\\n            Same                       7        sample.txt\n\n------------------------------------------------------------------------------\n\n               Total    Copied   Skipped  Mismatch    FAILED    Extras\n    Dirs :         1         0         0         0         0         0\n   Files :         1         1         0         0         0         0\n   Bytes :         7         7         0         0         0         0\n...\nPS C:\\temp> Get-Content .\\src\\sample.txt\ntest001\nPS C:\\temp> Get-Content .\\dst\\sample.txt\ntest002\nThe file is listed as copied, and since it becomes a same file after the first robocopy run at least the times are synced. However, even though seven bytes have been copied according to the output no data was actually written to the destination file in both cases despite the data flag being set (via /copyall). The behavior also doesn't change if the data flag is set explicitly (/copy:d).\nI had to modify the last write time to get robocopy to actually synchronize the data.\nPS C:\\temp> Set-ItemProperty src\\sample.txt -Name LastWriteTime -Value (Get-Date)\nPS C:\\temp> robocopy src dst sample.txt /is /it /copyall /mir\n...\n  Options : /S /E /COPYALL /PURGE /MIR /IS /IT /R:1000000 /W:30\n\n------------------------------------------------------------------------------\n\n                           1    C:\\temp\\src\\\n100%        Newer                      7        sample.txt\n\n------------------------------------------------------------------------------\n\n               Total    Copied   Skipped  Mismatch    FAILED    Extras\n    Dirs :         1         0         0         0         0         0\n   Files :         1         1         0         0         0         0\n   Bytes :         7         7         0         0         0         0\n...\nPS C:\\temp> Get-Content .\\dst\\sample.txt\ntest001\nAn admittedly ugly workaround would be to change the last write time of same/tweaked files to force robocopy to copy the data:\n& robocopy src dst /is /it /l /ndl /njh /njs /ns /nc |\n  Where-Object { $_.Trim() } |\n  ForEach-Object {\n    $f = Get-Item $_\n    $f.LastWriteTime = $f.LastWriteTime.AddSeconds(1)\n  }\n& robocopy src dst /copyall /mir\n\nSwitching to xcopy is probably your best option:\n& xcopy src dst /k/r/e/i/s/c/h/f/o/x/y\n\n\n\nA22:\n\nHere are the built-in ways to do head and tail. Don't use pipes because if you have a large file, it will be extremely slow. Using these built-in options will be extremely fast even for huge files.\ngc log.txt -head 10 \ngc log.txt -tail 10\ngc log.txt -tail 10 -wait # equivalent to tail -f\n\n\n\nA23:\n\nWith that, there seems to be some confusion on the \"working directory\" and PowerShell's \"location\". What most people here are doing, and saying to do is change PowerShell's \"location\". The \"working directory\" is actually different. Here is an article that explains it.\nFor those who don't want to read the article: Open PowerShell and use what others have said to do Set-Location \"C:\\some\\directory\". Notice that your \"working directory\" is still where your PowerShell was opened at. Either \"~\" or \"%SYSTEMROOT%\\system32\" depending on if you ran as administrator or not. To check the working directory, use [Environment]::CurrentDirectory.\nNote: in the article the author says to check the \"working directory\" by using this command:\n\\[Environment\\]::CurrentDirectory\n\nI am not sure if this works with older PowerShell versions, but with PowerShell 5 (and later) you have to use [Environment]::CurrentDirectory.\n\n\nA24:\n\nAren't there environment variables you can view on the other platforms for the OS?\nGet-ChildItem -Path Env:\n\nParticularly, on Windows at least, there's an OS environment variable, so you should be able to accomplish this by using $Env:OS.\n\nSince some time has passed and the PowerShell Core (v6) product is GA now (the Core branding has been dropped as of v7), you can more accurately determine your platform based on the following automatic boolean variables:\n$IsMacOS\n$IsLinux\n$IsWindows\n\n\n\nA25:\n\nCreate a PowerShell profile as follows. \n\nRun PowerShell as administrator and execute the following command:\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned\nThis will permit PowerShell to run local scripts and scripts downloaded from the Internet that have been signed. Read more about this command in the documentation.\nIn your Documents folder, find a folder named WindowsPowerShell for classic PowerShell or PowerShell for newer PowerShell Core. If it does not exist, that's ok; just create it.\nCreate a new file named profile.ps1 in the WindowsPowerShell folder (or PowerShell for PowerShell Core).\nOpen profile.ps1 and add the following command to set your default working directory:\nSet-Location C:\\my\\default\\working\\directory\n\nOpen a new PowerShell window... the changes should have taken effect.\n\n\n\nA26:\n\nI did this for a home folder where all the folders are on the desktops of the corresponding users, reachable through a shortcut which did not have the appropriate permissions, so that users couldn't see it even if it was there. So I used Robocopy with the parameter to overwrite the file with the right settings:\nFOR /F \"tokens=*\" %G IN ('dir /b') DO robocopy  \"\\\\server02\\Folder with shortcut\" \"\\\\server02\\home\\%G\\Desktop\" /S /A /V /log+:C:\\RobocopyShortcut.txt /XF *.url *.mp3 *.hta *.htm *.mht *.js *.IE5 *.css *.temp *.html *.svg *.ocx *.3gp *.opus *.zzzzz *.avi *.bin *.cab *.mp4 *.mov *.mkv *.flv *.tiff *.tif *.asf *.webm *.exe *.dll *.dl_ *.oc_ *.ex_ *.sy_ *.sys *.msi *.inf *.ini *.bmp *.png *.gif *.jpeg *.jpg *.mpg *.db *.wav *.wma *.wmv *.mpeg *.tmp *.old *.vbs *.log *.bat *.cmd *.zip /SEC /IT /ZB /R:0\n\nAs you see there are many file types which I set to ignore (just in case), just set them for your needs or your case scenario.\nIt was tested on Windows Server 2012, and every switch is documented on Microsoft's sites and others.\n\n\nA27:\n\nI had tried the above answers in Windows Server 2016 without success.\nBut I found this approach (it should be the same for Windows\u00a010) working for me.\n\nStart a PowerShell session\nIn the Taskbar, right-click and pin to keep a link there\nAgain right click the icon in taskbar and then right-click Windows PowerShell and choose Properties\nEnter your preferred directory in the Start in: input field and press OK\nStart from the taskbar icon\n\nDone!\nIn the same Properties dialog you can also change many other settings like fonts, colors, sizes and on the Shortcut tab there via button Advanced. You can select if that PowerShell session is to be run with administrator privileges.\n\n\nA28:\n\nFirst you can reach till that folder:\ncd 'C:\\MyDirectory'\nand then use:\n./MyCommand.cmd\n\n\nA29:\n\nTo run or convert batch files to PowerShell (particularly if you wish to sign all your scheduled task scripts with a certificate) I simply create a PowerShell script, for example, deletefolders.ps1.\nInput the following into the script:\ncmd.exe /c \"rd /s /q C:\\#TEMP\\test1\"\n\ncmd.exe /c \"rd /s /q C:\\#TEMP\\test2\"\n\ncmd.exe /c \"rd /s /q C:\\#TEMP\\test3\"\n\n*Each command needs to be put on a new line, calling cmd.exe again.\nThis script can now be signed and run from PowerShell outputting the commands to command prompt / cmd directly.\nIt is a much safer way than running batch files!\n\n\nA30:\n\nType this in PowerShell:\nNew-Item -path $profile -type file \u2013force\n\nIt creates a .ps1 file in the PowerShell folder. Open it, and edit it as:\nSet-location C:\\files\n\nDone\nRefer to this link. It works fine.\nChange PowerShell Start Directory\n\n\nA31:\n\nUsing just the command line, if a file exists already it will append to it:\n$(if (-Not (Test-Path ~\\Documents\\WindowsPowerShell\\)){ mkdir ~\\Documents\\WindowsPowerShell\\}) ; echo \"Set-Location c:\\THELOCATIONYOUWANT\" >> ~\\Documents\\WindowsPowerShell\\profile.ps1\n\n\n\nA32:\n\nTry invoking cmd /c C:\\MyDirectory\\MyCommand.cmd \u2013 that should work.\n\n\nA33:\n\n\nOpen file Microsoft.PowerShell_profile under C:\\Users\\yourusername\\Documents\\PowerShell\n\nAdd the following line:\nset-location \"C:\\Whatever\\path\\you\\want\\to\\set\\as\\worrkingdir\\\"\n\n\nRelaunch PowerShell\n\n\n\n\nA34:\n\nGo to C:\\MyDirectory and try this:\n.\\MyCommand.cmd\n\n\n\nA35:\n\nPutting Set-Location into your profile will unconditionally change the current working directory, which might have unwanted consequences in regards to the working directory for scripts that you execute via \"run with PowerShell\".\nAn alternative solution is to change the working directory for the .lnk files to PowerShell usually found in %USERPROFILE%\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Windows PowerShell. Right click on a link, and change the working directory from %HOMEDRIVE%%HOMEPATH% to the directory you want.\n\n\nA36:\n\nIn windows 11 I could fix this by setting the directory in the shortcut properties. Right click on Powershell in the taskbar, select properties and change the WorkingDirectory flag (default it was set to ~)\n\n\n\nA37:\n\nThis solution sets current working folder to location where script is located.  Be sure to place at beginning of your script, or at least before you try to use command relying on location path.\nSet-Location (Split-Path $MyInvocation.MyCommand.Path)\n\n\n\nA38:\n\nMake this the first line in your Profile.ps1 and PowerShell Core (pwsh) will open in the directory you are currently working in:\nset-location (get-location).path\n\n\n\nA39:\n\nSince the PowerShell versions 6.1 on Windows/Linux/OSX went to GA you can use the new properties of $PSVersionTable, OS, Platform and GitCommitId\nUpdate In v6.0.0-beta.3 there are some breaking changes:\n\nChange positional parameter for powershell.exe from -Command to -File\n\n\n$PSVersionTable on :  \nPlatform Win32NT OS Microsoft Windows 10.0.15063\nPS C:\\Users\\LotPings> $PSVersionTable\n\nName                           Value\n----                           -----\nPSVersion                      6.1.0\nPSEdition                      Core\nGitCommitId                    6.1.0\nOS                             Microsoft Windows 10.0.17134\nPlatform                       Win32NT\nPSCompatibleVersions           {1.0, 2.0, 3.0, 4.0...}\nPSRemotingProtocolVersion      2.3\nSerializationVersion           1.1.0.1\nWSManStackVersion              3.0\n\nPlatform Unix OS Linux (ubuntu)\nPS /home/LotPings> $PSVersionTable\n\nName                           Value\n----                           -----\nPSVersion                      6.1.0\nPSEdition                      Core\nGitCommitId                    6.1.0\nOS                             Linux 4.15.0-34-generic #37-Ubuntu SMP Mon Aug 27 15:21:48 UTC 2018\nPlatform                       Unix\nPSCompatibleVersions           {1.0, 2.0, 3.0, 4.0...}\nPSRemotingProtocolVersion      2.3\nSerializationVersion           1.1.0.1\nWSManStackVersion              3.0\n\nPlatform Unix OS Darwin\nPS /Users/LotPings> $PSVersionTable\n\nName                           Value\n----                           -----\nPSVersion                      6.1.0\nPSEdition                      Core\nGitCommitId                    6.1.0\nOS                             Darwin 17.7.0 Darwin Kernel Version 17.7.0: Thu Jun 21 22:53:14 PDT 2018; root:xnu-4570.71.2~1/RE...\nPlatform                       Unix\nPSCompatibleVersions           {1.0, 2.0, 3.0, 4.0...}\nPSRemotingProtocolVersion      2.3\nSerializationVersion           1.1.0.1\nWSManStackVersion              3.0\n\n\n\nA40:\n\nmore.exe exists on Windows, ports of less are easily found (and the PowerShell Community Extensions, PSCX, includes one).\nPowerShell doesn't really provide any alternative to separate programs for either, but for structured data Out-Grid can be helpful.\nHead and Tail can both be emulated with Select-Object using the -First and -Last parameters respectively.\nSed functions are all available but structured rather differently. The filtering options are available in Where-Object (or via Foreach-Object and some state for ranges). Other, transforming, operations can be done with Select-Object and Foreach-Object.\nHowever as PowerShell passes (.NET) objects \u2013 with all their typed structure, eg. dates remain DateTime instances \u2013 rather than just strings, which each command needs to parse itself, much of sed and other such programs are redundant.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 27, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\n\n\n\nI would like to create views or dataframes from an existing dataframe based on column selections.\nFor example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from it except two of them. I tried doing the following, but it didn't work:\nimport numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\n\n# Try to create a second dataframe df2 from df with all columns except 'B' and D\nmy_cols = set(df.columns)\nmy_cols.remove('B').remove('D')\n\n# This returns an error (\"unhashable type: set\")\ndf2 = df[my_cols]\n\nWhat am I doing wrong? Perhaps more generally, what mechanisms does pandas have to support the picking and exclusions of arbitrary sets of columns from a dataframe?\n\n\nCandidate answers:\nA1:\n\nI had the same question except that I wanted to combine the criteria into an OR condition.  The format given by Wouter Overmeire combines the criteria into an AND condition such that both must be satisfied:\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n\nBut I found that, if you wrap each condition in (... == True) and join the criteria with a pipe, the criteria are combined in an OR condition, satisfied whenever either of them is true: \ndf[((df.A==1) == True) | ((df.D==6) == True)]\n\n\n\nA2:\n\nYou just need to convert your set to a list\nimport pandas as pd\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\nmy_cols = set(df.columns)\nmy_cols.remove('B')\nmy_cols.remove('D')\nmy_cols = list(my_cols)\ndf2 = df[my_cols]\n\n\n\nA3:\n\nSince version 0.18.1 the .loc method accepts a callable for selection. Together with lambda functions you can create very flexible chainable filters:\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\ndf.loc[lambda df: df.A == 80]  # equivalent to df[df.A == 80] but chainable\n\ndf.sort_values('A').loc[lambda df: df.A > 80].loc[lambda df: df.B > df.A]\n\nIf all you're doing is filtering, you can also omit the .loc.\n\n\nA4:\n\nIf you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows:\npd.DataFrame = apply_masks()\n\nUsage:\nA = pd.DataFrame(np.random.randn(4, 4), columns=[\"A\", \"B\", \"C\", \"D\"])\nA.le_mask(\"A\", 0.7).ge_mask(\"B\", 0.2)... (May be repeated as necessary\n\nIt's a little bit hacky but it can make things a little bit cleaner if you're continuously chopping and changing datasets according to filters.\nThere's also a general purpose filter adapted from Daniel Velkov above in the gen_mask function which you can use with lambda functions or otherwise if desired.\nFile to be saved (I use masks.py):\nimport pandas as pd\n\ndef eq_mask(df, key, value):\n    return df[df[key] == value]\n\ndef ge_mask(df, key, value):\n    return df[df[key] >= value]\n\ndef gt_mask(df, key, value):\n    return df[df[key] > value]\n\ndef le_mask(df, key, value):\n    return df[df[key] <= value]\n\ndef lt_mask(df, key, value):\n    return df[df[key] < value]\n\ndef ne_mask(df, key, value):\n    return df[df[key] != value]\n\ndef gen_mask(df, f):\n    return df[f(df)]\n\ndef apply_masks():\n\n    pd.DataFrame.eq_mask = eq_mask\n    pd.DataFrame.ge_mask = ge_mask\n    pd.DataFrame.gt_mask = gt_mask\n    pd.DataFrame.le_mask = le_mask\n    pd.DataFrame.lt_mask = lt_mask\n    pd.DataFrame.ne_mask = ne_mask\n    pd.DataFrame.gen_mask = gen_mask\n\n    return pd.DataFrame\n\nif __name__ == '__main__':\n    pass\n\n\n\nA5:\n\nYou can do it with numpy.eye and a using the array element selection mechanism:\nimport numpy as np\nnb_classes = 6\ndata = [[2, 3, 4, 0]]\n\ndef indices_to_one_hot(data, nb_classes):\n    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n    targets = np.array(data).reshape(-1)\n    return np.eye(nb_classes)[targets]\n\nThe the return value of indices_to_one_hot(nb_classes, data) is now\narray([[[ 0.,  0.,  1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.],\n        [ 1.,  0.,  0.,  0.,  0.,  0.]]])\n\nThe .reshape(-1) is there to make sure you have the right labels format (you might also have [[2], [3], [4], [0]]).\n\n\nA6:\n\nYou can use numpy.eye function.\nimport numpy as np\n\ndef one_hot_encode(x, n_classes):\n    \"\"\"\n    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n    : x: List of sample Labels\n    : return: Numpy array of one-hot encoded labels\n     \"\"\"\n    return np.eye(n_classes)[x]\n\ndef main():\n    list = [0,1,2,3,4,3,2,1,0]\n    n_classes = 5\n    one_hot_list = one_hot_encode(list, n_classes)\n    print(one_hot_list)\n\nif __name__ == \"__main__\":\n    main()\n\nResult\nD:\\Desktop>python test.py\n[[ 1.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.]\n [ 0.  0.  1.  0.  0.]\n [ 0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  1.]\n [ 0.  0.  0.  1.  0.]\n [ 0.  0.  1.  0.  0.]\n [ 0.  1.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.]]\n\n\n\nA7:\n\nIf you set your columns to search as indexes, then you can use DataFrame.xs() to take a cross section. This is not as versatile as the query answers, but it might be useful in some situations.\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(\n    np.random.randint(3, size=(10, 5)),\n    columns=list('ABCDE')\n)\n\ndf\n# Out[55]: \n#    A  B  C  D  E\n# 0  0  2  2  2  2\n# 1  1  1  2  0  2\n# 2  0  2  0  0  2\n# 3  0  2  2  0  1\n# 4  0  1  1  2  0\n# 5  0  0  0  1  2\n# 6  1  0  1  1  1\n# 7  0  0  2  0  2\n# 8  2  2  2  2  2\n# 9  1  2  0  2  1\n\ndf.set_index(['A', 'D']).xs([0, 2]).reset_index()\n# Out[57]: \n#    A  D  B  C  E\n# 0  0  2  2  2  2\n# 1  0  2  1  1  0\n\n\n\nA8:\n\nI offer this for additional examples.  This is the same answer as https://stackoverflow.com/a/28159296/ \nI'll add other edits to make this post more useful.\npandas.DataFrame.query\nquery was made for exactly this purpose.  Consider the dataframe df\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(\n    np.random.randint(10, size=(10, 5)),\n    columns=list('ABCDE')\n)\n\ndf\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\n\nLet's use query to filter all rows where D > B\ndf.query('D > B')\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n7  6  2  6  6  5\n\nWhich we chain\ndf.query('D > B').query('C > B')\n# equivalent to\n# df.query('D > B and C > B')\n# but defeats the purpose of demonstrating chaining\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n4  3  6  7  7  4\n5  5  3  7  5  9\n7  6  2  6  6  5\n\n\n\nA9:\n\nHere's how to create a copy of a DataFrame excluding a list of columns:\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\ndf2 = df.drop(['B', 'D'], axis=1)\n\nBut be careful! You mention views in your question, suggesting that if you changed df, you'd want df2 to change too. (Like a view would in a database.)\nThis method doesn't achieve that:\n>>> df.loc[0, 'A'] = 999 # Change the first value in df\n>>> df.head(1)\n     A         B         C         D\n0  999 -0.742688 -1.980673 -0.920133\n>>> df2.head(1) # df2 is unchanged. It's not a view, it's a copy!\n          A         C\n0  0.251262 -1.980673\n\nNote also that this is also true of @piggybox's method. (Although that method is nice and slick and Pythonic. I'm not doing it down!!)\nFor more on views vs. copies see this SO answer and this part of the Pandas docs which that answer refers to.\n\n\nA10:\n\nHere is a solution using DictVectorizer and the Pandas DataFrame.to_dict('records') method.\n>>> import pandas as pd\n>>> X = pd.DataFrame({'income': [100000,110000,90000,30000,14000,50000],\n                      'country':['US', 'CAN', 'US', 'CAN', 'MEX', 'US'],\n                      'race':['White', 'Black', 'Latino', 'White', 'White', 'Black']\n                     })\n\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> v = DictVectorizer()\n>>> qualitative_features = ['country','race']\n>>> X_qual = v.fit_transform(X[qualitative_features].to_dict('records'))\n>>> v.vocabulary_\n{'country=CAN': 0,\n 'country=MEX': 1,\n 'country=US': 2,\n 'race=Black': 3,\n 'race=Latino': 4,\n 'race=White': 5}\n\n>>> X_qual.toarray()\narray([[ 0.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  1.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.,  1.],\n       [ 0.,  1.,  0.,  0.,  0.,  1.],\n       [ 0.,  0.,  1.,  1.,  0.,  0.]])\n\n\n\nA11:\n\nI'm not entirely sure what you want, and your last line of code does not help either, but anyway:\n\"Chained\" filtering is done by \"chaining\" the criteria in the boolean index.\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n\nIf you want to chain methods, you can add your own mask method and use that one.\nIn [90]: def mask(df, key, value):\n   ....:     return df[df[key] == value]\n   ....:\n\nIn [92]: pandas.DataFrame.mask = mask\n\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\n\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [97]: df.mask('A', 1)\nOut[97]:\n   A  B  C  D\na  1  4  9  1\nd  1  3  9  6\n\nIn [98]: df.mask('A', 1).mask('D', 6)\nOut[98]:\n   A  B  C  D\nd  1  3  9  6\n\n\n\nA12:\n\nYou don't really need to convert that into a set:\ncols = [col for col in df.columns if col not in ['B', 'D']]\ndf2 = df[cols]\n\n\n\nA13:\n\nFilters can be chained using a Pandas query:\ndf = pd.DataFrame(np.random.randn(30, 3), columns=['a','b','c'])\ndf_filtered = df.query('a > 0').query('0 < b < 2')\n\nFilters can also be combined in a single query:\ndf_filtered = df.query('a > 0 and 0 < b < 2')\n\n\n\nA14:\n\nMuch easier to use Pandas for basic one-hot encoding. If you're looking for more options you can use scikit-learn.\nFor basic one-hot encoding with Pandas you pass your data frame into the get_dummies function.\nFor example, if I have a dataframe called imdb_movies:\n\n...and I want to one-hot encode the Rated column, I do this:\npd.get_dummies(imdb_movies.Rated)\n\n\nThis returns a new dataframe with a column for every \"level\" of rating that exists, along with either a 1 or 0 specifying the presence of that rating for a given observation.\nUsually, we want this to be part of the original dataframe. In this case, we attach our new dummy coded frame onto the original frame using \"column-binding.\nWe can column-bind by using Pandas concat function:\nrated_dummies = pd.get_dummies(imdb_movies.Rated)\npd.concat([imdb_movies, rated_dummies], axis=1)\n\n\nWe can now run an analysis on our full dataframe.\nSIMPLE UTILITY FUNCTION\nI would recommend making yourself a utility function to do this quickly:\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    return(res)\n\nUsage:\nencode_and_bind(imdb_movies, 'Rated')\n\nResult:\n\nAlso, as per @pmalbu comment, if you would like the function to remove the original feature_to_encode then use this version:\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) \n\nYou can encode multiple features at the same time as follows:\nfeatures_to_encode = ['feature_1', 'feature_2', 'feature_3',\n                      'feature_4']\nfor feature in features_to_encode:\n    res = encode_and_bind(train_set, feature)\n\n\n\nA15:\n\nSo the way I see it is that you do two things when sub-setting your data ready for analysis.\n\nget rows\nget columns\n\nPandas has a number of ways of doing each of these and some techniques that help get rows and columns. For new Pandas users it can be confusing as there is so much choice.\nDo you use iloc, loc, brackets, query, isin, np.where, mask etc...\nMethod chaining\nNow method chaining is a great way to work when data wrangling. In R they have a simple way of doing it, you select() columns and you filter() rows.\nSo if we want to keep things simple in Pandas why not use the filter() for columns and the query() for rows. These both return dataframes and so no need to mess-around with boolean indexing, no need to add df[ ] round the return value.\nSo what does that look like:-\ndf.filter(['col1', 'col2', 'col3']).query(\"col1 == 'sometext'\")\n\nYou can then chain on any other methods like groupby, dropna(), sort_values(), reset_index() etc etc.\nBy being consistent and using filter() to get your columns and query() to get your rows it will be easier to read your code when coming back to it after a time.\nBut filter can select rows?\nYes this is true but by default query() get rows and filter() get columns. So if you stick with the default there is no need to use the axis= parameter.\nquery()\nquery() can be used with both and/or &/| you can also use comparison operators > , < , >= , <=, ==, !=. You can also use Python in, not in.\nYou can pass a list to query using @my_list\nSome examples of using query to get rows\ndf.query('A > B')\n\ndf.query('a not in b')\n\ndf.query(\"series == '2206'\")\n\ndf.query(\"col1 == @mylist\")\n\ndf.query('Salary_in_1000 >= 100 & Age < 60 & FT_Team.str.startswith(\"S\").values')\n\nfilter()\nSo filter is basicly like using bracket df[] or df[[]] in that it uses the labels to select columns. But it does more than the bracket notation.\nfilter has like= param so as to help select columns with partial names.\ndf.filter(like='partial_name',)\n\nfilter also has regex to help with selection\ndf.filter(regex='reg_string')\n\nSo to sum up this way of working might not work for ever situation e.g. if you want to use indexing/slicing then iloc is the way to go. But this does seem to be a solid way of working and can simplify your workflow and code.\n\n\nA16:\n\npandas as has inbuilt function \"get_dummies\" to get one hot encoding of that particular column/s.\none line code for one-hot-encoding:\ndf=pd.concat([df,pd.get_dummies(df['column name'],prefix='column name')],axis=1).drop(['column name'],axis=1)\n\n\n\nA17:\n\nOne hot encoding with pandas is very easy:\ndef one_hot(df, cols):\n    \"\"\"\n    @param df pandas DataFrame\n    @param cols a list of columns to encode \n    @return a DataFrame with one-hot encoding\n    \"\"\"\n    for each in cols:\n        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n        df = pd.concat([df, dummies], axis=1)\n    return df\n\nEDIT:\nAnother way to one_hot using sklearn's LabelBinarizer :\nfrom sklearn.preprocessing import LabelBinarizer \nlabel_binarizer = LabelBinarizer()\nlabel_binarizer.fit(all_your_labels_list) # need to be global or remembered to use it later\n\ndef one_hot_encode(x):\n    \"\"\"\n    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n    : x: List of sample Labels\n    : return: Numpy array of one-hot encoded labels\n    \"\"\"\n    return label_binarizer.transform(x)\n\n\n\nA18:\n\nMy answer is similar to the others. If you do not want to create a new function you can use what pandas has defined for you already. Use the pipe method.\ndf.pipe(lambda d: d[d['column'] == value])\n\n\n\nA19:\n\nApproach 1: You can use pandas' pd.get_dummies.\nExample 1:\nimport pandas as pd\ns = pd.Series(list('abca'))\npd.get_dummies(s)\nOut[]: \n     a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n\nExample 2:\nThe following will transform a given column into one hot. Use prefix to have multiple dummies.\nimport pandas as pd\n        \ndf = pd.DataFrame({\n          'A':['a','b','a'],\n          'B':['b','a','c']\n        })\ndf\nOut[]: \n   A  B\n0  a  b\n1  b  a\n2  a  c\n\n# Get one hot encoding of columns B\none_hot = pd.get_dummies(df['B'])\n# Drop column B as it is now encoded\ndf = df.drop('B',axis = 1)\n# Join the encoded df\ndf = df.join(one_hot)\ndf  \nOut[]: \n       A  a  b  c\n    0  a  0  1  0\n    1  b  1  0  0\n    2  a  0  0  1\n\nApproach 2: Use Scikit-learn\nUsing a OneHotEncoder has the advantage of being able to fit on some training data and then transform on some other data using the same instance. We also have handle_unknown to further control what the encoder does with unseen data.\nGiven a dataset with three features and four samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding.\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> enc = OneHotEncoder()\n>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])   \nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n   handle_unknown='error', n_values='auto', sparse=True)\n>>> enc.n_values_\narray([2, 3, 4])\n>>> enc.feature_indices_\narray([0, 2, 5, 9], dtype=int32)\n>>> enc.transform([[0, 1, 1]]).toarray()\narray([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])\n\nHere is the link for this example: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n\n\nA20:\n\nJust want to add a demonstration using loc to filter not only by rows but also by columns and some merits to the chained operation.\nThe code below can filter the rows by value.\ndf_filtered = df.loc[df['column'] == value]\n\nBy modifying it a bit you can filter the columns as well.\ndf_filtered = df.loc[df['column'] == value, ['year', 'column']]\n\nSo why do we want a chained method? The answer is that it is simple to read if you have many operations. For example,\nres =  df\\\n    .loc[df['station']=='USA', ['TEMP', 'RF']]\\\n    .groupby('year')\\\n    .agg(np.nanmean)\n\n\n\nA21:\n\nThis solution is more hackish in terms of implementation, but I find it much cleaner in terms of usage, and it is certainly more general than the others proposed.\nhttps://github.com/toobaz/generic_utils/blob/master/generic_utils/pandas/where.py\nYou don't need to download the entire repo: saving the file and doing\nfrom where import where as W\n\nshould suffice. Then you use it like this:\ndf = pd.DataFrame([[1, 2, True],\n                   [3, 4, False], \n                   [5, 7, True]],\n                  index=range(3), columns=['a', 'b', 'c'])\n# On specific column:\nprint(df.loc[W['a'] > 2])\nprint(df.loc[-W['a'] == W['b']])\nprint(df.loc[~W['c']])\n# On entire - or subset of a - DataFrame:\nprint(df.loc[W.sum(axis=1) > 3])\nprint(df.loc[W[['a', 'b']].diff(axis=1)['b'] > 1])\n\nA slightly less stupid usage example:\ndata = pd.read_csv('ugly_db.csv').loc[~(W == '$null$').any(axis=1)]\n\nBy the way: even in the case in which you are just using boolean cols,\ndf.loc[W['cond1']].loc[W['cond2']]\n\ncan be much more efficient than\ndf.loc[W['cond1'] & W['cond2']]\n\nbecause it evaluates cond2 only where cond1 is True.\nDISCLAIMER: I first gave this answer elsewhere because I hadn't seen this.\n\n\nA22:\n\n\nThis is unappealing as it requires I assign df to a variable before being able to filter on its values. \n\ndf[df[\"column_name\"] != 5].groupby(\"other_column_name\")\n\nseems to work: you can nest the [] operator as well. Maybe they added it since you asked the question.\n\n\nA23:\n\nYou can also leverage the numpy library for logical operations. Its pretty fast. \ndf[np.logical_and(df['A'] == 1 ,df['B'] == 6)]\n\n\n\nA24:\n\npandas provides two alternatives to Wouter Overmeire's answer which do not require any overriding. One is .loc[.] with a callable, as in\ndf_filtered = df.loc[lambda x: x['column'] == value]\n\nthe other is .pipe(), as in\ndf_filtered = df.pipe(lambda x: x.loc[x['column'] == value])\n\n\n\nA25:\n\nAlso have a look into the built-in DataFrame.filter function.\nMinimalistic but greedy approach (sufficient for the given df):\n\ndf.filter(regex=\"[^BD]\")\n\nConservative/lazy approach (exact matches only):\ndf.filter(regex=\"^(?!(B|D)$).*$\")\n\nConservative and generic:\nexclude_cols = ['B','C']\ndf.filter(regex=\"^(?!({0})$).*$\".format('|'.join(exclude_cols)))\n\n\n\nA26:\n\nYou can either Drop the columns you do not need OR Select the ones you need\n# Using DataFrame.drop\ndf.drop(df.columns[[1, 2]], axis=1, inplace=True)\n\n# drop by Name\ndf1 = df1.drop(['B', 'C'], axis=1)\n\n# Select the ones you want\ndf1 = df[['a','d']]\n\n\n\nA27:\n\nThe answer from @lodagro is great. I would extend it by generalizing the mask function as:\ndef mask(df, f):\n  return df[f(df)]\n\nThen you can do stuff like:\ndf.mask(lambda x: x[0] < 0).mask(lambda x: x[1] > 0)\n\n\n\nA28:\n\nFirstly, easiest way to one hot encode: use Sklearn.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\nSecondly, I don't think using pandas to one hot encode is that simple (unconfirmed though)\nCreating dummy variables in pandas for python\nLastly, is it necessary for you to one hot encode? One hot encoding exponentially increases the number of features, drastically increasing the run time of any classifier or anything else you are going to run. Especially when each categorical feature has many levels. Instead you can do dummy coding.\nUsing dummy encoding usually works well, for much less run time and complexity. A wise prof once told me, 'Less is More'. \nHere's the code for my custom encoding function if you want.\nfrom sklearn.preprocessing import LabelEncoder\n\n#Auto encodes any dataframe column of type category or object.\ndef dummyEncode(df):\n        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n        le = LabelEncoder()\n        for feature in columnsToEncode:\n            try:\n                df[feature] = le.fit_transform(df[feature])\n            except:\n                print('Error encoding '+feature)\n        return df\n\nEDIT: Comparison to be clearer:\nOne-hot encoding: convert n levels to n-1 columns.\nIndex  Animal         Index  cat  mouse\n  1     dog             1     0     0\n  2     cat       -->   2     1     0\n  3    mouse            3     0     1\n\nYou can see how this will explode your memory if you have many different types (or levels) in your categorical feature. Keep in mind, this is just ONE column.\nDummy Coding:\nIndex  Animal         Index  Animal\n  1     dog             1      0   \n  2     cat       -->   2      1 \n  3    mouse            3      2\n\nConvert to numerical representations instead. Greatly saves feature space, at the cost of a bit of accuracy.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "   (vs (vs ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 28, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nisJsonString('{ \"Id\": 1, \"Name\": \"Coke\" }')\n\nshould be true and\nisJsonString('foo')\nisJsonString('<div>foo</div>')\n\nshould be false.\nI'm looking for a solution that doesn't use try/catch because I have my debugger set to \"break on all errors\" and that causes it to break on invalid JSON strings.\n\n\nCandidate answers:\nA1:\n\nNPM: NPM stands for Node Package Manager and is the default package manager for Node.js. It was developed by Isaac Z. Schlueter and was originally released on January 12, 2010. It is entirely written in JavaScript. It consists of a command-line client npm which manages all node.js packages and modules. When node.js is installed, it is included in the installation.\nnpm run your-package-name\n\nNPX is a tool that use to execute packages.\nNPX is an acronym for Node Package Execute The NPX package comes with npm, so when you install npm above 5.2.0, NPX will be installed automatically.\nIt is an npm package runner that can execute any package that you want from the npm registry without even installing that package. The npx is useful during a single time use package. If you have installed npm below 5.2.0 then npx is not installed in your system.\nRun the following command to determine if npx is installed:\nnpx -v\n\nThe following command can be run if npx is not installed.\nnpm install -g npx\n\nUse npx to execute the package:\nnpx your-package-name\n\n\n\n\nA2:\n\nnpm - package manager\nnpx - Execute npm package\nThis is a difference with it.\n\n\nA3:\n\nArray.prototype.map() index:\nOne can access the index Array.prototype.map() via the second argument of the callback function. Here is an example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst map = array.map((x, index) => {\n  console.log(index);\n  return x + index;\n});\n\nconsole.log(map);\n\n\n\nOther arguments of Array.prototype.map():\n\nThe third argument of the callback function exposes the array on which map was called upon\nThe second argument of Array.map() is a object which will be the this value for the callback function. Keep in mind that you have to use the regular function keyword in order to declare the callback since an arrow function doesn't have its own binding to the this keyword.\n\nFor example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst thisObj = { prop1: 1 }\n\nconst map = array.map((x, index, array) => {\n  console.log(array);\n  console.log(this)\n}, thisObj);\n\n\n\n\n\nA4:\n\nIt means that the object you pass in the request (I guess it is pagedoc) has a circular reference, something like:\nvar a = {};\na.b = a;\n\nJSON.stringify cannot convert structures like this.\nN.B.: This would be the case with DOM nodes, which have circular references, even if they are not attached to the DOM tree. Each node has an ownerDocument which refers to document in most cases. document has a reference to the DOM tree at least through document.body and document.body.ownerDocument refers back to document again, which is only one of multiple circular references in the DOM tree.\n\n\nA5:\n\nUse a JSON parser like JSON.parse:\nfunction isJsonString(str) {\n    try {\n        JSON.parse(str);\n    } catch (e) {\n        return false;\n    }\n    return true;\n}\n\n\n\nA6:\n\nNPX is a tool for creating and executing some features in a new project\nNPM is the package manager that contains all of libraries\n\n\nA7:\n\nBased on zainengineer's answer... Another approach is to make a deep copy of the object and strip circular references and stringify the result.\n\n\nfunction cleanStringify(object) {\r\n    if (object && typeof object === 'object') {\r\n        object = copyWithoutCircularReferences([object], object);\r\n    }\r\n    return JSON.stringify(object);\r\n\r\n    function copyWithoutCircularReferences(references, object) {\r\n        var cleanObject = {};\r\n        Object.keys(object).forEach(function(key) {\r\n            var value = object[key];\r\n            if (value && typeof value === 'object') {\r\n                if (references.indexOf(value) < 0) {\r\n                    references.push(value);\r\n                    cleanObject[key] = copyWithoutCircularReferences(references, value);\r\n                    references.pop();\r\n                } else {\r\n                    cleanObject[key] = '###_Circular_###';\r\n                }\r\n            } else if (typeof value !== 'function') {\r\n                cleanObject[key] = value;\r\n            }\r\n        });\r\n        return cleanObject;\r\n    }\r\n}\r\n\r\n// Example\r\n\r\nvar a = {\r\n    name: \"a\"\r\n};\r\n\r\nvar b = {\r\n    name: \"b\"\r\n};\r\n\r\nb.a = a;\r\na.b = b;\r\n\r\nconsole.log(cleanStringify(a));\r\nconsole.log(cleanStringify(b));\n\n\n\n\n\nA8:\n\nHere's an example of what your app creation might look like using npx\n\nnpx create-react-app project-name --template all\n\n\n\nA9:\n\n\nsuppose you have an array like\n\n\n\n   const arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    \n    \n    arr.map((myArr, index) => {\n      console.log(`your index is -> ${index} AND value is ${myArr}`);\n    })\n\n\n\n> output will be\n index is -> 0 AND value is 1\n index is -> 1 AND value is 2\n index is -> 2 AND value is 3\n index is -> 3 AND value is 4\n index is -> 4 AND value is 5\n index is -> 5 AND value is 6\n index is -> 6 AND value is 7\n index is -> 7 AND value is 8\n index is -> 8 AND value is 9\n\n\n\nA10:\n\nSimple answer is like\nNPX: is used to execute any node package without installing the package on our machine.\nNPM: is used to install any node js package in our machine. We can use \"require(\"package-name')\" when we install any package using NPM. but we can not import the package when we use NPX.\nExample: You should run npm i axios\nin this case you are installing axios package in your local machine\nand npx create-react-app 'app-name'\nhere you are executing the create-react-app package directly on your machine without installing it's files.\n\n\nA11:\n\nI normally use the circular-json npm package to solve this.\n// Felix Kling's example\nvar a = {};\na.b = a;\n// load circular-json module\nvar CircularJSON = require('circular-json');\nconsole.log(CircularJSON.stringify(a));\n//result\n{\"b\":\"~\"}\n\nNote: circular-json has been deprecated, I now use flatted (from the creator of CircularJSON):\n// ESM\nimport {parse, stringify} from 'flatted/esm';\n\n// CJS\nconst {parse, stringify} = require('flatted/cjs');\n\nconst a = [{}];\na[0].a = a;\na.push(a);\n\nstringify(a); // [[\"1\",\"0\"],{\"a\":\"0\"}]\n\nfrom: https://www.npmjs.com/package/flatted\n\n\nA12:\n\nNPM stands for Node Package Manager. NPM is Node.JS's default package manager. It's written in Javascript. The role of NPM is to manage the package and modules of node.js.\nNPX stands for Node Package Execute. NPX comes with npm, when npm is installed above the 5.2.0 version, it gets installed automatically. NPX is an npm package runner and its role is to execute the package from the registry without even installing that package.\nNow, the differences between NPM and NPX are as below:\n\ni) NPM is used to install the packages while NPX is used to execute the packages.\n\n\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages which are used by npx don't need to be taken care of as they are not installed globally.\n\n\n\nA13:\n\nYou can use the javascript eval() function to verify if it's valid.\ne.g.\nvar jsonString = '{ \"Id\": 1, \"Name\": \"Coke\" }';\nvar json;\n\ntry {\n  json = eval(jsonString);\n} catch (exception) {\n  //It's advisable to always catch an exception since eval() is a javascript executor...\n  json = null;\n}\n\nif (json) {\n  //this is json\n}\n\nAlternatively, you can use JSON.parse function from json.org:\ntry {\n  json = JSON.parse(jsonString);\n} catch (exception) {\n  json = null;\n}\n\nif (json) {\n  //this is json\n}\n\nHope this helps.\nWARNING: eval() is dangerous if someone adds malicious JS code, since it will execute it. Make sure the JSON String is trustworthy, i.e. you got it from a trusted source.\nEdit For my 1st solution, it's recommended to do this.\n try {\n      json = eval(\"{\" + jsonString + \"}\");\n    } catch (exception) {\n      //It's advisable to always catch an exception since eval() is a javascript executor...\n      json = null;\n    }\n\nTo guarantee json-ness. If the jsonString isn't pure JSON, the eval will throw an exception.\n\n\nA14:\n\nnpm is a tool that use to install packages and npx is a tool that use to execute packages.\nnpm-If you wish to run package through npm then you have to specify that package in your package.json and install it locally.\nnpx-A package can be executable without installing the package. It is an npm package runner so if any packages aren\u2019t already installed it will install them automatically.\n\n\nA15:\n\nOne approach is to strip object and functions from main object. And stringify the simpler form\nfunction simpleStringify (object){\n    // stringify an object, avoiding circular structures\n    // https://stackoverflow.com/a/31557814\n    var simpleObject = {};\n    for (var prop in object ){\n        if (!object.hasOwnProperty(prop)){\n            continue;\n        }\n        if (typeof(object[prop]) == 'object'){\n            continue;\n        }\n        if (typeof(object[prop]) == 'function'){\n            continue;\n        }\n        simpleObject[prop] = object[prop];\n    }\n    return JSON.stringify(simpleObject); // returns cleaned up JSON\n};\n\nif you are using node js use inspect()\nimport {inspect} from \"util\";\nconsole.log(inspect(object));\n\n\n\nA16:\n\nIn my case I simply forgot to use async/await thing while building the route:\napp.get('/products', async (req, res) => {\n    const products = await Product.find();\n    res.send(products );\n});\n\n\n\nA17:\n\nA comment first. The question was about not using try/catch.\nIf you do not mind to use it, read the answer below.\nHere we just check a JSON string using a regexp, and it will work in most cases, not all cases.\nHave a look around the line 450 in https://github.com/douglascrockford/JSON-js/blob/master/json2.js\nThere is a regexp that check for a valid JSON, something like:\nif (/^[\\],:{}\\s]*$/.test(text.replace(/\\\\[\"\\\\\\/bfnrtu]/g, '@').\nreplace(/\"[^\"\\\\\\n\\r]*\"|true|false|null|-?\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d+)?/g, ']').\nreplace(/(?:^|:|,)(?:\\s*\\[)+/g, ''))) {\n\n  //the json is ok\n\n}else{\n\n  //the json is not ok\n\n}\n\nEDIT: The new version of json2.js makes a more advanced parsing than above, but still based on a regexp replace ( from the comment of @Mrchief )\n\n\nA18:\n\nSimple Definition:\nnpm - Javascript package manager \nnpx - Execute npm package binaries\n\n\nA19:\n\nIntroducing npx: an npm package runner\nNPM - Manages packages but doesn't make life easy executing any.NPX - A tool for executing Node packages.\n\nNPX comes bundled with NPM version 5.2+ \n\nNPM by itself does not simply run any package. It doesn't run any package as a matter of fact. If you want to run a package using NPM, you must specify that package in your package.json file.\nWhen executables are installed via NPM packages, NPM links to them:\n\nlocal installs have \"links\" created at ./node_modules/.bin/ directory.\nglobal installs have \"links\" created from the global bin/ directory (e.g. /usr/local/bin) on Linux or at %AppData%/npm on Windows.\n\nDocumentation you should read\n\nNPM:\nOne might install a package locally on a certain project:\nnpm install some-package\n\nNow let's say you want NodeJS to execute that package from the command line:\n$ some-package\n\nThe above will fail. Only globally installed packages can be executed by typing their name only.\nTo fix this, and have it run, you must type the local path:\n$ ./node_modules/.bin/some-package\n\nYou can technically run a locally installed package by editing your packages.json file and adding that package in the scripts section:\n{\n  \"name\": \"whatever\",\n  \"version\": \"1.0.0\",\n  \"scripts\": {\n    \"some-package\": \"some-package\"\n  }\n}\n\nThen run the script using npm run-script (or npm run):\nnpm run some-package\n\n\nNPX:\nnpx will check whether <command> exists in $PATH, or in the local project binaries, and execute it. So, for the above example, if you wish to execute the locally-installed package some-package all you need to do is type:\nnpx some-package\n\nAnother major advantage of npx is the ability to execute a package which wasn't previously installed:\n$ npx create-react-app my-app\n\nThe above example will generate a react app boilerplate within the path the command had run in, and ensures that you always use the latest version of a generator or build tool without having to upgrade each time you\u2019re about to use it.\n\nUse-Case Example:\nnpx command may be helpful in the script section of a package.json file,\nwhen it is unwanted to define a dependency which might not be commonly used or any other reason:\n\"scripts\": {\n    \"start\": \"npx gulp@3.9.1\",\n    \"serve\": \"npx http-server\"\n}\n\nCall with: npm run serve\n\nRelated questions:\n\nHow to use package installed locally in node_modules?\nNPM: how to source ./node_modules/.bin folder?\nHow do you run a js file using npm scripts?\n\n\n\nA20:\n\nNPM => Is a JS package manager.\nNPX => Is a tool for executing Node packages and execute npm package binaries.\nIt is easy to remember:\n-npm stands for MANAGER\n-npx stands for EXECUTE\n\n\nA21:\n\nwith npx:\nnpx create-react-app MyApp\n\nearlier:\nnpm install -g create-react-app\ncreate-react-app MyApp\n\nEarlier it was necessary to install a CLI tool locally before using it, with npx that is not required anymore.\n\n\nA22:\n\nSimply npm is the Node Package Manager and\nnpx is the executeable version that run npm packages\n\n\nA23:\n\nYou will be able to get the current iteration's index for the map method through its 2nd parameter.\nExample:\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nlist.map((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return currElement; //equivalent to list[index]\n});\n\nOutput:\nThe current iteration is: 0 <br>The current element is: h\n\nThe current iteration is: 1 <br>The current element is: e\n\nThe current iteration is: 2 <br>The current element is: l\n\nThe current iteration is: 3 <br>The current element is: l \n\nThe current iteration is: 4 <br>The current element is: o\n\nSee also: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array/map\n\nParameters\ncallback - \n    Function that produces an element of the new Array, taking three arguments:\n1) currentValue\n        The current element being processed in the array.\n2) index\n      The index of the current element being processed in the array.\n3) array\n      The array map was called upon.\n\n\n\nA24:\n\nnpm is package manager or installer on the other hand Packages used by npx are not installed globally so you have to carefree for the pollution for the long term.\n\n\nA25:\n\nHere's an example of NPX in action: npx cowsay hello\nIf you type that into your bash terminal you'll see the result. The benefit of this is that npx has temporarily installed cowsay. There is no package pollution since cowsay is not permanently installed. This is great for one off packages where you want to avoid package pollution. \nAs mentioned in other answers, npx is also very useful in cases where (with npm) the package needs to be installed then configured before running. E.g. instead of using npm to install and then configure the json.package file and then call the configured run command just use npx instead. A real example:\nnpx create-react-app my-app\n\n\nA26:\n\nNPM vs. NPX\nNPM stands for the Node Package Manager. A text based program for Nodejs package management.\nWhile NPX is a Node Package Runner. Its function is to execute the Nodejs package\nNPX will execute binary files from the Nodejs package, both installed and not.\nEven NPX can also help us use certain versions of Nodejs without having to use nvm (node.js version management), nave (node.js virtual environment), and nvm (node.js version management).\n\n\nA27:\n\nNPM - NODE PACKAGE MANAGER\n\nit's a package manager.\n\nnode.js  package can be installed using NPM\n\n\nNPX - NODE PACKAGE EXECUTE\n\nit execute the  node.js  package\n\ndoesn't matter whether you installed  node.js  package locally or\nglobally.\n\nNPX will temporarily install package and run it.\n\nNPX is used when you want to execute package for single time.\n\n\n.\n\n\nA28:\n\nIn prototypeJS, we have method isJSON. You can try that. Even json might help.\n\"something\".isJSON();\n// -> false\n\"\\\"something\\\"\".isJSON();\n// -> true\n\"{ foo: 42 }\".isJSON();\n// -> false\n\"{ \\\"foo\\\": 42 }\".isJSON();\n// -> true\n\n\n\nA29:\n\nNPX:\nFrom https://www.futurehosting.com/blog/npx-makes-life-easier-for-node-developers-plus-node-vulnerability-news/:\n\nWeb developers can have dozens of projects on their development\n  machines, and each project has its own particular set of npm-installed\n  dependencies. A few years back, the usual advice for dealing with CLI\n  applications like Grunt or Gulp was to install them locally in each\n  project and also globally so they could easily be run from the command\n  line.\nBut installing globally caused as many problems as it solved. Projects\n  may depend on different versions of command line tools, and polluting\n  the operating system with lots of development-specific CLI tools isn\u2019t\n  great either. Today, most developers prefer to install tools locally\n  and leave it at that.\nLocal versions of tools allow developers to pull projects from GitHub\n  without worrying about incompatibilities with globally installed\n  versions of tools. NPM can just install local versions and you\u2019re good\n  to go. But project specific installations aren\u2019t without their\n  problems: how do you run the right version of the tool without\n  specifying its exact location in the project or playing around with\n  aliases?\nThat\u2019s the problem npx solves. A new tool included in NPM 5.2, npx is\n  a small utility that\u2019s smart enough to run the right application when\n  it\u2019s called from within a project.\nIf you wanted to run the project-local version of mocha, for example,\n  you can run npx mocha inside the project and it will do what you\n  expect.\nA useful side benefit of npx is that it will automatically install npm\n  packages that aren\u2019t already installed. So, as the tool\u2019s creator Kat\n  March\u00e1n points out, you can run npx benny-hill without having to deal\n  with Benny Hill polluting the global environment.\nIf you want to take npx for a spin, update to the most recent version\n  of npm.\n\n\n\nA30:\n\nnpx is a npm package runner (x probably stands for eXecute). One common way to use npx is to download and run a package temporarily or for trials.\ncreate-react-app is an npm package that is expected to be run only once in a project's lifecycle. Hence, it is preferred to use npx to install and run it in a single step.\nAs mentioned in the main page https://www.npmjs.com/package/npx, npx can run commands in the PATH or from node_modules/.bin by default.\nNote:\nWith some digging, we can find that create-react-app points to a Javascript file (possibly to /usr/lib/node_modules/create-react-app/index.js on Linux systems) that is executed within the node environment. This is simply a global tool that does some checks. The actual setup is done by react-scripts, whose latest version is installed in the project. Refer https://github.com/facebook/create-react-app for more info.\n\n\nA31:\n\nFor my case I was getting that error when I was using async function on my server-side to fetch documents using mongoose. It turned out that the reason was I forgot to put await before calling find({}) method. Adding that part fixed my issue. \n\n\nA32:\n\nThe differences between NPM and NPX are as below:\ni) NPM is used to install the packages while NPX is used to execute the packages.\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages used by npx don't need to be taken care of as they are not installed globally.\n\n\nA33:\n\nNPM stands for Node Package Manager.\nIt comes pre-installed with Node.js. NPM helps to manage packages in your projects as dependencies.\nWhen using NPM, there are two ways to install a package into your local computer.\n\nLocally: When a package is installed locally, it is installed in\n./node_modules/.bin/ of the local project directory.\n\nGlobally: A global package is installed in the user environment\npath. /usr/local/bin for Linux and AppData%/npm for Windows.\n\n\nTo execute a locally installed package, it should be specified in the package.json scripts block as shown below.\n\"scripts\": {\n    \"your-package\":  \"your-package-name\"\n}\n\nThen, you can execute the package with:\nnpm run your-package-name\n\nNPX is an NPM package executor.\nCurrently, NPX is bundled with NPM when you install the NPM version 5.2.0 or higher.\nWhy NPX over NPM?\n\nNo need to edit the package.json file with node_modules paths.\n\nYou can directly execute the tool from the command line.\n\n\n\n\nA34:\n\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages. \nIt doesn't matter whether you installed that package globally or locally. NPX will temporarily install it and run it. NPM also can run packages if you configure a package.json file and include it in the script section. \n\nSo remember this, if you want to check/run a node package  quickly without installing locally or globally use NPX.\n\nnpM -  Manager\nnpX -  Execute - easy to remember\n\n\nA35:\n\nI resolve this problem on NodeJS like this:\nvar util = require('util');\n\n// Our circular object\nvar obj = {foo: {bar: null}, a:{a:{a:{a:{a:{a:{a:{hi: 'Yo!'}}}}}}}};\nobj.foo.bar = obj;\n\n// Generate almost valid JS object definition code (typeof string)\nvar str = util.inspect(b, {depth: null});\n\n// Fix code to the valid state (in this example it is not required, but my object was huge and complex, and I needed this for my case)\nstr = str\n    .replace(/<Buffer[ \\w\\.]+>/ig, '\"buffer\"')\n    .replace(/\\[Function]/ig, 'function(){}')\n    .replace(/\\[Circular]/ig, '\"Circular\"')\n    .replace(/\\{ \\[Function: ([\\w]+)]/ig, '{ $1: function $1 () {},')\n    .replace(/\\[Function: ([\\w]+)]/ig, 'function $1(){}')\n    .replace(/(\\w+): ([\\w :]+GMT\\+[\\w \\(\\)]+),/ig, '$1: new Date(\"$2\"),')\n    .replace(/(\\S+): ,/ig, '$1: null,');\n\n// Create function to eval stringifyed code\nvar foo = new Function('return ' + str + ';');\n\n// And have fun\nconsole.log(JSON.stringify(foo(), null, 4));\n\n\n\nA36:\n\nnpm - Package manager for JavaScript, just like: pip (Python), Maven (Java), NuGet (.NET), Composer (PHP), RubyGems (Ruby), ...\nnpx - runs a command of a package without installing it explicitly.\nUse cases:\n\nYou don't want to install packages neither globally nor locally.\nYou don't have permission to install it globally.\nJust want to test some commands.\nSometime, you want to have a script command (generate, convert something, ...) in package.json to execute something without installing these packages as project's dependencies.\n\nSyntax:\nnpx [options] [-p|--package <package>] <command> [command-arg]...\n\nPackage is optional:\nnpx   -p uglify-js         uglifyjs --output app.min.js app.js common.js\n      +----------------+   +--------------------------------------------+\n      package (optional)   command, followed by arguments\n\nFor example:\nStart a HTTP Server      : npx http-server\nLint code                : npx eslint ./src\n                         # Run uglifyjs command in the package uglify-js\nMinify JS                : npx -p uglify-js uglifyjs -o app.min.js app.js common.js\nMinify CSS               : npx clean-css-cli -o style.min.css css/bootstrap.css style.css\nMinify HTML              : npx html-minifier index-2.html -o index.html --remove-comments --collapse-whitespace\nScan for open ports      : npx evilscan 192.168.1.10 --port=10-9999\nCast video to Chromecast : npx castnow http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\n\nMore about command:\n\nhttps://docs.npmjs.com/files/package.json#bin\nhttps://github.com/mishoo/UglifyJS2/blob/master/package.json#L17\n\n\n\nA37:\n\nHere is the simple definition.\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages.\n\n\nA38:\n\nSimplest Definition:\nNPX\n\nThe npx stands for Node Package Execute and it comes with the npm,\nwhen you installed npm above 5.2.0 version then automatically npx will\ninstalled. It is an npm package runner that can execute any package\nthat you want from the npm registry without even installing that\npackage.\n\nNPM\n\nnpm is a package manager for the JavaScript programming language\nmaintained by npm, Inc. npm is the default package manager for the\nJavaScript runtime environment Node.js. It consists of a command line\nclient, also called npm, and an online database of public and paid-for\nprivate packages\n\n\n\nA39:\n\nAs per the JSON docs at Mozilla, JSON.stringify has a second parameter replacer which can be used to filter/ignore children items while parsing the tree. However, perhaps you can avoid the circular references.\nIn Node.js we cannot. So we can do something like this:\nfunction censor(censor) {\n  var i = 0;\n  \n  return function(key, value) {\n    if(i !== 0 && typeof(censor) === 'object' && typeof(value) == 'object' && censor == value) \n      return '[Circular]'; \n    \n    if(i >= 29) // seems to be a harded maximum of 30 serialized objects?\n      return '[Unknown]';\n    \n    ++i; // so we know we aren't using the original object anymore\n    \n    return value;  \n  }\n}\n\nvar b = {foo: {bar: null}};\n\nb.foo.bar = b;\n\nconsole.log(\"Censoring: \", b);\n\nconsole.log(\"Result: \", JSON.stringify(b, censor(b)));\n\nThe result:\nCensoring:  { foo: { bar: [Circular] } }\nResult: {\"foo\":{\"bar\":\"[Circular]\"}}\n\nUnfortunately there seems to be a maximum of 30 iterations before it automatically assumes it's circular. Otherwise, this should work. I even used areEquivalent from here, but JSON.stringify still throws the exception after 30 iterations. Still, it's good enough to get a decent representation of the object at a top level, if you really need it. Perhaps somebody can improve upon this though? In Node.js for an HTTP request object, I'm getting:\n{\n\"limit\": null,\n\"size\": 0,\n\"chunks\": [],\n\"writable\": true,\n\"readable\": false,\n\"_events\": {\n    \"pipe\": [null, null],\n    \"error\": [null]\n},\n\"before\": [null],\n\"after\": [],\n\"response\": {\n    \"output\": [],\n    \"outputEncodings\": [],\n    \"writable\": true,\n    \"_last\": false,\n    \"chunkedEncoding\": false,\n    \"shouldKeepAlive\": true,\n    \"useChunkedEncodingByDefault\": true,\n    \"_hasBody\": true,\n    \"_trailer\": \"\",\n    \"finished\": false,\n    \"socket\": {\n        \"_handle\": {\n            \"writeQueueSize\": 0,\n            \"socket\": \"[Unknown]\",\n            \"onread\": \"[Unknown]\"\n        },\n        \"_pendingWriteReqs\": \"[Unknown]\",\n        \"_flags\": \"[Unknown]\",\n        \"_connectQueueSize\": \"[Unknown]\",\n        \"destroyed\": \"[Unknown]\",\n        \"bytesRead\": \"[Unknown]\",\n        \"bytesWritten\": \"[Unknown]\",\n        \"allowHalfOpen\": \"[Unknown]\",\n        \"writable\": \"[Unknown]\",\n        \"readable\": \"[Unknown]\",\n        \"server\": \"[Unknown]\",\n        \"ondrain\": \"[Unknown]\",\n        \"_idleTimeout\": \"[Unknown]\",\n        \"_idleNext\": \"[Unknown]\",\n        \"_idlePrev\": \"[Unknown]\",\n        \"_idleStart\": \"[Unknown]\",\n        \"_events\": \"[Unknown]\",\n        \"ondata\": \"[Unknown]\",\n        \"onend\": \"[Unknown]\",\n        \"_httpMessage\": \"[Unknown]\"\n    },\n    \"connection\": \"[Unknown]\",\n    \"_events\": \"[Unknown]\",\n    \"_headers\": \"[Unknown]\",\n    \"_headerNames\": \"[Unknown]\",\n    \"_pipeCount\": \"[Unknown]\"\n},\n\"headers\": \"[Unknown]\",\n\"target\": \"[Unknown]\",\n\"_pipeCount\": \"[Unknown]\",\n\"method\": \"[Unknown]\",\n\"url\": \"[Unknown]\",\n\"query\": \"[Unknown]\",\n\"ended\": \"[Unknown]\"\n}\n\nI created a small Node.js module to do this here: https://github.com/ericmuyser/stringy Feel free to improve/contribute!\n\n\nA40:\n\nUsing Ramda:\nimport {addIndex, map} from 'ramda';\n\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nconst mapIndexed = addIndex(map);\nmapIndexed((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return 'X';\n}, list);\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " X,   (i.e. \"The answer is: \" \"The answer is: \" \"The last thing is: \" \"The last thing is"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 29, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIf I have a multi-threaded program that reads a cache-type memory by reference. Can I change this pointer by the main thread without risking any of the other threads reading unexpected values.\nAs I see it, if the change is atomic the other threads will either read the older value or the newer value; never random memory (or null pointers), right?\nI am aware that I should probably use synchronisation methods anyway, but I'm still curious.\nAre pointer changes atomic?\nUpdate: My platform is 64-bit Linux (2.6.29), although I'd like a cross-platform answer as well :)\n\n\nCandidate answers:\nA1:\n\nIn practice, he's correct and a memory barrier is implied in this specific case. \nBut the point is that if its presence is \"debatable\", the code is already too complex and unclear.\nReally guys, use a mutex or other proper constructs. It's the only safe way to deal with threads and to write maintainable code.\nAnd maybe you'll see other errors, like that the code is unpredictable if send() is called more than one time.\n\n\nA2:\n\nThe only thing guaranteed by the standard is the sig_atomic_t type.\nAs you've seen from the other answers, it is likely to be OK when targeting generic x86 architecture, but very risky with more \"specialty\" hardware.\nIf you're really desperate to know, you can compare sizeof(sig_atomic_t) to sizeof(int*) and see what they are you your target system.\n\n\nA3:\n\nNo, it is not.  \nWeb archive link (original has gone dead)\n\n\nA4:\n\nA Mutex can be either local to a process or system-wide. MSDN:\n\nMutexes are of two types: local mutexes, which are unnamed, and named system mutexes. A local mutex exists only within your process.\n\nFurthermore, one should take special care - detailed on the same page as well - when using a system-wide mutex on a system with Terminal Services.\nOne of the differences between Mutex and lock is that Mutex utilizes a kernel-level construct, so synchronization will always require at least a user space-kernel space transition.\nlock - that is really a shortcut to the Monitor class, on the other hand tries to avoid allocating kernel resources and transitioning to kernel code (and is thus leaner & faster - if one has to find a WinAPI construct that it resembles, it would be CriticalSection).\nThe other difference is what others point out: a named Mutex can be used across processes.\nUnless one has special needs or requires synchronization across processes, it is just better to stick to lock (aka Monitor)\u02db\nThere are several other \"minor\" differences, like how abandonment is handled, etc.\nThe same can be said about ReaderWriterLock and ReaderWriterLockSlim in 3.5, Semaphore and the new SemaphoreSlim in .NET 4.0 etc.\nIt is true that the latter xxSlim classes cannot be used as a system-wide sync primitives, but they were never meant to - they were \"only\" meant to be faster and more resource friendly.\n\n\nA5:\n\n\nIntroduction to parallel computing: https://computing.llnl.gov/tutorials/parallel_comp/\nPOSIX threads programming: https://computing.llnl.gov/tutorials/pthreads/\n\n\n\nA6:\n\n\nIn practice, these tend to be implemented with CAS and LL/SC.\n(...and some spinning before giving up the time slice of the thread - usually by calling into a kernel function that switches context.)\nIf you only need a spinlock, wikipedia gives you an example which trades CAS for lock prefixed xchg on x86/x64. So in a strict sense, a CAS is not needed for crafting a spinlock - but some kind of atomicity is still required. In this case, it makes use of an atomic operation that can write a register to memory and return the previous contents of that memory slot in a single step. (To clarify a bit more: the lock prefix asserts the #LOCK signal that ensures that the current CPU has exclusive access to the memory. On todays CPUs it is not necessarily carried out this way, but the effect is the same. By using xchg we make sure that we will not get preempted somewhere between reading and writing, since instructions will not be interrupted half-way. So if we had an imaginary lock mov reg0, mem / lock mov mem, reg1 pair (which we don't), that would not quite be the same - it could be preempted just between the two movs.)\nOn current architectures, as pointed out in the comments, you mostly end up using the atomic primitives of the CPU and the coherency protocols provided by the memory subsystem.\nFor this reason, you not only have to use these primitives, but also account for the cache/memory coherency guaranteed by the architecture.\nThere may be implementation nuances as well. Considering e.g. a spinlock:\n\n\ninstead of a naive implementation, you should probably use e.g. a TTAS spin-lock with some exponential backoff, \non a Hyper-Threaded CPU, you should probably issue pause instructions that serve as  hints that you're spinning - so that the core you are running on can do something useful during this\nyou should really give up on spinning and yield control to other threads after a while\netc...\n\nthis is still user mode - if you are writing a kernel, you might have some other tools that you can use as well (since you are the one that schedules threads and handles/enables/disables interrupts).\n\n\n\nA7:\n\nMutex is a cross process and there will be a classic example of not running more than one instance of an application.\n2nd example is say you are having a file and you don't want different process to access the same file , you can implement a Mutex but remember one thing Mutex is a operating system wide and cannot used between two remote process.\nLock is a simplest way to protect section of your code and it is appdomain specific , you can replace lock with Moniters if you want more controlled synchronization.\n\n\nA8:\n\nA lot has been said already, but to make it simple, here's my take.\nlock -> Simple to use, wrapper on monitor, locks across threads in an AppDomain.\nunnamed mutex -> similar to lock except locking scope is more and it's across AppDomain in a process.\nNamed mutex -> locking scope is even more than unnamed mutex and it's across process in an operating system.\nSo now options are there, you need to choose the one fits best in your case.\n\n\nA9:\n\nC++/CLI does have a lock class. All you need to do is declare a lock variable using stack-based semantics, and it will safely exit the monitor when its destructor is called, e.g.:\n#include <msclr\\lock.h>\n{    \n    msclr::lock l(m_lock);\n\n    // Do work\n\n} //destructor of lock is called (exits monitor).  \n\nm_lock declaration depends on whether you are synchronising access to an instance or static member. \nTo protect instance members, use this:\nObject^ m_lock = gcnew Object(); // Each class instance has a private lock - \n                                 // protects instance members.\n\nTo protect static members, use this:\nstatic Object^ m_lock = gcnew Object(); // Type has a private lock -\n                                        // protects static members.\n\n\n\nA10:\n\nIt depends on which implementation of the C runtime library you're using. If you're using MSVC for example then there's a compiler option which lets you specify which version of the library you want to build with (i.e. a run-time library that supports multi-threading by being tread-safe, or not).\n\n\nA11:\n\nmalloc and free are not reentrant, because they use a static data structure which records what memory blocks are free. As a result, no library functions that allocate or free memory are reentrant.\n\n\nA12:\n\nGCC Atomic Built-ins\n\n\nA13:\n\nIf you will be working with UNIX-like systems, then I recommend Programming With POSIX Threads by David R. Butenhof.\nIf you will be working with Microsoft Windows, then I recommend Writing Multithreaded Applications in Win32 by Jim Beveridge and Robert Wiener.\nIrrespective of which threading package(s) you will end up using, I recommend you look at two presentations I wrote: Generic Synchronization Policies and Multi-threaded Performance Pitfalls. Those short presentations contain useful information that, unfortunately, is not discussed in many other books and articles.\n\n\nA14:\n\nYes, under POSIX.1-2008 malloc is thread-safe.\n\n2.9.1 Thread-Safety\nAll functions defined by this volume of POSIX.1-2008 shall be thread-safe, except that the following functions1 need not be thread-safe.\n[ a list of functions that does not contain malloc ]\n\n\n\nA15:\n\nlock is a compiler keyword, not an actual class or object.  It's a wrapper around the functionality of the Monitor class and is designed to make the Monitor easier to work with for the common case.\nThe Monitor (and the lock keyword) are, as Darin said, restricted to the AppDomain.  Primarily because a reference to a memory address (in the form of an instantiated object) is required to manage the \"lock\" and maintain the identity of the Monitor\nThe Mutex, on the other hand, is a .Net wrapper around an operating system construct, and can be used for system-wide synchronization, using string data (instead of a pointer to data) as its identifier.  Two mutexes that reference two strings in two completely different memory addresses, but having the same data, will actually utilize the same operating-system mutex.\n\n\nA16:\n\nThis file is missing. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58016\nIt was fixed only in gcc 4.9, as its release notes says (http://gcc.gnu.org/gcc-4.9/changes.html)\n\n\nA17:\n\nQuestion: \"is malloc reentrant\"?\nAnswer: no, it is not. Here is one definition of what makes a routine reentrant.\nNone of the common versions of malloc allow you to re-enter it (e.g. from a signal handler). Note that a reentrant routine may not use locks, and almost all malloc versions in existence do use locks (which makes them thread-safe), or global/static variables (which makes them thread-unsafe and non-reentrant).\nAll the answers so far answer \"is malloc thread-safe?\", which is an entirely different question. To that question the answer is it depends on your runtime library, and possibly on the compiler flags you use. On any modern UNIX, you'll get a thread-safe malloc by default. On Windows, use /MT, /MTd, /MD or /MDd flags to get thread-safe runtime library.\n\n\nA18:\n\nThe cop-out answer is that the C spec does not require a pointer assignment to be atomic, so you can't count on it being atomic.\nThe actual answer would be that it probably depends on your platform, compiler, and possibly the alignment of the stars on the day you wrote the program.\n\n\nA19:\n\nIf you are working with GLIBC, the answer is: Yes, BUT.\nSpecifically, yes, BUT, please, please be aware that while malloc and free are thread-safe, the debugging functions are not.\nSpecifically, the extremely useful mtrace(), mcheck(), and mprobe() functions are not thread-safe.  In one of the shortest, straightest answers you will ever see from a GNU project, this is explained here:\nhttps://sourceware.org/bugzilla/show_bug.cgi?id=9939\nYou will need to consider alternate techniques, such as ElectricFence, valgrind, dmalloc, etc.\nSo, if you mean, \"are the malloc() and free() functions threadsafe\", the answer is yes.  But if you mean, \"is the entire malloc/free suite threadsafe\", the answer is NO.\n\n\nA20:\n\nI use a Mutex to check see if I already have a copy of the application running on the same machine.\nbool firstInstance;\nMutex mutex = new Mutex(false, @\"Local\\DASHBOARD_MAIN_APPLICATION\", out firstInstance);\n\nif (!firstInstance)\n{\n    //another copy of this application running \n}\nelse\n{\n    //run main application loop here.\n}\n// Refer to the mutex down here so garbage collection doesn't chuck it out.\nGC.KeepAlive(mutex);\n\n\n\nA21:\n\nI have also been looking for such a book, they are very hard to come by. This one will be released in May, if that's any help:\nhttp://www.manning.com/williams/\nI purchased this book:\nhttp://www.amazon.co.uk/gp/product/0123705916/ref=oss_product\nIt's very good, it's in java, but most of the principles apply to c/c++ anyway.\n\n\nA22:\n\nShort answer: yes, as of C11, which is the first version of the C standard that includes the concept of threads, malloc and friends are required to be thread-safe.  Many operating systems that included both threads and a C runtime made this guarantee long before the C standard did, but I'm not prepared to swear to all.  However, malloc and friends are not and never have been required to be reentrant.\nThat means, it is safe to call malloc and free from multiple threads simultaneously and not worry about locking, as long as you aren't breaking any of the other rules of memory allocation (e.g. call free once and only once on each pointer returned by malloc).  But it is not safe to call these functions from a signal handler that might have interrupted a call to malloc or free in the thread handling the signal.  Sometimes, using functionality beyond ISO C, you can guarantee that the thread handling the signal did not interrupt a call to malloc or free, e.g. with sigprocmask and sigpause, but try not to do that unless you have no other option, because it's hard to get perfectly right.\n\nLong answer with citations: The C standard added a concept of threads in the 2011 revision (link is to document N1570, which is the closest approximation to the official text of the 2011 standard that is publicly available at no charge).  In that revision, section 7.1.4 paragraph 5 states:\n\nUnless explicitly stated otherwise in the detailed descriptions that follow, library functions shall prevent data races as follows: A library function shall not directly or indirectly access objects accessible by threads other than the current thread unless the objects are accessed directly or indirectly via the function's arguments. A library function shall not directly or indirectly modify objects accessible by threads other than the current thread unless the objects are accessed directly or indirectly via the function's non-const arguments. Implementations may share their own internal objects between threads if the objects are not visible to users and are protected against data races.\n[footnote: This means, for example, that an implementation is not permitted to use a static object for internal purposes without synchronization because it could cause a data race even in programs that do not explicitly share objects between threads. Similarly, an implementation of memcpy is not permitted to copy bytes beyond the specified length of the destination object and then restore the original values because it could cause a data race if the program shared those bytes between threads.]\n\nAs I understand it, this is a long-winded way of saying that the library functions defined by the C standard are required to be thread-safe (in the usual sense: you can call them from multiple threads simultaneously, without doing any locking yourself, as long as they don't end up clashing on the data passed as arguments) unless the documentation for a specific function specifically says it isn't.\nThen, 7.22.3p2 confirms that malloc, calloc, realloc, aligned_alloc, and free in particular are thread-safe:\n\nFor purposes of determining the existence of a data race, memory allocation functions behave as though they accessed only memory locations accessible through their arguments and not other static duration storage. These functions may, however, visibly modify the storage that they allocate or deallocate. A call to free or realloc that deallocates a region p of memory synchronizes with any allocation call that allocates all or part of the region p. This synchronization occurs after any access of p by the deallocating function, and before any such access by the allocating function.\n\nContrast what it says about strtok, which is not and never has been thread-safe, in 7.24.5.8p6:\n\nThe strtok function is not required to avoid data races with other calls to the strtok function.\n[footnote: The strtok_s function can be used instead to avoid data races.]\n\n(comment on the footnote: don't use strtok_s, use strsep.)\nOlder versions of the C standard said nothing whatsoever about thread safety.  However, they did say something about reentrancy, because signals have always been part of the C standard.  And this is what they said, going back to the original 1989 ANSI C standard (this document has nigh-identical wording to, but very different section numbering from, the ISO C standard that came out the following year):\n\nIf [a] signal occurs other than as the result of calling the abort\nor raise function, the behavior is undefined if the signal handler\ncalls any function in the standard library other than the signal\nfunction itself or refers to any object with static storage duration\nother than by assigning a value to a static storage duration variable\nof type volatile sig_atomic_t .  Furthermore, if such a call to the\nsignal function results in a SIG_ERR return, the value of errno is\nindeterminate.\n\nWhich is a long-winded way of saying that C library functions are not required to be reentrant as a general rule.  Very similar wording still appears in C11, 7.14.1.1p5:\n\nIf [a] signal occurs other than as the result of calling the abort or raise function, the behavior is undefined if the signal handler refers to any object with static or thread storage duration that is not a lock-free atomic object other than by assigning a value to an object declared as volatile sig_atomic_t, or the signal handler calls any function in the standard library other than the abort function, the _Exit function, the quick_exit function, or the signal function with the first argument equal to the signal number corresponding to the signal that caused the invocation of the handler. Furthermore, if such a call to the signal function results in a SIG_ERR return, the value of errno is indeterminate.\n[footnote: If any signal is generated by an asynchronous signal handler, the behavior is undefined.]\n\nPOSIX requires a much longer, but still short compared to the overall size of the C library, list of functions to be safely callable from an \"asynchronous signal handler\", and also defines in more detail the circumstances under which a signal might \"occur other than as the result of calling the abort or raise function.\"  If you're doing anything nontrivial with signals, you are probably writing code intended to be run on an OS with the Unix nature (as opposed to Windows, MVS, or something embedded that probably doesn't have a complete hosted implementation of C in the first place), and you should familiarize yourself with the POSIX requirements for them, as well as the ISO C requirements.\n\n\nA23:\n\nIn practice, a function call is a compiler barrier, meaning that the compiler will not move global memory accesses past the call. A caveat to this is functions which the compiler knows something about, e.g. builtins, inlined functions (keep in mind IPO!) etc.\nSo a processor memory barrier (in addition to a compiler barrier) is in theory needed to make this work. However, since you're calling read and write which are syscalls that change the global state, I'm quite sure that the kernel issues memory barriers somewhere in the implementation of those. There is no such guarantee though, so in theory you need the barriers.\n\n\nA24:\n\nYou didn't mention a platform.  So I think a slightly more accurate question would be \n\nAre pointer changes guaranteed to be atomic?\n\nThe distinction is necessary because different C/C++ implementations may vary in this behavior.  It's possible for a particular platform to guarantee atomic assignments and still be within the standard.  \nAs to whether or not this is guaranteed overall in C/C++, the answer is No.  The C standard makes no such guarantees.  The only way to guarantee a pointer assignment is atomic is to use a platform specific mechanism to guarantee the atomicity of the assignment.  For instance the Interlocked methods in Win32 will provide this guarantee.  \nWhich platform are you working on?\n\n\nA25:\n\nThis is quite old question and I want to bring freshness according current state of things.\nYes, currently malloc() is thread-safe.\nFrom the GNU C Library Reference Manual of glibc-2.20 [released 2014-09-07]:\n\nvoid * malloc (size_t size)\nPreliminary: MT-Safe | ...\n...\n  1.2.2.1 POSIX Safety Concepts:\n... MT-Safe or Thread-Safe functions are safe to call in the presence\n  of other threads. MT, in MT-Safe, stands for Multi Thread.\nBeing MT-Safe does not imply a function is atomic, nor that it uses\n  any of the memory synchronization mechanisms POSIX exposes to users.\n  It is even possible that calling MT-Safe functions in sequence does\n  not yield an MT-Safe combination. For example, having a thread call\n  two MT-Safe functions one right after the other does not guarantee\n  behavior equivalent to atomic execution of a combination of both\n  functions, since concurrent calls in other threads may interfere in a\n  destructive way.\nWhole-program optimizations that could inline functions across library\n  interfaces may expose unsafe reordering, and so performing inlining\n  across the GNU C Library interface is not recommended. The documented\n  MT-Safety status is not guaranteed underwhole-program optimization.\n  However, functions defined in user-visible headers are designed to be\n  safe for inlining.\n\n\n\nA26:\n\nThe x86 architecture, has long had an instruction called xchg which will exchange the contents of a register with a memory location. xchg has always been atomic. \nThere has also always been a lock prefix that could be applied to any a single instruction to make that instruction atomic.  Before there were multi processor systems, all this really did was to prevent an interrupt from being delivered in the middle of a locked instruction. (xchg was implicitly locked).\nThis article has some sample code using xchg to implement a spinlock\nhttp://en.wikipedia.org/wiki/Spinlock\nWhen multi CPU and later multi Core systems began to be built, more sophisticated systems were needed to insure that lock and xchg would synchronize all of the memory subsystems, including l1 cache on all of the processors.  About this time, new research into locking and lockless algorithms showed that atomic CompareAndSet was a more flexible primitive to have, so more modern CPUs have that as an instruction. \nAddendum: In comments andras supplied a \"dusty old\" list of instructions which allow the lock prefix.  http://pdos.csail.mit.edu/6.828/2007/readings/i386/LOCK.htm\n\n\nA27:\n\nHere is an excerpt from malloc.c of glibc :\nThread-safety: thread-safe unless NO_THREADS is defined\nassuming NO_THREADS is not defined by default, malloc is thread safe at least on linux.\n\n\nA28:\n\nA lock is specific to the AppDomain, while Mutex to the Operating System allowing you to perform inter-process locking and synchronization (IPC).\n\n\nA29:\n\nAs others have mentioned, there is nothing in the C language that guarantees this, and it is dependent on your platform.\nOn most contemporary desktop platforms, the read/write to a word-sized, aligned location will be atomic.  But that really doesn't solve your problem, due to processor and compiler re-ordering of reads and writes.\nFor example, the following code is broken:\nThread A:\nDoWork();\nworkDone = 1;\n\nThread B:\nwhile(workDone != 0);\n\nReceiveResultsOfWork();\n\nAlthough the write to workDone is atomic, on many systems there is no guarantee by the processor that the write to workDone will be visible to other processors before writes done via DoWork() are visible.  The compiler may also be free to re-order the write to workDone to before the call to DoWork().  In both cases, ReceiveResultsOfWork() might start working on incomplete data.\nDepending on your platform, you may need to insert memory fences and so on to ensure proper ordering.  This can be very tricky to get right.\nOr just use locks.  Much simpler, much easier to verify as correct, and in most cases more than performant enough.\n\n\nA30:\n\nFew more minor differences which were not mentioned in the answers:\n\nIn the case of using locks, you can be sure that the lock will be released when an exception happens inside the lock's block.\nThat's because the lock uses monitors under the hood and is implemented this way:\n object __lockObj = x;\n bool __lockWasTaken = false;\n try\n {\n     System.Threading.Monitor.Enter(__lockObj, ref __lockWasTaken);\n     // Your code...\n }\n finally\n {\n     if (__lockWasTaken) System.Threading.Monitor.Exit(__lockObj);\n }\n\nThus, in any case, the lock is released, and you don't need to release it manually (like you'd do for the mutexes).\n\nFor Locks, you usually use a private object to lock (and should use).\nThis is done for many reasons. (More info: see this answer and official documentation).\n\n\nSo, in case of locks, you can't (accidentally gain) access to the locked object from the outside and cause some damage.\nBut in case of Mutex, you can, as it's common to have a Mutex which is marked public and used from anywhere.\n\n\nA31:\n\nTry Threading.Monitor. And catch.\n\n\nA32:\n\nThe C language says nothing about whether any operations are atomic. I've worked on microcontrollers with 8 bit buses and 16-bit pointers; any pointer operation on these systems would potentially be non-atomic. I think I remember Intel 386s (some of which had 16-bit buses) raising similar concerns. Likewise, I can imagine systems that have 64-bit CPUs, but 32-bit data buses, which might then entail similar concerns about non-atomic pointer operations. (I haven't checked to see whether any such systems actually exist.)\nEDIT: Michael's answer is well worth reading. Bus size vs. pointer size is hardly the only consideration regarding atomicity; it was simply the first counterexample that came to mind for me.\n\n\nA33:\n\nThe basic rule is: the compiler must make the global state appear to be exactly as you coded it, but if it can prove that a given function doesn't use global variables then it can implement the algorithm any way it chooses.\nThe upshot is that traditional compilers always treated functions in another compilation unit as a memory barrier because they couldn't see inside those functions. Increasingly, modern compilers are growing \"whole program\" or \"link time\" optimization strategies which break down these barriers and will cause poorly written code to fail, even though it's been working fine for years.\nIf the function in question is in a shared library then it won't be able to see inside it, but if the function is one defined by the C standard then it doesn't need to -- it already knows what the function does -- so you have to be careful of those also. Note that a compiler will not recognise a kernel call for what it is, but the very act of inserting something that the compiler can't recognise (inline assembler, or a function call to an assembler file) will create a memory barrier in itself.\nIn your case, notify will either be a black box the compiler can't see inside (a library function) or else it will contain a recognisable memory barrier, so you are most likely safe.\nIn practice, you have to write very bad code to fall over this.\n\n\nA34:\n\nMemory barriers aren't just to prevent instruction reordering. Even if instructions aren't reordered it can still cause problems with cache coherence. As for the reordering - it depends on your compiler and settings. ICC is particularly agressive with reordering. MSVC w/ whole program optimization can be, too. \nIf your shared data variable is declared as volatile, even though it's not in the spec most compilers will generate a memory variable around reads and writes from the variable and prevent reordering. This is not the correct way of using volatile, nor what it was meant for.\n(If I had any votes left, I'd +1 your question for the narration.)\n\n\nA35:\n\nThe Lock and Monitors are basically used to provide thread safety for threads that are generated by the application itself i.e. Internal Threads. On the other hand, Mutex ensures thread safety for threads that are generated by the external applications i.e. External Threads. Using Mutex, only one external thread can access our application code at any given point in time.\nread this\n\n\nA36:\n\nI suggest reading \n\n\u00a731.1 Thread Safety (and Reentrancy Revisited)\n\nof the book The Linux Programming Interface, it explains the difference between thread safety and reentrancy, as well as malloc.\nExcerpt\uff1a\n\nA function is said to be thread-safe if it can safely be invoked by\n  multiple threads at the same time; put conversely, if a function is\n  not thread-safe, then we can\u2019t call it from one thread while it is\n  being executed in another thread.\n....\n   This function illustrates the typical reason that a function is not\n  thread-safe: it employs global or static variables that are shared by all threads.\n...\n  Although the use of critical sections to implement thread safety is a significant\n  improvement over the use of per-function mutexes, it is still somewhat inefficient\n  because there is a cost to locking and unlocking a mutex. A reentrant function\n  achieves thread safety without the use of mutexes. It does this by avoiding the use\n  of global and static variables. \n...\n  However, not all functions can\n  be made reentrant. The usual reasons are the following:\n\nBy their nature, some functions must access global data structures. The functions in the malloc library provide a good example. These functions maintain a\n  global linked list of free blocks on the heap. The functions of the malloc library\n  are made thread-safe through the use of mutexes.  \n\n....\n\nDefinitely worth a read.\nAnd to answer your question, malloc is thread safe but not reentrant.\n\n\nA37:\n\nThe equivelent to a lock / SyncLock would be to use the Monitor class.\nIn .NET 1-3.5sp, lock(obj) does:\nMonitor.Enter(obj);\ntry\n{\n    // Do work\n}\nfinally\n{\n    Monitor.Exit(obj);\n}\n\nAs of .NET 4, it will be:\nbool taken = false;\ntry\n{\n    Monitor.Enter(obj, ref taken);\n    // Do work\n}\nfinally\n{\n    if (taken)\n    {\n        Monitor.Exit(obj);\n    }\n}\n\nYou could translate this to C++ by doing:\nSystem::Object^ obj = gcnew System::Object();\nMonitor::Enter(obj);\ntry\n{\n    // Do work\n}\nfinally\n{\n    Monitor::Exit(obj);\n}\n\n\n\nA38:\n\nI read somewhere that if you compile with -pthread, malloc becomes thread safe. I\u00b4m pretty sure its implementation dependant though, since malloc is ANSI C and threads are not.\nIf we are talking gcc:\n\nCompile and link with -pthread and\n  malloc() will be thread-safe, on x86\n  and AMD64.\n\nhttp://groups.google.com/group/comp.lang.c.moderated/browse_thread/thread/2431a99b9bdcef11/ea800579e40f7fa4\nAnother opinion, more insightful\n\n{malloc, calloc, realloc, free,\n  posix_memalign} of glibc-2.2+ are\n  thread safe\n\nhttp://linux.derkeiler.com/Newsgroups/comp.os.linux.development.apps/2005-07/0323.html\n\n\nA39:\n\nI like to think of thread synchronization as a bottom up where processor and operating system provide construct that are primitive to more sophisticated\nAt the processor level you have CAS and LL/SC which allow you to perform a test and store in a single atomic operation ... you also have other processor constructs that allow you to disable and enable interrupt (however they are considered dangerous ... under certain circumstances you have no other option but to use them)\noperating system provides the ability to context switch between tasks which can happen every time a thread has used its time slice ... or it can happen due to otgher reasons (I will come to that)\nthen there are higher level constructs like mutexes which uses these primitive mechanisms provided by processor (think spinning mutex) ... which will continuously wait for the condition to become true and checks for that condition atomically\nthen these spinning mutex can use the functionality provided by OS (context switch and system calls like yield which relinquishes the control to another thread) and gives us mutexes\nthese constructs are further utilized by higher level constructs like conditional variables (which can keep track of how many threads are waiting for the mutex and which thread to allow first when the mutex becomes available)\nThese constructs than can be further used to provide more sophisticated synchronization constructs ... example : semaphores etc\n\n\nA40:\n\nThere's no equivalent of the lock keyword in C++. You could do this instead:\nMonitor::Enter(instanceToLock);\ntry\n{\n    // Only one thread could execute this code at a time\n}\nfinally\n{\n    Monitor::Exit(instanceToLock);\n}\n\n\n\nA41:\n\nNo, it is not thread-safe. There may actually be a malloc_lock() and malloc_unlock() function available in your C library. I know that these exist for the Newlib library. I had to use this to implement a mutex for my processor, which is multi-threaded in hardware.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " (vs. \"I am not a programmer, I am a programmer, I am a programmer, I am a programmer, I am a, I am a"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 30, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI'm running this command on a batch file: \nfor %I in (*.txt *.doc) do copy %I c:\\test2\n\n...and it keeps returning:\n\nI was unexpected at this time.\n\nWhat is the cause of this error?\n\n\nCandidate answers:\nA1:\n\nFOR /F \"tokens=* USEBACKQ\" %%F IN (`command`) DO (\nSET var=%%F\n)\nECHO %var%\n\nI always use the USEBACKQ so that if you have a string to insert or a long file name, you can use your double quotes without screwing up the command.\nNow if your output will contain multiple lines, you can do this\nSETLOCAL ENABLEDELAYEDEXPANSION\nSET count=1\nFOR /F \"tokens=* USEBACKQ\" %%F IN (`command`) DO (\n  SET var!count!=%%F\n  SET /a count=!count!+1\n)\nECHO %var1%\nECHO %var2%\nECHO %var3%\nENDLOCAL\n\n\n\nA2:\n\nI'll assume you are talking about Windows, right? I don't believe you can change the icon of a batch file directly. Icons are embedded in .EXE and .DLL files, or pointed to by .LNK files.\nYou could try to change the file association, but that approach may vary based on the version of Windows you are using. This is down with the registry in XP, but I'm not sure about Vista.\n\n\nA3:\n\nIf you don't want to output to a temp file and then read into a variable, this code stores result of command direct into a variable:\nFOR /F %i IN ('findstr testing') DO set VARIABLE=%i\necho %VARIABLE%\n\nIf you want to enclose search string in double quotes:\nFOR /F %i IN ('findstr \"testing\"') DO set VARIABLE=%i\n\nIf you want to store this code in a batch file, add an extra % symbol:\nFOR /F %%i IN ('findstr \"testing\"') DO set VARIABLE=%%i\n\nA useful example to count the number of files in a directory & store in a variable:\n(illustrates piping)\nFOR /F %i IN ('dir /b /a-d \"%cd%\" ^| find /v /c \"?\"') DO set /a count=%i\n\nNote the use of single quotes instead of double quotes \" or grave accent ` in the command brackets. This is cleaner alternative to delims, tokens or usebackq in for loop.\nUpdate 27/8/2021:\nAnother method is to set errorlevel variable, although many would discourage setting errorlevel on large scripts or when new to cmd flavor of the installed OS variant.\nThis method works where (return) value to be stored fits in a 32-bit integer.\neg.\nto count the number of files in a directory & store in a variable called count:\n(dir /b /a-d ^| find /v /c \"?\") | (set /p myVar=& cmd /c exit /b %myVar%)\nset count=%errorlevel%\n\nVerbose - Win CMD integer limits tests:\n\nWin CMD arithmetic limit: 2147483647 (32-bit integers)\nStack overflow would continue count at -2147483648\nand reset again after reaching 2147483647\n\nREM See tests below:\ncmd /c exit /b 2147483647\necho %errorlevel%\n2147483647\n\ncmd /c exit /b 2147483648\necho %errorlevel%\n-2147483648\n\ncmd /c exit /b 2147483649\necho %errorlevel%\n-2147483647\n\nAbove method can be modified to return encoded strings to be decoded in parent process (within 32 bit limitation).\nA 3rd illustration, although of limited use (because the variable is set in child process, not parent) is:\n(dir /b /a-d ^| find /v /c \"?\") | (set /p myVar=& set myVar)\n\nIn this case the value of myVar is set to the number of files in the directory\nTested on Win 10 CMD.\n\n\nA4:\n\nActually, you don't even need a greater feature. All you need to do is add\ngoto homepagename\n\nThen you will be taken there if none of the if commands execute a goto command.\nFor example, this will fix your code:\n@echo off\n:Start2 \ncls\ngoto Start\n:Start\ntitle Frequently Used Websites\necho Please select a website from the list\necho with the corresponding key\necho --------------------------------------\necho [1] Google\necho [2] Wikipedia\necho [3] Facebook\necho [4] Youtube\necho [5] Yahoo\nset input=\nset /p input= Choice:\nif %input%==1 goto Z if NOT goto Start2\nif %input%==2 goto X if NOT goto Start2\nif %input%==3 goto C if NOT goto Start2\nif %input%==4 goto V if NOT goto Start2\nif %input%==5 goto B if NOT goto Start2\nif %input%>=6 goto N\ngoto Start\n\n\n\nA5:\n\nIf you want an icon for a batch file,\nfirst create a link for the batch file as follows\nRight click in window folder where you want the link\nselect New -> Shortcut, then specify where the .bat file is.\nThis creates the .lnk file you wanted.\nThen you can specify an icon for the link,\non its properties page.\nSome nice icons are available here:\n%SystemRoot%\\System32\\SHELL32.dll\nNote For me on Windows 10:\n%SystemRoot% == C:\\Windows\\ \nMore Icons are here:\nC:\\Windows\\System32\\imageres.dll\nAlso you might want to have the first line\nin the batch file to be \"cd ..\"\nif you stash your batch files in a bat subdirectory\none level below where your shortcuts,  are supposed to execute.\n\n\nA6:\n\nThe simple naive way that is slow because it opens and positions the file pointer to End-Of-File multiple times.\n@echo off\ncommand1 >output.txt\ncommand2 >>output.txt\n...\ncommandN >>output.txt\n\nA better way - easier to write, and faster because the file is opened and positioned only once.\n@echo off\n>output.txt (\n  command1\n  command2\n  ...\n  commandN\n)\n\nAnother good and fast way that only opens and positions the file once\n@echo off\ncall :sub >output.txt\nexit /b\n\n:sub\ncommand1\ncommand2\n...\ncommandN\n\nEdit 2020-04-17\nEvery now and then you may want to repeatedly write to two or more files. You might also want different messages on the screen. It is still possible to to do this efficiently by redirecting to undefined handles outside a parenthesized block or subroutine, and then use the & notation to reference the already opened files.\ncall :sub 9>File1.txt 8>File2.txt\nexit /b\n\n:sub\necho Screen message 1\n>&9 echo File 1 message 1\n>&8 echo File 2 message 1\necho Screen message 2\n>&9 echo File 1 message 2\n>&8 echo File 2 message 2\nexit /b\n\nI chose to use handles 9 and 8 in reverse order because that way is more likely to avoid potential permanent redirection due to a Microsoft redirection implementation design flaw when performing multiple redirections on the same command. It is highly unlikely, but even that approach could expose the bug if you try hard enough. If you stage the redirection than you are guaranteed to avoid the problem.\n3>File1.txt ( 4>File2.txt call :sub)\nexit /b\n\n:sub\netc.\n\n\n\nA7:\n\nTo read a file...\nset /P Variable=<File.txt\n\nTo Write a file\n@echo %DataToWrite%>File.txt\n\nnote; having spaces before the <> character causes a space to be added at the end of the variable, also\nTo add to a file,like a logger program,\nFirst make a file with a single enter key in it called e.txt\nset /P Data=<log0.log\nset /P Ekey=<e.txt\n@echo %Data%%Ekey%%NewData%>log0.txt\n\nyour log will look like this\nEntry1\nEntry2 \n\nand so on\nAnyways a couple useful things\n\n\nA8:\n\nYou could use a Bat to Exe converter from here:\nhttps://web.archive.org/web/20190304134631/http://www.f2ko.de/en/b2e.php\nThis will convert your batch file to an executable, then you can set the icon for the converted file.\n\n\nA9:\n\nSome notes and some tricks.\nThe 'official' way to assign result to a variable is with FOR /F though in the other answers is shown how a temporary file can be used also.\nFor command processing FOR command has two forms depending if the usebackq option is used. In the all examples below the whole output is used without splitting it.\nFOR /f \"tokens=* delims=\" %%A in ('whoami') do @set \"I-Am=%%A\"\nFOR /f \"usebackq tokens=* delims=\" %%A in (`whoami`) do @set \"I-Am=%%A\"\n\nand if used directly in the console:\nFOR /f \"tokens=* delims=\" %A in ('whoami') do set \"I-Am=%A\"\nFOR /f \"usebackq tokens=* delims=\" %A in (`whoami`) do set \"I-Am=%A\"\n\n%%A is a temporary variable available only on the FOR command context and is called token.The two forms can be useful in case when you are dealing with arguments containing specific quotes. It is especially useful with REPL interfaces of other languages or WMIC.\nThough in both cases the expression can be put in double quotes and it still be processed.\nHere's an example with python (it is possible to transition the expression in the brackets on a separate line which is used for easier reading):\n@echo off\n\nfor /f \"tokens=* delims=\" %%a in (\n  '\"python -c \"\"print(\"\"\"Message from python\"\"\")\"\"\"'\n) do (\n    echo processed message from python: \"%%a\"\n)\n\nTo use an assigned variable in the same FOR block check also the DELAYED EXPANSION\nAnd some tricks\nTo save yourself from writing all the arguments for the FOR command you can use MACRO for assigning the result to variable:\n@echo off\n\n::::: ---- defining the assign macro ---- ::::::::\nsetlocal DisableDelayedExpansion\n(set LF=^\n%=EMPTY=%\n)\nset ^\"\\n=^^^%LF%%LF%^%LF%%LF%^^\"\n\n::set argv=Empty\nset assign=for /L %%n in (1 1 2) do ( %\\n%\n   if %%n==2 (%\\n%\n      setlocal enableDelayedExpansion%\\n%\n      for /F \"tokens=1,2 delims=,\" %%A in (\"!argv!\") do (%\\n%\n         for /f \"tokens=* delims=\" %%# in ('%%~A') do endlocal^&set \"%%~B=%%#\" %\\n%\n      ) %\\n%\n   ) %\\n%\n) ^& set argv=,\n\n::::: -------- ::::::::\n\n\n:::EXAMPLE\n%assign% \"WHOAMI /LOGONID\",result\necho %result%\n\nthe first argument to the macro is the command and the second the name of the variable we want to use and both are separated by , (comma). Though this is suitable only for straight forward scenarios.\nIf we want a similar macro for the console we can use DOSKEY\ndoskey assign=for /f \"tokens=1,2 delims=,\" %a in (\"$*\") do @for /f \"tokens=* delims=\" %# in ('\"%a\"') do @set \"%b=%#\"\nrem  -- example --\nassign WHOAMI /LOGONID,my-id\necho %my-id%\n\nDOSKEY does accept double quotes as enclosion for arguments so this also is useful for more simple scenarios.\nFOR also works well with pipes which can be used for chaining commands (though it is not so good for assigning a variable.\nhostname |for /f \"tokens=* delims=\" %%# in ('more') do @(ping %%#)\n\nWhich also can be beautified with macros:\n@echo off\n:: --- defining chain command macros ---\nset \"result-as-[arg]:=|for /f \"tokens=* delims=\" %%# in ('more') do @(\"\nset \"[arg]= %%#)\"\n:::  --------------------------  :::\n\n::Example:\nhostname %result-as-[arg]:% ping %[arg]%\n\nAnd for completnes macros for the temp file approach (no doskey definition ,but it also can be easy done.If you have a SSD this wont be so slow):\n@echo off\n\n:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\nset \"[[=>\"#\" 2>&1&set/p \"&set \"]]==<# & del /q # >nul 2>&1\"\n:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\nchcp %[[%code-page%]]%\necho ~~%code-page%~~\n\nwhoami %[[%its-me%]]%\necho ##%its-me%##\n\nFor /f with another macro:\n::::::::::::::::::::::::::::::::::::::::::::::::::\n;;set \"{{=for /f \"tokens=* delims=\" %%# in ('\" &::\n;;set \"--=') do @set \"\"                        &::\n;;set \"}}==%%#\"\"                               &::\n::::::::::::::::::::::::::::::::::::::::::::::::::\n\n:: --examples\n\n::assigning ver output to %win-ver% variable\n%{{% ver %--%win-ver%}}%\necho 3: %win-ver%\n\n\n::assigning hostname output to %my-host% variable\n%{{% hostname %--%my-host%}}%\necho 4: %my-host%\n\n\n\nA10:\n\nUsing parameters in batch files: %0 and %9\nBatch files can refer to the words passed in as parameters with the tokens: %0 to %9.\n%0 is the program name as it was called.\n%1 is the first command line parameter\n%2 is the second command line parameter\nand so on till %9.\n\nparameters passed in on the commandline must be alphanumeric characters and delimited by spaces.  Since %0 is the program name as it was called, in DOS %0 will be empty for AUTOEXEC.BAT if started at boot time.\nExample:\nPut the following command in a batch file called mybatch.bat:\n@echo off\n@echo hello %1 %2\npause\n\nInvoking the batch file like this: mybatch john billy would output:\nhello john billy\n\nGet more than 9 parameters for a batch file, use: %*\nThe Percent Star token %* means \"the rest of the parameters\".  You can use a for loop to grab them, as defined here:\nhttp://www.robvanderwoude.com/parameters.php\nNotes about delimiters for batch parameters\nSome characters in the command line parameters are ignored by batch files, depending on the DOS version, whether they are \"escaped\" or not, and often depending on their location in the command line:\ncommas (\",\") are replaced by spaces, unless they are part of a string in \ndouble quotes\n\nsemicolons (\";\") are replaced by spaces, unless they are part of a string in \ndouble quotes\n\n\"=\" characters are sometimes replaced by spaces, not if they are part of a \nstring in double quotes\n\nthe first forward slash (\"/\") is replaced by a space only if it immediately \nfollows the command, without a leading space\n\nmultiple spaces are replaced by a single space, unless they are part of a \nstring in double quotes\n\ntabs are replaced by a single space\n\nleading spaces before the first command line argument are ignored\n\n\n\nA11:\n\nAdd these two lines near the top of your batch file, all stdout and stderr after will be redirected to log.txt:\nif not \"%1\"==\"STDOUT_TO_FILE\"  %0 STDOUT_TO_FILE %*  >log.txt 2>&1\nshift /1\n\n\n\nA12:\n\n    if %var% geq 1\n\nis the easiest way\n\n\nA13:\n\nThe icon displayed by the Shell (Explorer) for batch files is determined by the registry key\nHKCR\\batfile\\DefaultIcon\n\nwhich, on my computer is\n%SystemRoot%\\System32\\imageres.dll,-68\n\nYou can set this to any icon you like.\nThis will however change the icons of all batch files (unless they have the extension .cmd).\n\n\nA14:\n\nAssuming you're referring to MS-DOS batch files: as it is simply a text file with a special extension, a .bat file doesn't store an icon of its own.\nYou can, however, create a shortcut in the .lnk format that stores an icon.\n\n\nA15:\n\nI know this is an older post, but someone will stumble across it in a Google search and it also looks like some questions the OP asked in comments weren't specifically addressed. Also, please go easy on me since this is my first answer posted on SO. :)\nTo redirect the output to a file using a dynamically generated file name, my go-to (read: quick & dirty) approach is the second solution offered by @dbenham. So for example, this:\n@echo off\n> filename_prefix-%DATE:~-4%-%DATE:~4,2%-%DATE:~7,2%_%time:~0,2%%time:~3,2%%time:~6,2%.log (\necho Your Name Here\necho Beginning Date/Time: %DATE:~-4%-%DATE:~4,2%-%DATE:~7,2%_%time:~0,2%%time:~3,2%%time:~6,2%.log\nREM do some stuff here\necho Your Name Here\necho Ending Date/Time: %DATE:~-4%-%DATE:~4,2%-%DATE:~7,2%_%time:~0,2%%time:~3,2%%time:~6,2%.log\n)\n\nWill create a file like what you see in this screenshot of the file in the target directory\nThat will contain this output:\nYour Name Here\nBeginning Date/Time: 2016-09-16_141048.log\nYour Name Here\nEnding Date/Time: 2016-09-16_141048.log\n\nAlso keep in mind that this solution is locale-dependent, so be careful how/when you use it.\n\n\nA16:\n\ntry this:\nif 3 gtr 2 @echo \"biggger\"\n\nThis outputs:\n\"biggger\"\n\n\nThe other operators are:\n\nEQU - equal\nNEQ - not equal\nLSS - less than\nLEQ - less than or equal\nGTR - greater than\nGEQ - greater than or equal\n\nReference\n\nMicrosoft TechNet | Windows Server 2012 | Command-line Reference: If (Archived here)\nSS64: If (Archived here)\n\n\n\nA17:\n\nOne of the way you can achieve this is:\n\nCreate an executable Jar file\nCreate a batch file to run the above jar and launch the desktop java application.\nUse Batch2Exe converter and covert to batch file to Exe.\nDuring above conversion, you can change the icon to that of your choice.(must of valid .ico file)\nPlace the short cut for the above exe on desktop.\n\nNow your java program can be opened in a fancy way just like any other MSWindows apps.! :) \n\n\nA18:\n\nI most cases, creating a temporary file named after your variable name might be acceptable. (as you are probably using meaningful variables name...)\nHere, my variable name is SSH_PAGEANT_AUTH_SOCK\ndir /w \"\\\\.\\pipe\\\\\"|find \"pageant\" > %temp%\\SSH_PAGEANT_AUTH_SOCK && set /P SSH_PAGEANT_AUTH_SOCK=<%temp%\\SSH_PAGEANT_AUTH_SOCK\n\n\n\nA19:\n\necho some output >\"your logfile\"\n\nor\n(\n echo some output\n echo more output\n)>\"Your logfile\"\n\nshould fill the bill.\nIf you want to APPEND the output, use >> instead of >. > will start a new logfile.\n\n\nA20:\n\nIf you're running within a batch/cmd file, you need to double the % markers:\nfor %%i in (*.txt *.doc) do copy %%i c:\\test2\n\nThe single % variant only works from the command line.\n\n\nA21:\n\nif you want both out and err streams redirected\ndir >> a.txt 2>&1\n\n\n\nA22:\n\nThese answers were all so close to the answer that I needed.  This is an attempt to expand on them.\nIn a Batch file\nIf you're running from within a .bat file and you want a single line that allows you to export a complicated command like jq -r \".Credentials.AccessKeyId\" c:\\temp\\mfa-getCreds.json to a variable named AWS_ACCESS_KEY then you want this:\nFOR /F \"tokens=* USEBACKQ\" %%g IN (`jq -r \".Credentials.AccessKeyId\" c:\\temp\\mfa-getCreds.json`) do (SET \"AWS_ACCESS_KEY=%%g\")\n\nOn the Command Line\nIf you're at the C:\\ prompt you want a single line that allows you to run a complicated command like jq -r \".Credentials.AccessKeyId\" c:\\temp\\mfa-getCreds.json to a variable named AWS_ACCESS_KEY then you want this:\nFOR /F \"tokens=* USEBACKQ\" %g IN (`jq -r \".Credentials.AccessKeyId\" c:\\temp\\mfa-getCreds.json`) do (SET \"AWS_ACCESS_KEY=%g\")\n\nExplanation\nThe only difference between the two answers above is that on the command line, you use a single % in your variable.  In a batch file, you have to double up on the percentage signs (%%).\nSince the command includes colons, quotes, and parentheses, you need to include the USEBACKQ line in the options so that you can use backquotes to specify the command to run and then all kinds of funny characters inside of it.\n\n\nA23:\n\nIn a single line:\nFOR /F \"tokens=*\" %%g IN ('your command') do (SET VAR=%%g)\n\nThe command output will be set in %g then in VAR.\nMore information here: https://ss64.com/nt/for_cmd.html\n\n\nA24:\n\nBatch Files automatically pass the text after the program so long as their are variables to assign them to. They are passed in order they are sent; e.g. %1 will be the first string sent after the program is called, etc.\nIf you have Hello.bat and the contents are:\n@echo off\necho.Hello, %1 thanks for running this batch file (%2)\npause\n\nand you invoke the batch in command via\n\nhello.bat APerson241 %date%\n\nyou should receive this message back:\nHello, APerson241 thanks for running this batch file (01/11/2013)\n\n\nA25:\n\n@echo off\n>output.txt (\necho Checking your system infor, Please wating...\n\nsysteminfo | findstr /c:\"Host Name\" \nsysteminfo | findstr /c:\"Domain\"\n\nipconfig /all | find \"Physical Address\" \n\nipconfig | find \"IPv4\" \nipconfig | find \"Default Gateway\"\n\n)\n\n@pause\n\n\n\nA26:\n\nUse variables i.e. the .BAT variables and called %0 to %9\n\n\nA27:\n\ncd %windir%\\system32\\inetsrv\n\n@echo off\n\nfor /F \"tokens=* USEBACKQ\" %%x in (      \n        `appcmd list apppool /text:name`\n       ) do (\n            echo|set /p=  \"%%x - \" /text:name & appcmd.exe list apppool \"%%x\" /text:processModel.identityType\n       )\n\necho %date% & echo %time%\n\npause\n\n\n\nA28:\n\ni recommand to use BAT to EXE converter for your desires \n\n\nA29:\n\nAs others have already said, parameters passed through the command line can be accessed in batch files with the notation %1 to %9. There are also two other tokens that you can use:\n\n%0 is the executable (batch file) name as specified in the command line.\n%* is all parameters specified in the command line -- this is very useful if you want to forward the parameters to another program.\n\nThere are also lots of important techniques to be aware of in addition to simply how to access the parameters.\nChecking if a parameter was passed\nThis is done with constructs like IF \"%~1\"==\"\", which is true if and only if no arguments were passed at all. Note the tilde character which causes any surrounding quotes to be removed from the value of %1; without a tilde you will get unexpected results if that value includes double quotes, including the possibility of syntax errors.\nHandling more than 9 arguments (or just making life easier)\nIf you need to access more than 9 arguments you have to use the command SHIFT. This command shifts the values of all arguments one place, so that %0 takes the value of %1, %1 takes the value of %2, etc. %9 takes the value of the tenth argument (if one is present), which was not available through any variable before calling SHIFT (enter command SHIFT /? for more options).\nSHIFT is also useful when you want to easily process parameters without requiring that they are presented in a specific order. For example, a script may recognize the flags -a and -b in any order. A good way to parse the command line in such cases is\n:parse\nIF \"%~1\"==\"\" GOTO endparse\nIF \"%~1\"==\"-a\" REM do something\nIF \"%~1\"==\"-b\" REM do something else\nSHIFT\nGOTO parse\n:endparse\nREM ready for action!\n\nThis scheme allows you to parse pretty complex command lines without going insane.\nSubstitution of batch parameters\nFor parameters that represent file names the shell provides lots of functionality related to working with files that is not accessible in any other way. This functionality is accessed with constructs that begin with %~.\nFor example, to get the size of the file passed in as an argument use\nECHO %~z1\n\nTo get the path of the directory where the batch file was launched from (very useful!) you can use\nECHO %~dp0\n\nYou can view the full range of these capabilities by typing CALL /? in the command prompt.\n\n\nA30:\n\nYou can write this (easier)\n@echo off\n\n:Start2\ncls\ngoto Start\n\n:Start\ntitle Frequently Used Websites\necho Please select a website from the list\necho with the corresponding key\necho --------------------------------------\necho [1] Google\necho [2] Wikipedia\necho [3] Facebook\necho [4] Youtube\necho [5] Yahoo\n\nset /p input= Choice: \n\nif %input%==1 goto Z\nif %input%==2 goto X\nif %input%==3 goto C\nif %input%==4 goto V\nif %input%==5 goto B\necho Invalid selection!\necho.\necho Press any key to go back!\npause >nul\ncls\ngoto start2\n\n:Z\ncls\necho You have selected Google\npause\nstart www.google.com\nexit\n\n:X\ncls\necho You have selected Wikipedia\npause\nstart www.wikipedia.com\nexit\n\n:C\ncls\necho You have selected Facebook\npause\nstart www.facebook.com\nexit\n\n:V\ncls\necho You have selected Youtube\npause\nstart www.youtube.com\nexit\n\n:B\ncls\necho You have selected Yahoo\npause\nstart www.Yahoo.com\nexit\n\n:N\ncls\necho Invalid Selection! Try again\npause\ngoto start2\n\n\n\nA31:\n\nIf being run from a batch file, variables need to be denoted with two percent signs, like %%I, only from the command line you use one\n\n\nA32:\n\nYou can just create a shortcut and then right click on it -> properties -> change icon, and just browse for your desired icon.\nHope this help.\nTo set an icon of a shortcut programmatically, see this article using SetIconLocation:\nHow Can I Change the Icon for an Existing Shortcut?:\nhttps://devblogs.microsoft.com/scripting/how-can-i-change-the-icon-for-an-existing-shortcut/\nConst DESKTOP = &H10&\nSet objShell = CreateObject(\"Shell.Application\")\nSet objFolder = objShell.NameSpace(DESKTOP)\nSet objFolderItem = objFolder.ParseName(\"Test Shortcut.lnk\")\nSet objShortcut = objFolderItem.GetLink\nobjShortcut.SetIconLocation \"C:\\Windows\\System32\\SHELL32.dll\", 13\nobjShortcut.Save\n\n\n\nA33:\n\nTry BatToExe converter. It will convert your batch file to an executable, and allow you to set an icon for it.\n\n\nA34:\n\nI found this thread on that there Interweb thing. Boils down to:\n@echo off \nsetlocal enableextensions \nfor /f \"tokens=*\" %%a in ( \n'VER' \n) do ( \nset myvar=%%a \n) \necho/%%myvar%%=%myvar% \npause \nendlocal \n\nYou can also redirect the output of a command to a temporary file, and then put the contents of that temporary file into your variable, likesuchashereby. It doesn't work with multiline input though.\ncmd > tmpFile \nset /p myvar= < tmpFile \ndel tmpFile \n\nCredit to the thread on Tom's Hardware.\n\n\nA35:\n\n@Jon's :parse/:endparse scheme is a great start, and he has my gratitude for the initial pass, but if you think that the Windows torturous batch system would let you off that easy\u2026 well, my friend, you are in for a shock. I have spent the whole day with this devilry, and after much painful research and experimentation I finally managed something viable for a real-life utility.\nLet us say that we want to implement a utility foobar. It requires an initial command. It has an optional parameter --foo which takes an optional value (which cannot be another parameter, of course); if the value is missing it defaults to default. It also has an optional parameter --bar which takes a required value. Lastly it can take a flag --baz with no value allowed. Oh, and these parameters can come in any order.\nIn other words, it looks like this:\nfoobar <command> [--foo [<fooval>]] [--bar <barval>] [--baz]\n\nComplicated? No, that seems pretty typical of real life utilities. (git anyone?)\nWithout further ado, here is a solution:\n@ECHO OFF\nSETLOCAL\nREM FooBar parameter demo\nREM By Garret Wilson\n\nSET CMD=%~1\n\nIF \"%CMD%\" == \"\" (\n  GOTO usage\n)\nSET FOO=\nSET DEFAULT_FOO=default\nSET BAR=\nSET BAZ=\n\nSHIFT\n:args\nSET PARAM=%~1\nSET ARG=%~2\nIF \"%PARAM%\" == \"--foo\" (\n  SHIFT\n  IF NOT \"%ARG%\" == \"\" (\n    IF NOT \"%ARG:~0,2%\" == \"--\" (\n      SET FOO=%ARG%\n      SHIFT\n    ) ELSE (\n      SET FOO=%DEFAULT_FOO%\n    )\n  ) ELSE (\n    SET FOO=%DEFAULT_FOO%\n  )\n) ELSE IF \"%PARAM%\" == \"--bar\" (\n  SHIFT\n  IF NOT \"%ARG%\" == \"\" (\n    SET BAR=%ARG%\n    SHIFT\n  ) ELSE (\n    ECHO Missing bar value. 1>&2\n    ECHO:\n    GOTO usage\n  )\n) ELSE IF \"%PARAM%\" == \"--baz\" (\n  SHIFT\n  SET BAZ=true\n) ELSE IF \"%PARAM%\" == \"\" (\n  GOTO endargs\n) ELSE (\n  ECHO Unrecognized option %1. 1>&2\n  ECHO:\n  GOTO usage\n)\nGOTO args\n:endargs\n\nECHO Command: %CMD%\nIF NOT \"%FOO%\" == \"\" (\n  ECHO Foo: %FOO%\n)\nIF NOT \"%BAR%\" == \"\" (\n  ECHO Bar: %BAR%\n)\nIF \"%BAZ%\" == \"true\" (\n  ECHO Baz\n)\n\nREM TODO do something with FOO, BAR, and/or BAZ\nGOTO :eof\n\n:usage\nECHO FooBar\nECHO Usage: foobar ^<command^> [--foo [^<fooval^>]] [--bar ^<barval^>] [--baz]\nEXIT /B 1\n\nYes, it really is that bad. See my similar post at https://stackoverflow.com/a/50653047/421049, where I provide more analysis of what is going on in the logic, and why I used certain constructs.\nHideous. Most of that I had to learn today. And it hurt.\n\n\nA36:\n\nYou may use a program like BAT to EXE converter for example that one: link\n\nThis program permits you to add your custom icon.\n\n\nA37:\n\ntry with shortcutjs.bat to create a shortcut:\ncall shortcutjs.bat -linkfile mybat3.lnk -target \"%cd%\\Ascii2All.bat\" -iconlocation \"%SystemRoot%\\System32\\SHELL32.dll,77\"\n\nyou can use the -iconlocation switch to point to a icon .\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 31, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI believe that all of these (and even die() or die(0)) are identical.  If they are not identical, which is preferred for exiting a script successfully?  If they are identical, is there any preferred standard to indicate successful script completion?  I tend to use exit;.\nEDIT: All of the answers have \"die() and exit() are identical\" even though I say that in my question.  I updated to the title to hopefully make it clearer that this is NOT my question.  I want to clearly indicate success from a command line script.\n\n\nCandidate answers:\nA1:\n\nGenerally speaking, the first way is more popular overall because those with prior programming knowledge can easily transition to PHP and get work done in an object-oriented fashion. The first way is more universal. My advice would be to stick with what is tried and true across many languages. Then, when and if you use another language, you'll be ready to get something accomplished (instead of spending time reinventing the wheel).\n\n\nA2:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA3:\n\nEncapsulation is important in any OO language, popularity has nothing to do with it. In dynamically typed languages, like PHP, it is especially useful because there is little ways to ensure a property is of a specific type without using setters.\nIn PHP, this works:\nclass Foo {\n   public $bar; // should be an integer\n}\n$foo = new Foo;\n$foo->bar = \"string\";\n\nIn Java, it doesn't:\nclass Foo {\n   public int bar;\n}\nFoo myFoo = new Foo();\nmyFoo.bar = \"string\"; // error\n\nUsing magic methods (__get and __set) also works, but only when accessing a property that has lower visibility than the current scope can access. It can easily give you headaches when trying to debug, if it is not used properly.\n\n\nA4:\n\nYou can use php magic methods __get and __set.\n<?php\nclass MyClass {\n  private $firstField;\n  private $secondField;\n\n  public function __get($property) {\n    if (property_exists($this, $property)) {\n      return $this->$property;\n    }\n  }\n\n  public function __set($property, $value) {\n    if (property_exists($this, $property)) {\n      $this->$property = $value;\n    }\n\n    return $this;\n  }\n}\n?>\n\n\n\nA5:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA6:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\nA7:\n\nGoogle already published a guide on optimization of PHP and the conclusion was:\nNo getter and setter Optimizing PHP\nAnd no, you must not use magic methods.  For PHP, Magic Methods are evil. Why?\n\nThey are hard to debug.\nThere is a negative performance impact.\nThey require writing more code.\n\nPHP is not Java, C++, or C#. PHP is different and plays with different rules. \n\n\nA8:\n\ndie(); is just a synonym for exit(); and is functionally identical.\nThe standard way is to use exit code zero to signify success, and anything else to denote an error condition.\n\n\nA9:\n\nWell, PHP does have magic methods __get, __set, __isset & __unset, which is always a start. Alas proper (get it?) OO properties is more than magic methods. The main problem with PHP's implementation is that magic methods are called for all inaccessible properties. Which means you have to Repeat Yourself (eg. by calling property_exists()) in the magic methods when determining if name is actually a property of your object. And you can't really solve this general problem with a base class unless all your classes inherit from ie. ClassWithProperties, since PHP lacks multiple inheritance.\nIn contrast, Python new style classes gives you property(), which lets you explicitly define all your properties. C# has special syntax. \nhttp://en.wikipedia.org/wiki/Property_(programming)\n\n\nA10:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\nA11:\n\nValidating + Formatting/Deriving Values\nSetters let you to validate data and getters let you format or derive data. Objects allow you to encapsulate data and its validation and formatting code into a neat package that encourages DRY.\nFor example, consider the following simple class that contains a birth date.\nclass BirthDate {\n\n    private $birth_date;\n\n    public function getBirthDate($format='Y-m-d') {\n        //format $birth_date ...\n        //$birth_date = ...\n        return $birth_date;\n    }\n\n    public function setBirthDate($birth_date) {                   \n        //if($birth_date is not valid) throw an exception ...          \n        $this->birth_date = $birth_date;\n    }\n\n    public function getAge() {\n        //calculate age ...\n        return $age;\n    }\n\n    public function getDaysUntilBirthday() {\n        //calculate days until birth days\n        return $days;\n    }\n}\n\nYou'll want to validate that the value being set is\n\nA valid date\nNot in the future\n\nAnd you don't want to do this validation all over your application (or over multiple applications for that matter). Instead, it's easier to make the member variable protected or private (in order to make the setter the only access point) and to validate in the setter because then you'll know that the object contains a valid birth date no matter which part of the application the object came from and if you want to add more validation then you can add it in a single place.\nYou might want to add multiple formatters that operate on the same member variable i.e. getAge() and getDaysUntilBirthday() and you might want to enforce a configurable format in getBirthDate() depending on locale. Therefore I prefer consistently accessing values via getters as opposed to mixing $date->getAge() with $date->birth_date.\ngetters and setters are also useful when you extend objects. For example, suppose your application needed to allow 150+ year birth dates in some places but not in others. One way to solve the problem without repeating any code would be to extend the BirthDate object and put the additional validation in the setter.\nclass LivingBirthDate extends BirthDate {\n\n    public function setBirthDate($birth_date) {\n        //if $birth_date is greater than 150 years throw an exception\n        //else pass to parent's setter\n        return parent::setBirthDate($birth_date);\n    }\n}\n\n\n\nA12:\n\nIn addition to the already great and respected answers in here, I would like to expand on PHP having no setters/getters.\nPHP does not have getter and setter syntax. It provides subclassed or magic methods to allow \"hooking\" and overriding the property lookup process, as pointed out by Dave.\nMagic allows us lazy programmers to do more with less code at a time at which we are actively engaged in a project and know it intimately, but usually at the expense of readability.\nPerformance Every unnecessary function, that results from forcing a getter/setter-like code-architecture in PHP, involves its own memory stack-frame upon invocation and is wasting CPU cycles. \nReadability: The codebase incurs bloating code-lines, which impacts code-navigation as more LOC mean more scrolling,.\nPreference: Personally, as my rule of thumb, I take the failure of static code analysis \nas a sign to avoid going down the magical road as long as obvious long-term benefits elude me at that time.\nFallacies:\nA common argument is readability. For instance that $someobject->width is easier to read than $someobject->width(). However unlike a planet's circumference or width, which can be assumed to be  static, an object's instance such as $someobject, which requires a width function, likely takes a measurement of the object's instance width.\nTherefore readability increases mainly because of assertive naming-schemes and not by hiding the function away that outputs a given property-value.\n__get / __set uses:\n\npre-validation and pre-sanitation of property values\nstrings e.g.\n\"\nsome {mathsobj1->generatelatex} multi\nline text {mathsobj1->latexoutput}\nwith lots of variables for {mathsobj1->generatelatex}\n some reason\n\"\n\nIn this case generatelatex would adhere to a naming scheme of actionname + methodname\nspecial, obvious cases\n$dnastringobj->homeobox($one_rememberable_parameter)->gattaca->findrelated()\n$dnastringobj->homeobox($one_rememberable_parameter)->gttccaatttga->findrelated()\n\n\nNote: PHP chose not to implement getter/setter syntax. I am not claiming that getters/setter are generally bad.\n\n\nA13:\n\nclass MyClass {\n    private $firstField;\n    private $secondField;\n    private $thirdField;\n\n    public function __get( $name ) {\n        if( method_exists( $this , $method = ( 'get' . ucfirst( $name  ) ) ) )\n            return $this->$method();\n        else\n            throw new Exception( 'Can\\'t get property ' . $name );\n    }\n\n    public function __set( $name , $value ) {\n        if( method_exists( $this , $method = ( 'set' . ucfirst( $name  ) ) ) )\n            return $this->$method( $value );\n        else\n            throw new Exception( 'Can\\'t set property ' . $name );\n    }\n\n    public function __isset( $name )\n    {\n        return method_exists( $this , 'get' . ucfirst( $name  ) ) \n            || method_exists( $this , 'set' . ucfirst( $name  ) );\n    }\n\n    public function getFirstField() {\n        return $this->firstField;\n    }\n\n    protected function setFirstField($x) {\n        $this->firstField = $x;\n    }\n\n    private function getSecondField() {\n        return $this->secondField;\n    }\n}\n\n$obj = new MyClass();\n\necho $obj->firstField; // works\n$obj->firstField = 'value'; // works\n\necho $obj->getFirstField(); // works\n$obj->setFirstField( 'value' ); // not works, method is protected\n\necho $obj->secondField; // works\necho $obj->getSecondField(); // not works, method is private\n\n$obj->secondField = 'value'; // not works, setter not exists\n\necho $obj->thirdField; // not works, property not exists\n\nisset( $obj->firstField ); // returns true\nisset( $obj->secondField ); // returns true\nisset( $obj->thirdField ); // returns false\n\nReady!\n\n\nA14:\n\nWhy use getters and setters?\n\nScalability: It's easier refactor a getter than search all the var assignments in a project code.\nDebugging: You can put breakpoints at setters and getters.\nCleaner: Magic functions are not good solution for writting less, your IDE will not suggest the code. Better use templates for fast-writting getters.\n\n\n\n\nA15:\n\nAfter reading the other advices, I'm inclined to say that:\nAs a GENERIC rule, you will not always define setters for ALL properties, specially \"internal\" ones (semaphores, internal flags...). Read-only properties will not have setters, obviously, so some properties will only have getters; that's where __get() comes to shrink the code:\n\ndefine a __get() (magical global getters) for all those properties which are alike,\ngroup them in arrays so:\n\nthey'll share common characteristics: monetary values will/may come up properly formatted, dates in an specific layout (ISO, US, Intl.), etc.\nthe code itself can verify that only existing & allowed properties are being read using this magical method.\nwhenever you need to create a new similar property, just declare it and add its name to the proper array and it's done. That's way FASTER than defining a new getter, perhaps with some lines of code REPEATED again and again all over the class code.\n\n\nYes! we could write a private method to do that, also, but then again, we'll have MANY methods declared (++memory) that end up calling another, always the same, method. Why just not write a SINGLE method to rule them all...? [yep! pun absolutely intended! :)]\nMagic setters can also respond ONLY to specific properties, so all date type properties can be screened against invalid values in one method alone. If date type properties were listed in an array, their setters can be defined easily. Just an example, of course. there are way too many situations.\nAbout readability... Well... That's another debate: I don't like to be bound to the uses of an IDE (in fact, I don't use them, they tend to tell me (and force me) how to write... and I have my likes about coding \"beauty\"). I tend to be consistent about naming, so using ctags and a couple of other aids is sufficient to me... Anyway: once all this magic setters and  getters are done, I write the other setters that are too specific or \"special\" to be generalized in a __set() method. And that covers all I need about getting and setting properties. Of course: there's not always a common ground, or there are such a few properties that is not worth the trouble of coding a magical method, and then there's still the old good traditional setter/getter pair.\nProgramming languages are just that: human artificial languages. So, each of them has its own intonation or accent, syntax and flavor, so I won't pretend to write a Ruby or Python code using the same \"accent\" than Java or C#, nor I would write a JavaScript or PHP to resemble Perl or SQL... Use them the way they're meant to be used.\n\n\nA16:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA17:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA18:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA19:\n\nIf you preffer to use the __call function, you can use this method. It works with\n\nGET => $this->property()\nSET => $this->property($value)\nGET => $this->getProperty()\nSET => $this->setProperty($value)\n\nkalsdas\npublic function __call($name, $arguments) {\n\n    //Getting and setting with $this->property($optional);\n\n    if (property_exists(get_class($this), $name)) {\n\n\n        //Always set the value if a parameter is passed\n        if (count($arguments) == 1) {\n            /* set */\n            $this->$name = $arguments[0];\n        } else if (count($arguments) > 1) {\n            throw new \\Exception(\"Setter for $name only accepts one parameter.\");\n        }\n\n        //Always return the value (Even on the set)\n        return $this->$name;\n    }\n\n    //If it doesn't chech if its a normal old type setter ot getter\n    //Getting and setting with $this->getProperty($optional);\n    //Getting and setting with $this->setProperty($optional);\n    $prefix = substr($name, 0, 3);\n    $property = strtolower($name[3]) . substr($name, 4);\n    switch ($prefix) {\n        case 'get':\n            return $this->$property;\n            break;\n        case 'set':\n            //Always set the value if a parameter is passed\n            if (count($arguments) != 1) {\n                throw new \\Exception(\"Setter for $name requires exactly one parameter.\");\n            }\n            $this->$property = $arguments[0];\n            //Always return the value (Even on the set)\n            return $this->$name;\n        default:\n            throw new \\Exception(\"Property $name doesn't exist.\");\n            break;\n    }\n}\n\n\n\nA20:\n\nAs several people have mentioned, die() and exit() are exactly the same.\nIf you look at the PHP documentation, there are two options for arguments:\n\nAn numeric value.  This is only useful if you are using PHP from the command line, as opposed to a web server.  A value of zero indicates success.  Nonzero indicates a failure condition occurred.\nA string value.  This will be displayed to the browser when the exit occurs.\n\nInstead of die() or exit(), I recommend using exceptions and a custom top-level exception handler to manage failure conditions.  \nYou have more flexibility that way to do things like automatic error logging.  Also, if you're using PHP to implement a JSON API, this exception handler can hand back a valid, error-indicating JSON snippet instead.\n\n\nA21:\n\nThere are many ways to create sourcecode in a netbeans-convention. This is nice. It makes thinks such easyer === FALSE. Just use the traditionel, specially if you are not sure which one of the properties should be encapsuled and which one not. I know, it is a boi.... pla... code, but for debugging-works and many other thinks it is the better, clear way. Dont spend to much time with thousend of arts how to make simple getters and setters. You cannot implement too some design patterns like the demeter-rule and so on, if you use magics. In specific situation you can use magic_calls or for small, fast and clear solutions. Sure you could make solutions for design-patters in this way too, but why to make you live more difficult. \n\n\nA22:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA23:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA24:\n\ndie() is typically used to kill the script with an error output:\ndie(\"There was a fatal error\");\n\nwhere-as exit is typically used as a successful exit (At least in my coding)\nThe PHP Manual says that the functions are identical.\n\n\nA25:\n\nThese are all identical. I'm pretty sure die() is just a straight-up alias to exit(), but even if it isn't, it still acts identically.\nWhen one of these functions is given a string argument, it prints out the string before terminating the process. When it encounters an integer under 255, that integer is considered the return code for the process, which gets passed back to the process which invoked the PHP script. This is particularly useful when writing command line applications (PHP isn't web-only!).\nAs far as the difference between exit, exit(), and exit(0), there really is none. There is definitely no difference between the first two because exit is technically a language construct, not a function, so it can be called with or without parentheses, just like echo. Returning a code of 0 means \"this program ran successfully/without errors\", and while I don't know what exactly happens when you don't pass an argument, PHP.net says that an argument-less exit indicates success, so I would bet it returns 0, though again PHP.net doesn't show a default for the argument.\n\n\nA26:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA27:\n\ndie is exactly equivalent to exit.\nFrom the manual:\n\nIf status is an integer, that value will be used as the exit status..\n\nThis is only useful if you have some sort of wrapper that does something based on the exit status. Unless you have a specific need to report an exit code to the outside world, just exit;.\n\n\nA28:\n\nI made an experiment using the magic method __call.\nNot sure if I should post it (because of all the \"DO NOT USE MAGIC METHODS\" warnings in the other answers and comments) but i'll leave it here.. just in case someone find it useful.\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = substr($_name, 4);\n\n    if (isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nJust add that method above in your class, now you can type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_foo(); // return \"bar\"\n$C->get_bom(); // return \"bim\"\n\n// as setter\n$C->set_foo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_bom(\"zam\"); // set \"zam\" as new value of bom\n\n\n\nThis way you can get/set everything in your class if it exist so, if you need it for only a few specific elements, you could use a \"whitelist\" as filter.  \nExample:\nprivate $callWhiteList = array(\n    \"foo\" => \"foo\",\n    \"fee\" => \"fee\",\n    // ...\n);\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = $this->callWhiteList[substr($_name, 4)];\n\n    if (!is_null($varName) && isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nNow you can only get/set \"foo\" and \"fee\".\nYou can also use that \"whitelist\" to assign custom names to access to your vars.\nFor example,\nprivate $callWhiteList = array(\n    \"myfoo\" => \"foo\",\n    \"zim\" => \"bom\",\n    // ...\n);\n\nWith that list you can now type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // private $callWhiteList = array( ... )\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_myfoo(); // return \"bar\"\n$C->get_zim(); // return \"bim\"\n\n// as setter\n$C->set_myfoo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_zim(\"zam\"); // set \"zam\" as new value of bom\n\n\n.\n.\n.\nThat's all.\n\nDoc:\n__call() is triggered when invoking inaccessible methods in an object context. \n\n\nA29:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA30:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n( ( ( ( ( ( ( ( \"\n( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 32, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI'm adding this table:\nCREATE TABLE contenttype (\n        contenttypeid INT UNSIGNED NOT NULL AUTO_INCREMENT,\n        class VARBINARY(50) NOT NULL,\n        packageid INT UNSIGNED NOT NULL,\n        canplace ENUM('0','1') NOT NULL DEFAULT '0',\n        cansearch ENUM('0','1') NOT NULL DEFAULT '0',\n        cantag ENUM('0','1') DEFAULT '0',\n        canattach ENUM('0','1') DEFAULT '0',\n        isaggregator ENUM('0', '1') NOT NULL DEFAULT '0',\n        PRIMARY KEY (contenttypeid),\n        UNIQUE KEY packageclass (packageid, class)\n);\n\nAnd I get a 1050 \"table already exists\"\nBut the table does NOT exist. Any ideas?\nEDIT: more details because everyone seems to not believe me :)\nDESCRIBE contenttype\n\nyields:\n\n1146 - Table 'gunzfact_vbforumdb.contenttype' doesn't exist\n\nand\nCREATE TABLE gunzfact_vbforumdb.contenttype(\ncontenttypeid INT UNSIGNED NOT NULL AUTO_INCREMENT ,\nclass VARBINARY( 50 ) NOT NULL ,\npackageid INT UNSIGNED NOT NULL ,\ncanplace ENUM( '0', '1' ) NOT NULL DEFAULT '0',\ncansearch ENUM( '0', '1' ) NOT NULL DEFAULT '0',\ncantag ENUM( '0', '1' ) DEFAULT '0',\ncanattach ENUM( '0', '1' ) DEFAULT '0',\nisaggregator ENUM( '0', '1' ) NOT NULL DEFAULT '0',\nPRIMARY KEY ( contenttypeid ) ,\n\nYields:\n\n1050 - Table 'contenttype' already exists\n\n\n\nCandidate answers:\nA1:\n\nThe classic way would be to add commas to the left and right:\nselect * from shirts where CONCAT(',', colors, ',') like '%,1,%'\n\nBut find_in_set also works:\nselect * from shirts where find_in_set('1',colors) <> 0\n\n\n\nA2:\n\nUse MySQL's FIELD() function:\nSELECT name, description, ...\nFROM ...\nWHERE id IN([ids, any order])\nORDER BY FIELD(id, [ids in order])\n\nFIELD() will return the index of the first parameter that is equal to the first parameter (other than the first parameter itself).\nFIELD('a', 'a', 'b', 'c')\nwill return 1\nFIELD('a', 'c', 'b', 'a')\nwill return 3\nThis will do exactly what you want if you paste the ids into the IN() clause and the FIELD() function in the same order.\n\n\nA3:\n\nThis will work for sure, and I actually tried it out:\nlwdba@localhost (DB test) :: DROP TABLE IF EXISTS shirts;\nQuery OK, 0 rows affected (0.08 sec)\n\nlwdba@localhost (DB test) :: CREATE TABLE shirts\n    -> (<BR>\n    -> id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    -> ticketnumber INT,\n    -> colors VARCHAR(30)\n    -> );<BR>\nQuery OK, 0 rows affected (0.19 sec)\n\nlwdba@localhost (DB test) :: INSERT INTO shirts (ticketnumber,colors) VALUES\n    -> (32423,'1,2,5,12,15'),\n    -> (32424,'1,5,12,15,30'),\n    -> (32425,'2,5,11,15,28'),\n    -> (32426,'1,2,7,12,15'),\n    -> (32427,'2,4,8,12,15');\nQuery OK, 5 rows affected (0.06 sec)\nRecords: 5  Duplicates: 0  Warnings: 0\n\nlwdba@localhost (DB test) :: SELECT * FROM shirts WHERE LOCATE(CONCAT(',', 1 ,','),CONCAT(',',colors,',')) > 0;\n+----+--------------+--------------+\n| id | ticketnumber | colors       |\n+----+--------------+--------------+\n|  1 |        32423 | 1,2,5,12,15  |\n|  2 |        32424 | 1,5,12,15,30 |\n|  4 |        32426 | 1,2,7,12,15  |\n+----+--------------+--------------+\n3 rows in set (0.00 sec)\n\nGive it a Try !!!\n\n\nA4:\n\nAns to get sorted data.\nSELECT ...\nFROM ...\nORDER  BY FIELD(user_id,5,3,2,...,50)  LIMIT 10\n\n\n\nA5:\n\nThis is actually how your query works and is a normal behaviour. Using LIMIT you will not limit the count or sum but only the returned rows. So your query will return n rows as stated in your LIMIT clause. And since your query actually returns only one row, applying a (non-zero) limit has no effect on the results.\nHowever, your second query will work as expected and is an established way of solving this problem.\n\n\nA6:\n\nSee following how to get sorted data.\nSELECT ...\n  FROM ...\n WHERE zip IN (91709,92886,92807,...,91356)\n   AND user.status=1\nORDER \n    BY provider.package_id DESC \n     , FIELD(zip,91709,92886,92807,...,91356)\nLIMIT 10\n\n\n\nA7:\n\nFor others looking for a way to order by any field, including joined field, in postgresql, use a subquery:\nSELECT * FROM(\nSELECT DISTINCT ON(availables.bookdate) `availables`.* \nFROM `availables` INNER JOIN `rooms` ON `rooms`.id = `availables`.room_id \nWHERE (rooms.hotel_id = 5056 \nAND availables.bookdate BETWEEN '2009-11-22' AND '2009-11-24')\n) AS distinct_selected\nORDER BY availables.updated_at\n\nor arel:\n\nsubquery = SomeRecord.select(\"distinct on(xx.id) xx.*, jointable.order_field\")\n      .where(\"\").joins(\")\nresult = SomeRecord.select(\"*\").from(\"(#{subquery.to_sql}) AS distinct_selected\").order(\" xx.order_field ASC, jointable.order_field ASC\")\n\n\n\nA8:\n\nIf you don't have an aggregate function in your where clause, another possible source of the 1111 - Invalid use of group function error is if you have nested aggregate functions:\nselect sum(avg(close)) from prices;\n(1111, 'Invalid use of group function')\n\nYou can get around this by breaking up the problem into two steps:\n\nSave the inner aggregation into a variable\n\nselect @avg:=avg(close) from prices;\n\n\nRun the outer aggregation against the variable\n\nselect sum(@avg) from prices;\n\n\n\nA9:\n\nFIND_IN_SET is your friend in this case\nselect * from shirts where FIND_IN_SET(1,colors) \n\n\n\nA10:\n\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic static function interpolateQuery($query, $params) {\n    $keys = array();\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n    }\n\n    $query = preg_replace($keys, $params, $query, 1, $count);\n\n    #trigger_error('replaced '.$count.' keys');\n\n    return $query;\n}\n\n\n\nA11:\n\nTwo solutions that spring to mind:\n\norder by case id when 123 then 1 when 456 then 2 else null end asc\norder by instr(','||id||',',',123,456,') asc\n\n(instr() is from Oracle; maybe you have locate() or charindex() or something like that)\n\n\nA12:\n\nGive this a shot:\nSELECT name, description, ...\nWHERE id IN\n    (SELECT id FROM table1 WHERE...)\nORDER BY\n    (SELECT display_order FROM table1 WHERE...),\n    (SELECT name FROM table1 WHERE...)\n\nThe WHEREs will probably take a little tweaking to get the correlated subqueries working properly, but the basic principle should be sound.\n\n\nA13:\n\nAlternative method:\nSELECT * FROM x WHERE CHARINDEX('$x', field1, 0)<1 \n\nExplanation: It searches for occurrences of '$x' starting from position 0. And only selects where count is 'less than one'.\n\n\nA14:\n\nAdded a little bit more to the code by Mike - walk the values to add single quotes \n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_array($value))\n            $values[$key] = implode(',', $value);\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n    // Walk the array to see if we can add single-quotes to strings\n    array_walk($values, create_function('&$v, $k', 'if (!is_numeric($v) && $v!=\"NULL\") $v = \"\\'\".$v.\"\\'\";'));\n\n    $query = preg_replace($keys, $values, $query, 1, $count);\n\n    return $query;\n}\n\n\n\nA15:\n\nI think you should manage to store your data in a way that you will simply do a join and it will be perfect, so no hacks and complicated things going on.\nI have for instance a \"Recently played\" list of track ids, on SQLite i simply do:\nSELECT * FROM recently NATURAL JOIN tracks;\n\n\n\nA16:\n\n1. For MySQL:\nSELECT FIND_IN_SET(5, columnname) AS result \nFROM table\n\n2.For Postgres SQL :\nSELECT * \nFROM TABLENAME f\nWHERE 'searchvalue' = ANY (string_to_array(COLUMNNAME, ','))\n\nExample\nselect * \nfrom customer f\nwhere '11' = ANY (string_to_array(customerids, ','))\n\n\n\nA17:\n\nI just tried to do this is MS SQL Server where we do not have FIELD():\nSELECT table1.id\n... \nINNER JOIN\n    (VALUES (10,1),(3,2),(4,3),(5,4),(7,5),(8,6),(9,7),(2,8),(6,9),(5,10)\n    ) AS X(id,sortorder)\n        ON X.id = table1.id\n    ORDER BY X.sortorder\n\nNote that I am allowing duplication too.\n\n\nA18:\n\nAccording to MySQL's \"Debuking GROUP BY Myths\" http://dev.mysql.com/tech-resources/articles/debunking-group-by-myths.html. SQL (2003 version of the standard) doesn't requires columns referenced in the SELECT list of a query to also appear in the GROUP BY clause.\n\n\nA19:\n\nFirst, the error you're getting is due to where you're using the COUNT function -- you can't use an aggregate (or group) function in the WHERE clause.\nSecond, instead of using a subquery, simply join the table to itself:\nSELECT a.pid \nFROM Catalog as a LEFT JOIN Catalog as b USING( pid )\nWHERE a.sid != b.sid\nGROUP BY a.pid\n\nWhich I believe should return only rows where at least two rows exist with the same pid but there is are at least 2 sids.  To make sure you get back only one row per pid I've applied a grouping clause.\n\n\nA20:\n\nA bit late probably but now there is PDOStatement::debugDumpParams\n\nDumps the informations contained by a prepared statement directly on\n  the output. It will provide the SQL query in use, the number of\n  parameters used (Params), the list of parameters, with their name,\n  type (paramtype) as an integer, their key name or position, and the\n  position in the query (if this is supported by the PDO driver,\n  otherwise, it will be -1).\n\nYou can find more on the official php docs\nExample:\n<?php\n/* Execute a prepared statement by binding PHP variables */\n$calories = 150;\n$colour = 'red';\n$sth = $dbh->prepare('SELECT name, colour, calories\n    FROM fruit\n    WHERE calories < :calories AND colour = :colour');\n$sth->bindParam(':calories', $calories, PDO::PARAM_INT);\n$sth->bindValue(':colour', $colour, PDO::PARAM_STR, 12);\n$sth->execute();\n\n$sth->debugDumpParams();\n\n?>\n\n\n\nA21:\n\nYou can achieve this by following function.\nRun following query to create function.\nDELIMITER ||\nCREATE FUNCTION `TOTAL_OCCURANCE`(`commastring` TEXT, `findme`     VARCHAR(255)) RETURNS int(11)\nNO SQL\n-- SANI: First param is for comma separated string and 2nd for string to find.\nreturn ROUND (   \n    (\n        LENGTH(commastring)\n        - LENGTH( REPLACE ( commastring, findme, \"\") ) \n    ) / LENGTH(findme)        \n);\n\nAnd call this function like this\nmsyql> select TOTAL_OCCURANCE('A,B,C,A,D,X,B,AB', 'A');\n\n\n\nA22:\n\nselect * from shirts where find_in_set('1',colors) <> 0\n\nWorks for me\n\n\nA23:\n\nYou need to use HAVING, not WHERE.\nThe difference is: the WHERE clause filters which rows MySQL selects. Then MySQL groups the rows together and aggregates the numbers for your COUNT function.\nHAVING is like WHERE, only it happens after the COUNT value has been computed, so it'll work as you expect. Rewrite your subquery as:\n(                  -- where that pid is in the set:\nSELECT c2.pid                  -- of pids\nFROM Catalog AS c2             -- from catalog\nWHERE c2.pid = c1.pid\nHAVING COUNT(c2.sid) >= 2)\n\n\n\nA24:\n\nI think that .uniq [1] will solve your problem. \n[1] Available.select('...').uniq\n\nTake a look at http://guides.rubyonrails.org/active_record_querying.html#selecting-specific-fields\n\n\nA25:\n\nIf I remember correctly, in PostgreSQL you have to add every column you fetch from the table where the GROUP BY clause applies to the GROUP BY clause.\n\n\nA26:\n\nI modified the method to include handling output of arrays for statements like WHERE IN (?). \nUPDATE: Just added check for NULL value and duplicated $params so actual $param values are not modified. \nGreat work bigwebguy and thanks!\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_string($value))\n            $values[$key] = \"'\" . $value . \"'\";\n\n        if (is_array($value))\n            $values[$key] = \"'\" . implode(\"','\", $value) . \"'\";\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n\n    $query = preg_replace($keys, $values, $query);\n\n    return $query;\n}\n\n\n\nA27:\n\nI spent a good deal of time researching this situation for my own needs. This and several other SO threads helped me a great deal, so I wanted to share what I came up with.\nWhile having access to the interpolated query string is a significant benefit while troubleshooting, we wanted to be able to maintain a log of only certain queries (therefore, using the database logs for this purpose was not ideal). We also wanted to be able to use the logs to recreate the condition of the tables at any given time, therefore, we needed to make certain the interpolated strings were escaped properly. Finally, we wanted to extend this functionality to our entire code base having to re-write as little of it as possible (deadlines, marketing, and such; you know how it is).\nMy solution was to extend the functionality of the default PDOStatement object to cache the parameterized values (or references), and when the statement is executed, use the functionality of the PDO object to properly escape the parameters when they are injected back in to the query string. We could then tie in to execute method of the statement object and log the actual query that was executed at that time (or at least as faithful of a reproduction as possible).\nAs I said, we didn't want to modify the entire code base to add this functionality, so we overwrite the default bindParam() and bindValue() methods of the PDOStatement object, do our caching of the bound data, then call parent::bindParam() or parent::bindValue(). This allowed our existing code base to continue to function as normal.\nFinally, when the execute() method is called, we perform our interpolation and provide the resultant string as a new property E_PDOStatement->fullQuery. This can be output to view the query or, for example, written to a log file.\nThe extension, along with installation and configuration instructions, are available on github:\nhttps://github.com/noahheck/E_PDOStatement\nDISCLAIMER:\nObviously, as I mentioned, I wrote this extension. Because it was developed with help from many threads here, I wanted to post my solution here in case anyone else comes across these threads, just as I did.\n\n\nA28:\n\nThe IN clause describes a set of values, and sets do not have order.\nYour solution with a join and then ordering on the display_order column is the most nearly correct solution; anything else is probably a DBMS-specific hack (or is doing some stuff with the OLAP functions in standard SQL).  Certainly, the join is the most nearly portable solution (though generating the data with the display_order values may be problematic).  Note that you may need to select the ordering columns; that used to be a requirement in standard SQL, though I believe it was relaxed as a rule a while ago (maybe as long ago as SQL-92).\n\n\nA29:\n\nMy first thought was to write a single query, but you said that was not possible because one is run by the user and the other is run in the background.  How are you storing the list of ids to pass from the user to the background process?  Why not put them in a temporary table with a column to signify the order.\nSo how about this:\n\nThe user interface bit runs and inserts values into a new table you create.  It would insert the id, position and some sort of job number identifier)\nThe job number is passed to the background process (instead of all the ids)\nThe background process does a select from the table in step 1 and you join in to get the other information that you require.  It uses the job number in the WHERE clause and orders by the position column.\nThe background process, when finished, deletes from the table based on the job identifier.\n\n\n\nA30:\n\nUse MySQL FIND_IN_SET function:\n  SELECT * \n    FROM table_name \n   WHERE id IN (..,..,..,..) \nORDER BY FIND_IN_SET (coloumn_name, .., .., ..);\n\n\n\nA31:\n\nPostgreSQL is more SQL compliant than MySQL. All fields - except computed field with aggregation function - in the output must be present in the GROUP BY clause.\n\n\nA32:\n\nFirst check if you are in the right database USE yourDB and try Select * from contenttype just to see what is it and if it exists really...\n\n\nA33:\n\nA solution is to voluntarily put an error in the query and to print the error's message:\n//Connection to the database\n$co = new PDO('mysql:dbname=myDB;host=localhost','root','');\n//We allow to print the errors whenever there is one\n$co->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n//We create our prepared statement\n$stmt = $co->prepare(\"ELECT * FROM Person WHERE age=:age\"); //I removed the 'S' of 'SELECT'\n$stmt->bindValue(':age','18',PDO::PARAM_STR);\ntry {\n    $stmt->execute();\n} catch (PDOException $e) {\n    echo $e->getMessage();\n}\n\nStandard output:\n\nSQLSTATE[42000]: Syntax error or access violation: [...] near 'ELECT * FROM Person WHERE age=18' at line 1\n\nIt is important to note that it only prints the first 80 characters of the query.\n\n\nA34:\n\nWhat kind of field is this? The IN operator cannot be used with a single field, but is meant to be used in subqueries or with predefined lists:\n-- subquery\nSELECT a FROM x WHERE x.b NOT IN (SELECT b FROM y);\n-- predefined list\nSELECT a FROM x WHERE x.b NOT IN (1, 2, 3, 6);\n\nIf you are searching a string, go for the LIKE operator (but this will be slow):\n-- Finds all rows where a does not contain \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE '%text%';\n\nIf you restrict it so that the string you are searching for has to start with the given string, it can use indices (if there is an index on that field) and be reasonably fast:\n-- Finds all rows where a does not start with \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE 'text%';\n\n\n\nA35:\n\nMySQL's GROUP BY can be used without an aggregate function (which is contrary to the SQL standard), and returns the first row in the group (I don't know based on what criteria), while PostgreSQL must have an aggregate function (MAX, SUM, etc) on the column, on which the GROUP BY clause is issued.\n\n\nA36:\n\nIf you're using MySQL, there is a method REGEXP that you can use...\nhttp://dev.mysql.com/doc/refman/5.1/en/regexp.html#operator_regexp\nSo then you would use:\nSELECT * FROM `shirts` WHERE `colors` REGEXP '\\b1\\b'\n\n\n\nA37:\n\nSELECT * FROM table WHERE field1 NOT LIKE '%$x%';   (Make sure you escape $x properly beforehand to avoid SQL injection)\nEdit: NOT IN does something a bit different - your question isn't totally clear so pick which one to use.   LIKE 'xxx%' can use an index.  LIKE '%xxx' or LIKE '%xxx%' can't.\n\n\nA38:\n\nYou should actually fix your database schema so that you have three tables:\nshirt: shirt_id, shirt_name\ncolor: color_id, color_name\nshirtcolor: shirt_id, color_id\n\nThen if you want to find all of the shirts that are red, you'd do a query like:\nSELECT *\nFROM shirt, color\nWHERE color.color_name = 'red'\n  AND shirt.shirt_id = shirtcolor.shirt_id\n  AND color.color_id = shirtcolor.color_id\n\n\n\nA39:\n\nNot the prettiest solution, but changing the group parameter to output every column in model works in PostgreSQL:\nexpiration = Available.find(:all,\n:joins => [ :room ],\n:conditions => [ \"rooms.hotel_id = ? AND availables.bookdate BETWEEN ? AND ?\", hostel_id, date.to_s, (date+days-1).to_s ],\n:group => Available.column_names.collect{|col| \"availables.#{col}\"},\n:order => 'availables.updated_at')\n\n\n\nA40:\n\nSELECT ORDER_NO, DELIVERY_ADDRESS \nfrom IFSAPP.PURCHASE_ORDER_TAB \nwhere ORDER_NO in ('52000077','52000079','52000167','52000297','52000204','52000409','52000126') \nORDER BY instr('52000077,52000079,52000167,52000297,52000204,52000409,52000126',ORDER_NO)\n\nworked really great\n\n\nA41:\n\nIf the set of colors is more or less fixed, the most efficient and also most readable way would be to use string constants in your app and then use MySQL's SET type with FIND_IN_SET('red',colors) in your queries. When using the SET type with FIND_IN_SET, MySQL uses one integer to store all values and uses binary \"and\" operation to check for presence of values which is way more efficient than scanning a comma-separated string.\nIn SET('red','blue','green'), 'red' would be stored internally as 1, 'blue' would be stored internally as 2 and 'green' would be stored internally as 4. The value 'red,blue' would be stored as 3 (1|2) and 'red,green' as 5 (1|4).\n\n\nA42:\n\nTake a look at the FIND_IN_SET function for MySQL.\nSELECT * \n    FROM shirts \n    WHERE FIND_IN_SET('1',colors) > 0\n\n\n\nA43:\n\nI assume you mean that you want the final SQL query, with parameter values interpolated into it.  I understand that this would be useful for debugging, but it is not the way prepared statements work.  Parameters are not combined with a prepared statement on the client-side, so PDO should never have access to the query string combined with its parameters.\nThe SQL statement is sent to the database server when you do prepare(), and the parameters are sent separately when you do execute().  MySQL's general query log does show the final SQL with values interpolated after you execute().  Below is an excerpt from my general query log.  I ran the queries from the mysql CLI, not from PDO, but the principle is the same.\n081016 16:51:28 2 Query       prepare s1 from 'select * from foo where i = ?'\n                2 Prepare     [2] select * from foo where i = ?\n081016 16:51:39 2 Query       set @a =1\n081016 16:51:47 2 Query       execute s1 using @a\n                2 Execute     [2] select * from foo where i = 1\n\nYou can also get what you want if you set the PDO attribute PDO::ATTR_EMULATE_PREPARES.  In this mode, PDO interpolate parameters into the SQL query and sends the whole query when you execute().  This is not a true prepared query.  You will circumvent the benefits of prepared queries by interpolating variables into the SQL string before execute().\n\nRe comment from @afilina:\nNo, the textual SQL query is not combined with the parameters during execution. So there's nothing for PDO to show you.\nInternally, if you use PDO::ATTR_EMULATE_PREPARES, PDO makes a copy of the SQL query and interpolates parameter values into it before doing the prepare and execute. But PDO does not expose this modified SQL query. \nThe PDOStatement object has a property $queryString, but this is set only in the constructor for the PDOStatement, and it's not updated when the query is rewritten with parameters.\nIt would be a reasonable feature request for PDO to ask them to expose the rewritten query. But even that wouldn't give you the \"complete\" query unless you use PDO::ATTR_EMULATE_PREPARES.\nThis is why I show the workaround above of using the MySQL server's general query log, because in this case even a prepared query with parameter placeholders is rewritten on the server, with parameter values backfilled into the query string. But this is only done during logging, not during query execution.\n\n\nA44:\n\nSounds like you have Schroedinger's table... \nSeriously now, you probably have a broken table. Try:\n\nDROP TABLE IF EXISTS contenttype\nREPAIR TABLE contenttype\nIf you have sufficient permissions, delete the data files (in /mysql/data/db_name)\n\n\n\nA45:\n\nCorrect, the solution to fixing this is to use :select and to select each field that you wish to decorate the resulting object with and group by them.\nNasty - but it is how group by should work as opposed to how MySQL works with it by guessing what you mean if you don't stick fields in your group by.\n\n\nA46:\n\nFor Oracle, John's solution using instr() function works. Here's slightly different solution that worked - \nSELECT id \nFROM table1 \nWHERE id IN (1, 20, 45, 60) \nORDER BY instr('1, 20, 45, 60', id)\n\n\nA47:\n\nMySQL's totally non standards compliant GROUP BY can be emulated by Postgres' DISTINCT ON. Consider this:\nMySQL:\nSELECT a,b,c,d,e FROM table GROUP BY a\n\nThis delivers 1 row per value of a (which one, you don't really know). Well actually you can guess, because MySQL doesn't know about hash aggregates, so it will probably use a sort... but it will only sort on a, so the order of the rows could be random. Unless it uses a multicolumn index instead of sorting. Well, anyway, it's not specified by the query.\nPostgres:\nSELECT DISTINCT ON (a) a,b,c,d,e FROM table ORDER BY a,b,c\n\nThis delivers 1 row per value of a, this row will be the first one in the sort according to the ORDER BY specified by the query. Simple.\nNote that here, it's not an aggregate I'm computing. So GROUP BY actually makes no sense. DISTINCT ON makes a lot more sense.\nRails is married to MySQL, so I'm not surprised that it generates SQL that doesn't work in Postgres.\n\n\nA48:\n\nIf you want to do arbitrary sorting on a query using values inputted by the query in MS SQL Server 2008+, it can be done by creating a table on the fly and doing a join like so (using nomenclature from OP).\nSELECT table1.name, table1.description ... \nFROM (VALUES (id1,1), (id2,2), (id3,3) ...) AS orderTbl(orderKey, orderIdx) \nLEFT JOIN table1 ON orderTbl.orderKey=table1.id\nORDER BY orderTbl.orderIdx\n\nIf you replace the VALUES statement with something else that does the same thing, but in ANSI SQL, then this should work on any SQL database.\nNote: \nThe second column in the created table (orderTbl.orderIdx) is necessary when querying record sets larger than 100 or so. I originally didn't have an orderIdx column, but found that with result sets larger than 100 I had to explicitly sort by that column; in SQL Server Express 2014 anyways.\n\n\nA49:\n\nPDOStatement has a public property $queryString. It should be what you want.\nI've just notice that PDOStatement has an undocumented method debugDumpParams() which you may also want to look at.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "So, we have a query that has a tableif you is a the best answer, and a query that is a single user, and a user, and"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 33, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI am currently working on a project that is a spinoff (fork) from a framework I have been working on.\nThis project is intended to be pretty generic, but now I need to fork the codebase again for a client of mine.\nAt this moment, I have created a custom branch for my client, but I'd rather have a standalone repository for this.\n\nIs is possible to 'fork a fork'?\nIf not, what alternatives do I have?\n\nOutline of the situation:\n\nFramework repository (original)\n\n\nGeneric application repository (fork)\n\n\n(not yet) Client repository (fork of application)\n\n\n\nNote: when trying to  'fork a fork' in Github, you will receive a notification that you have already forked the project:\n\n\n\nCandidate answers:\nA1:\n\nIn my repo's directory on my machine in the terminal I had to reset the origin url:\ngit remote set-url origin git@github.com:repoaccountname/repo-name.git\n\n\n\nA2:\n\nI had more luck with navigating in my terminal to the directory I wanted to add to the repository, then (assuming you're working on a branch called master):\n    git init\n    git add .\n    git commit -m \"my commit\"\n    git remote add origin <remote repository URL>\n    git push origin master\n\nHere's a link to an article explaining how to do it in more detail: https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/\nNote that you won't be able to run the \"git add .\" line if the directory in question is open.\n\n\nA3:\n\n1. first create a git repostry.\n2. second open git bash in  existing or uploading project.\n3. perform git init \n4. git add .\n5. git commit -m \"print message\"\n6. git remote add github<repostry url>\n7. git remote -v\n8. git push github master\n\nOR\ngit push origin master\n\nif  you get any error, you may use it\ngit push -f origin master\n\n\n\nA4:\n\nYou might have changed your repository name\nIn your local repository edit the file:\n.git/config\n\nThen check:\n[remote \"origin\"]\n   url = \n\nthat the URL matches your remote repository\n\n\nA5:\n\nGo to Sourctree -> Tools -> Options -> Git Select \n\n\nA6:\n\nThe easiest way is to create an app password and use that password.\nHere you can see how you can create an app password on Bitbucket:\nhttps://docs.snyk.io/tutorials/getting-started/atlassian-integrations/atlassian-bitbucket-app-password\n\n\nA7:\n\nSo i had this project that wasnt under source control i made some changes to and wanted to keep stuff i changed.\ngit init\ngit remote add origin <url>\ngit fetch\ngit branch master origin/master\ngit restore --staged .\n\n\n\nA8:\n\nI got the same problem while using a github repository, and connecting to it via https, while using the OS X Keychain Credential helper.\nMy problem was that I had the wrong credentials stored in OS X's Keychain (I was using the email address that I used to sign up for github.com rather than the [username]@github.com address it provides you). I deleted the old account in the keychain and only left the @github.com one and it fixed the problem.\nNot sure if it is related, but when I checked the user.email local config:\ngit config -l\n\nit showed the incorrect email address as well, so I updated the local git user.email to use the correct account too:\ngit config user.email <username>@github.com\n\n\n\nA9:\n\nif you have more files in my case i have 7000 image files when i try to add them from project's route folder it hasn't added them but when i go to the image folder everything is ok. Go through the target folder and command like abows\ngit add .\ngit commit -am \"image uploading\"\ngit push origin master\n\n\n\ngit push origin master Enumerating objects: 6574, done. Counting objects: 100% (6574/6574), done. Delta compression using up to 4\n    threads Compressing objects: 100% (6347/6347), done. Writing objects: \n    28% (1850/6569), 142.17 MiB | 414.00 KiB/s\n\n\n\n\nA10:\n\nI am using a SSH key, but SourceTree was asking me a password every time.\nFound on the community of Atlassian, this solution worked for me:\n\nOpen the terminal, get into your project directory\nType git config credential.helper store\nType git pull\nInput username/password (if asked)\n\nVoil\u00e0 !\n\n\nA11:\n\nIn addition to setting up the Personal Access Token on GitHub, and doing what A_01 states above, this may be helpful (it's how I finally got it working)\n\nI made sure had the latest version of Sourcetree \nI upgraded to the latest version of and used System Git (Tools -> Options -> Git)\nUpgraded to the latest Git Credential Manager for Windows (this was via an installer I downloaded and double-clicked, very easy)\nEnsured I was using OAuth for my GitHub account stored in Tools -> Options -> Authentication\nMade sure I checked off the box \"Allow Sourcetree to manage my credentials using the Git Credential Manager\" (this was important!!) on Tools -> Options -> Git\nThen when I triggered the GitHub dialog (via pushing a new branch, just an example), I logged in there (this is where it used to fail).\nI next got the 2 factor auth dialog, and the code was sent to my phone as expected. I was able to enter the code from my phone into this dialog.\n\n\n\nA12:\n\nMy issue was that I used the clone https url widget provided by github. That URL doesn't work for private repositories as you need to add a username to the front of it.  \nExample: a private repo owned by john and named widget with collaborator sam the correct url would be: \nhttps://sam@github.com/john/widget.git\nThe github provided url: \nhttps://github.com/john/widget.git\nThe error message leaves much to be desired.\n\n\nA13:\n\nMaybe an obvious thing, but...\nIf you have problem with the index, use git-gui. You get a very good view how the index (staging area) actually works.\nAnother source of information that helped me understand the index was Scott Chacons \"Getting Git\" page 259 and forward.\nI started off using the command line because most documentation only showed that...\nI think git-gui and gitk actually make me work faster, and I got rid of bad habits like \"git pull\" for example... Now I always fetch first... See what the new changes really are before I merge.\n\n\nA14:\n\nIn my case, the password that Sourcetree was asking for was for the ssh private key password. If I remove the password from the ssh key, this prompt doesn't come up.\nHowever since I wanted the password to be there, I kept investigating why it wasn't accepting my ssh key password.\nTurns out there is a wrinkle. If you have multiple ssh private keys which have different passwords, Sourcetree will give you a password prompt for each password.\nThink of it this way - if you have a fresh shell in your system, and you just started ssh-agent up, and ran ssh-add, what are the password prompts that you get? For every such password prompt, Sourcetree will give you this dialog box. In fact, if you add passwords to the dialog in the same order as you would in the shell to an ssh-add, this would work. In fact, I am guessing Sourcetree is doing exactly the same in the background.\nSo my final solution was 1) keep the passwords on the ssh keys 2) make all these passwords the same. So now Sourcetree will give me a single password prompt, and entering the ssh private key password will work.\n\n\nA15:\n\nIf you are using \"Username & Password\" authentication,you can try to change to use SSH keys for the authentication.\nI have met this kind of situation when I commit to GitLab everytime.When I changed the way of authentication:SSH keys,the problem has been solved.\n\n\nA16:\n\nHere is what worked for me.\nI tried almost all the solutions mentioned here and on the web a lot of times. I gave up multiple times before and was using it this way for more than a year I think. Finally, I was able to solve this annoying issue.\n\nAs suggested by user \"AEQ\" in his answer, I deleted all the passwords related to GitHub from my Keychain Access.\nTried deleting and re-adding the Github account. But still, the problem persisted. (generated a personal access token from \"https://github.com/settings/tokens\" and use it as the password)\n\nWhat made it work was\nMake sure to use HTTPS and not SSH when adding the new user account. If you have already added the account, just edit it. That's it.\nFinally, It feels good.\nAttaching screenshots for ease.\n\n\n\nA17:\n\nI copied a small sub project I had that was under Git source control into another project and forgot to delete the .git folder. When I went to commit I got the same message as above and couldn't clear it until I deleted the .git folder.\nIt is a bit silly, but it is worth checking you don't have a .git folder under the folder that doesn't commit.\n\n\nA18:\n\nGo to the directory where you code is,\ngit init\ngit add .\ngit commit -m \"Your message\"\n\nNow add your address go to your git hub copy the clone address,\ngit remote add origin <remote repository URL>\n\nNow add push your code with,\ngit push -u -f origin master\n\nAnd you are done.\n\n\nA19:\n\nAlso make sure the repo you've entered is cased correctly (it's case sensitive).\n\n\nA20:\n\nIn my case, I was trying to clone a private repository in our group Github account to me laptop:\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone https://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nfatal: https://github.com/nyusngroup/velocity_CCSNe.git/info/refs?service=git-upload-pack not found: did you run git update-server-info on the server?\n\nI found two ways can solve this.\n(1) Use http instead of https. It asks me the name and password of our group Github account. After I entered the information, I can clone it.\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone http://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nUsername for 'http://github.com':nyusngroup\nPassword for 'http://nyusngroup@github.com': \n\n(2) Add my Github account to the collaborators of the private repository in our group Github account, as pointed by Monkey King's answer above.\n\n\nA21:\n\nFor future references, I solved this problem by changing the GIT system used in Sourcetree, from Embedded to System.\nHow to change the git system:\n \n1. Find the Options submenu (located under Tools menu) and click on it to view the Options popup \n2. On the Options setting popup, find the Git tab, then click on it to view the Git setting \n3. Find Git version setting, then check the version used in it. In my case, I changed it to system because previously I'm using Git bash cmd, and it looks like that the git embedded in Sourcetree clashes with each other. \n4. Restart Sourcetree\n\n\nA22:\n\nYou didn't add the changes. Either specifically add them via\ngit add filename1 filename2\n\nor add all changes (from root path of the project)\ngit add .\n\nor use the shorthand -a while commiting:\ngit commit -a -m \"message\".\n\n\n\nA23:\n\nIt looks like that's a private (or deleted) repository; if you visit the repository page while logged it'll give you the real URL, which'll probably be https://TeaCodie@github.com/TeaCodie/TeaCodie-Website.git , i.e. with a username specified?\n\n\nA24:\n\nA solution worked for me given by Andrew Magill, Copied from Source : link\nThe problem is that SourceTree's embedded Git client comes with git-credential-manager v1.12, which no longer works with Github since they disabled TLS 1.1 on their site.  You need to get git-credential-manager v1.14.  Easiest way to do that is to install a current copy of the Git client separately, and then switch SourceTree over to use that (\"system git\") instead of its embedded client.  Alternatively, you can update git-credential-manager in your embedded client by replacing its files with the newest version from Microsoft.\n\n\nA25:\n\nThis error could also happen if the repository you are attempting to reach was deleted.\n\n\nA26:\n\nIn my case, the Sourcetree login screen repeatedly showed when pushing to GitHub. Logging in from that screen failed. I finally determined it was due to an expired OAuth token, which is trivial to fix.\n\nNavigate to the GitHub account config screen: Tools > Options > Authentication\nClick your GitHub account and then click edit\nOn the Edit Hosting Account dialog click the Refresh OAuth Token button\n\n\n\nA27:\n\nIf you are not using GitHub 2FA, another reason for asking you your password is:  \nYour GitHub repo url is an ssh one, not an https one.\nSince git does not find your ssh keys (or your ssh key is not registered to your GitHub account), it fails back to account/password mechanism. \nIf you are using 2FA (and an https url), then you need a personal token as a password.\n\n\nA28:\n\nif you have a subfolder, which was cloned from other git-Repository, first you have to remove the $.git$ file from the child-Repository:\nrm -rf .git\nafter that you can change to parent folder and use git add -A. \n\n\nA29:\n\n\ngit init\ngit add .\ngit commit -m \"initial commit\"\ngit remote add origin [URL]\ngit push origin masteror\n\ngit push -f origin master\n\ngit push --force.\nIt does force the update\n\n\n\nA30:\n\nSince this question is specifically about SourceTree and Github, the answer in 2021 is simple. Quoting bcr:\n\nEnsured I was using OAuth for my GitHub account stored in Tools ->\nOptions -> Authentication\n\n\nGo there. Select Add (new account).\nSelect OAuth.\nClick Refresh OAuth Token. A browser page will open (github.com) requesting renewal of your token.\nLogin to accept. SourceTree will immediately respond by acknowledging the token provided by github.com.\n\n\n\nA31:\n\nIf you are using two-factor authentication with GitHub you will need to create a personal access token and use it with SourceTree:\n\nTo work with GitHub's two-factor authentication in SourceTree you can simply use your access token instead of your password. The steps to do this are as follows:\n\nGo to your Personal Access Tokens settings in GitHub.\nClick on the Generate new token button.\nName the token something descriptive.\nSelect which scopes you wish to grant this token.\nClick the Generate token button.\nCopy the token and use it as a password in your hosted repositories.\n\nYou can find more information about this on GitHub's help here.\n\nMac Users:\nIf your SourceTree keeps on asking for the password, go to the Terminal and type this:\ngit config --global credential.helper osxkeychain\n\n\n\nA32:\n\nI received this same error when I attempted to git clone something from Kiln what was actually a Mercurial repository.\n\n\nA33:\n\nI had an issue where I was doing commit --amend even after issuing a git add . and it still wasn't working.  Turns out I made some .vimrc customizations and my editor wasn't working correctly.  Fixing these errors so that vim returns the correct code resolved the issue.\n\n\nA34:\n\nI think it is very preferable if you first pull the existing Github repo on the local and then add the new files to the Github repo\nThis link will help: https://stackoverflow.com/a/61285076/5840973\n\n\nA35:\n\nMac:\nThis solution works for me.\nAfter clone code successfully, you open terminal and run this code\ngit config --global credential.helper osxkeychain\n\n\n\nA36:\n\nI find this problem appearing when I've done a git add . in a subdirectory below where my .gitignore file lives (the home directory of my repository, so to speak).  Try changing directories to your uppermost directory and running git add . followed by git commit -m \"my commit message\".\n\n\nA37:\n\nI contacted GitHub support about this 'issue' and they confirmed this is not possible. Also, it is very unlikely that such a feature will be implemented in the future.\n\n\nA38:\n\nAssume that I would like to add FreeRTOS repository, which URL is https://github.com/FreeRTOS/FreeRTOS-Kernel.git, into my repository, example URL is https://github.com/username/example as a submodule\ngit submodule add https://github.com/FreeRTOS/FreeRTOS-Kernel.git\ngit add .\ngit commit -m 'add a submodule'\ngit push\n\nTo clone using HTTPS:\ngit clone https://github.com/username/example.git --recurse-submodules\n\nUsing SSH:\ngit clone git@github.com:username/example.git --recurse-submodules\n\nIf you have downloaded the repo without using the --recurse-submodules argument, you need to run:\ngit submodule update --init --recursive\n\n\n\nA39:\n\nLets say you have remote repo with files and local repo with the same files.\nAnd you want to add Git to local files, and dont want to push.\nThen you can do such commands on local repo:\ngit init\ngit remote add origin <url>\ngit fetch --all\ngit reset --hard origin/master\n\nAfter that your repository is synced with remote repo.\nYou can change files, git add, push and so on.\n\n\nA40:\n\nI had a similar thing after an update; I tried re-adding my bitbucket credentials, changing the protocols, etc. with no luck, until...\n\nI opened my keychain and searched for bitbucket (probably the same for github, just search for github instead) under the Passwords category. \nI found that I had multiple Access Keys for my user, so I closed my repository, deleted all the keys of \"application password\" Kind, opened the SourceTree preferences > Accounts > removed my account and re-added it (login as usual - I used Basic with HTTPS). \nKeychain asks for permission to save the password, which I said yes and now I only see one key of the kind \"application password\". \n\nI opened my repository from the repo browser and hey presto, no more password popups!\nThis bugged me for a whole week! If this helped you, you can do a little dance with me now :)\n\n\nA41:\n\nAll the answers above seems to guide about creating a new repository in git but the question is about adding a folder to existing repo. To do that following steps can be followed.\n\nClone your existing repo using following command:\ngit clone https://github.com/company/our_repo.git \nManually take your project folder to the desired location i.e. trunk/bin\nNow commit and then push in the repo using the commands:\ngit commit -m \"message\" and git push origin master \n\n\n\nA42:\n\nYou could have done a:\ngit add -u -n\n\nTo check which files you modified and are going to be added (dry run: -n option), and then\ngit add -u\n\nTo add just modified files\n\n\nA43:\n\nI met up with the same problem.\nHow I solved this problem is:\nI use git bash to create a new repo, when I typed \"git push origin master\"\nIt reported \n\"fatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\"\nFinally, I found there was not a repo on the github at all.\nYou'd better create a new repo first on github. \nMaybe this experience can help somebody.\n\n\nA44:\n\nOpen your Terminal, access to this folder and write:\ngit init\ngit add .\ngit commit -m \"my commit\"\ngit remote set-url origin git@github.com:username/repo.git\ngit push origin master\n\n\n\nA45:\n\nopen terminal and clone your repo. repository url must have your usedid,\nFor ex:\n$ git clone https://<username>@github.com/<userId>/<reponame>.git\n$ git clone https://mark@github.com/mak123/<reponame>.git\n\nThen drag and drop the cloned folder into the sourceTree window.You can also do by clicking \"+New Repository\" button and from the menu by selecting \"Add Existing Local Repository\".\nIt may ask for password again but this time it will get added to your keychain.\n\n\nA46:\n\nReally struggled with this on Mac, couldn't get it to work with SSH. Ended up following this guide which worked: https://www.youtube.com/watch?v=otRHRPnHDeI.\nThere were a couple things which could have been the cause, it may not have been the SSH itself. For instance, Oauth apparently doesn't work on SourceTree despite nothing appearing wrong immediately when using it. Making sure you use a Personal Access Token and not your real password among others.\nA good rule of thumb is, if after connecting your account it only shows your public repos, it's already broken. https://github.com/settings/tokens\n\n\nA47:\n\nIn my case, this was happening with a submodule that had defaulted to https even though the main project was using ssh. I fixed it by changing the submodule's repo path in SourceTree's Repository Settings to use the ssh path instead of https.\n\n\nA48:\n\nDid you create a new repository on the http://github.com with the same name? \nIf not, do it! And make sure each letter is correct and case sensitive.\n\n\nA49:\n\nAs the message says:\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nGit has a \"staging area\" where files need to be added before being committed, you can read an explanation of it here.\n\nFor your specific example, you can use:\ngit commit -am \"save arezzo files\"\n\n(note the extra a in the flags, can also be written as git commit -a -m \"message\" - both do the same thing)\nAlternatively, if you want to be more selective about what you add to the commit, you use the git add command to add the appropriate files to the staging area, and git status to preview what is about to be added (remembering to pay attention to the wording used).\nYou can also find general documentation and tutorials for how to use git on the git documentation page which will give more detail about the concept of staging/adding files.\n\nOne other thing worth knowing about is interactive staging - this allows you to add parts of a file to the staging area, so if you've made three distinct code changes (for related but different functionality), you can use interactive mode to split the changes and add/commit each part in turn. Having smaller specific commits like this can be helpful.\n\n\nA50:\n\nFollow to @Chris answer in case you've enabled 2FA, to use Personal Access Token in SourceTree, you need to add your acc with this token in SourceTree > Preferences with following information:\n\nAuth Type: Basic  \nUsername: {your-github-account} \nPassword: {your-personal-access-token} \nProtocol: HTTPS\n\nAfter that, you are able to clone your Github repo.\n\n\n\nA51:\n\ndelete each .git file in all projects\nyou can use this command\nrm -rf .git\n\n\n\nA52:\n\nThis happened to me and at first it was not apparent what had gone wrong. The previous day I had renamed my repo on github.com only changing the first character of the repo name from a lowercase letter to uppercase. I was not aware that the repo name in the origin URL was case-sensitive unit I received the error which contained the text, \"did you run git update-server-info on the server.\" Once I edited the .git/config file to correct the case of the same letter in the origin URL, the problem was solved, the error no longer occurred and I was once again able to push my changes up to the repo on github.com. Another bit of advice: if you make changes to your repo on github, make sure to test sync'ing your local repo right away. In my case, I did not and it was the next day when I had tried to sync and couldn't because of the error and I forgot that I had renamed the repo, so I was scratching my head for a bit. Had I tested sync'ing right away, I would have caught this problem right away.\n\n\nA53:\n\nI finally was able to solve this after months of no luck. On a Mac :\n\nOpen 'Keychain Access' on a Mac by using Launchpad or Spotlight Search\nNarrow down your search by selecting \"login\" under Keychains and \"Passwords\" under the Category (both filters on the left side of the window)\nIn the top right hand corner of the window, do a search for \"git\"\nYou'll likely see a keychain there. I right-clicked and deleted the one entry I had.\nGo into sourcetree and try to do a pull/fetch/anything. You'll be prompted for a password. This should re-initiate your password with Sourcetree. To double check this, go into your keychain and repeat steps 3 & 4. Click on the keychain and look at the access control tab. SourcetreeLogin should be listed as always having access now.\n\n\n\n\nA54:\n\nI encountered this message when using Jenkins 2.176.1 and git plugin 3.10.0 using a very old command line git version (1.7.1) as included on CentOS 6.\nThe message does not occur on newer versions of command line git (1.8 or later) with the Jenkins git plugin.\nUpgrading to a newer command line git version resolves the problem.\nOfficially, the Jenkins git plugin does not support command line git 1.7.1.  The minimum supported version is command line git 1.7.10. Many Jenkins git plugin use cases require at least command line git 1.9.0.\nCommand line git 1.7.1 behaves differently than later versions when a repository has been initialized (with git init) and then a git fetch is performed with a refspec which references 'origin'.  Later versions of command line git do not have the same problem.\n\n\nA55:\n\nSimple answer: Yes and no.\nNo, for a single account as you are unable to create two repositories with the same name (and forks always have the same name).\nYes, for multiple accounts. So you could setup an organization for your second fork.\n\n\nA56:\n\nThe reason why this is happening is because you have a folder that is already being tracked by Git inside another folder that is also tracked by Git. For example, I had a project and I added a subfolder to it. Both of them were being tracked by Git before I put one inside the other. In order to stop tracking the one inside, find it and remove the Git file with:\nrm -rf .git\n\nIn my case I had a WordPress application and the folder I added inside was a theme. So I had to go to the theme root, and remove the Git file, so that the whole project would now be tracked by the parent, the WordPress application.\n\n\nA57:\n\nIn my case I needed to put my ssh password instead of my personal access token. Then it worked.\n\n\nA58:\n\nIn my case I was using the wrong protocol in the repository URL (http instead of https)\n\n\nA59:\n\nIn my case my github account did not have permissions to the repo. Added the github account as a collaborator for the repo and that fixed it.\n\n\nA60:\n\nIn my case, I cloned the repo using HTTPS based url. Then I configured my githib account with 2-factor authentication using SSH key. So whenever I was pulling/pushing changes, it was showing me dialog to provide my password.\nI fixed it by enabling SSH access by right clicking on project repo-menu on SourceTree.\n\n\n\nA61:\n\nI had a very similar issue with the same error message. \"Changes not staged for commit\", yet when I do a diff it shows differences.  I finally figured out that a while back I had changed a directories case. ex. \"PostgeSQL\" to \"postgresql\".  As I remember now sometimes git will leave a file or two behind in the old case directory.  Then you will commit a new version to the new case.\nThus git doesn't know which one to rely on.  So to resolve it, I had to go onto the github's website. Then you're able to view both cases.  And you must delete all the files in the incorrect cased directory. Be sure that you have the correct version saved off or in the correct cased directory.\nOnce you have deleted all the files in the old case directory, that whole directory will disappear. Then do a commit.\nAt this point you should be able to do a Pull on your local computer and not see the conflicts any more.  Thus being able to commit again.  :)\n\n\nA62:\n\nMake sure that your user account is added to the repository as a collaborator.\nSetting --> Collaborators\n\n\nA63:\n\nIn my case I had old version of the git. Upgrade to latest version fixed this issue.\n\n\nA64:\n\nI got this issue when using GitStack. I looked into C:/GitStack/apache/logs/error.log and found that GitStack was looking for the repository in the wrong location although its settings said D:/Repositories (my custom location), GitStack looked for the repositories in its default location. So I simply went and save the settings again from GitStack admin page. It solved the problem.\n\n\nA65:\n\nThis error mostly caused by WRONG URL, please check:\n\nhttp or https\nURL Name\nusername@git_url\nwrong git name\n\n\n\nA66:\n\nprobably you were trying to clone like this: \ngit clone https://github.com/TeaCodie/TeaCodie-Website.git\nThen you got this error:\nfatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\nHere is what worked for me:\ngit clone https://github.com/TeaCodie/TeaCodie-Website.git/.git\nbecause the directory \"info/refs\" is in the .git directory.\n\n\nA67:\n\nJust another stupid reason: you may have changed credentials on your laptop, but there is another development PC running but not frequently used, that has SourceTree started and uses the old credentials frequently to poll status.\nSounds stupid, but took me some time to solve :-)\n\n\nA68:\n\nFor windows user\n\nchange SSH Client from Putty/Plink to OpenSSH\n\nin Tools -> Option -> SSHClient\n\n\n\nA69:\n\nThis is what I discovered (after searching for 'sourcetree' in my drive C:):\n\nUninstall SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\Atlassian\\SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\SourceTree\nReinstall SourceTree\n\n\n\nA70:\n\nSourceTree version 2.4.8.0\nThe problem for me seemed to be the order of setup between SourceTree and GitHub. I setup SourceTree first and just clicked all the \"setup later\" options.\nAn easy fix - Uninstall and re-install SourceTree. The non-obvious bit was needing to delete the actual program files manually.\nFrom any \"SourceTree.exe\" shortcut, open file location. Mine forked at \"C:\\Users\\myusername\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Atlassian\" where the shortcut pointed to another shortcut. \nOpen that shortcut's file location. I eventually ended up at \"C:\\Users\\myusername\\AppData\\Local\\SourceTree\". Delete the whole \"SourceTree\" folder. Go up one level to \"...\\Local\" and delete the \"Atlassian\" folder as well.\nNow uninstall SourceTree. Make sure your GitHub is setup the way you desire (for me, I also created and saved an SSH key that I used for SourceTree setup). Re-install SourceTree using all the correct credentials, point to the right repository and such. \nAfter all this, SourceTree asked for my GitHub credentials twice, with two different dialog boxes, then stopped and worked! No more endless loop. Good luck!\n\n\nA71:\n\nYou have to use -f when you are going to push on already existing repo.\ngit init\ngit add *\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin <repo url>\ngit push -f origin main\n\n\n\nA72:\n\nYou should do:\ngit commit . -m \"save arezzo files\"\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( vs ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 34, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIf C does not support passing a variable by reference, why does this work?\n#include <stdio.h>\n\nvoid f(int *j) {\n  (*j)++;\n}\n\nint main() {\n  int i = 20;\n  int *p = &i;\n  f(p);\n  printf(\"i = %d\\n\", i);\n\n  return 0;\n}\n\nOutput:\n$ gcc -std=c99 test.c\n$ a.exe\ni = 21 \n\n\n\nCandidate answers:\nA1:\n\nConsider the below figure and program to understand this concept better.\n\nAs per the figure, ptr1 is a single pointer which is having address of variable num.\nptr1 = &num;\n\nSimilarly ptr2 is a pointer to pointer(double pointer) which is having the address of pointer ptr1. \nptr2 = &ptr1;\n\nA pointer which points to another pointer is known as double pointer. In this example ptr2 is a double pointer.\nValues from above diagram :\nAddress of variable num has : 1000\nAddress of Pointer ptr1 is: 2000\nAddress of Pointer ptr2 is: 3000\n\nExample:\n#include <stdio.h>\n\nint main ()\n{\n   int  num = 10;\n   int  *ptr1;\n   int  **ptr2;\n\n   // Take the address of var \n   ptr1 = &num;\n\n   // Take the address of ptr1 using address of operator &\n   ptr2 = &ptr1;\n\n   // Print the value\n   printf(\"Value of num = %d\\n\", num );\n   printf(\"Value available at *ptr1 = %d\\n\", *ptr1 );\n   printf(\"Value available at **ptr2 = %d\\n\", **ptr2);\n}\n\nOutput:\nValue of num = 10\nValue available at *ptr1 = 10\nValue available at **ptr2 = 10\n\n\n\nA2:\n\nBecause when you say \n*ipp = ip2\n\nyou're saying the 'object pointed by ipp' to point the direction of memory that ip2 is pointing.\nYou're not saying ipp to point ip2.\n\n\nA3:\n\n'Pass by reference' (by using pointers) has been in C from the beginning. Why do you think it's not?\n\n\nA4:\n\nThe order of precedence for the 3 operators involved in your question is the following :\npost-increment ++ > dereference * > assignment +=\nYou can check this page for further details on the subject.\n\nWhen parsing an expression, an operator which is listed on some row will be bound tighter (as if by parentheses) to its arguments than any operator that is listed on a row further below it. For example, the expression *p++ is parsed as *(p++), and not as (*p)++.\n\nLong story short, in order to express this assignment *ptr+=1 using the post-increment operator you need to add parentheses to the dereference operator to give that operation precedence over ++ as in this (*ptr)++\n\n\nA5:\n\nhope  this piece of code can help.\n#include <iostream>\n#include <stdio.h>\nusing namespace std;\n\nint main()\n{\n    int i = 5, j = 6, k = 7;\n    int *ip1 = &i, *ip2 = &j;\n    int** ipp = &ip1;\n    printf(\"address of value i: %p\\n\", &i);\n    printf(\"address of value j: %p\\n\", &j);\n    printf(\"value ip1: %p\\n\", ip1);\n    printf(\"value ip2: %p\\n\", ip2);\n    printf(\"value ipp: %p\\n\", ipp);\n    printf(\"address value of ipp: %p\\n\", *ipp);\n    printf(\"value of address value of ipp: %d\\n\", **ipp);\n    *ipp = ip2;\n    printf(\"value ipp: %p\\n\", ipp);\n    printf(\"address value of ipp: %p\\n\", *ipp);\n    printf(\"value of address value of ipp: %d\\n\", **ipp);\n}\n\nit outputs: \n\n\n\nA6:\n\n*ptr += 1 : Increment data that ptr points to.\n*ptr++ : Increment pointer that is point to next memory location instead of the data that pointer points to.\n\n\nA7:\n\nA pointer to a pointer is also called a handle. One usage for it is often when an object can be moved in memory or removed. One is often responsible to lock and unlock the usage of the object so it will not be moved when accessing it.\nIt's often used in memory restricted environment, ie the Palm OS.\n\ncomputer.howstuffworks.com Link>>\nwww.flippinbits.com Link>>\n\n\n\nA8:\n\nHow do pointers to pointers work in C?\nFirst a pointer is a variable, like any other variable, but that holds the address of a variable.\nA pointer to a pointer is a variable, like any other variable, but that holds the address of a variable.  That variable just happens to be a pointer.\nWhen would you use them?\nYou can use them when you need to return a pointer to some memory on the heap, but not using the return value. \nExample:\nint getValueOf5(int *p)\n{\n  *p = 5;\n  return 1;//success\n}\n\nint get1024HeapMemory(int **p)\n{\n  *p = malloc(1024);\n  if(*p == 0)\n    return -1;//error\n  else \n    return 0;//success\n}\n\nAnd you call it like this:\nint x;\ngetValueOf5(&x);//I want to fill the int varaible, so I pass it's address in\n//At this point x holds 5\n\nint *p;    \nget1024HeapMemory(&p);//I want to fill the int* variable, so I pass it's address in\n//At this point p holds a memory address where 1024 bytes of memory is allocated on the heap\n\nThere are other uses too, like the main() argument of every C program has a pointer to a pointer for argv, where each element holds an array of chars that are the command line options.  You must be careful though when you use pointers of pointers to point to 2 dimensional arrays, it's better to use a pointer to a 2 dimensional array instead.  \nWhy it's dangerous?\nvoid test()\n{\n  double **a;\n  int i1 = sizeof(a[0]);//i1 == 4 == sizeof(double*)\n\n  double matrix[ROWS][COLUMNS];\n  int i2 = sizeof(matrix[0]);//i2 == 240 == COLUMNS * sizeof(double)\n}\n\nHere is an example of a pointer to a 2 dimensional array done properly:\nint (*myPointerTo2DimArray)[ROWS][COLUMNS]\n\nYou can't use a pointer to a 2 dimensional array though if you want to support a variable number of elements for the ROWS and COLUMNS.  But when you know before hand you would use a 2 dimensional array.\n\n\nA9:\n\nConsidere each variable represented like this:\ntype  : (name, adress, value)\n\nso your variables should be represented like this\nint   : ( i ,  &i , 5 ); ( j ,  &j ,  6); ( k ,  &k , 5 )\n\nint*  : (ip1, &ip1, &i); (ip1, &ip1, &j)\n\nint** : (ipp, &ipp, &ip1)\n\nAs the value of ipp is &ip1 so the inctruction:\n*ipp = ip2;\n\nchanges the value at the addess &ip1 to the value of ip2, which means ip1 is changed:\n(ip1, &ip1, &i) -> (ip1, &ip1, &j)\n\nBut ipp still:\n(ipp, &ipp, &ip1)\n\nSo the value of ipp still &ip1 which means it still points to ip1.\n\n\nA10:\n\nYour example works because you are passing the address of your variable to a function that manipulates its value with the dereference operator. \nWhile C does not support reference data types, you can still simulate passing-by-reference by explicitly passing pointer values, as in your example.\nThe C++ reference data type is less powerful but considered safer than the pointer type inherited from C. This would be your example, adapted to use C++ references:\nvoid f(int &j) {\n  j++;\n}\n\nint main() {\n  int i = 20;\n  f(i);\n  printf(\"i = %d\\n\", i);\n\n  return 0;\n}\n\n\n\nA11:\n\nLet's apply parentheses to show the order of operations\na + b / c\na + (b/c)\n\nLet's do it again with\n*ptr   += 1\n(*ptr) += 1\n\nAnd again with\n*ptr++\n*(ptr++)\n\n\nIn *ptr += 1, we increment the value of the variable our pointer points to.\nIn *ptr++, we increment the pointer after our entire statement (line of code) is done, and return a reference to the variable our pointer points to.\n\nThe latter allows you to do things like:\nfor(int i = 0; i < length; i++)\n{\n    // Copy value from *src and store it in *dest\n    *dest++ = *src++;\n\n    // Keep in mind that the above is equivalent to\n    *(dest++) = *(src++);\n}\n\nThis is a common method used to copy a src array into another dest array.\n\n\nA12:\n\nLet's assume an 8 bit computer with 8 bit addresses (and thus only 256 bytes of memory). This is part of that memory (the numbers at the top are the addresses):\n  54   55   56   57   58   59   60   61   62   63   64   65   66   67   68   69\n+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n|    | 58 |    |    | 63 |    | 55 |    |    | h  | e  | l  | l  | o  | \\0 |    |\n+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n\nWhat you can see here, is that at address 63 the string \"hello\" starts. So in this case, if this is the only occurrence of \"hello\" in memory then,\nconst char *c = \"hello\";\n\n... defines c to be a pointer to the (read-only) string \"hello\", and thus contains the value 63. c must itself be stored somewhere: in the example above at location 58. Of course we can not only point to characters, but also to other pointers. E.g.:\nconst char **cp = &c;\n\nNow cp points to c, that is, it contains the address of c (which is 58). We can go even further. Consider:\nconst char ***cpp = &cp;\n\nNow cpp stores the address of cp. So it has value 55 (based on the example above), and you guessed it: it is itself stored at address 60.\n\nAs to why one uses pointers to pointers:\n\nThe name of an array usually yields the address of its first element. So if the array contains elements of type t, a reference to the array has type t *. Now consider an array of arrays of type t: naturally a reference to this 2D array will have type (t *)* = t **, and is hence a pointer to a pointer.\nEven though an array of strings sounds one-dimensional, it is in fact two-dimensional, since strings are character arrays. Hence: char **.\nA function f will need to accept an argument of type t ** if it is to alter a variable of type t *.\nMany other reasons that are too numerous to list here.\n\n\n\nA13:\n\nThe difference is due to operator precedence.\nThe post-increment operator ++ has higher precedence than the dereference operator *. So *ptr++ is equivalent to *(ptr++). In other words, the post increment modifies the pointer, not what it points to.\nThe assignment operator += has lower precedence than the dereference operator *, so *ptr+=1 is equivalent to (*ptr)+=1. In other words, the assignment operator modifies the value that the pointer points to, and does not change the pointer itself. \n\n\nA14:\n\n*ipp = ip2; implies:\nAssign ip2 to the variable pointed to by ipp. So this is equivalent to:\nip1 = ip2;\n\nIf you want the address of ip2 to be stored in ipp, simply do:\nipp = &ip2;\n\nNow ipp points to ip2.\n\n\nA15:\n\nPointers to Pointers\n\nSince we can have pointers to int, and pointers to char, and pointers to any structures we've defined, and in fact pointers to any type in C, it shouldn't come as too much of a surprise that we can have pointers to other pointers.\n\n\n\nA16:\n\nVery good question.\nIn K&R \"C programming language\" \"5.1 Pointers and Addresses\", we can get an answer for this.\n\"The unary operators * and & bind more tightly than arithmetic operators\"\n*ptr += 1      //Increment what ptr points to.\n\n\"Unary operators like * and ++ associate right to left.\"\n*ptr++        //Increment prt instead of what ptr point to.\n\n//It works like *(ptr++).\nThe correct way is:\n(*ptr)++      //This will work.\n\n\n\nA17:\n\nI like this \"real world\" code example of pointer to pointer usage, in Git 2.0, commit 7b1004b:\n\nLinus once said:\n\nI actually wish more people understood the really core low-level kind of coding. Not big, complex stuff like the lockless name lookup, but simply good use of pointers-to-pointers etc.\nFor example, I've seen too many people who delete a singly-linked list entry by keeping track of the \"prev\" entry, and then to delete the entry, doing something like:\n\n   if (prev)\n     prev->next = entry->next;\n   else\n     list_head = entry->next;\n\nand whenever I see code like that, I just go \"This person doesn't understand pointers\". And it's sadly quite common.\n\nPeople who understand pointers just use a \"pointer to the entry pointer\", and initialize that with the address of the list_head. And then as they traverse the list, they can remove the entry without using any conditionals, by just doing a\n*pp =  entry->next\n\n\nApplying that simplification lets us lose 7 lines from this function even while adding 2 lines of comment.\n- struct combine_diff_path *p, *pprev, *ptmp;\n+ struct combine_diff_path *p, **tail = &curr;\n\n\n\nChris points out in the comments to the 2016 video \"Linus Torvalds's Double Pointer Problem\".\n\nkumar points out in the comments the blog post \"Linus on Understanding Pointers\", where Grisha Trubetskoy explains:\n\nImagine you have a linked list defined as:\n   typedef struct list_entry {\n       int val;\n       struct list_entry *next;\n   } list_entry;\n\nYou need to iterate over it from the beginning to end and remove a specific element whose value equals the value of to_remove.\nThe more obvious way to do this would be:\n   list_entry *entry = head; /* assuming head exists and is the first entry of the list */\n   list_entry *prev = NULL;\n   \n   while (entry) { /* line 4 */\n       if (entry->val == to_remove)     /* this is the one to remove ; line 5 */\n           if (prev)\n              prev->next = entry->next; /* remove the entry ; line 7 */\n           else\n               head = entry->next;      /* special case - first entry ; line 9 */\n   \n       /* move on to the next entry */\n       prev = entry;\n       entry = entry->next;\n   }\n\nWhat we are doing above is:\n\niterating over the list until entry is NULL, which means we\u2019ve reached the end of the list (line 4).\nWhen we come across an entry we want removed (line 5),\nwe assign the value of current next pointer to the previous one,\nthus eliminating the current element (line 7).\n\nThere is a special case above - at the beginning of the iteration there is no previous entry (prev is NULL), and so to remove the first entry in the list you have to modify head itself (line 9).\nWhat Linus was saying is that the above code could be simplified by making the previous element a pointer to a pointer rather than just a pointer.\nThe code then looks like this:\n   list_entry **pp = &head; /* pointer to a pointer */\n   list_entry *entry = head;\n\n   while (entry) {\n       if (entry->val == to_remove)\n           *pp = entry->next;\n       else\n            pp = &entry->next;\n       entry = entry->next;\n   }\n\nThe above code is very similar to the previous variant, but notice how we no longer need to watch for the special case of the first element of the list, since pp is not NULL at the beginning. Simple and clever.\nAlso, someone in that thread commented that the reason this is better is because *pp = entry->next is atomic. It is most certainly NOT atomic.\nThe above expression contains two dereference operators (* and ->) and one assignment, and neither of those three things is atomic.\nThis is a common misconception, but alas pretty much nothing in C should ever be assumed to be atomic (including the ++ and -- operators)!\n\n\n\nA18:\n\nLike most beginner questions in the C tag, this question can be answered by going back to first principles:\n\nA pointer is a kind of value.\nA variable contains a value.\nThe & operator turns a variable into a pointer.\nThe * operator turns a pointer into a variable.\n\n(Technically I should say \"lvalue\" instead of \"variable\", but I feel it is more clear to describe mutable storage locations as \"variables\".)\nSo we have variables:\nint i = 5, j = 6;\nint *ip1 = &i, *ip2 = &j;\n\nVariable ip1 contains a pointer. The & operator turns i into a pointer and that pointer value is assigned to ip1. So ip1 contains a pointer to i.\nVariable ip2 contains a pointer. The & operator turns j into a pointer and that pointer is assigned to ip2. So ip2 contains a pointer to j.\nint **ipp = &ip1;\n\nVariable ipp contains a pointer. The & operator turns variable ip1 into a pointer and that pointer value is assigned to ipp. So ipp contains a pointer to ip1.\nLet's sum up the story so far:\n\ni contains 5\nj contains 6\nip1 contains \"pointer to i\"\nip2 contains \"pointer to j\"\nipp contains \"pointer to ip1\"\n\nNow we say\n*ipp = ip2;\n\nThe * operator turns a pointer back into a variable. We fetch the value of ipp, which is \"pointer to ip1 and turn it into a variable. What variable? ip1 of course!\nTherefore this is simply another way of saying \nip1 = ip2;\n\nSo we fetch the value of ip2. What is it? \"pointer to j\". We assign that pointer value to ip1, so ip1 is now \"pointer to j\"\nWe only changed one thing: the value of ip1:\n\ni contains 5\nj contains 6\nip1 contains \"pointer to j\"\nip2 contains \"pointer to j\"\nipp contains \"pointer to ip1\"\n\n\nWhy does ipp still point to ip1 and not ip2?\n\nA variable changes when you assign to it. Count the assignments; there cannot be more changes to variables than there are assignments!  You start by assigning to i, j, ip1, ip2 and ipp.  You then assign to *ipp, which as we've seen means the same as \"assign to ip1\". Since you didn't assign to ipp a second time, it didn't change!\nIf you wanted to change ipp then you'll have to actually assign to ipp:\nipp = &ip2;\n\nfor instance.\n\n\nA19:\n\nBecause you are changing the pointer of *ipp. It means  \n\nipp (varaiable name)----go inside.\ninside ipp is address of ip1.\nnow *ipp so go to (adress of inside) ip1.  \n\nNow we are at ip1.\n*ipp(i.e.ip1) = ip2.\nip2 contain address of j.so ip1 content will be replace by contain of ip2(i.e. address of j),\nWE ARE NOT CHANGING ipp CONTENT.\nTHAT'S IT. \n\n\nA20:\n\nipp = &ip1;\n\nNo later assignment has changed the value of ipp. This is why it still points to ip1.\nWhat you do with *ipp, i.e., with ip1, does not change the fact that ipp points to ip1.\n\n\nA21:\n\nMy very personal opinion is that pictures with arrows pointing this way or that make pointers harder to understand. It does make them seem like some abstract, mysterious entities. They are not. \nLike everything else in your computer, pointers are numbers. The name \"pointer\" is just a fancy way of saying \"a variable containing an address\".\nTherefore, let me stir things around by explaining how a computer actually works.\nWe have an int, it has the name i and the value 5. This is stored in memory. Like everything stored in memory, it needs an address, or we wouldn't be able to find it. Lets say i ends up at address 0x12345678 and its buddy j with value 6 ends up just after it. Assuming a 32-bit CPU where int is 4 bytes and pointers are 4 bytes, then the variables are stored in physical memory like this:\nAddress     Data           Meaning\n0x12345678  00 00 00 05    // The variable i\n0x1234567C  00 00 00 06    // The variable j\n\nNow we want to point at these variables. We create one pointer to int, int* ip1, and one int* ip2. Like everything in the computer, these pointer variables get allocated somewhere in memory too. Lets assume they end up at the next adjacent addresses in memory, immediately after j. We set the pointers to contain the addresses of the variables previously allocated: ip1=&i; (\"copy the address of i into ip1\") and ip2=&j. What happens between the lines is:\nAddress     Data           Meaning\n0x12345680  12 34 56 78    // The variable ip1(equal to address of i)\n0x12345684  12 34 56 7C    // The variable ip2(equal to address of j)\n\nSo what we got were just yet some 4 byte chunks of memory containing numbers. There's no mystical or magical arrows anywhere in sight. \nIn fact, just by looking at a memory dump, we can't tell whether the address 0x12345680 contains an int or int*. The difference is how our program chooses to use the contents stored at this address. (The task of our program is actually just to tell the CPU what to do with these numbers.)\nThen we add yet another level of indirection with int** ipp = &ip1;. Again, we just get a chunk of memory:\nAddress     Data           Meaning\n0x12345688  12 34 56 80    // The variable ipp\n\nThe pattern does seem familiar. Yet another chunk of 4 bytes containing a number.\nNow, if we had a memory dump of the above fictional little RAM, we could manually check where these pointers point. We peek at what's stored at the address of the ipp variable and find the contents 0x12345680. Which is of course the address where ip1 is stored. We can go to that address, check the contents there, and find the address of i, and then finally we can go to that address and find the number 5.\nSo if we take the contents of ipp, *ipp, we will get the address of the pointer variable ip1. By writing *ipp=ip2 we copy ip2 into ip1, it is equivalent to ip1=ip2. In either case we would get\nAddress     Data           Meaning\n0x12345680  12 34 56 7C    // The variable ip1\n0x12345684  12 34 56 7C    // The variable ip2\n\n\n(These examples were given for a big endian CPU)\n\n\nA22:\n\nForget for a second about the pointing analogy.  What a pointer really contains is a memory address.  The & is the \"address of\" operator - i.e. it returns the address in memory of an object.  The * operator gives you the object a pointer refers to, i.e. given a pointer containing an address, it returns the object at that memory address.  So when you do *ipp = ip2, what you are doing is *ipp get the object at the address held in ipp which is   ip1 and then assign to ip1 the value stored in ip2, which is the address of j. \nSimply\n& --> Address of\n* --> Value at\n\n\nA23:\n\nNotice the assignments:\nipp = &ip1;\n\nresults ipp to point to ip1.\nso for ipp to point to ip2, we should change in the similar manner,\nipp = &ip2;\n\nwhich we are clearly not doing. Instead we are changing the value at address pointed by ipp.\nBy doing the folowing\n*ipp = ip2;\n\nwe are just replacing the value stored in ip1.\nipp = &ip1 , means *ipp = ip1 = &i, \nNow, *ipp = ip2 = &j.\nSo, *ipp = ip2 is essentially same as ip1 = ip2.\n\n\nA24:\n\nBecause you changed the value pointed to by ipp not the value of ipp. So, ipp still points to ip1 (the value of ipp), ip1's value is now the same as ip2's value, so they both point to j.\nThis:\n*ipp = ip2;\n\nis the same as:\nip1 = ip2;\n\n\n\nA25:\n\nWhen covering pointers on a programming course at university, we were given two hints as to how to begin learning about them. The first was to view Pointer Fun With Binky. The second was to think about the Haddocks' Eyes passage from Lewis Carroll's Through the Looking-Glass\n\n\u201cYou are sad,\u201d the Knight said in an anxious tone: \u201cLet me sing you a song to comfort you.\u201d\n\u201cIs it very long?\u201d Alice asked, for she had heard a good deal of poetry that day.\n\u201cIt's long,\u201d said the Knight, \u201cbut it's very, very beautiful. Everybody that hears me sing it - either it brings the tears to their eyes, or else -\u201d\n\u201cOr else what?\u201d said Alice, for the Knight had made a sudden pause.\n\u201cOr else it doesn't, you know. The name of the song is called \u2018Haddocks' Eyes.\u2019\u201d\n\u201cOh, that's the name of the song, is it?\" Alice said, trying to feel interested.\n\u201cNo, you don't understand,\u201d the Knight said, looking a little vexed. \u201cThat's what the name is called. The name really is \u2018The Aged Aged Man.\u2019\u201d\n\u201cThen I ought to have said \u2018That's what the song is called\u2019?\u201d Alice corrected herself.\n\u201cNo, you oughtn't: that's quite another thing! The song is called \u2018Ways And Means\u2019: but that's only what it's called, you know!\u201d\n\u201cWell, what is the song, then?\u201d said Alice, who was by this time completely bewildered.\n\u201cI was coming to that,\u201d the Knight said. \u201cThe song really is \u2018A-sitting On A Gate\u2019: and the tune's my own invention.\u201d\n\n\n\nA26:\n\nIf you add the dereference operator * to the pointer, you redirect from the pointer to the pointed-to object.\nExamples:\nint i = 0;\nint *p = &i; // <-- N.B. the pointer declaration also uses the `*`\n             //     it's not the dereference operator in this context\n*p;          // <-- this expression uses the pointed-to object, that is `i`\np;           // <-- this expression uses the pointer object itself, that is `p`\n\nTherefore:\n*ipp = ip2; // <-- you change the pointer `ipp` points to, not `ipp` itself\n            //     therefore, `ipp` still points to `ip1` afterwards.\n\n\n\nA27:\n\n\nIn C, Pass-by-reference is simulated\n  by passing the address of a variable\n  (a pointer) and dereferencing that\n  address within the function to read or\n  write the actual variable.  This will\n  be referred to as \"C style\n  pass-by-reference.\"\n\nSource: www-cs-students.stanford.edu\n\n\nA28:\n\n\nMy question is: Why in the second picture, ipp is still point to ip1 but not ip2?\n\nyou placed nice pictures, I'm going to try to make nice ascii art:\nLike @Robert-S-Barnes said in his answer: forget about pointers, and what points to what, but think in terms of memory. Basically, an int* means that it contains the address of a variable and an int** contains the address of a variable that contains the address of a variable. Then you can use the pointer's algebra to access the values or the addresses: &foo means address of foo, and *foo means value of the address contained in foo.\nSo, as pointers is about dealing with memory, the best way to actually make that \"tangible\" is to show what the pointers algebra does to the memory.\nSo, here's your program's memory (simplified for the purpose of the example):\nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [   |   |   |   |   ]\n\nwhen you do your initial code:\nint i = 5, j = 6;\nint *ip1 = &i, *ip2 = &j;\n\nhere's how your memory looks like:\nname:    i   j ip1 ip2\naddr:    0   1   2   3\nmem : [  5|  6|  0|  1]\n\nthere you can see ip1 and ip2 gets the addresses of i and j and ipp still does not exists.\nDon't forget that addresses are simply integers stored with a special type.\nThen you declare and defined ipp such as:\nint **ipp = &ip1;\n\nso here's your memory:\nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [  5|  6|  0|  1|  2]\n\nand then, you're changing the value pointed by the address stored in ipp, which is\nthe address stored in ip1:\n*ipp = ip2;\n\nthe program's memory is \nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [  5|  6|  1|  1|  2]\n\nN.B.: as int* is a special type, I prefer to always avoid declaring multiple pointers on the same line, as I think the int *x; or int *x, *y; notation can be misleading. I prefer to write int* x; int* y;\nHTH\n\n\nA29:\n\nA pointer-to-a-pointer is used when a reference to a pointer is required.  For example, when you wish to modify the value (address pointed to) of a pointer variable declared in a calling function's scope inside a called function.\nIf you pass a single pointer in as an argument, you will be modifying local copies of the pointer, not the original pointer in the calling scope.  With a pointer to a pointer, you modify the latter.\n\n\nA30:\n\nBecause you're passing the value of the pointer to the method and then dereferencing it to get the integer that is pointed to.\n\n\nA31:\n\nVery beginning you set,\nipp = &ip1;\n\nNow dereference it as,\n*ipp = *&ip1 // Here *& becomes 1  \n*ipp = ip1   // Hence proved \n\n\n\nA32:\n\nBecause there is no pass-by-reference in the above code. Using pointers (such as void func(int* p)) is pass-by-address.\nThis is pass-by-reference in C++ (won't work in C):\nvoid func(int& ref) {ref = 4;}\n\n...\nint a;\nfunc(a);\n// a is 4 now\n\n\n\nA33:\n\nIf you'd want ipp to point to ip2, you'd have to say ipp = &ip2;. However, this would leave ip1 still pointing to i.\n\n\nA34:\n\nipp can hold a value of (i.e point to) a pointer to pointer type object. When you do  \nipp = &ip2;  \n\nthen the ipp contains the address of the variable (pointer) ip2, which is (&ip2) of type pointer to pointer. Now the arrow of ipp in second pic will point to ip2.   \nWiki says:\nThe * operator is a dereference operator operates on pointer variable, and returns an l-value (variable) equivalent to the value at pointer address. This is called dereferencing the pointer.  \nApplying * operator on ipp derefrence it to a l-value of pointer to int type. The dereferenced l-value *ipp is of type pointer to int, it can hold the address of an int type data. After the statement   \nipp = &ip1;\n\nipp is holding the address of ip1 and *ipp is holding the address of (pointing to) i. You can say that *ipp is an alias of ip1. Both **ipp and *ip1 are alias for i.\nBy doing  \n *ipp = ip2;  \n\n*ipp and ip2 both points to same location but ipp is still pointing to ip1.   \nWhat *ipp = ip2; does actually is that it copies the contents of ip2 (the address of j) to ip1 (as *ipp is an alias for ip1), in effect making both pointers ip1 and ip2 pointing to the same object (j).\nSo, in the second figure, arrow of ip1 and ip2 is pointing to j while ipp is still pointing to ip1 as no modification is done to change the value of ipp.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 35, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI have a library I created,\nFile mylib.c:\n#include <mylib.h>\n\nint\ntestlib() {\n    printf(\"Hello, World!\\n\");\n    return (0);\n}\n\nFile mylib.h:\n#include <stdio.h>\nextern int testlib();\n\nIn my program, I've attempted to call this library function:\nFile myprogram.c:\n#include <mylib.h>\n\nint\nmain (int argc, char *argv[]) {\n    testlib();\n    return (0);\n}\n\nWhen I attempt to compile this program I get the following error:\nIn file included from myprogram.c:1\nmylib.h:2 warning: function declaration isn't a prototype\nI'm using: gcc (GCC) 3.4.5 20051201 (Red Hat 3.4.5-2)\nWhat is the proper way to declare a function prototype?\n\n\nCandidate answers:\nA1:\n\nIt can take time for the network close to be observed - the total time is nominally about 2 minutes (yes, minutes!) after a close before the packets destined for the port are all assumed to be dead.  The error condition is detected at some point.  With a small write, you are inside the MTU of the system, so the message is queued for sending.  With a big write, you are bigger than the MTU and the system spots the problem quicker.  If you ignore the SIGPIPE signal, then the functions will return EPIPE error on a broken pipe - at some point when the broken-ness of the connection is detected.\n\n\nA2:\n\nIn C int foo() and int foo(void) are different functions. int foo() accepts an arbitrary number of arguments, while int foo(void) accepts 0 arguments. In C++ they mean the same thing. I suggest that you use void consistently when you mean no arguments.\nIf you have a variable a, extern int a; is a way to tell the compiler that a is a symbol that might be present in a different translation unit (C compiler speak for source file), don't resolve it until link time. On the other hand, symbols which are function names are anyway resolved at link time. The meaning of a storage class specifier on a function (extern, static) only affects its visibility and extern is the default, so extern is actually unnecessary.\nI suggest removing the extern, it is extraneous and is usually omitted. \n\n\nA3:\n\nI think Treb is on the right track. It's more important for integers that you have an unsigned corresponding type. Those are the ones that are used in bit-shifting and used in bit-maps. A sign bit just gets into the way. For example, right-shifting a negative value, the resulting value is implementation defined in C++. Doing that with an unsigned integer or overflowing such one has perfectly defined semantics because there is no such bit in the way. \nSo for integers at least, the need for a separate unsigned type is stronger than just giving warnings. All the above points do not need to be considered for floats. So, there is, i think, no real need for hardware support for them, and C will already don't support them at that point. \n\n\nA4:\n\nWell quite a few programming languages are written in C. And some of them support functions as first class citizens, languages in that area are ecl (embbedabble common lisp IIRC), Gnu Smalltalk (gst) (Smalltalk has blocks), then there are libraries for \"closures\" e.g in glib2 http://library.gnome.org/devel/gobject/unstable/chapter-signal.html#closure \nwhich at least got near functional programming. So maybe using some of those implementations to do functional programming may be an option. \nWell  or you can go learning Ocaml, Haskell, Mozart/Oz or the like ;-)\nRegards\n\n\nA5:\n\nLook at Hartel & Muller's book, Functional C\n\nhttps://ris.utwente.nl/ws/portalfiles/portal/5128727/book.pdf\nhttp://www.cs.bris.ac.uk/~henkm/f2c/index.html \u2014 missing since 2016\nhttp://www.cs.bris.ac.uk/~henkm/f2c/index.html \u2014 via Internet Archive aka WayBack Machine.\n\n\n\nA6:\n\nI believe the unsigned int was created because of the need for a larger value margin than the signed int could offer. \nA float has a much larger margin, so there was never a 'physical' need for an unsigned float. And as you point out yourself in your question, the additional 1 bit precision is nothing to kill for.\nEdit:\nAfter reading the answer by Brian R. Bondy, I have to modify my answer: \nHe is definitely right that the underlying CPUs did not have unsigned float operations.  However, I maintain my belief that this was a design decision based on the reasons I stated above ;-)\n\n\nA7:\n\nWhy C++ doesn't have support for unsigned floats is because there is no equivalent machine code operations for the CPU to execute.  So it would be very inefficient to support it.  \nIf C++ did support it, then you would be sometimes using an unsigned float and not realizing that your performance has just been killed.  If C++ supported it then every floating point operation would need to be checked to see if it is signed or not.  And for programs that do millions of floating point operations, this is not acceptable. \nSo the question would be why don't hardware implementers support it.  And I think the answer to that is that there was no unsigned float standard defined originally.  Since languages like to be backwards compatible, even if it were added languages couldn't make use of it.  To see the floating point spec you should look at the IEEE standard 754 Floating-Point.\nYou can get around not having an unsigned floating point type though by creating a unsigned float class that encapsulates a float or double and throws warnings if you try to pass in a negative number.   This is less efficient, but probably if you aren't using them intensely you won't care about that slight performance loss.\nI definitely see the usefulness of having an unsigned float.  But C/C++ tends to chose efficiency that works best for everyone over safety. \n\n\nA8:\n\nendianness are talking about byte orders not bit orders. Nowadays , it is 99% sure that bit orders are fixed. However, when using bitfields, endianness should be taken in count. See the example below.\n#include <stdio.h>\n\ntypedef struct tagT{\n\n    int a:4;\n    int b:4;\n    int c:8;\n    int d:16;\n}T;\n\n\nint main()\n{\n    char data[]={0x12,0x34,0x56,0x78};\n    T *t = (T*)data;\n    printf(\"a =0x%x\\n\" ,t->a);\n    printf(\"b =0x%x\\n\" ,t->b);\n    printf(\"c =0x%x\\n\" ,t->c);\n    printf(\"d =0x%x\\n\" ,t->d);\n\n    return 0;\n}\n\n//- big endian :  mips24k-linux-gcc (GCC) 4.2.3 - big endian\na =0x1\nb =0x2\nc =0x34\nd =0x5678\n 1   2   3   4   5   6   7   8\n\\_/ \\_/ \\_____/ \\_____________/\n a   b     c           d\n\n// - little endian : gcc (Ubuntu 4.3.2-1ubuntu11) 4.3.2\na =0x2\nb =0x1\nc =0x34\nd =0x7856\n 7   8   5   6   3   4   1   2\n\\_____________/ \\_____/ \\_/ \\_/\n       d           c     b   a\n\n\n\nA9:\n\nYou can use GCC's nested functions to simulate lambda expressions, in fact, I have a macro to do it for me:\n#define lambda(return_type, function_body) \\\n  ({ \\\n    return_type anon_func_name_ function_body \\\n    anon_func_name_; \\\n  })\n\nUse like this:\nint (*max)(int, int) = lambda (int, (int x, int y) { return x > y ? x : y; });\n\n\n\nA10:\n\nThe way I went about doing functional programming in C was to write a functional language interpreter in C.  I named it Fexl, which is short for \"Function EXpression Language.\"\nThe interpreter is very small, compiling down to 68K on my system with -O3 enabled.  It's not a toy either - I'm using it for all the new production code I write for my business (web-based accounting for investment partnerships.)\nNow I write C code only to (1) add a built-in function that calls a system routine (e.g. fork, exec, setrlimit, etc.), or (2) optimize a function that could otherwise be written in Fexl (e.g. search for a substring).\nThe module mechanism is based on the concept of a \"context\".  A context is a function (written in Fexl) which maps a symbol to its definition.  When you read a Fexl file, you can resolve it with any context you like.  This allows you to create custom environments, or run code in a restricted \"sandbox.\"\nhttp://fexl.com\n\n\nA11:\n\nNo, it will not be fully-portable.  Packing options for structs are extensions, and are themselves not fully portable.  In addition to that, C99 \u00a76.7.2.1, paragraph 10 says: \"The order of allocation of bit-fields within a unit (high-order to low-order or low-order to high-order) is implementation-defined.\"\nEven a single compiler might lay the bit field out differently depending on the endianness of the target platform, for example.\n\n\nA12:\n\nPrerequisite for functional programming style is a first class function.\nIt could be simulated in portable C if you tolerate next:\n\nmanual management of lexical scope bindings, aka closures.\nmanual management of function variables lifetime.\nalternative syntax of function application/call.\n\n/* \n * with constraints desribed above we could have\n * good approximation of FP style in plain C\n */\n\nint increment_int(int x) {\n  return x + 1;\n}\n\nWRAP_PLAIN_FUNCTION_TO_FIRST_CLASS(increment, increment_int);\n\nmap(increment, list(number(0), number(1)); // --> list(1, 2)\n\n\n/* composition of first class function is also possible */\n\nfunction_t* computation = compose(\n  increment,\n  increment,\n  increment\n);\n\n*(int*) call(computation, number(1)) == 4;\n\nruntime for such code could be as small as one below\n\nstruct list_t {\n  void* head;\n  struct list_t* tail;\n};\n\nstruct function_t {\n   void* (*thunk)(list_t*);\n   struct list_t* arguments;\n}\n\nvoid* apply(struct function_t* fn, struct list_t* arguments) {\n  return fn->thunk(concat(fn->arguments, arguments));\n}\n\n/* expansion of WRAP_PLAIN_FUNCTION_TO_FIRST_CLASS */\nvoid* increment_thunk(struct list_t* arguments) {\n  int x_arg = *(int*) arguments->head;\n  int value = increment_int(x_arg);\n  int* number = malloc(sizeof *number);\n\n  return number ? (*number = value, number) : NULL;\n}\n\nstruct function_t* increment = &(struct function_t) {\n  increment_thunk,\n  NULL\n};\n\n/* call(increment, number(1)) expands to */\napply(increment, &(struct list_t) { number(1), NULL });\n\nIn essence we imitate first class function with closures represented as pair of function/arguments plus bunch of macroses. Complete code could be found here.\n\n\nA13:\n\nThe current state of a socket is determined by 'keep-alive' activity. In your case, this is possible that when you are issuing the send call, the keep-alive activity tells that the socket is active and so the send call will write the required data (40 bytes) in to the buffer and returns without giving any error.\nWhen you are sending a bigger chunk, the send call goes in to blocking state.\nThe send man page also confirms this:\n\nWhen the message does not fit into the send buffer of the socket, send() normally blocks, unless the socket has been placed in non-blocking I/O mode. In non-blocking mode it would return EAGAIN in this case\n\nSo, while blocking for the free available buffer, if the caller is notified (by keep-alive mechanism) that the other end is no more present, the send call will fail.\nPredicting the exact scenario is difficult with the mentioned info, but I believe, this should be the reason for you problem.\n\n\nA14:\n\nFunctional programming is not about lambdas, it is all about pure functions.  So the following broadly promote functional style:\n\nOnly use function arguments, do not use global state.\nMinimise side effects i.e. printf, or any IO.  Return data describing IO which can be executed instead of causing the side effects directly in all functions.  \n\nThis can be achieved in plain c, no need for magic.\n\n\nA15:\n\nMaybe the 40 bytes fits into the pipe buffer, and the 40000 bytes doesn't?\nEdit:\nThe sending process is sent a SIGPIPE signal when you try to write to a closed pipe. I don't know exactly when the signal is sent, or what effect the pipe buffer has on this. You may be able to recover by trapping the signal with the sigaction call.\n\n\nA16:\n\nTry:\nextern int testlib(void);\n\n\n\nA17:\n\nThe heap usage comes from the C++ standard library. It allocates memory for internal library use on startup. If you don't link against it, there should be zero difference between the C and C++ version. With GCC and Clang, you can compile the file with:\n\ng++ -Wl,--as-needed main.cpp\n\nThis will instruct the linker to not link against unused libraries. In your example code, the C++ library is not used, so it should not link against the C++ standard library.\nYou can also test this with the C file. If you compile with:\n\ngcc main.c -lstdc++\n\nThe heap usage will reappear, even though you've built a C program.\nThe heap use is obviously dependant to the specific C++ library implementation you're using. In your case, that's the GNU C++ library, libstdc++. Other implementations might not allocate the same amount of memory, or they might not allocate any memory at all (at least not on startup.) The LLVM C++ library (libc++) for example does not do heap allocation on startup, at least on my Linux machine:\n\nclang++ -stdlib=libc++ main.cpp\n\nThe heap use is the same as not linking at all against it.\n(If compilation fails, then libc++ is probably not installed. The package name usually contains \"libc++\" or \"libcxx\".)\n\n\nA18:\n\nThere is a significant difference between signed and unsigned integers in C/C++:\nvalue >> shift\n\nsigned values leave the top bit unchanged (sign extend), unsigned values clear the top bit.\nThe reason there is no unsigned float is that you quickly run into all sorts of problems if there are no negative values. Consider this:\nfloat a = 2.0f, b = 10.0f, c;\nc = a - b;\n\nWhat value does c have? -8. But what would that mean in a system without negative numbers. FLOAT_MAX - 8 perhaps? Actually, that doesn't work as FLOAT_MAX - 8 is FLOAT_MAX due to precision effects so things are even more screwy. What if it was part of a more complex expression:\nfloat a = 2.0f, b = 10.0f, c = 20.0f, d = 3.14159f, e;\ne = (a - b) / d + c;\n\nThis isn't a problem for integers due to the nature of the 2's complement system.\nAlso consider standard mathematical functions: sin, cos and tan would only work for half their input values, you couldn't find the log of values < 1, you couldn't solve quadratic equations: x = (-b +/- root (b.b - 4.a.c)) / 2.a, and so on. In fact, it probably wouldn't work for any complex function as these tend to be implemented as polynomial approximations which would use negative values somewhere.\nSo, unsigned floats are pretty useless.\nBut that doesn't mean to say that a class that range checks float values isn't useful, you may want to clamp values to a given range, for example RGB calculations.\n\n\nA19:\n\nBit fields vary widely from compiler to compiler, sorry.\nWith GCC, big endian machines lay out the bits big end first and little endian machines lay out the bits little end first.\nK&R says \"Adjacent [bit-]field members of structures are packed into implementation-dependent storage units in an implementation-dependent direction. When a field following another field will not fit ... it may be split between units or the unit may be padded. An unnamed field of width 0 forces this padding...\"\nTherefore, if you need machine independent binary layout you must do it yourself.\nThis last statement also applies to non-bitfields due to padding -- however all compilers seem to have some way of forcing byte packing of a structure, as I see you already discovered for GCC.\n\n\nA20:\n\nIf you want to implement closures, you'll have to get groady with assembly language and stack swapping/management. Not recommending against it, just saying that's what you'll have to do.\nNot sure how you'll handle anonymous functions in C. On a von Neumann machine, you could do anonymous functions in asm, though.\n\n\nA21:\n\nThanks @BenVoigt for your very useful comment starting \n\nNo, they were created to save memory.\n\nLinux source does use a bit field to match to an external structure: /usr/include/linux/ip.h has this code for the first byte of an IP datagram\nstruct iphdr {\n#if defined(__LITTLE_ENDIAN_BITFIELD)\n        __u8    ihl:4,\n                version:4;\n#elif defined (__BIG_ENDIAN_BITFIELD)\n        __u8    version:4,\n                ihl:4;\n#else\n#error  \"Please fix <asm/byteorder.h>\"\n#endif\n\nHowever in light of your comment I'm giving up trying to get this to work for the multi-byte bit field frag_off.\n\n\nA22:\n\nWhen peer close, you just do not know whether it just stop sending or both sending and receiving.Because TCP allows this, btw, you should know the difference between close and shutdown. \nIf peer both stop sending and receiving, first you send some bytes, it will succeed. But the peer kernel will send you RST. So subsequently you send some bytes, your kernel will send you SIGPIPE signal, if you catch or ignore this signal, when your send returns, you just get Broken pipe error, or if you don't , the default behavior of your program is crashing.\n\n\nA23:\n\nUnsigned integer types in C are defined in such a way as to obey the rules of an abstract algebraic ring.  For example, for any value X and Y, adding X-Y to Y will yield X.  Unsigned integer types are guaranteed to obey these rules in all cases which do not involve conversion to or from any other numeric type [or unsigned types of different sizes], and that guarantee is one of the most important feature of such types.  In some cases, it's worthwhile to give up the ability to represent negative numbers in exchange for the extra guarantees only unsigned types can provide.  Floating-point types, whether signed or not, cannot abide by all the rules of an algebraic ring [e.g. they cannot guarantee that X+Y-Y will equal X], and indeed IEEE doesn't even allow them to abide by the rules of an equivalence class [by requiring that certain values compare unequal to themselves].  I don't think an \"unsigned\" floating-point type could abide by any axioms which an ordinary floating-point type could not, so I'm not sure what advantages it would offer.\n\n\nA24:\n\nMost of the time, probably, but don't bet the farm on it, because if you're wrong, you'll lose big.\nIf you really, really need to have identical binary information, you'll need to create bitfields with bitmasks - e.g. you use an unsigned short (16 bit) for Message, and then make things like versionMask = 0xE000 to represent the three topmost bits.\nThere's a similar problem with alignment within structs. For instance, Sparc, PowerPC, and 680x0 CPUs are all big-endian, and the common default for Sparc and PowerPC compilers is to align struct members on 4-byte boundaries. However, one compiler I used for 680x0 only aligned on 2-byte boundaries - and there was no option to change the alignment!\nSo for some structs, the sizes on Sparc and PowerPC are identical, but smaller on 680x0, and some of the members are in different memory offsets within the struct.\nThis was a problem with one project I worked on, because a server process running on Sparc would query a client and find out it was big-endian, and assume it could just squirt binary structs out on the network and the client could cope. And that worked fine on PowerPC clients, and crashed big-time on 680x0 clients. I didn't write the code, and it took quite a while to find the problem. But it was easy to fix once I did.\n\n\nA25:\n\nNeither GCC nor Clang are compilers -- they're actually toolchain driver programs.  That means they invoke the compiler, the assembler, and the linker.\nIf you compile your code with a C or a C++ compiler you will get the same assembly produced.  The Assembler will produce the same objects.  The difference is that the toolchain driver will provide different input to the linker for the two different languages:  different startups (C++ requires code for executing constructors and destructors for objects with static or thread-local storage duration at namespace level, and requires infrastructure for stack frames to support unwinding during exception processing, for example), the C++ standard library (which also has objects of static storage duration at namespace level), and probably additional runtime libraries (for example, libgcc with its stack-unwinding infrastructure).\nIn  short, it's not the compiler causing the increase in footprint, it's the linking in of stuff you've chose to use by choosing the C++ language.\nIt's true that C++ has the \"pay only for what you use\" philosophy, but by using the language, you pay for it.  You can disable parts of the language (RTTI, exception handling) but then you're not using C++ any more.  As mentioned in another answer, if you don't use the standard library at all you can instruct the driver to leave that out (--Wl,--as-needed) but if you're not going to use any of the features of C++ or its library, why are you even choosing C++ as a programming language?\n\n\nA26:\n\n\nA square-root will definately never return a negative number. There are other places as well where a negative float value has no meaning. Perfect candidate for an unsigned float.\n\nC99 supports complex numbers, and a type generic form of sqrt, so sqrt( 1.0 * I) will be negative.\n\nThe commentors highlighted a slight gloss above, in that I was referring to the type-generic sqrt macro rather than the function, and it will return a scalar floating point value by truncation of the complex to its real component: \n#include <complex.h>\n#include <tgmath.h>\n\nint main () \n{\n    complex double a = 1.0 + 1.0 * I;\n\n    double f = sqrt(a);\n\n    return 0;\n}\n\nIt also contains a brain-fart, as the real part of the sqrt of any complex number is positive or zero, and sqrt(1.0*I) is sqrt(0.5) + sqrt(0.5)*I not -1.0.\n\n\nA27:\n\nBitfields should be avoided - they aren't very portable between compilers even for the same platform. from the C99 standard 6.7.2.1/10 - \"Structure and union specifiers\" (there's similar wording in the C90 standard):\n\nAn implementation may allocate any addressable storage unit large enough to hold a bitfield. If enough space remains, a bit-field that immediately follows another bit-field in a structure shall be packed into adjacent bits of the same unit. If insufficient space remains, whether a bit-field that does not fit is put into the next unit or overlaps adjacent units is implementation-defined. The order of allocation of bit-fields within a unit (high-order to low-order or low-order to high-order) is implementation-defined. The alignment of the addressable storage unit is unspecified.\n\nYou cannot guarantee whether a bit field will 'span' an int boundary or not and you can't specify whether a bitfield starts at the low-end of the int or the high end of the int (this is independant of whether the processor is big-endian or little-endian).\nPrefer bitmasks.  Use inlines (or even macros) to set, clear and test the bits.\n\n\nA28:\n\nFFCALL lets you build closures in C -- callback = alloc_callback(&function, data) returns a function pointer such that callback(arg1, ...) is equivalent to calling function(data, arg1, ...).  You will have to handle garbage collection manually, though.\nRelatedly, blocks have been added to Apple's fork of GCC; they're not function pointers, but they let you pass around lambdas while avoiding the need to build and free storage for captured variables by hand (effectively, some copying and reference counting happens, hidden behind some syntactic sugar and runtime libraries).\n\n\nA29:\n\nSession timeout settings may be the reason of broken pipe.\n For example: Server session timeout 3 hours and load balancer has 1 hour.\n Load balancer blocks after 1 hour, but server keeps sending response. In this case, one end of the pipe is broken.\nBut it can be also a user behavior. User closes the page during download etc.\n\n\nA30:\n\n(As an aside, Perl 6 lets you write\nsubset Nonnegative::Float of Float where { $_ >= 0 };\n\nand then you can use Nonnegative::Float just like you would any other type.)\nThere's no hardware support for unsigned floating point operations, so C doesn't offer it.    C is mostly designed to be \"portable assembly\", that is, as close to the metal as you can be without being tied down to a specific platform.\n[edit]\nC is like assembly: what you see is exactly what you get.  An implicit \"I'll check that this float is nonnegative for you\" goes against its design philosophy.  If you really want it, you can add assert(x >= 0) or similar, but you have to do that explicitly.\n\n\nA31:\n\nWhat is it about C that you want to make functional, the syntax or the semantics? The semantics of functional programming could certainly be added to the C compiler, but by the time you were done, you'd essentially have the equivalent of one of the existing functional languages, such as Scheme, Haskell, etc.\nIt would be a better use of time to just learn the syntax of those languages which directly support those semantics.\n\n\nA32:\n\nIHMO it's because supporting both signed and unsigned floating-point types in either hardware or software would be too troublesome\nFor integer types we can utilize the same logic unit for both signed and unsigned integer operations in most situations using the nice property of 2's complement, because the result is identical in those cases for add, sub, non-widening mul and most bitwise operations. For operations that differentiate between signed and unsigned version we can still share the majority of the logic. For example\n\nArithmetic and logical shift need only a slight change in the filler for the top bits\nWidening multiplication can use the same hardware for the main part and then some separate logic to adjust the result to change the signness. Not that it's used in real multipliers but it's possible to do\nSigned comparison can be converted to unsigned comparison and vice versa easily by toggling the top bit or adding INT_MIN. Also theoretically possible, it's probably not used on hardware, yet it's useful on systems that support only one type of comparison (like 8080 or 8051)\n\nSystems that use 1's complement also just need a little modification to the logic because it's simply the carry bit wrapped around to the least significant bit. Not sure about sign-magnitude systems but it seems like they use 1's complement internally so the same thing applies\nUnfortunately we don't that luxury for floating-point types. By simply freeing the sign bit we'll have the unsigned version. But then what should we use that bit for?\n\nIncrease the range by adding it to the exponent\nIncrease the precision by adding it to the mantissa. This is often more useful, as we generally need more precision than range\n\nBut both choices need a bigger adder to accommodate for the wider value range. That increases the complexity of the logic while the adder's top bit sits there unused most of the time. Even more circuitry will be needed for multiplications, divisions or other complex operations\nOn systems that use software floating-point you need 2 versions for each function which wasn't expected during the time memory was so much expensive, or you'd have to find some \"tricky\" way to share parts of the signed and unsigned functions\nHowever floating-point hardware existed long before C was invented, so I believe the choice in C was due to the lack of hardware support because of the reason I mentioned above\nThat said, there exists several specialized unsigned floating-point formats, mainly for image processing purposes, like Khronos group's 10 and 11-bit floating-point type\n\n\nA33:\n\nI guess it depends on that the IEEE floating-point specifications only are signed and that most programming languages use them.\nWikipedia article on IEEE-754 floating-point numbers\nEdit: Also, as noted by others, most hardware does not support non-negative floats, so the normal kind of floats are more efficient to do since there is hardware support.\n\n\nA34:\n\n\nI'm not aware of any programming language that supports unsigned\nfloating-point numbers. Any idea why they don't exist?\n\nUnsigned floats exist. See the unsigned float16 (11 fractions bit, 5 exponent bits, 0 sign bits) for GPU hardware, HDR format DXGI_FORMAT_BC6H. It's just that they're uncommon enough across most computing hardware that mainstream programming languages omit them. In this usage, the sign is omitted because colors darker than black make no sense anyway.\nEven the far more common IEEE half or signed float16_t, which is used quite frequently in the field of graphics and machine learning for HDR images and lower bandwidth tensors, hasn't received the honor of being incorporated into C/C++ (though, more domain-specific languages like CUDA/HLSL do have half/float16_t, and there have been C++ proposals too). So if even signed float16 can't make into C++ outside of compiler specific extensions (e.g. gcc __fp16), then an unsigned float16 has little hope :b, and not even CUDA or HLSL have the unsigned type in the language, just in the texture definition itself (found in a .DDS file or in GPU texture memory). Until then, we'll have to continue to implement more exotic types without compiler help via helper libraries.\nupdate 2022-10-09 C++23 includes signed float16_t from proposal https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p1467r9.html, albeit as optional extensions. I don't see anything for an unsigned float though.\n\n\nA35:\n\nI think the main reason is that unsigned floats would have really limited uses compared to unsigned ints. I don't buy the argument that it's because the hardware doesn't support it. Older processors had no floating point capabilities at all, it was all emulated in software. If unsigned floats were useful they would have been implemented in software first and the hardware would have followed suit.\n\n\nA36:\n\nThe Felix language compiles to C++. Maybe that could be a step stone, if you don't mind C++.\n\n\nA37:\n\nThe main thing that comes to mind is the use of code generators.  Would you be willing to program in a different language that provided the functional programming and then generate the C code from that?\nIf that's not an attractive option, then you could abuse CPP to get part of the way there.  The macro system should let you emulate some functional programming ideas.  I've heard tell that gcc is implemented this way but I've never checked.\nC can of course pass functions around using function pointers, the main problems are lack of closures and the type system tends to get in the way.  You could explore more powerful macro systems than CPP such as M4.  I guess ultimately, what I'm suggesting is that true C isn't up to the task without great effort but you could extend C to make it be up to the task.  That extension would look the most like C if you use CPP or you could go to the other end of the spectrum and generate C code from some other language.\n\n\nA38:\n\nHartel & Muller's book, Functional C, can nowadays (2012-01-02) be found at: http://eprints.eemcs.utwente.nl/1077/ (there is a link to PDF version).\n\n\nA39:\n\nYou do not need to send code through a network to get this error. For example, this Python 3 code (now my favourite piece of code):\nwhile 1:print()\n\nwill print a lot of newlines, and eventually a BrokenPipeError will be raised. So you will get this error if you write too much data to an IO or other stream.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n(vs (vs ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 36, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI'm trying to find the best way to set default values for objects in Rails.  \nThe best I can think of is to set the default value in the new method in the controller. \nDoes anyone have any input if this is acceptable or if there's a better way to do it?\n\n\nCandidate answers:\nA1:\n\nI had the same, issue on Debian Squeeze, amd64. Installing libxrender1 solved the issue for me.\nsudo apt-get install libxrender1\n\n\n\nA2:\n\nI was having the exact same problem! The solution, to me, was to install the 64bit version of wkhtmltoimage.\n\n\nA3:\n\nI know this is a very late answer for this issue, but I got on something similar and went down the full path to understand what was really causing the issue.\nTurned out that the default windows jscript engine is still on es3, and many gems are taking advantage of es5 or es6 features.\nUnfortunately if this happen (you are using a gem or a piece of code that leverage es5 or es6 features), there is no way to let it work on windows with the native js engine.\nThis is the reason why installing node.js solves the problem (node is at least es5).\nHope this can help some folks struggling with a runtime error of jsexec.\nMy 2 cents advise is to install node(very easy) or install v8, and not removing the //=require_tree.\nNote execjs will automatically use node if detected. Otherwise force its use, adding in boot something like: \nENV['EXECJS_RUNTIME'] = 'Node'\n\nTo set the env to node.\n\n\nA4:\n\nIf your system is 64 bits, and your software requires 32 bits libs, you need to add the flag \"i386\", \nfor example on ubuntu:\nsudo apt-get install libxrender1:i386\n\n\n\nA5:\n\nWe didn't have a myapp/bin directory in our rails 4 app, so we created one and then copied in the my app/script/rails file, plus the bundle and rake files from under rvm/ruby/bin and then added these to the repo for git and pushed it up to heroku and all was well.\n\n\nA6:\n\nWith Ubuntu 16 (from a fresh Docker install):\napt install -y libxrender1 libxext6\n\n\n\nA7:\n\nThough people have given long answers I suggest very simple and concise answer\nJust go to \n\nconfig/initializers/assets.rb\n  and\n\nAdd the following line\n\nRails.application.config.assets.precompile += %w( style.css )\n\nwhere style.css can be replaced by your any file name for css\n\n\nA8:\n\nIf you are referring to ActiveRecord objects, you have (more than) two ways of doing this:\n1. Use a :default parameter in the DB\nE.G.\nclass AddSsl < ActiveRecord::Migration\n  def self.up\n    add_column :accounts, :ssl_enabled, :boolean, :default => true\n  end\n\n  def self.down\n    remove_column :accounts, :ssl_enabled\n  end\nend\n\nMore info here: http://api.rubyonrails.org/classes/ActiveRecord/Migration.html\n2. Use a callback\nE.G. before_validation_on_create\nMore info here: http://api.rubyonrails.org/classes/ActiveRecord/Callbacks.html#M002147\n\n\nA9:\n\nFor beginners like me:\n\nNavigate to \\app\\views\\layouts\\application.html.erb\nChange line 6 from:\n'<%= javascript_include_tag 'application', 'data-turbolinks-track' => true %>'\n\nto\n<%= javascript_include_tag 'defaults', 'data-turbolinks-track' => true %>\n\nSource from tutorial to fix here\n\n\nA10:\n\nFor windows users, this may work. There is a problem with coffee-script-source >1.9.0 running on windows.\nIt seems you have to add this to your gemfile:\ngem 'coffee-script-source', '1.8.0'\nthen do\nbundle update coffee-script-source\nI tried all the above options, and also mixed up a few combinations of them, till I found this Rails-4, ExecJS::ProgramError in Pages#welcome and had done multiple system gem updates and bundle installs and updates.\nI reverted all my trials and downgraded my coffee-script-source and it works. Posting here to help out anyone else, who may have a similar issue.\nUpdating files in vendor/cache\ncoffee-script-source-1.8.0.gem Removing outdated .gem files from vendor/cache\ncoffee-script-source-1.9.1.1.gem Bundle updated!\n\n\nA11:\n\nI can confirm running rake rails:update:bin works, as said by @Ryan Taylor.\nI think I had this problem because I originally created this Rails app on Windows. Running the command above on Linux solved for me.\nAlso, on changing from Windows to Linux for development, it is a good idea to delete Gemfile.lock file and run bundle install to generate it again without Windows specific gems listed there.\n\n\nA12:\n\nWhat I have found using ActiveAdmin 0.6.1 is that ActiveAdmin will look for helpers in app/helpers/active_admin/*_helper.rb, but the name doesn't really matter.\nWhat does matter is:\n\nthe filename must end in \"_helper.rb\"\nthe module name must be the camel-case of the file name\nthe file must be in app/helpers/active_admin/ directory. \n\nIf anyone knows where this is officially documented, that would be awesome.\nHere is an example: https://gist.github.com/afred/7035a657e8ec5ec08d3b\n\n\nA13:\n\nThe Unix packages x11-libs/libXext and x11-libs/libXrender are dependencies\nGentoo:\nsudo emerge libXext libXrender\n\nPDFkit wiki also explains in more detail how to get wkhtmltopdf working on engine yard.\nPDFkit wiki\nDebian or Ubuntu:\nsudo apt-get install libxrender1\n\n\n\nA14:\n\napp/helpers/active_admin/view_helpers.rb\n\ndidn't help me\nEDITED: i changed it to views_helper.rb & ViewsHelper accordingly and it worked\n*but if you want to define it only for certain resource, you can do it in my way\n\ni had to define\n#app/helpers/active_admin/categories_helper.rb\n\nmodule ActiveAdmin::CategoriesHelper\n\n  def helper_method\n\n  end\n\nend\n\nfor my active_admin resource app/admin/categories.rb\n\n\nA15:\n\nDid you change the location of your code from C:\\Users\\this-user\\yo-app?\nWhen I was young in rails I have created an app and the default location of my app was C:\\Users\\Duncan\\my-app and then, when I changed my-app and placed it in D:\\All-my-Apps-folder i had that error....\nI scratched my head, tried 1,2,3 and more .....nothing! Until I returned all code to default folder location and to my amazement, I was rolling again :)\nIn case someone may find this useful (I can't explain why that happened, maybe someone may without speculation)\n\n\nA16:\n\nLong Answer + Explanation\nI think the correct fix is to add the file to the precompiled assets, as recommended by the error message. Maybe that isn't fixing the issue for you because you've got an erb file that needs to be rendered at run time. I imagine if the file was a static json file then you would not still experience the issue after adding it to the precompiled assets.\nWhen you use the image_path helper, Sprockets is assuming that you've got a static asset. The fact that your app didn't raise errors before sprockets-rails 3.0 is somewhat surprising. This new version is doing a better job, apparently, at enforcing the standards. (it also looks like there are other problems with 3.0 that might be updated shortly)\nIf you need to have erb inside the manifest, then it would be best practice to use a route path helper rather than image_path or asset_path to get the url. This would require you to add a manifest route to your config/routes.rb file and render the json file through a controller action. The view file would be your .erb manifest.\n\nShort Answer\nThis started happening to me after doing a bundler update that changed my sprockets-rails version from 2.3.3 to 3.0.0. A simple fix is to revert sprockets-rails back to version 2.3.3 in your Gemfile and running bundle install again:\ngem 'sprockets-rails', '2.3.3'\nAs an aside: I was experiencing this issue in development environment and was able to fix it there by running rake assets:precompile. Unfortunately, it didn't get my tests passing.\n\n\nA17:\n\nHad the same issue\nOS- Windows 8\nError- 'ExecJS::RuntimeError...'\nSolution- missing Node.js\n\ninstall Node.js from http://www.nodejs.org/download/\nRestart the computer\n\n\n\nA18:\n\nI had this issue because the permissions on my ~/bin directory were 644 instead of 755.  Running rake rails:update:bin locally (on Mac/*nix) and then pushing the changes fixed the problem.\n\n\nA19:\n\nYou can try this\nUser.find(:all, limit: 10,\n            joins:  \"LEFT JOIN `user_points` ON user_points.user_id = users.id\" ,\n            select: \"users.*, count(user_points.id)\", \n            group:  \"user_points.user_id\")\n\n\n\nA20:\n\nI had the very same problem that you did. The issue lied in the fact that the bin folder was never pushed to the heroku repository. \nI looked, I looked, and then I looked again, there was no rule in the .gitignore file for the bin/ folder...\nThen, after a lot of pain and anguish, I realized that a couple of months before I had created a global .gitignore that would ignore all bin folders of all my repositories (why lord, why??).\nI deleted the global .gitignore, and everything worked fine.\n\n\nA21:\n\nMy friend was attempting a Rails tutorial on Win 8 RTM a few months ago and ran into this error.  Not sure if this issue exists in Windows 7 as well, but this may help.\nOptions:\n1) Removing  //= require_tree . / Ignoring the issue - As ColinR stated above, this line should not be causing an issue in the first place.  There is an actual problem with ExecJS working properly with the JavaScript runtime on your system and removing this line is just ignoring that fact.\n2) Installing Node.js / Running away -  Many people seem to just end up installing Node.js and using that instead of the JavaScript runtime already on their system.  While that is a valid option, it also requires additional software and only avoids the original issue, which is that ExecJS is not working properly with the JavaScript runtime already on your system.  If the existing JavaScript runtime on your system is supposed to work, why not make it work instead of installing more software?  According to the ExecJS creator, the runtime already built into Windows is in fact supported...\n\nExecJS lets you run JavaScript code from Ruby. It automatically picks the best runtime available to evaluate your JavaScript program, then returns the result to you as a Ruby object.\nExecJS supports these runtimes:\n\ntherubyracer - Google V8 embedded within Ruby\ntherubyrhino - Mozilla Rhino embedded within JRuby\nNode.js\nApple JavaScriptCore - Included with Mac OS X\nMicrosoft Windows Script Host (JScript)\n\n(from github.com/sstephenson/execjs#execjs )\n\n3) Actually fixing the issue / Learning - Use the knowledge of options 1 and 2 to search for other solutions.  I can't tell you how many webpages I closed upon seeing options 1 or 2 was the accepted solution before actually finding information about the root issue we were having.  The only reason we kept looking was that we couldn't believe the Rails team would (1) insert a line of code in every scaffold generated project that caused an issue, or (2) require that we install additional software just to run that default line of code.  And so we eventually arrived at a fix for our root issue (your miles may vary).\nThe Fix that worked for us:\nOn the system having issues, find ExecJS's runtimes.rb file.  It looks like this. Make a copy of the found file for backup. Open the original runtimes.rb for editing.  Find the section that starts with the line JScript = ExternalRuntime.new(. In that section, on the line containing :command     => \"cscript //E:jscript //Nologo //U\", - remove the //U only.  Then on the line containing :encoding    => 'UTF-16LE' # CScript with //U returns UTF-16LE - change UTF-16LE to UTF-8 .  Save the changes to the file.  This section of the file should now read:\nJScript = ExternalRuntime.new(\n    :name        => \"JScript\",\n    :command     => \"cscript //E:jscript //Nologo\",\n    :runner_path => ExecJS.root + \"/support/jscript_runner.js\",\n    :encoding    => 'UTF-8' # CScript with //U returns UTF-16LE\n)\n\nNext, stop then restart your Rails server and refresh the page in your browser that produced the original error. Hopefully the page loads without error now.  Here's the ExecJS issue thread where we originally posted our results: https://github.com/sstephenson/execjs/issues/81#issuecomment-9892952\nIf this did not fix the issue, you can always overwrite the modified runtimes.rb with the backup copy you (hopefully) made and everything will be back to square one.  In that case, consider option 3 and keep searching.  Let us know what eventually works for you.. unless it's removing the require_tree or installing node.js, there's plenty of that going around already. :)\n\n\nA22:\n\nQuick and dirty solution: remove //= require_tree . from application.js.\nAs I explain in the comments for the question, this doesn't actually solve the underlying issue that is causing the error, but merely sidesteps it.\n\n\nA23:\n\nIf you are just setting defaults for certain attributes of a database backed model I'd consider using sql default column values - can you clarify what types of defaults you are using?\nThere are a number of approaches to handle it, this plugin looks like an interesting option.\n\n\nA24:\n\nI need also do after:\nsudo apt-get install libfontconfig1\n\n\n\nA25:\n\nI had this problem and was scowering the internet I am running Windows 8 with this rails gem file\nsource 'https://rubygems.org'\n\ngem 'rails', '3.2.9'\n\n# Bundle edge Rails instead:\n# gem 'rails', :git => 'git://github.com/rails/rails.git'\n\ngroup :development do gem 'sqlite3', '1.3.5' \nend\n\n# Gems used only for assets and not required # in production environments by default. \n\ngroup :assets do \n    gem 'sass-rails', '3.2.5' \n    gem 'coffee-rails', '3.2.2'\n\ngem 'uglifier', '1.2.3' \nend\n\ngem 'jquery-rails', '2.0.2'\n\ngroup :production do \n    gem 'pg', '0.12.2' \nend\n\nWent to http://nodejs.org/download/  installed - restarted the machine and everything worked.\n\n\nA26:\n\nI can make it work in ActiveAdmin 0.6.1 (finally!). The solution is to create a helper module as following:\n# app/helpers/active_admin_helpers.rb\nmodule ActiveAdminHelpers\n  # make this method public (compulsory)\n  def self.included(dsl)\n    # nothing ...\n  end\n\n  # define helper methods here ...\n  def helper_method\n    ...\n  end\nend\n\nthen include this module this way:\n# app/admin/[resource].rb\ninclude ActiveAdminHelpers\n\nActiveAdmin.register [Resource] do\n  ...\n\nend\n\nActually, it's not a nice solution but it's DRY and working good. I have already read and tried a lot of methods and solutions such as ViewHelpers module (put under 'app/helpers' or 'app/admin/active_admin'), ActiveAdmin::DSL monkey patching, ... but those never worked in version 0.6.1 (I don't have any ideas about other versions) :(\n\n\nA27:\n\nActiveRecord::Base.connection will provide you with everything you ever wanted to know about the database connection established by boot.rb and environment.rb\nActiveRecord::Base.connection returns a lot of information. So you've got to know exactly what you're looking for.\nAs Marcel points out:\nActiveRecord::Base.connection.instance_of? \n  ActiveRecord::ConnectionAdapters::MysqlAdapter \n\nis probably the best method of determining if your database MySQL.\nDespite relying on internal information that could change between ActiveRecord release, I prefer doing it this way:\nActiveRecord::Base.connection.instance_values[\"config\"][:adapter] == \"mysql\"\n\n\n\nA28:\n\nWhat worked for me with Rails 3.2.11 and and gem activeadmin (0.5.1) was not adding the app/active_admin/view_helpers.rb file, or declaring any modules in config/initializers/active_admin.rb\nI put my helpers logically, by model, into the app/*_helpers.rb files. Then inside the app/admin/model.rb file I used:\n# app/admin/[resource].rb\nActiveAdmin.register [Resource] do\n  ...\n  filter :gender, as: :select, collection: proc{genders}\n  ...\nend\n\nTo use the helper in filters, to display a drop down list of genders to filter on, in the list view. For the corresponding create form fields, I used:\n# app/admin/[resource].rb\nActiveAdmin.register [Resource] do\n  form do |f|\n    f.inputs \"Case Manager\" do\n      ...\n      f.input :gender, as: :radio, collection: genders\n      ...\n      f.buttons\n    end\n  end\nend\n\nTo display radio buttons for the input form.\nNot sure why the proc{} is required outside of the form do |f| block, but if anyone can explain why it's a bad idea, I'll find a different way.\n\n\nA29:\n\nAfter struggling with this for a bit, I noticed that my Rails 4 project had a /bin directory, unlike some older Rails 3 projects I had cloned.  /bin contains 3 files, bundle, rails, and rake, but these weren't making it to Heroku because I had bin in my global .gitignore file.  \nThis is a pretty common ignore rule if you work with Git and other languages (Java, etc.), so to fix this:\n\nRemove bin from ~/.gitignore\nRun bundle install\nCommit your\nchanges with git add . and git commit -m \"Add bin back\"\nPush your changes to Heroku with git push heroku master\n\n\n\nA30:\n\nUsing activeadmin 1.0.0.pre1 from git://github.com/activeadmin/activeadmin.git\nRails 4.2.1\nThis worked for me...\nmy_app/app/helpers/active_admin/resources_helper.rb\nmodule ActiveAdmin\n  module ResourcesHelper\n    def resource_form_for(_resource, _params, _options = {}, &_block)\n      url = if _resource.new_record?\n              UrlBuilder.resources_path(_resource.class, _params)\n            else\n              UrlBuilder.resource_path(_resource.class, _params)\n            end\n\n      method = _resource.new_record? ? :post : :put\n\n      options = { url: url, method: method, builder: ActiveAdmin::FormBuilder }\n      options.merge!(_options)\n\n      semantic_form_for([:admin, _resource], options) do |f|\n        _block.call(f)\n      end\n    end\n  end\nend\n\nmy_app/app/admin/balance_sheets.rb\nActiveAdmin.register BalanceSheet do\n  form partial: 'form'\nend\n\nmy_app/app/views/admin/balance_sheets/_form.html.erb\n<%= resource_form_for(resource, params) do |f| %>\n  <%= f.inputs \"Fields\" do %>\n    <%= f.input :progress_status %>\n    <%= f.input :crew %>\n    <%= f.input :shift %>\n    <%= f.input :expected_progress %>\n    <%= f.input :real_progress %>\n    <%= f.input :analyst, collection: User.analysts %>\n    <%= f.input :activity_ids, as: :check_boxes, collection: Activity.balance_sheet_activities %>\n    <%= f.input :worker_ids, as: :check_boxes, collection: Worker.all %>\n  <% end %>\n  <%= f.actions %>\n<% end %>\n\n\n\nA31:\n\nI favoured the Learning route. It seems the problem stems from \nIO.popen(command, options) { |f| output = f.read }\n\nreturning an empty string in execjs\\external_runtine.rb (line 173 in version 1.4.0). This is why the error message contains no text. The changes suggested did not work for me. I changed UTF-16LE to UTF-8, but it still returned an empty string. I removed \\\\U from the command - this at least returned text, but it was in the wrong encoding - in the browser it displayed as Chinese characters.\nAccording to this MSDN blog post, using the //U flag and redirecting to a file causes cscript to return the result using UTF-16.\nAnd then, magically, it worked (@#%$&^@$%!!!?!?!) using command as \"cscript //E:jscript //Nologo\" and encoding as \"UTF-8\". Oh well.\n\n\nA32:\n\nYou need to put your helper functions in app/helpers/active_admin/views_helper.rb file \nExample: \nmodule ActiveAdmin::ViewsHelper #camelized file name\n  def my_helper \n       # do something \n  end \nend \n\n\n\nA33:\n\nJust for future reference, adding :all gives a deprecated message. In later versions of rails you can simply chain the methods like this:\nUser.joins(\"LEFT JOIN `user_points` ON user_points.user_id = users.id\").select(\"users.*, count(user_points.id)\").group(\"user_points.user_id\")\n\nOR use a scope like this:\nscope :my_scope_name_here, -> { \n        joins(\"LEFT JOIN `user_points` ON user_points.user_id = users.id\")\n        .select(\"users.*, count(user_points.id)\")\n        .group(\"user_points.user_id\")\n}\n\nYou can also chain .where between the .join and the .select.\nHope this helps someone in the future.\n\n\nA34:\n\nOn rails 5.2.6, rake app:update:bin worked for me. Now deployment to heroku is working.\n\n\nA35:\n\nI got a similar error.\nI did not modify assets.rb or anything, just restart my server and no error anymore.\n\nActionView::Template::Error (Asset was not declared to be precompiled in production.\nAdd Rails.application.config.assets.precompile += %w( rails.png ) to config/initializers/assets.rb and restart your server):\n    10:   <%= link_to \"Sign up now!\", '#', class: \"btn btn-lg btn-primary\" %>\n    11: \n    12: \n    13: <%= link_to image_tag(\"rails.png\", alt: \"Rails logo\"),\n    14:             'http://rubyonrails.org/' %>\n  app/views/static_pages/home.html.erb:13:in `_app_views_static_pages_home_html_erb___1806898863626708249_70312070486240'\n\n\nA36:\n\nOne thing you can do to make it more readable is: \n((Time.zone.now - myActiveRecord.visit_date) / 1.day).to_i\n\nEdit:\nActually you can get rid of one set of the brackets with:\n(Time.zone.now - myActiveRecord.visit_date).to_i / 1.day\n\n\n\nA37:\n\nDefining ActiveAdmin::ViewHelpers in app/admin/active_admin/view_helpers.rb works for me with activeadmin 0.3.4 and 0.5.0.\n\n\nA38:\n\nEven more shorter call\nActiveRecord::Base.connection.adapter_name == 'MySQL'\n\n\n\nA39:\n\nRunning Win 8 64 bit\nrails 4.2.5\nruby 2.1.7\nThis one worked for me\n\n\nA40:\n\nIn Rails 3, (maybe earlier, but I'm using Rails 3 currently) using ActiveRecord::ConnectionAdapters::MysqlAdapter is a poor way to go about it, as it's only initialized if the database adapter in use is MySQL.  Even if you have the MySQL gem installed, if it's not your connection type, that call wil fail:\nLoading development environment (Rails 3.0.3)\n>> ActiveRecord::Base.connection.instance_of? ActiveRecord::ConnectionAdapters::MysqlAdapter\nNameError: uninitialized constant ActiveRecord::ConnectionAdapters::MysqlAdapter\nfrom (irb):1\n\nSo, I'd recommend stasl's answer and use the adapter_name property of the connection.\n\n\nA41:\n\nI used the solution number 2 because previously i had have this mistake, but in this ocation didn't work, then I added the \ngem 'coffee-script-source', '1.8.0'\nand run \nbundle install\n\nand my problem was fixed\n\n\nA42:\n\nYou can define them in app/helpers/ as you tried but you need to include them trough the active admin's initializer like this:\n# in config/initializers/active_admin.rb\nActiveAdmin.setup do |config|\n    ....\nend\n\nmodule ActiveAdmin::ViewHelpers\n  include ApplicationHelper\nend\n\n\n\nA43:\n\n\"Correct\" is a dangerous word in Ruby.  There's usually more than one way to do anything.  If you know you'll always want that default value for that column on that table, setting them in a DB migration file is the easiest way:\nclass SetDefault < ActiveRecord::Migration\n  def self.up\n    change_column :people, :last_name, :type, :default => \"Doe\"\n  end\n\n  def self.down\n    # You can't currently remove default values in Rails\n    raise ActiveRecord::IrreversibleMigration, \"Can't remove the default\"\n  end\nend\n\nBecause ActiveRecord autodiscovers your table and column properties, this will cause the same default to be set in any model using it in any standard Rails app.\nHowever, if you only want default values set in specific cases -- say, it's an inherited model that shares a table with some others -- then another elegant way is do it directly in your Rails code when the model object is created:\nclass GenericPerson < Person\n  def initialize(attributes=nil)\n    attr_with_defaults = {:last_name => \"Doe\"}.merge(attributes)\n    super(attr_with_defaults)\n  end\nend\n\nThen, when you do a GenericPerson.new(), it'll always trickle the \"Doe\" attribute up to Person.new() unless you override it with something else.\n\n\nA44:\n\nI had to add my nodejs folder to my Windows Path environment variable.  In Windows 8 open the Control Panel, go to System, Advanced system settings (on the left), click Environment Variables on the left, and edit the Path variable to include the directory to your nodejs folder (probably in Program Files).\nOf course you have to have Node.js installed (use the Windows installer) and have installed CoffeeScript through NPM.\n\n\nA45:\n\nRan into this today on a RHEL7 machine running the static version of wkhtmltopdf.  Solution was to install a few dependencies: \nyum install libXrender libXext fontconfig \n\n\n\nA46:\n\nI know this question is a bit dated but I came across it while Googling for a similar problem. In my case I needed to know the difference in whole days on a macro and micro scale.\nFor example, I needed my code to be able to tell me that Dec 31, 2010 is 366 days before Jan 1, 2012 and that Dec 31, 2010 23:59 is 1 day away from Jan 1, 2011 00:00. The method above works in the former but in the case of the latter, it says they are 0 days apart.\nWhat I ended up doing was using Ruby's Date class to do the math for me. Using the code above my method looks like this:\n(Time.zone.now.to_date - myActiveRecord.visit_date.to_date).to_i\nThis will work with inputs in Time or DateTime due to the conversion. Another possible solution would be to first call beginning_of_day on each of the Times or DateTimes but in my case, the minutes were important.\n\n\nA47:\n\nHere's a less complicated solution, for beginners:\nIf you are just working through the tutorial, you are probably working with the default Gemfile (or very nearly). You can open it up in your text editor, and remove the pound sign from the front of this line:\n# gem 'therubyracer', :platforms => :ruby\n\nYou will need to re-run bundle install, which will likely download a few things. But once it does, you should be able to start the server without any problem.\nAt least, that worked for me.\nThis also works on Ubuntu 12.04, by the way.\n\n\nA48:\n\nRails actually has a method built in for just this sort of thing.\ncheckout #time_ago_in_words\nSo, for the original case...\n((Time.zone.now - myActiveRecord.visit_date)/86400).to_i\nvs\ntime_ago_in_words(myActiveRecord.visit_date)\n\n\nA49:\n\nI had a similar error and had to edit my manifest.js file in order to get it to work.\nEdit /assets/config.manifest.js and then\n// manifest.js\n//= link_tree ../images\n//= link_tree ../stylesheets .css\n\nThen do a bundle exec rake assets:precompile\n\n\nA50:\n\nSteps : \n\n\nbundle config --delete bin         # Turn off Bundler's stub generator\nrake rails:update:bin               # Use the new Rails 4 executables\ngit add bin or git add bin -f      # Add bin/ to source control\ngit commit -a -m \"you commit message\"\ngit push heroku master\nheroku open\n\n\n\n\nA51:\n\nThere is an adapter_name in AbstractAdapter and that is there since Rails2.\nSo it's easier to use in the migration like this:\nadapter_type = connection.adapter_name.downcase.to_sym\ncase adapter_type\nwhen :mysql, :mysql2\n  # do the MySQL part\nwhen :sqlite\n  # do the SQLite3 part\nwhen :postgresql\n  # etc.\nelse\n  raise NotImplementedError, \"Unknown adapter type '#{adapter_type}'\"\nend\n\n\n\nA52:\n\nAnother way to do this is to make the specific ActiveAdmin controller generated behind-the-scenes include the helper.  This method will allow making the inclusion of the helpers explicit per file rather than global.\nActiveAdmin.register MyModel do\n  controller do\n    include MyHelper\n  end\nend\n\n\n\nA53:\n\nRails 5 has a left_outer_joins method. So you can do\nUser.left_outer_joins(:user_points)\n\nor use the alias\nUser.left_joins(:user_points)\n\n\n\nA54:\n\nYou can also use ActiveAdmin partials :\nrender partial: 'admin/my_partial', locals: { var: my_var }\nAnd inside app/views/admin/_my_partial.html.arb your active_admin ruby code.\n\n\nA55:\n\nI had this problem also since I upgraded to rails 4.0.0\nRun this command\nrake rails:update:bin\n\nYou can go here for more info\nhttps://devcenter.heroku.com/articles/rails4\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "    |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  "}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 37, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIf you run a .bat or .cmd file with %0|%0 inside, your computer starts to use a lot of memory and after several minutes, is restarted. Why does this code block your Windows? And what does this code programmatically do? Could it be considered a \"bug\"?\n\n\nCandidate answers:\nA1:\n\nI found a simpler solution at http://candidcode.com/2010/01/12/a-guide-to-converting-from-mercurial-hg-to-git-on-a-windows-client/\nAnd then I found a yet simpler solution myself:\nTo use the hg-git Mercurial extension on Windows: \n\ninstall the official Mercurial binaries\nput dulwich folder\nfrom dulwich sources and hggit folder from hg-git sources to the\nroot of library.zip in Mercurial installation folder\nadd the following to %USERPROFILE%\\Mercurial.ini:\n\n\n[extensions]\nhgext.bookmarks=\nhggit=\n\nTo have SSH support you need plink.exe from PuTTY family. After that you should add the following to Mercurial.ini:\n[ui]\nusername = John Doe <[email\u00a0protected]>\nssh=d:/home/lib/dll/plink.exe -i \"d:/home2/ssh-private-key.ppk\"\n\nWhen connecting to a SSH server for the first time, you should start putty.exe and try to connect using it. It will add the server key fingerprint to the registry. Otherwise plink will ask you to accept the fingerprint but it doesn't accept any input.\nYou can use puttygen.exe to generate private keys. Either use keys without a passphrase or use Pageant.exe ssh authentication agent.\n\n\nA2:\n\nInstall gVim on your window and enable \".bat\" when you install gvim and click next, done.\nYou can use vim on window.\n\n\nA3:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA4:\n\nThis is the Windows version of a fork bomb.\n%0 is the name of the currently executing batch file. A batch file that contains just this line:\n%0|%0\n\nIs going to recursively execute itself forever, quickly creating many processes and slowing the system down.\nThis is not a bug in windows, it is just a very stupid thing to do in a batch file.\n\n\nA5:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA6:\n\nIf you can install TortoiseHg, it includes dulwich and other requirements.\n\n\nA7:\n\nTry following configuration (change to your path), which works for me:\n[extensions]\n; hg-git extention\nhgext.bookmarks =\nhggit = C:\\Python26\\Lib\\site-packages\\hg_git-0.2.1-py2.6.egg\\hggit\n\nIn my case when I have empty value for hggit =, I get the same error as you do in this case. But I can import dulwich without problem in python shell, so you should check your easy-install.pth (as pointed out by David) if it contains dulwich-0.5.0-py2.5.egg.\nI did install pure version of dulwich as well.\n\n\nA8:\n\n%0 will never end, but it never creates more than one process because it instantly transfers control to the 2nd batch script (which happens to be itself).\nBut a Windows pipe creates a new process for each side of the pipe, in addition to the parent process. The parent process can't finish until each side of the pipe terminates. So the main program with a simple pipe will have 3 processes. You can see how the bomb quickly get's out of control if each side of the pipe recursively calls the parent batch!\n\n\nA9:\n\nAfter a long research I finally managed to setup hggit properly.\nWhat I have done:\n\nI have installed the newest version of TortoiseHg (in my case Version 5.9.1)\nThis official version (automatically) contains Python-2.7.18 (According to Help/About)\nDownload dulwich: cd C:\\dulwich, git clone https://github.com/dulwich/dulwich.git .\nTortoiseHg uses Python 2.7, dulwich doesn't support this (old) Python version anymore. We have to go back to an older version of dulwich (<0.20)\ngit reset --hard c6993451a745d313f61e5d080b9f9d8611a8d7f4\nThe subfolder C:\\dulwich\\dulwich has to be copied inside the zip file C:\\Program Files\\TortoiseHg\\lib\\library.zip so that there is a path C:\\Program Files\\TortoiseHg\\lib\\library.zip\\dulwich\nThis can be done by the following commands:\na) Extract the currenty content of TortoiseHg's library\nmd \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\ntar -xf \"C:\\Program Files\\TortoiseHg\\lib\\library.zip\" -C \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\nb) Copy dulwich files into directory\nxcopy C:\\dulwich\\dulwich \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\\dulwich\\\"\nc) Save old library.zip and create new library.zip\nren \"C:\\Program Files\\TortoiseHg\\lib\\library.zip\" library_old.zip\ncd \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\ntar -acf \"C:\\Program Files\\TortoiseHg\\lib\\library.zip\" *.*\nd) Cleanup\ncd ..\nrd /q /s \"C:\\Program Files\\TortoiseHg\\lib\\ExtractedLibrary\"\nrd /q /s C:\\dulwich\nDownload hg-git: cd C:\\hg-git, hg clone https://foss.heptapod.net/mercurial/hg-git .\nThe current version of hg-git also does not support Python 2.7 anymore. We have to go back to the last working version 0.9.0 (=revision 1320 in TortoiseHg):\nhg update 6d7d6f174df8\nCreate a new empty repository which will be \"connected\" to a remote git repository\ncd C:\\MyLocalHgRepoWhichIsConnectedToGit, hg init\nCreate a textfile C:\\MyLocalHgRepoWhichIsConnectedToGit\\.hg\\hgrc with the following content:\n\n\n[paths]\ndefault = C:\\ConnectedGitRepoForPushAndPull\n\n[extensions]\nhgext.bookmarks = \nhggit = C:\\hg-git\\hggit\n\n\nRemark: Alternatively you can also copy the folder C:\\hg-git\\hggit inside library.zip and leave the path in the hgrc file empty. Indeed this is my favourite way. Do it the same way as I have explained under step 4 with dulwich.\nThat's it. Now the Mercurial Repo is linked to the Git Repo\nand you can execute  hg pull or hg push inside the Hg Repo.\n\n\nIf you don't want to read all my explanations you can simply use this batch file and execute it:\nhggit.bat\n@echo off\nrem Installation von dulwich+hggit f\u00fcr TortoiseHg\nrem Michael Hutter, 22.09.2021\nrem https://stackoverflow.com/a/69279657/9134997\n\nif \"%1\"==\"install\" goto install\nif \"%1\"==\"clonegitrepo\" goto clonegitrepo\necho Syntax:\necho %0 install (installiert dulwich und hggit in TortoiseHg)\necho %0 clonegitrepo C:\\HgRepoToCreate C:\\GitRepoToClone (Klonen eines Git-Repos in ein Hg-Repo)\npause\ngoto ende\n\n\n:install\nset LibraryPath=C:\\Program Files\\TortoiseHg\\lib\nset TempPathDulwich=C:\\Temp\\dulwich\nset TempPathHggit=C:\\Temp\\hg-git\n\nif not exist \"%LibraryPath%\\library.zip\" (\necho Die Datei %LibraryPath%\\library.zip existiert nicht!\npause\ngoto Ende\n)\n\nrem Installation von dulwich\nmd %TempPathDulwich%\ncd %TempPathDulwich%\ngit clone https://github.com/dulwich/dulwich.git .\ngit reset --hard c6993451a745d313f61e5d080b9f9d8611a8d7f4\npause\n\nrem Installation von hg-git\nmd %TempPathHggit%\ncd %TempPathHggit%\nhg clone https://foss.heptapod.net/mercurial/hg-git .\nhg update 6d7d6f174df8\npause\n\nrem dulwich und hggit in library.zip aufnehmen\nmd \"%LibraryPath%\\ExtractedLibrary\"\ntar -xf \"%LibraryPath%\\library.zip\" -C \"%LibraryPath%\\ExtractedLibrary\"\nxcopy %TempPathDulwich%\\dulwich \"%LibraryPath%\\ExtractedLibrary\\dulwich\\\"\nxcopy %TempPathHggit%\\hggit \"%LibraryPath%\\ExtractedLibrary\\hggit\\\"\nren \"%LibraryPath%\\library.zip\" library_old.zip\nif exist \"%LibraryPath%\\library.zip\" del \"%LibraryPath%\\library.zip\"\ncd \"%LibraryPath%\\ExtractedLibrary\"\ntar -acf \"%LibraryPath%\\library.zip\" *.*\n\nrem Aufr\u00e4umen\ncd ..\nrd /q /s \"%LibraryPath%\\ExtractedLibrary\"\nrd /q /s %TempPathDulwich%\nrd /q /s %TempPathHggit%\npause\ngoto ende\n\n\n:clonegitrepo\nrem Klonen eines Git-Repos in ein lokales Hg-Repo\necho Erstelle neues HgRepo %2\nmd %2\ncd %2\nif not exist %2 (\necho \"Fehler: %2 existiert nicht!\"\npause\ngoto ende\n)\nhg init\necho [paths] > .hg\\hgrc\necho default = %3 >> .hg\\hgrc\necho. >> .hg\\hgrc\necho [extensions] >> .hg\\hgrc\necho hgext.bookmarks = >> .hg\\hgrc\necho hggit = >> .hg\\hgrc\nhg pull\nhg update\npause\n\n:ende\n\n\n\nA10:\n\nHow to sign your app\nUse Microsoft's SignTool to sign your app.\nYou download it as part of the Windows SDK. Note that it's also possible to install SignTool without installing the entire SDK. Once installed you can use SignTool from the command line like so:\nsigntool sign /a /fd SHA256 /tr http://timestamp.digicert.com /td SHA256 MyFile.exe\n\nThis will sign MyFile.exe. Explanation of the used command line options:\n\n/a will automatically use the certificate that is valid for the longest time. If you have no certificate, SignTool will display an error.\n/fd SHA256 will use the SHA-256 digest algorithm for the file signature. Using SHA256 is recommended and considered to be more secure than the default SHA1 digest algorithm.\n/tr http://timestamp.digicert.com adds a timestamp to your signed apps. This is extremely important because this will allow the signature to remain valid even after the certificate itself has already expired. The argument for the /tr option is a timestamp URL. You can use any of the timestamp URL's from this list of free RFC 3161 timestamp servers.\n/td SHA256 will use the SHA-256 digest algorithm for the timestamp signature. As before, using SHA256 is recommended and considered to be more secure.\n\nHow and when to use self-signed certificates\nIf you'd like to get a hold of a certificate that you can use to test your process of signing the executable, you can use MakeCert to create a self-signed certificate.\nOnce you've created your own certificate and have used it to sign your executable, you'll need to manually add it as a Trusted Root CA for your machine in order for UAC to accept your self-signed certificate as a trusted source. Note that you can only do this on your own development machines. You usually can not do this on your user's computers, since most users will not accept to install a new Root CA for good reasons.\nHow to get rid of the \"unrecognized app\" warning\nEven if your app is signed, you might still see the following warning message when trying to run the app:\n\nMicrosoft Defender SmartScreen prevented an unrecognized app from\nstarting. Running this app might put your PC at risk.\n\nHow to avoid this warning is a somewhat complex topic. Please see this answer to get the whole picture about these Microsoft SmartScreen warnings and what you can do and should know about it.\n\n\nA11:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA12:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\nA13:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA14:\n\nBecause of restrictions on my workstation, I had to install my instance of vim locally to the following folder: %LocalAppData%\\Vim\nBut just changing an environmental variable like others suggest is insufficient because the batch files aren't in the installation directory (such as vimdiff). So I figured out where these come from, and made my own batch files to add them to an environmental variable. To help you make these batch files, I've provided the list of commands below for you to run in the vim installation directory (if saving the commands to an install.bat file first, replace all % characters with %% to escape it):\necho @echo off > evim.bat\necho gvim.exe -y %* >> evim.bat\n\necho @echo off > view.bat\necho vim.exe -R %* >> view.bat\n\necho @echo off > gview.bat\necho gvim.exe -R %* >> gview.bat\n\necho @echo off > vimdiff.bat\necho vim.exe -d %* >> vimdiff.bat\n\necho @echo off > gvimdiff.bat\necho vim.exe -d %* >> gvimdiff.bat\n\nThen you have to go to Start > Search 'Edit environment variables for your account' > Find 'Path' > Edit > Add a new subentry for %LocalAppData%\\Vim\\vim82. Last you may need to close and open the command prompt, or kill explorer.exe and restart it.\n\n\nA15:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\nA16:\n\nBased on techtonik's explanation of the reason for the failing import of dulwich.errors, I found a solution which appears simpler to me than the already proposed ones:\nOn the Mercurial download page there is offered\n\nMercurial <x.y.z> for Python 2.7 on Windows x86 (source install)\n\nand\n\nMercurial <x.y.z> for Python 2.7 on Windows x64 (source install)\n\nas .EXE files which install Mercurial as a Python module into an existing Python 2.7 (x86 or x64) installation.\nIf hg-git and dulwich have been installed to this Python installation as well, the import errors should vanish.\nIf you are not sure what to do now, I hope this step-by-step explanation helps:\nPrerequisites\n\nPython 2.7 is installed\n<python 2.7 install dir> and <python 2.7 install dir>\\Scripts are in the PATH environment variable \nhg-git (and dulwich) have been installed into this Python installation via the instructions from here\n\nSteps\n\nDeinstall any existing Mercurial installation\nDownload and install one of the above-quoted EXE files, depending on whether your Python 2.7 installation is 32bit or 64bit (If you get a \"Python 2.7 cannot be found in registry\" error, you probably need the other one.)\n\nNow hg clone <some git repo> should work on the console.\n\n\nA17:\n\n\nThat makes me think dulwich is not\n  installed correctly, or not in the\n  path.\n\nYou're absolutely right. Mercurial binary distributions for Windows are 'frozen' - they use the Python code and interpreter bundled with them and therefore independent of packages installed in system PYTHONPATH. When you specify path to hggit extension in Mercurial.ini, hg tries to import it using direct path, but dulwich library is not imported explicitly by hg and doesn't bundled with its library, so the import fails.\nIt is possible to add both Dulwich and HgGit into library.zip that is installed along with hg.exe, but for me the best way is to install everything from source including Mercurial and execute commands using .bat files installed into \\Python\\Scripts. In this case you will need to:\n\nInstall Mercurial from source. This builds \"pure\" version, because Windows users usually don't have Visual Studio or alternative compiler for compiling C speedups.\nInstall Dulwich - I'd use latest trunk snapshot for both Git and Dulwich.\npython setup.py --pure install\nInstall latest HgGit snapshot\npython setup.py install\nEdit Mercurial.ini to enable hggit =\nLaunch Mercurial using your \\Python\\Scripts\\hg.bat\n\n\n\nA18:\n\nUntil you get import dulwich to work, hggit won't work. Check that the dulwich egg file is in your easy-install.pth file under site-packages. \nFor further debugging you can try ask pkg_resources about it:\nimport pkg_resources\npkg_resources.require(\"dulwich\")\n\n\n\nA19:\n\nWindows 10 has linux subsystem for windows. So you can install bash in windows and from bash you can use vim.I found it more convenient.\n\n\nA20:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA21:\n\nHad this problem today when installing the latest TortoiseHg.\nGet the latest python 2.7 (I used 2.7.16) setup on your system, you probably have this already.\nGet a command window and go to the c:\\Python27 folder\nTo run pip use Scripts\\pip or easy_install use Scripts\\easy_install\nTry pip -V to make sure you get the 2.7 version and not some 3.X version\nWrong:\nc:\\Python27>pip -V\npip 20.2.4 from c:\\python38\\lib\\site-packages\\pip (python 3.8)\n\nRight:\nc:\\Python27>Scripts\\pip -V\npip 20.2.4 from c:\\python27\\lib\\site-packages\\pip (python 2.7)\n\nIf dulwich or hg-git are installed already\nScripts\\pip uninstall dulwich\nScripts\\pip uninstall hg-git\n\nInstall hg-git\nScripts\\easy_install install hg-git\n\nYou should now have two folders\nC:\\Python27\\Lib\\site-packages\\dulwich-0.19.16-py2.7-win-amd64.egg\\dulwich\nC:\\Python27\\Lib\\site-packages\\hg_git-0.9.0-py2.7.egg\\hggit\n\nIt will only work if Dulwich is version 0.19.16 (less than 0.20.x)\nCopy these folders (dulwich and hggit) into the zip-file\nC:\\Program Files\\TortoiseHg\\lib\\library.zip\nfolder in zip file\n\n\nA22:\n\nI got this error as well even after downloading the latest Tortoisehg and making sure the hggit plugin was installed as well as my .ini & hgrc files had the right entry to enable hggit.\nTurns out my problem was that I had both mercurial and tortoisehg in my path. So when I ran any hg commands, it was using the hg.exe in mercurial folder as opposed to the hg.exe in the torsoisehg directory.\nThis makes sense but my mercurial installation did not have the plug ins. My fix was to remove mercurial from my path so hg commands go through the tortoisehg directory since it has hg completely bundled. Note however, the recommended option might be to upgrade mercurual to a version that has the plugins that one needs but this is what worked for me. I tried replacing the library.zip in mercurial with the one in tortoisehg and this worked but it led to other errors as one would imagine.\n@techtonik's answer led me down this road for which I am grateful.\nRecap: verify which hg exe is running your hg commands by checking your path because that hg.exe does not find the plugins for whatever reason.\n\n\nA23:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA24:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA25:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA26:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA27:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\nA28:\n\nWhen you install gVim:\nPlease make sure [\u2713] Create .bat files for command line use is checked.\nIt'll create several .bat files in C:\\Windows\\:\nC:\\>cd %windir%\nC:\\WINDOWS>dir /b *.bat\nevim.bat\ngview.bat\ngvim.bat\ngvimdiff.bat\nview.bat\nvim.bat\nvimdiff.bat\nvimtutor.bat\n\nNotice that: C:\\WINDOWS is already in the PATH environment variable.\nWhen you type vim in command line, C:\\WINDOWS\\vim.bat will be launched.\nIf you leave the checkbox mentioned above unchecked, you need to modify PATH manually.\n\n\nA29:\n\n\nsudo apt-get install python-dev # Dam you dependency!\nsudo easy_install dulwich\n\nSuccess!\n\n\nA30:\n\nI ran into this problem too with dulwich.errors.  Instead of installing everything from scratch.  I just copied dulwich from my default site-packages to the mercurial site-packages.  worked with no problems.\n\n\nA31:\n\nJust to supplement, I'm on a fairly highly controlled Windows workstation right now, and don't have access to much.  Downloading the \"executable installer\" that I usually use did not create the bat files nor, for some reason, vim.exe, though gvim.exe was installed in the vim74 dir for me and worked fine.  So though I also needed to set the PATH, that there was no bat file in C:\\WiNDOWS nor any command line executable in my VIm runtime folder to call meant that callingvim from the command line (or Powershell) didn't work.\nI'm guessing some portion of the install that's creating the command-line related stuff, apparently including vim.exe, isn't recovering gracefully when you don't have admin permissions.\nEither way, the \"right\" thing to do appears to be to set your PATH to your vim executable folder as usual (note that this might be a little more difficult than usual if you don't have admin privs), then download the \"Win32 console executable\" from the usual download page that matches the version of gvim that you've already installed, dig vim.exe out of that zip you just downloaded, and place that into the same folder as gvim.exe.\nLooking on another box where the executable installer did work \"fully\", there's some jive in the vim.bat file that wasn't installed for me about \"collect the arguments in VIMARGS for Win95\" and if .%OS%==.Windows_NT goto ntaction, etc etc, but not having any of that doesn't seem to be a problem on Win7, at least.  ;^)\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " V3, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 38, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIs there any actual difference between these two lines of code?\nini_set('max_execution_time', 20*60);\nset_time_limit(20*60);\n\n\n\nCandidate answers:\nA1:\n\nA slight improvement on RiggsFolly's script above, if you set:\nPATH=%phpver%;%PATH%\n\nand add your new PHP ver path at the beginning; this allows you to set a default path in your Environment setting and then you only need this script when you want to change to a different version.\nAlso, if like me, you want to run this in a git bash shell, just call make a bash script to call the .bat file:\n#!/bin/bash\neval phppath.bat $1\n\n\n\nA2:\n\nValidating + Formatting/Deriving Values\nSetters let you to validate data and getters let you format or derive data. Objects allow you to encapsulate data and its validation and formatting code into a neat package that encourages DRY.\nFor example, consider the following simple class that contains a birth date.\nclass BirthDate {\n\n    private $birth_date;\n\n    public function getBirthDate($format='Y-m-d') {\n        //format $birth_date ...\n        //$birth_date = ...\n        return $birth_date;\n    }\n\n    public function setBirthDate($birth_date) {                   \n        //if($birth_date is not valid) throw an exception ...          \n        $this->birth_date = $birth_date;\n    }\n\n    public function getAge() {\n        //calculate age ...\n        return $age;\n    }\n\n    public function getDaysUntilBirthday() {\n        //calculate days until birth days\n        return $days;\n    }\n}\n\nYou'll want to validate that the value being set is\n\nA valid date\nNot in the future\n\nAnd you don't want to do this validation all over your application (or over multiple applications for that matter). Instead, it's easier to make the member variable protected or private (in order to make the setter the only access point) and to validate in the setter because then you'll know that the object contains a valid birth date no matter which part of the application the object came from and if you want to add more validation then you can add it in a single place.\nYou might want to add multiple formatters that operate on the same member variable i.e. getAge() and getDaysUntilBirthday() and you might want to enforce a configurable format in getBirthDate() depending on locale. Therefore I prefer consistently accessing values via getters as opposed to mixing $date->getAge() with $date->birth_date.\ngetters and setters are also useful when you extend objects. For example, suppose your application needed to allow 150+ year birth dates in some places but not in others. One way to solve the problem without repeating any code would be to extend the BirthDate object and put the additional validation in the setter.\nclass LivingBirthDate extends BirthDate {\n\n    public function setBirthDate($birth_date) {\n        //if $birth_date is greater than 150 years throw an exception\n        //else pass to parent's setter\n        return parent::setBirthDate($birth_date);\n    }\n}\n\n\n\nA3:\n\njust do these steps  if you don't need your old php version:\n\nopen wamp and right click on wamp manager than go : tools/Change PHP CLI Version than change php version to latest\nanother time  right click on wamp manager than go : tools/Delete unuserd versions and delete the oldest version which your system insist on it to be your pc php version :D\ngo to control panel/user account/change my environment variables and in PATH variable click edit and add your latest php version path which is in your wamp server bin folder\nclose all command lines or IDEs and restart them and check for php -v\n\nthis works well\n\n\nA4:\n\nTry using batch file\n\nOpen notepad\ntype php -S localhost:8000\nsave file as .bat extension, server.bat\nnow click on server.bat file your server is ready on http://localhost:8000\n\nDependency\nif you got error php not recognize any internal or external command\n    then goto environment variable and edit path to php.exe\n    \"C:\\wamp\\bin\\php\\php5.4.3\"\n\n\nA5:\n\nI made an experiment using the magic method __call.\nNot sure if I should post it (because of all the \"DO NOT USE MAGIC METHODS\" warnings in the other answers and comments) but i'll leave it here.. just in case someone find it useful.\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = substr($_name, 4);\n\n    if (isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nJust add that method above in your class, now you can type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_foo(); // return \"bar\"\n$C->get_bom(); // return \"bim\"\n\n// as setter\n$C->set_foo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_bom(\"zam\"); // set \"zam\" as new value of bom\n\n\n\nThis way you can get/set everything in your class if it exist so, if you need it for only a few specific elements, you could use a \"whitelist\" as filter.  \nExample:\nprivate $callWhiteList = array(\n    \"foo\" => \"foo\",\n    \"fee\" => \"fee\",\n    // ...\n);\n\npublic function __call($_name, $_arguments){\n    $action  = substr($_name, 0, 4);\n    $varName = $this->callWhiteList[substr($_name, 4)];\n\n    if (!is_null($varName) && isset($this->{$varName})){\n        if ($action === \"get_\") return $this->{$varName};\n        if ($action === \"set_\") $this->{$varName} = $_arguments[0];\n    }\n}\n\nNow you can only get/set \"foo\" and \"fee\".\nYou can also use that \"whitelist\" to assign custom names to access to your vars.\nFor example,\nprivate $callWhiteList = array(\n    \"myfoo\" => \"foo\",\n    \"zim\" => \"bom\",\n    // ...\n);\n\nWith that list you can now type:\nclass MyClass{\n    private foo = \"bar\";\n    private bom = \"bim\";\n    // ...\n    // private $callWhiteList = array( ... )\n    // public function __call(){ ... }\n    // ...\n}\n$C = new MyClass();\n\n// as getter\n$C->get_myfoo(); // return \"bar\"\n$C->get_zim(); // return \"bim\"\n\n// as setter\n$C->set_myfoo(\"abc\"); // set \"abc\" as new value of foo\n$C->set_zim(\"zam\"); // set \"zam\" as new value of bom\n\n\n.\n.\n.\nThat's all.\n\nDoc:\n__call() is triggered when invoking inaccessible methods in an object context. \n\n\nA6:\n\nIn addition to the already great and respected answers in here, I would like to expand on PHP having no setters/getters.\nPHP does not have getter and setter syntax. It provides subclassed or magic methods to allow \"hooking\" and overriding the property lookup process, as pointed out by Dave.\nMagic allows us lazy programmers to do more with less code at a time at which we are actively engaged in a project and know it intimately, but usually at the expense of readability.\nPerformance Every unnecessary function, that results from forcing a getter/setter-like code-architecture in PHP, involves its own memory stack-frame upon invocation and is wasting CPU cycles. \nReadability: The codebase incurs bloating code-lines, which impacts code-navigation as more LOC mean more scrolling,.\nPreference: Personally, as my rule of thumb, I take the failure of static code analysis \nas a sign to avoid going down the magical road as long as obvious long-term benefits elude me at that time.\nFallacies:\nA common argument is readability. For instance that $someobject->width is easier to read than $someobject->width(). However unlike a planet's circumference or width, which can be assumed to be  static, an object's instance such as $someobject, which requires a width function, likely takes a measurement of the object's instance width.\nTherefore readability increases mainly because of assertive naming-schemes and not by hiding the function away that outputs a given property-value.\n__get / __set uses:\n\npre-validation and pre-sanitation of property values\nstrings e.g.\n\"\nsome {mathsobj1->generatelatex} multi\nline text {mathsobj1->latexoutput}\nwith lots of variables for {mathsobj1->generatelatex}\n some reason\n\"\n\nIn this case generatelatex would adhere to a naming scheme of actionname + methodname\nspecial, obvious cases\n$dnastringobj->homeobox($one_rememberable_parameter)->gattaca->findrelated()\n$dnastringobj->homeobox($one_rememberable_parameter)->gttccaatttga->findrelated()\n\n\nNote: PHP chose not to implement getter/setter syntax. I am not claiming that getters/setter are generally bad.\n\n\nA7:\n\nYou can use php magic methods __get and __set.\n<?php\nclass MyClass {\n  private $firstField;\n  private $secondField;\n\n  public function __get($property) {\n    if (property_exists($this, $property)) {\n      return $this->$property;\n    }\n  }\n\n  public function __set($property, $value) {\n    if (property_exists($this, $property)) {\n      $this->$property = $value;\n    }\n\n    return $this;\n  }\n}\n?>\n\n\n\nA8:\n\nThe following solution is specifically for wamp environments: \nThis foxed me for a little while, tried all the other suggestions, $PATH etc even searched the windows registry looking for clues:\nThe GUI (wampmanager) indicates I have version 7 selected and yes if I phpinfo() in a page in the browser it will tell me its version 7.x.x yet php -v in the command prompt reports a 5.x.x\nIf you right click on the wampmanager head to icon->tools->delete unused versions and remove the old version, let it restart the services then the command prompt will return a 7.x.x\nThis solution means you no longer have the old version if you want to switch between php versions but there is a configuration file in C:\\wamp64\\wampmanager.conf which appears to specify the version to use with CLI (the parameter is called phpCliVersion). I changed it, restarted the server ... thought I had solved it but no effect perhaps I was a little impatient so I have a feeling there may be some mileage in that.\nHope that helps someone\n\n\nA9:\n\nIn windows, put your php.exe file in windows/system32 or any other system executable folders and then go to command line and type php and hit enter following it, if it doesnt generate any error then you are ready to use PHP on command line. If you have set your php.exe somewhere else than default system folders then you need to set the path of it in the environment variables! You can get there in following path....\ncontrol panel -> System -> Edith the environment variables of your account -> Environment Vaiables -> path -> edit then set the absolute path of your php.exe there and follow the same procedure as in first paragraph, if nothing in the error department, then you are ready to use php from command line!\n\n\nA10:\n\nI remember one time when I stumbled upon this issue a few years ago, it's because windows don't have readline, therefore no interactive shell, to use php interactive mode without readline support, you can do this instead:\nC:\\>php -a \nInteractive mode enabled \n\n<?php \necho \"Hello, world!\"; \n?> \n^Z \nHello, world!\n\nAfter entering interactive mode, type using opening (<?php) and closing (?>) php tag, and end with control Z (^Z) which denotes the end of file.\nI also recall that I found the solution from php's site user comment: http://www.php.net/manual/en/features.commandline.interactive.php#105729\n\n\nA11:\n\nThere are many ways to create sourcecode in a netbeans-convention. This is nice. It makes thinks such easyer === FALSE. Just use the traditionel, specially if you are not sure which one of the properties should be encapsuled and which one not. I know, it is a boi.... pla... code, but for debugging-works and many other thinks it is the better, clear way. Dont spend to much time with thousend of arts how to make simple getters and setters. You cannot implement too some design patterns like the demeter-rule and so on, if you use magics. In specific situation you can use magic_calls or for small, fast and clear solutions. Sure you could make solutions for design-patters in this way too, but why to make you live more difficult. \n\n\nA12:\n\nEncapsulation is important in any OO language, popularity has nothing to do with it. In dynamically typed languages, like PHP, it is especially useful because there is little ways to ensure a property is of a specific type without using setters.\nIn PHP, this works:\nclass Foo {\n   public $bar; // should be an integer\n}\n$foo = new Foo;\n$foo->bar = \"string\";\n\nIn Java, it doesn't:\nclass Foo {\n   public int bar;\n}\nFoo myFoo = new Foo();\nmyFoo.bar = \"string\"; // error\n\nUsing magic methods (__get and __set) also works, but only when accessing a property that has lower visibility than the current scope can access. It can easily give you headaches when trying to debug, if it is not used properly.\n\n\nA13:\n\nUPDATED\nAfter few research, best solution was to use that info another stackoverflow thread to avoid ctrl+z input and also from the scree output.\nSo, instead of php -a you should use call \"php.exe\" -f NAMED_SCRIPT.php\nOLD\nReadline not possible under Windows, so none of existent php shells written in php will work. But there's a workaround using -a interactive mode.\n2 commmon problems here. You cannot see result until executes CTRL Z command to indicate the final of code/file like EOF. When you do, result in most cases is printed result and fast closed window. Anyway, you will be returned to cmd not the -a interactive mode.\nSave this content into a .bat file, and define your PHP PATH into Windows variables, or modify php.exe to \"full path to exe\" instead:\n::\n:: PHP Shell launch wrapper\n::\n@ECHO off\ncall \"php.exe\" -a\n\necho.\necho.\n\ncall \"PHP Shell.bat\"\n\nThis is a simple Batch launching -a mode of php.exe. When it launchs php, stop script even no pause is wrote because is \"into\" the interactive waiting for input. When you hit CTRL Z, gets the SIGSTEP (next step) not the SIGSTOP (close, CTRL+C usually), then read the next intruction, wich is a recursive call to .bat itself. Because you're always into PHP -a mode, no exit command. You must use CTRL+C or hit the exit cross with mouse. (No alt+f4)\nYou can also use \"Bat to Exe\" converter to easy use.\n\n\nA14:\n\nAfter reading the other advices, I'm inclined to say that:\nAs a GENERIC rule, you will not always define setters for ALL properties, specially \"internal\" ones (semaphores, internal flags...). Read-only properties will not have setters, obviously, so some properties will only have getters; that's where __get() comes to shrink the code:\n\ndefine a __get() (magical global getters) for all those properties which are alike,\ngroup them in arrays so:\n\nthey'll share common characteristics: monetary values will/may come up properly formatted, dates in an specific layout (ISO, US, Intl.), etc.\nthe code itself can verify that only existing & allowed properties are being read using this magical method.\nwhenever you need to create a new similar property, just declare it and add its name to the proper array and it's done. That's way FASTER than defining a new getter, perhaps with some lines of code REPEATED again and again all over the class code.\n\n\nYes! we could write a private method to do that, also, but then again, we'll have MANY methods declared (++memory) that end up calling another, always the same, method. Why just not write a SINGLE method to rule them all...? [yep! pun absolutely intended! :)]\nMagic setters can also respond ONLY to specific properties, so all date type properties can be screened against invalid values in one method alone. If date type properties were listed in an array, their setters can be defined easily. Just an example, of course. there are way too many situations.\nAbout readability... Well... That's another debate: I don't like to be bound to the uses of an IDE (in fact, I don't use them, they tend to tell me (and force me) how to write... and I have my likes about coding \"beauty\"). I tend to be consistent about naming, so using ctags and a couple of other aids is sufficient to me... Anyway: once all this magic setters and  getters are done, I write the other setters that are too specific or \"special\" to be generalized in a __set() method. And that covers all I need about getting and setting properties. Of course: there's not always a common ground, or there are such a few properties that is not worth the trouble of coding a magical method, and then there's still the old good traditional setter/getter pair.\nProgramming languages are just that: human artificial languages. So, each of them has its own intonation or accent, syntax and flavor, so I won't pretend to write a Ruby or Python code using the same \"accent\" than Java or C#, nor I would write a JavaScript or PHP to resemble Perl or SQL... Use them the way they're meant to be used.\n\n\nA15:\n\nIf you preffer to use the __call function, you can use this method. It works with\n\nGET => $this->property()\nSET => $this->property($value)\nGET => $this->getProperty()\nSET => $this->setProperty($value)\n\nkalsdas\npublic function __call($name, $arguments) {\n\n    //Getting and setting with $this->property($optional);\n\n    if (property_exists(get_class($this), $name)) {\n\n\n        //Always set the value if a parameter is passed\n        if (count($arguments) == 1) {\n            /* set */\n            $this->$name = $arguments[0];\n        } else if (count($arguments) > 1) {\n            throw new \\Exception(\"Setter for $name only accepts one parameter.\");\n        }\n\n        //Always return the value (Even on the set)\n        return $this->$name;\n    }\n\n    //If it doesn't chech if its a normal old type setter ot getter\n    //Getting and setting with $this->getProperty($optional);\n    //Getting and setting with $this->setProperty($optional);\n    $prefix = substr($name, 0, 3);\n    $property = strtolower($name[3]) . substr($name, 4);\n    switch ($prefix) {\n        case 'get':\n            return $this->$property;\n            break;\n        case 'set':\n            //Always set the value if a parameter is passed\n            if (count($arguments) != 1) {\n                throw new \\Exception(\"Setter for $name requires exactly one parameter.\");\n            }\n            $this->$property = $arguments[0];\n            //Always return the value (Even on the set)\n            return $this->$name;\n        default:\n            throw new \\Exception(\"Property $name doesn't exist.\");\n            break;\n    }\n}\n\n\n\nA16:\n\nYou can run php pages using php.exe \ncreate some php file with php code and in the cmd write \"[PATH to php.ext]\\php.exe [path_to_file]\\file.php\" \n\n\nA17:\n\nThe problem you are describing sounds like your version of PHP might be missing the readline PHP module, causing the interactive shell to not work. I base this on this PHP bug submission.\nTry running \nphp -m\n\nAnd see if \"readline\" appears in the output.\nThere might be good reasons for omitting readline from the distribution. PHP is typically executed by a web server; so it is not really need for most use cases. I am sure you can execute PHP code in a file from the command prompt, using:\nphp file.php\n\nThere is also the phpsh project which provides a (better) interactive shell for PHP. However, some people have had trouble running it under Windows (I did not try \nthis myself).\nEdit:\nAccording to the documentation here, readline is not supported under Windows:\n\nNote: This extension is not available on Windows platforms.\n\nSo, if that is correct, your options are:\n\nAvoid the interactive shell, and just execute PHP code in files from the command line - this should work well\nTry getting phpsh to work under Windows\n\n\n\nA18:\n\nThat is because you are in 'Interactive Mode' where php evaluates everything you type. To see the end result, you do 'ctrl+z' and Enter. You should see the evaluated result now :)\np.s. run the cmd as Administrator!\n\n\nA19:\n\nThe PHP CLI as its called ( php for the Command Line Interface ) is called php.exe\nIt lives in c:\\wamp\\bin\\php\\php5.x.y\\php.exe ( where x and y are the version numbers of php that you have installed )\nIf you want to create php scrips to run from the command line then great its easy and very useful.\nCreate yourself a batch file like this, lets call it phppath.cmd :\nPATH=%PATH%;c:\\wamp\\bin\\php\\phpx.y.z\nphp -v\n\nChange x.y.z to a valid folder name for a version of PHP that you have installed within WAMPServer\nSave this into one of your folders that is already on your PATH, so you can run it from anywhere.\nNow from a command window, cd into your source folder and run >phppath.\nThen run\n\nphp your_script.php\n\nIt should work like a dream.\nHere is an example that configures PHP Composer and PEAR if required and they exist\n@echo off\n\nREM **************************************************************\nREM * PLACE This file in a folder that is already on your PATH\nREM * Or just put it in your C:\\Windows folder as that is on the\nREM * Search path by default\nREM * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nREM * EDIT THE NEXT 3 Parameters to fit your installed WAMPServer\nREM **************************************************************\n\n\nset baseWamp=D:\\wamp\nset defaultPHPver=7.4.3\nset composerInstalled=%baseWamp%\\composer\nset phpFolder=\\bin\\php\\php\n\nif %1.==. (\n    set phpver=%baseWamp%%phpFolder%%defaultPHPver%\n) else (\n    set phpver=%baseWamp%%phpFolder%%1\n)\n\nPATH=%PATH%;%phpver%\nphp -v\necho ---------------------------------------------------------------\n\n\nREM IF PEAR IS INSTALLED IN THIS VERSION OF PHP\n\nIF exist %phpver%\\pear (\n    set PHP_PEAR_SYSCONF_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_INSTALL_DIR=%baseWamp%%phpFolder%%phpver%\\pear\n    set PHP_PEAR_DOC_DIR=%baseWamp%%phpFolder%%phpver%\\docs\n    set PHP_PEAR_BIN_DIR=%baseWamp%%phpFolder%%phpver%\n    set PHP_PEAR_DATA_DIR=%baseWamp%%phpFolder%%phpver%\\data\n    set PHP_PEAR_PHP_BIN=%baseWamp%%phpFolder%%phpver%\\php.exe\n    set PHP_PEAR_TEST_DIR=%baseWamp%%phpFolder%%phpver%\\tests\n\n    echo PEAR INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n) else (\n    echo PEAR DOES NOT EXIST IN THIS VERSION OF php\n    echo ---------------------------------------------------------------\n)\n\nREM IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM **************************************************************\nREM * IF A GLOBAL COMPOSER EXISTS ADD THAT TOO\nREM *\nREM * This assumes that composer is installed in /wamp/composer\nREM *\nREM **************************************************************\nIF EXIST %composerInstalled% (\n    ECHO COMPOSER INCLUDED IN THIS CONFIG\n    echo ---------------------------------------------------------------\n    set COMPOSER_HOME=%baseWamp%\\composer\n    set COMPOSER_CACHE_DIR=%baseWamp%\\composer\n\n    PATH=%PATH%;%baseWamp%\\composer\n\n    rem echo TO UPDATE COMPOSER do > composer self-update\n    echo ---------------------------------------------------------------\n) else (\n    echo ---------------------------------------------------------------\n    echo COMPOSER IS NOT INSTALLED\n    echo ---------------------------------------------------------------\n)\n\nset baseWamp=\nset defaultPHPver=\nset composerInstalled=\nset phpFolder=\nset phpver=\nset phpFolder=\n\nCall this command file like this to use the default version of PHP\n> phppath\n\nOr to get a specific version of PHP like this\n> phppath 5.6.30\n\n\n\nA20:\n\nWell, PHP does have magic methods __get, __set, __isset & __unset, which is always a start. Alas proper (get it?) OO properties is more than magic methods. The main problem with PHP's implementation is that magic methods are called for all inaccessible properties. Which means you have to Repeat Yourself (eg. by calling property_exists()) in the magic methods when determining if name is actually a property of your object. And you can't really solve this general problem with a base class unless all your classes inherit from ie. ClassWithProperties, since PHP lacks multiple inheritance.\nIn contrast, Python new style classes gives you property(), which lets you explicitly define all your properties. C# has special syntax. \nhttp://en.wikipedia.org/wiki/Property_(programming)\n\n\nA21:\n\nLooking at the current source: \n/* {{{ proto bool set_time_limit(int seconds)\n   Sets the maximum time a script can run */\nPHP_FUNCTION(set_time_limit)\n{\n    zend_long new_timeout;\n    char *new_timeout_str;\n    int new_timeout_strlen;\n    zend_string *key;\n\n    if (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"l\", &new_timeout) == FAILURE) {\n        return;\n    }\n\n    new_timeout_strlen = zend_spprintf(&new_timeout_str, 0, ZEND_LONG_FMT, new_timeout);\n\n    key = zend_string_init(\"max_execution_time\", sizeof(\"max_execution_time\")-1, 0);\n    if (zend_alter_ini_entry_chars_ex(key, new_timeout_str, new_timeout_strlen, PHP_INI_USER, PHP_INI_STAGE_RUNTIME, 0 TSRMLS_CC) == SUCCESS) {\n        RETVAL_TRUE;\n    } else {\n        RETVAL_FALSE;\n    }\n    zend_string_release(key);\n    efree(new_timeout_str);\n}\n/* }}} */\n\nset_time_limit() is indeed just a convenience wrapper around the according ini_set() call. It doesn't even seem to perform the advertised timer reset. (But I would guess the \"timer\" actually isn't a separate entity, but the ini value itself is used as such.)\n\n\nA22:\n\nclass MyClass {\n    private $firstField;\n    private $secondField;\n    private $thirdField;\n\n    public function __get( $name ) {\n        if( method_exists( $this , $method = ( 'get' . ucfirst( $name  ) ) ) )\n            return $this->$method();\n        else\n            throw new Exception( 'Can\\'t get property ' . $name );\n    }\n\n    public function __set( $name , $value ) {\n        if( method_exists( $this , $method = ( 'set' . ucfirst( $name  ) ) ) )\n            return $this->$method( $value );\n        else\n            throw new Exception( 'Can\\'t set property ' . $name );\n    }\n\n    public function __isset( $name )\n    {\n        return method_exists( $this , 'get' . ucfirst( $name  ) ) \n            || method_exists( $this , 'set' . ucfirst( $name  ) );\n    }\n\n    public function getFirstField() {\n        return $this->firstField;\n    }\n\n    protected function setFirstField($x) {\n        $this->firstField = $x;\n    }\n\n    private function getSecondField() {\n        return $this->secondField;\n    }\n}\n\n$obj = new MyClass();\n\necho $obj->firstField; // works\n$obj->firstField = 'value'; // works\n\necho $obj->getFirstField(); // works\n$obj->setFirstField( 'value' ); // not works, method is protected\n\necho $obj->secondField; // works\necho $obj->getSecondField(); // not works, method is private\n\n$obj->secondField = 'value'; // not works, setter not exists\n\necho $obj->thirdField; // not works, property not exists\n\nisset( $obj->firstField ); // returns true\nisset( $obj->secondField ); // returns true\nisset( $obj->thirdField ); // returns false\n\nReady!\n\n\nA23:\n\nNo there isn't.\necho ini_get('max_execution_time'); // 30\nset_time_limit(100);\necho ini_get('max_execution_time'); // 100\n\nRegarding timer reset, it is reset in both cases:\nini_set('max_execution_time', 10);\n\nfor ($i=0; $i<50000000; $i++) {\n\n}\n\nini_set('max_execution_time', 10); // timer is reset, just as it would be with set_time_limit\n\nfor ($i=0; $i<50000000; $i++) {\n\n}\n\necho 'done';\n\n\n\nA24:\n\nGoogle already published a guide on optimization of PHP and the conclusion was:\nNo getter and setter Optimizing PHP\nAnd no, you must not use magic methods.  For PHP, Magic Methods are evil. Why?\n\nThey are hard to debug.\nThere is a negative performance impact.\nThey require writing more code.\n\nPHP is not Java, C++, or C#. PHP is different and plays with different rules. \n\n\nA25:\n\nAccording to the php manual, set_time_limit() will reset the execution timer when called.  I don't believe ini_set() has the same side-effect, which would be the difference between the two.\nSee http://php.net/manual/en/function.set-time-limit.php for more information.\nUpdate: since examining various portions of the php source code (including that referenced by mario's answer), it is my conclusion that ini_set() and set_time_limit() are precisely equivalent.\nini_set() does indeed reset the timer (though I'm still at a loss as to how either function performs the reset, I would have to look up the function that kills the script when the timer ends to figure that one out).\n\n\nA26:\n\nWhy use getters and setters?\n\nScalability: It's easier refactor a getter than search all the var assignments in a project code.\nDebugging: You can put breakpoints at setters and getters.\nCleaner: Magic functions are not good solution for writting less, your IDE will not suggest the code. Better use templates for fast-writting getters.\n\n\n\n\nA27:\n\nIf you want to just run a quick code snippet you can use the -r option:\nphp -r \"echo 'hi';\"\n\n-r allows to run code without using script tags <?..?>\n\n\nA28:\n\nGenerally speaking, the first way is more popular overall because those with prior programming knowledge can easily transition to PHP and get work done in an object-oriented fashion. The first way is more universal. My advice would be to stick with what is tried and true across many languages. Then, when and if you use another language, you'll be ready to get something accomplished (instead of spending time reinventing the wheel).\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "This is aThe answer to the question, \"Which is the best-looking color for a light blue?\""}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 39, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nWe have a web app that exports CSV files containing foreign characters with UTF-8, no BOM. Both Windows and Mac users get garbage characters in Excel. I tried converting to UTF-8 with BOM; Excel/Win is fine with it, Excel/Mac shows gibberish. I'm using Excel 2003/Win, Excel 2011/Mac.\nHere's all the encodings I tried:\nEncoding  BOM      Win                            Mac\n--------  ---      ----------------------------   ------------\nutf-8     --       scrambled                      scrambled\nutf-8     BOM      WORKS                          scrambled\nutf-16    --       file not recognized            file not recognized\nutf-16    BOM      file not recognized            Chinese gibberish\nutf-16LE  --       file not recognized            file not recognized\nutf-16LE  BOM      characters OK,                 same as Win\n                   row data all in first field\n\nThe best one is UTF-16LE with BOM, but the CSV is not recognized as such. The field separator is comma, but semicolon doesn't change things.\nIs there any encoding that works in both worlds?\n\n\nCandidate answers:\nA1:\n\nEASIEST SOLUTION\nI just figured this out today.\n\nOpen in Word\nReplace all hyphens with en dashes\nSave and Close\nOpen in Excel\n\nOnce you are done editing, you can always open it back up in Word again to replace the en dashes with hyphens again.\n\n\nA2:\n\nI believe if you used the double data type, the re-calculation in Excel would work just fine.\n\n\nA3:\n\n(Assuming Excel 2003...)\nWhen using the Text-to-Columns Wizard has, in Step 3 you can dictate the data type for each of the columns.  Click on the column in the preview and change the misbehaving column from \"General\" to \"Text.\"\n\n\nA4:\n\nIt seems to my case that Excel 2011 for Mac OS is not using Encoding.GetEncoding(\"10000\") as i thought and wasted 2 days with but the same iso as on Microsoft OS.\nThe best proof for this is to make a file in Excel 2011 for MAC with special chars, save it as CSV and then open it in MAC text editor and the chars are scrambled.\nFor me this approach worked - meaning that csv export on Excel 2011 on MAC OS has special western europeean chars inside:\nEncoding isoMacOS = Encoding.GetEncoding(\"iso-8859-1\");\nEncoding defaultEncoding = Encoding.Default; \n\n// Convert the string into a byte array.\nbyte[] defaultEncodingBytes = defaultEncoding.GetBytes(exportText);\n\n// Perform the conversion from one encoding to the other.\nbyte[] ansiBytes = Encoding.Convert(defaultEncoding, isoMacOS, defaultEncodingBytes);\n\ndecodedString = isoMacOS.GetString(ansiBytes);\n\n\n\nA5:\n\nHi I have the same issue, \nI write this vbscipt to create another CSV file. The new CSV file will have a space in font of each field, so excel will understand it as text. \nSo you create a .vbs file with the code below (for example Modify_CSV.vbs), save and close it. Drag and Drop your original file to your vbscript file. It will create a new file with \"SPACE_ADDED\" to file name in the same location.\nSet objArgs = WScript.Arguments\n\nSet objFso = createobject(\"scripting.filesystemobject\")\n\ndim objTextFile\ndim arrStr ' an array to hold the text content\ndim sLine  ' holding text to write to new file\n\n'Looping through all dropped file\nFor t = 0 to objArgs.Count - 1\n    ' Input Path\n    inPath = objFso.GetFile(wscript.arguments.item(t))\n\n    ' OutPut Path\n    outPath = replace(inPath, objFso.GetFileName(inPath), left(objFso.GetFileName(inPath), InStrRev(objFso.GetFileName(inPath),\".\") - 1) & \"_SPACE_ADDED.csv\")\n\n    ' Read the file\n    set objTextFile = objFso.OpenTextFile(inPath)\n\n\n    'Now Creating the file can overwrite exiting file\n    set aNewFile = objFso.CreateTextFile(outPath, True) \n    aNewFile.Close  \n\n    'Open the file to appending data\n    set aNewFile = objFso.OpenTextFile(outPath, 8) '2=Open for writing 8 for appending\n\n    ' Reading data and writing it to new file\n    Do while NOT objTextFile.AtEndOfStream\n        arrStr = split(objTextFile.ReadLine,\",\")\n\n        sLine = \"\"  'Clear previous data\n\n        For i=lbound(arrStr) to ubound(arrStr)\n            sLine = sLine + \" \" + arrStr(i) + \",\"\n        Next\n\n        'Writing data to new file\n        aNewFile.WriteLine left(sLine, len(sLine)-1) 'Get rid of that extra comma from the loop\n\n\n    Loop\n\n    'Closing new file\n    aNewFile.Close  \n\nNext ' This is for next file\n\nset aNewFile=nothing\nset objFso = nothing\nset objArgs = nothing\n\n\n\nA6:\n\n\n\"yyyy-mm-dd hh:mm:ss.000\" format does not work in all locales. For\n  some (at least Danish) \"yyyy-mm-dd hh:mm:ss,000\" will work better.\n\nas replied by user662894.\nI want to add: Don't try to get the microseconds from, say, SQL Server's datetime2 datatype: Excel can't handle more than 3 fractional seconds (i.e. milliseconds).\nSo \"yyyy-mm-dd hh:mm:ss.000000\" won't work, and when Excel is fed this kind of string (from the CSV file), it will perform rounding rather than truncation.\nThis may be fine except when microsecond precision matters, in which case you are better off by NOT triggering an automatic datatype recognition but just keep the string as string...\n\n\nA7:\n\n\"yyyy-MM-dd hh:mm:ss.000\" format does not work in all locales. For some (at least Danish) \"yyyy-MM-dd hh:mm:ss,000\" will work better.\n\n\nA8:\n\nI have found that putting an '=' before the double quotes will accomplish what you want. It forces the data to be text.\neg. =\"2008-10-03\",=\"more text\"\nEDIT (according to other posts): because of the Excel 2007 bug noted by Jeffiekins one should use the solution proposed by Andrew: \"=\"\"2008-10-03\"\"\"\n\n\nA9:\n\nThe earlier suggestion to use \"yyyy-MM-dd HH:mm:ss\" is fine, though I believe Excel has much finer time resolution than that.  I find this post rather credible (follow the thread and you'll see lots of arithmetic and experimenting with Excel), and if it's correct, you'll have your milliseconds.  You can just tack on decimal places at the end, i.e. \"yyyy-mm-dd hh:mm:ss.000\".\nYou should be aware that Excel may not necessarily format the data (without human intervention) in such a way that you will see all of that precision.  On my computer at work, when I set up a CSV with \"yyyy-mm-dd hh:mm:ss.000\" data (by hand using Notepad), I get \"mm:ss.0\" in the cell and \"m/d/yyyy\u00a0 hh:mm:ss AM/PM\" in the formula bar.\nFor maximum information[1] conveyed in the cells without human intervention, you may want to split up your timestamp into two or three pieces: a date portion, a time portion to the second, and optionally a fractional seconds portion.  It looks to me like Excel wants to give you at most three visible \"levels\" (where fractions of a second are their own level) in any given cell, and you want seven: years, months, days, hours, minutes, seconds, and fractions of a second.\nOr, if you don't need the timestamp to be human-readable but you want it to be as accurate as possible, you might prefer just to store it as a big number (internally, Excel is just using the number of days, including fractional days, since an \"epoch\" date).\n\n[1]That is, information stored numerically and suitable for use in calculations.  If you simply want to visually see as much information as possible, or only need it for sorting/filtering (no date or time arithmetic), you could make up some format which Excel will definitely parse as a string, and thus leave alone; e.g. \"yyyymmdd.hhmmss.000\".\n\n\nA10:\n\nStill an issue in Microsoft Office 2016 release, rather disturbing for those of us working with gene names such as MARC1, MARCH1, SEPT1 etc. \nThe solution I've found to be the most practical after generating a \".csv\" file in R, that will then be opened/shared with Excel users:\n\nOpen the CSV file as text (notepad)\nCopy it (ctrl+a, ctrl+c).\nPaste it in a new excel sheet -it will all paste in one column as long text strings.\nChoose/select this column. \nGo to Data- \"Text to columns...\", on the window opened choose \"delimited\" (next). Check that \"comma\" is marked (marking it will already show the separation of the data to columns below) (next), in this window you can choose the column you want and mark it as text (instead of general) (Finish).\n\nHTH\n\n\nA11:\n\ninstead of csv, trying outputting html with an XLS extension and \"application/excel\" mime-type. I know this will work in Windows, but can't speak for MacOS\n\n\nA12:\n\nWorking off of Jarod's solution and the issue brought up by Jeffiekins, you could modify\n\"May 16, 2011\"\n\nto\n\"=\"\"May 16, 2011\"\"\"\n\n\n\nA13:\n\nIn my case, \"Sept8\" in a csv file generated using R was converted into \"8-Sept\" by Excel 2013. The problem was solved by using write.xlsx2() function in the xlsx package to generate the output file in xlsx format, which can be loaded by Excel without unwanted conversion. So, if you are given a csv file, you can try loading it into R and converting it into xlsx using the write.xlsx2() function. \n\n\nA14:\n\nGo to the language settings in the Control Panel, then Format Options, select a locale and see the actual date format for the chosen locale used by Windows by default. Yes, that timestamp format is locale-sensitive. Excel uses those formats when parsing CSV.\nEven further, if the locale uses characters beyond ASCII, you'll have to emit CSV in the corresponding pre-Unicode Windows \"ANSI\" codepage, e.g. CP1251. Excel won't accept UTF-8.\n\n\nA15:\n\nWithout modifying your csv file you can:\n\nChange the excel Format Cells option to \"text\"\nThen using the \"Text Import Wizard\" to define the csv cells.\nOnce imported delete that data\nthen just paste as plain text\n\nexcel will properly format and separate your csv cells as text formatted ignoring auto date formats.\nKind of a silly work around, but it beats modifying the csv data before importing.   Andy Baird and Richard sort of eluded to this method, but missed a couple important steps.\n\n\nA16:\n\nI know this is an old thread. For the ones like me, who still have this problem using Office 2013 via PowerShell COM object can use the opentext method. The problem is that this method has many arguments, that are sometimes mutual exclusive. To resolve this issue you can use the invoke-namedparameter method introduced in this post.\nAn example would be\n$ex = New-Object -com \"Excel.Application\"\n$ex.visible = $true\n$csv = \"path\\to\\your\\csv.csv\"\nInvoke-NamedParameter ($ex.workbooks) \"opentext\" @{\"filename\"=$csv; \"Semicolon\"= $true}\n\nUnfortunately I just discovered that this method somehow breaks the CSV parsing when cells contain line breaks. This is supported by CSV but Microsoft's implementation seems to be bugged.\nAlso it did somehow not detect German-specific chars. Giving it the correct culture did not change this behaviour. All files (CSV and script) are saved with utf8 encoding.\nFirst I wrote the following code to insert the CSV cell by cell.\n$ex = New-Object -com \"Excel.Application\"\n$ex.visible = $true;\n$csv = \"path\\to\\your\\csv.csv\";\n$ex.workbooks.add();\n$ex.activeWorkbook.activeSheet.Cells.NumberFormat = \"@\";\n$data = import-csv $csv -encoding utf8 -delimiter \";\"; \n$row = 1; \n$data | %{ $obj = $_; $col = 1; $_.psobject.properties.Name |%{if($row -eq1){$ex.ActiveWorkbook.activeSheet.Cells.item($row,$col).Value2= $_ };$ex.ActiveWorkbook.activeSheet.Cells.item($row+1,$col).Value2 =$obj.$_; $col++ }; $row++;}\n\nBut this is extremely slow, which is why I looked for an alternative. Apparently, Excel allows you to set the values of a range of cells with a matrix. So I used the algorithm in this blog to transform the CSV in a multiarray.\nfunction csvToExcel($csv,$delimiter){\n     $a = New-Object -com \"Excel.Application\"\n     $a.visible = $true\n     \n    $a.workbooks.add()\n     $a.activeWorkbook.activeSheet.Cells.NumberFormat = \"@\"\n     $data = import-csv -delimiter $delimiter $csv; \n     $array = ($data |ConvertTo-MultiArray).Value\n     $starta = [int][char]'a' - 1\n     if ($array.GetLength(1) -gt 26) {\n         $col = [char]([int][math]::Floor($array.GetLength(1)/26) + $starta) + [char](($array.GetLength(1)%26) + $Starta)\n     } else {\n         $col = [char]($array.GetLength(1) + $starta)\n     }\n     $range = $a.activeWorkbook.activeSheet.Range(\"a1:\"+$col+\"\"+$array.GetLength(0))\n     $range.value2 = $array;\n     $range.Columns.AutoFit();\n     $range.Rows.AutoFit();\n     $range.Cells.HorizontalAlignment = -4131\n     $range.Cells.VerticalAlignment = -4160\n}\n\n function ConvertTo-MultiArray {\n     param(\n         [Parameter(Mandatory=$true, Position=1, ValueFromPipeline=$true)]\n         [PSObject[]]$InputObject\n     )\n     BEGIN {\n         $objects = @()\n         [ref]$array = [ref]$null\n     }\n     Process {\n         $objects += $InputObject\n     }\n     END {\n         $properties = $objects[0].psobject.properties |%{$_.name}\n         $array.Value = New-Object 'object[,]' ($objects.Count+1),$properties.count\n         # i = row and j = column\n         $j = 0\n         $properties |%{\n             $array.Value[0,$j] = $_.tostring()\n             $j++\n         }\n         $i = 1\n         $objects |% {\n             $item = $_\n             $j = 0\n             $properties | % {\n                 if ($item.($_) -eq $null) {\n                     $array.value[$i,$j] = \"\"\n                 }\n                 else {\n                     $array.value[$i,$j] = $item.($_).tostring()\n                 }\n                 $j++\n             }\n             $i++\n         }\n         $array\n     } \n} \ncsvToExcel \"storage_stats.csv\" \";\"\n\nYou can use above code as is; it should convert any CSV into Excel. Just change the path to the CSV and the delimiter character at the bottom.\n\n\nA17:\n\nA workaround using Google Drive (or Numbers if you're on a Mac):\n\nOpen the data in Excel\nSet the format of the column with incorrect data to Text (Format > Cells > Number > Text)\nLoad the .csv into Google Drive, and open it with Google Sheets\nCopy the offending column\nPaste column into Excel as Text (Edit > Paste Special > Text)\n\nAlternatively if you're on a Mac for step 3 you can open the data in Numbers.\n\n\nA18:\n\nIn Excel 2010 open a new sheet.\nOn the Data ribbon click \"Get External Data From Text\".\nSelect your CSV file then click \"Open\".\nClick \"Next\".\nUncheck \"Tab\", place a check mark next to \"Comma\", then click \"Next\".\nClick anywhere on the first column.\nWhile holding the shift key drag the slider across until you can click in the last column, then release the shift key.\nClick the \"text\" radio button then click \"Finish\"\nAll columns will be imported as text, just as they were in the CSV file.\n\n\nA19:\n\nThis is a only way I know how to accomplish this without messing inside the file itself. As usual with Excel, I learned this by beating my head on the desk for hours.\nChange the .csv file extension to .txt; this will stop Excel from auto-converting the file when it's opened. Here's how I do it: open Excel to a blank worksheet, close the blank sheet, then File => Open and choose your file with the .txt extension. This forces Excel to open the \"Text Import Wizard\" where it'll ask you questions about how you want it to interpret the file. First you choose your delimiter (comma, tab, etc...), then (here's the important part) you choose a set columns of columns and select the formatting. If you want exactly what's in the file then choose \"Text\" and Excel will display just what's between the delimiters. \n\n\nA20:\n\nNone of the solutions offered here is a good solution. It may work for individual cases, but only if you're in control of the final display. Take my example: my work produces list of products they sell to retail. This is in CSV format and contain part-codes, some of them start with zero's, set by manufacturers (not under our control). Take away the leading zeroes and you may actually match another product. \nRetail customers want the list in CSV format because of back-end processing programs, that are also out of our control and different per customer, so we cannot change the format of the CSV files. No prefixed'=', nor added tabs. The data in the raw CSV files is correct; it's when customers open those files in Excel the problems start. And many customers are not really computer savvy. They can just about open and save an email attachment.\nWe are thinking of providing the data in two slightly different formats: one as Excel Friendly (using the options suggested above by adding a TAB, the other one as the 'master'. But this may be wishful thinking as some customers will not understand why we need to do this. Meanwhile we continue to keep explaining why they sometimes see 'wrong' data in their spreadsheets.\nUntil Microsoft makes a proper change I see no proper resolution to this, as long as one has no control over how end-users use the files.\n\n\nA21:\n\n(EXCEL 2007 and later)\nHow to force excel not to \"detect\" date formats without editing the source file\nEither:\n\nrename the file as .txt\nIf you can't do that, instead of opening the CSV file directly in excel, create a new workbook then go to \nData > Get external data > From Text and select your CSV.\n\nEither way, you will be presented with import options, simply select each column containing dates and tell excel to format as \"text\" not \"general\".\n\n\nA22:\n\n2018\nThe only proper solution that worked for me (and also without modifying the CSV).\nExcel 2010:\n\nCreate new workbook\nData > From Text > Select your CSV file\nIn the popup, choose \"Delimited\" radio button, then click \"Next >\"\nDelimiters checkboxes: tick only \"Comma\" and uncheck the other options, then click \"Next >\"\nIn the \"Data preview\", scroll to the far right, then hold shift and click on the last column (this will select all columns). Now in the \"Column data format\" select the radio button \"Text\", then click \"Finish\".\n\nExcel office365: (client version)\n\nCreate new workbook\nData > From Text/CSV > Select your CSV file\nData type detection > do not detect\n\nNote: Excel office365 (web version), as I'm writing this, you will not be able to do that.\n\n\nA23:\n\nI know this is an old question, but the problem is not going away soon. CSV files are easy to generate from most programming languages, rather small, human-readable in a crunch with a plain text editor, and ubiquitous.\nThe problem is not only with dates in text fields, but anything numeric also gets converted from text to numbers. A couple of examples where this is problematic: \n\nZIP/postal codes\ntelephone numbers\ngovernment ID numbers\n\nwhich sometimes can start with one or more zeroes (0), which get thrown away when converted to numeric. Or the value contains characters that can be confused with mathematical operators (as in dates: /, -).\nTwo cases that I can think of that the \"prepending =\" solution, as mentioned previously, might not be ideal is\n\nwhere the file might be imported into a program other than MS Excel (MS Word's Mail Merge function comes to mind),\nwhere human-readability might be important.\n\nMy hack to work around this\nIf one pre/appends a non-numeric and/or non-date character in the value, the value will be recognized as text and not converted. A non-printing character would be good as it will not alter the displayed value. However, the plain old space character (\\s, ASCII 32) doesn't work for this as it gets chopped off by Excel and then the value still gets converted. But there are various other printing and non-printing space characters that will work well. The easiest however is to append (add after) the simple tab character (\\t, ASCII 9).\nBenefits of this approach:\n\nAvailable from keyboard or with an easy-to-remember ASCII code (9),\nIt doesn't bother the importation,\nNormally does not bother Mail Merge results (depending on the template layout - but normally it just adds a wide space at the end of a line). (If this is however a problem, look at other characters e.g. the zero-width space (ZWSP, Unicode U+200B)\nis not a big hindrance when viewing the CSV in Notepad (etc),\nand could be removed by find/replace in Excel (or Notepad etc).\nYou don't need to import the CSV, but can simply double-click to open the CSV in Excel.\n\nIf there's a reason you don't want to use the tab, look in an Unicode table for something else suitable.\nAnother option\nmight be to generate XML files, for which a certain format also is accepted for import by newer MS Excel versions, and which allows a lot more options similar to .XLS format, but I don't have experience with this.\nSo there are various options. Depending on your requirements/application, one might be better than another.\n\nAddition\nIt needs to be said that newer versions (2013+) of MS Excel don't open the CSV in spreadsheet format any more - one more speedbump in one's workflow making Excel less useful... At least, instructions exist for getting around it. See e.g. this Stackoverflow: How to correctly display .csv files within Excel 2013?\n.\n\n\nA24:\n\nI had a similar problem and this is the workaround that helped me without having to edit the csv file contents:\nIf you have the flexibility to name the file something other than \".csv\", you can name it with a \".txt\" extension, such as \"Myfile.txt\" or \"Myfile.csv.txt\".  Then when you open it in Excel (not by drag and drop, but using File->Open or the Most Recently Used files list), Excel will provide you with a \"Text Import Wizard\".\nIn the first page of the wizard, choose \"Delimited\" for the file type.\nIn the second page of the wizard choose \",\" as the delimiter and also choose the text qualifier if you have surrounded your values by quotes\nIn the third page, select every column individually and assign each the type \"Text\" instead of \"General\" to stop Excel from messing with your data.\nHope this helps you or someone with a similar problem!\n\n\nA25:\n\nIts not the Excel. Windows does recognize the formula, the data as a date and autocorrects. You have to change the Windows settings.\n\"Control Panel\" (-> \"Switch to Classic View\") -> \"Regional and Language\nOptions\" -> tab \"Regional Options\" -> \"Customize...\" -> tab \"Numbers\" -> And\nthen change the symbols according to what you want.\nhttp://www.pcreview.co.uk/forums/enable-disable-auto-convert-number-date-t3791902.html\nIt will work on your computer, if these settings are not changed for example on your customers' computer they will see dates instead of data.\n\n\nA26:\n\nTry MM/dd/yyyy hh:mm:ss a format.\nJava code to create XML file.\n\n\nxmlResponse.append(\"mydate>\").append(this.formatDate(resultSet.getTimestamp(\"date\"),\n    \"MM/dd/yyyy hh:mm:ss a\")).append(\"\");\n\n\npublic String formatDate(Date date, String format)\n{\n    String dateString = \"\";\n    if(null != date)\n    {\n        SimpleDateFormat dateFormat = new SimpleDateFormat(format);\n        dateString = dateFormat.format(date);\n    }\n    return dateString;\n}\n\n\n\nA27:\n\nAs for timezones.  I have to store the UTC offset as seconds from UTC that way formulas in Excel/OpenOffice can eventually localize datetimes.  I found this to be easier than storing any number that has a 0 in front of it.  -0900 didn't parse well in any spreadsheet system and importing it was nearly impossible to train people to do.\n\n\nA28:\n\n(EXCEL 2016 and later, actually I have not tried in older versions)\n\nOpen new blank page\nGo to tab \"Data\"\nClick \"From Text/CSV\" and choose your csv file\nCheck in preview whether your data is correct.\nIn \u0441ase when some column is converted to date click \"edit\" and then select type Text by clicking on calendar in head of column\nClick \"Close & Load\"\n\n\n\nA29:\n\nIf someone still looking for answer, the line below worked perfectly for me\nI entered =(\"my_value\").\ni.e. =(\"04SEP2009\") displayed as 04SEP2009 not as 09/04/2009\nThe same worked for integers more than 15 digits. They weren't getting trimmed anymore.\n\n\nA30:\n\nGiven a csv file with a datetime column in this format: yyyy-mm-dd hh:mm:ss\nExcel shows it in this format: dd/mm/yyyy hh:mm\ne.g. 2020-05-22 16:40:55 shows as 22/05/2020 16:40\nThis is evidently determined by the Short date and Short time format selected in Windows; for example, if I change the Short date format in Windows to yyyy-mm-dd, Excel shows 2020-05-22 16:40.\nAnnoyingly, I can't find any way to make Excel show the seconds automatically (I have to manually format the column in Excel). But if the csv file includes a time column in hh:mm:ss format (e.g. 16:40:55), that's what shows in Excel, including the seconds. \n\n\nA31:\n\nThe lowdown is: There is no solution. Excel 2011/Mac cannot correctly interpret a CSV file containing umlauts and diacritical marks no matter what encoding or hoop jumping you do. I'd be glad to hear someone tell me different!\n\n\nA32:\n\nFor second accuracy, yyyy-MM-dd HH:mm:ss should do the trick.\nI believe Excel is not very good with fractions of a second (loses them when interacting with COM object IIRC).\n\n\nA33:\n\nWARNING: Excel '07 (at least) has a(nother) bug: if there's a comma in the contents of a field, it doesn't parse the =\"field, contents\" correctly, but rather puts everything after the comma into the following field, regardless of the quotation marks.  \nThe only workaround I've found that works is to eliminate the = when the field contents include a comma.\nThis may mean that there are some fields that are impossible to represent exactly \"right\" in Excel, but by now I trust no-one is too surprised.\n\n\nA34:\n\nI have jus this week come across this convention, which seems to be an excellent approach, but I cannot find it referenced anywhere.  Is anyone familiar with it?  Can you cite a source for it?  I have not looked for hours and hours but am hoping someone will recognize this approach.\nExample 1: =(\"012345678905\") displays as 012345678905\nExample 2: =(\"1954-12-12\") displays as 1954-12-12, not 12/12/1954.\n\n\nA35:\n\nSo, weirdly excel imports a csv date in different ways. And, displays them differently depending on the format used in the csv file. Unfortunately the ISO 8061 format comes in as a string. Which prevents you from possibly reformatting the date yourself.\nAll the ones the do come in as a date... contain the entire information... but they format differently... if you don't like it you can choose a new format for the column in excel and it will work. (Note: you can tell it came in as a valid date/time as it will right justify... if it comes in as a string it will left justify)\nHere are formats I tested:\n\"yyyy-MM-dd\" shows up as a date of course when opened in excel. (also \"MM/dd/yyyy\" works)\n\"yyyy-MM-dd HH:mm:ss\" default display format is \"MM/dd/yyyy HH:mm\" (date and time w/out seconds)\n\"yyyy-MM-dd HH:mm:ss.fff\" default display format is \"HH:mm:ss\" (time only w/ seconds)\n\n\nA36:\n\nFor UTF-16LE with BOM if you use tab characters as your delimiters instead of commas Excel will recognise the fields. The reason it works is that Excel actually ends up using its Unicode *.txt parser.\nCaveat: If the file is edited in Excel and saved, it will be saved as tab-delimited ASCII. The problem now is that when you re-open the file Excel assumes it's real CSV (with commas), sees that it's not Unicode, so parses it as comma-delimited - and hence will make a hash of it!\nUpdate: The above caveat doesn't appear to be happening for me today in Excel 2010 (Windows) at least, although there does appear to be a difference in saving behaviour if:\n\nyou edit and quit Excel (tries to save as 'Unicode *.txt')\n\ncompared to:\n\nediting and closing just the file (works as expected).\n\n\n\nA37:\n\nHere is the simple method we use at work here when generating the csv file in the first place, it does change the values a bit so it is not suitable in all applications:\nPrepend a space to all values in the csv\nThis space will get stripped off by excel from numbers such as \" 1\",\" 2.3\" and \" -2.9e4\" but will remain on dates like \" 01/10/1993\" and booleans like \" TRUE\", stopping them being converted into excel's internal data types.\nIt also stops double quotes being zapped on read in, so a foolproof way of making text in a csv remain unchanged by excel EVEN IF is some text like \"3.1415\" is to surround it with double quotes AND prepend the whole string with a space, i.e. (using single quotes to show what you would type) ' \"3.1415\"'. Then in excel you always have the original string, except it is surrounded by double quotes and prepended by a space so you need to account for those in any formulas etc.  \n\n\nA38:\n\nWhile creating the string to be written to my CSV file in C# I had to format it this way:\n\"=\\\"\" + myVariable + \"\\\"\"\n\n\n\nA39:\n\nI wrote my timestamps to the CSV file as yyyy-mm-dd hh:mm:ss.\nIn Excel 365 I create a blank workbook, go to the Data tab and choose \"From Text/CSV.\nA dialog opens with a preview, which weirdly leaves the first column of timestamps alone, but shows it is going to convert the second column.\nYou can either \"Load\" the data, or \"Transform Data\", which is what I choose.\nNow you're in the \"Power Query Editor\" and you can massage how Excel will bring the columns in.\nIn my case I undo any automatic changes it made, and tell it both columns are Data Type \"Date/Time\" using a little button on the ribbon.\nPressing Close & Load brings it into Excel as a data source that you can sort etc.\n\n\nA40:\n\nWhat I have done for this same problem was to add the following before each csv value:\n\"=\"\"\"\nand one double quote after each CSV value, before opening the file in Excel.  Take the following values for example:\n012345,00198475\n\nThese should be altered before opening in Excel to:\n\"=\"\"\"012345\",\"=\"\"\"00198475\"\n\nAfter you do this, every cell value appears as a formula in Excel and so won't be formatted as a number, date, etc.  For example, a value of 012345 appears as:\n=\"012345\"\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( vs ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 40, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI have a table of tags and want to get the highest count tags from the list.\nSample data looks like this\nid (1) tag ('night')\nid (2) tag ('awesome')\nid (3) tag ('night')\n\nusing\nSELECT COUNT(*), `Tag` from `images-tags`\nGROUP BY `Tag`\n\ngets me back the data I'm looking for perfectly. However, I would like to organize it, so that the highest tag counts are first, and limit it to only send me the first 20 or so.\nI tried this...\nSELECT COUNT(id), `Tag` from `images-tags`\nGROUP BY `Tag`\nORDER BY COUNT(id) DESC\nLIMIT 20\n\nand I keep getting an \"Invalid use of group function - ErrNr 1111\"\nWhat am I doing wrong?\nI'm using MySQL 4.1.25-Debian\n\n\nCandidate answers:\nA1:\n\nCorrect, the solution to fixing this is to use :select and to select each field that you wish to decorate the resulting object with and group by them.\nNasty - but it is how group by should work as opposed to how MySQL works with it by guessing what you mean if you don't stick fields in your group by.\n\n\nA2:\n\nA solution is to voluntarily put an error in the query and to print the error's message:\n//Connection to the database\n$co = new PDO('mysql:dbname=myDB;host=localhost','root','');\n//We allow to print the errors whenever there is one\n$co->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n//We create our prepared statement\n$stmt = $co->prepare(\"ELECT * FROM Person WHERE age=:age\"); //I removed the 'S' of 'SELECT'\n$stmt->bindValue(':age','18',PDO::PARAM_STR);\ntry {\n    $stmt->execute();\n} catch (PDOException $e) {\n    echo $e->getMessage();\n}\n\nStandard output:\n\nSQLSTATE[42000]: Syntax error or access violation: [...] near 'ELECT * FROM Person WHERE age=18' at line 1\n\nIt is important to note that it only prints the first 80 characters of the query.\n\n\nA3:\n\nI assume you mean that you want the final SQL query, with parameter values interpolated into it.  I understand that this would be useful for debugging, but it is not the way prepared statements work.  Parameters are not combined with a prepared statement on the client-side, so PDO should never have access to the query string combined with its parameters.\nThe SQL statement is sent to the database server when you do prepare(), and the parameters are sent separately when you do execute().  MySQL's general query log does show the final SQL with values interpolated after you execute().  Below is an excerpt from my general query log.  I ran the queries from the mysql CLI, not from PDO, but the principle is the same.\n081016 16:51:28 2 Query       prepare s1 from 'select * from foo where i = ?'\n                2 Prepare     [2] select * from foo where i = ?\n081016 16:51:39 2 Query       set @a =1\n081016 16:51:47 2 Query       execute s1 using @a\n                2 Execute     [2] select * from foo where i = 1\n\nYou can also get what you want if you set the PDO attribute PDO::ATTR_EMULATE_PREPARES.  In this mode, PDO interpolate parameters into the SQL query and sends the whole query when you execute().  This is not a true prepared query.  You will circumvent the benefits of prepared queries by interpolating variables into the SQL string before execute().\n\nRe comment from @afilina:\nNo, the textual SQL query is not combined with the parameters during execution. So there's nothing for PDO to show you.\nInternally, if you use PDO::ATTR_EMULATE_PREPARES, PDO makes a copy of the SQL query and interpolates parameter values into it before doing the prepare and execute. But PDO does not expose this modified SQL query. \nThe PDOStatement object has a property $queryString, but this is set only in the constructor for the PDOStatement, and it's not updated when the query is rewritten with parameters.\nIt would be a reasonable feature request for PDO to ask them to expose the rewritten query. But even that wouldn't give you the \"complete\" query unless you use PDO::ATTR_EMULATE_PREPARES.\nThis is why I show the workaround above of using the MySQL server's general query log, because in this case even a prepared query with parameter placeholders is rewritten on the server, with parameter values backfilled into the query string. But this is only done during logging, not during query execution.\n\n\nA4:\n\nThis is actually how your query works and is a normal behaviour. Using LIMIT you will not limit the count or sum but only the returned rows. So your query will return n rows as stated in your LIMIT clause. And since your query actually returns only one row, applying a (non-zero) limit has no effect on the results.\nHowever, your second query will work as expected and is an established way of solving this problem.\n\n\nA5:\n\nI just tried to do this is MS SQL Server where we do not have FIELD():\nSELECT table1.id\n... \nINNER JOIN\n    (VALUES (10,1),(3,2),(4,3),(5,4),(7,5),(8,6),(9,7),(2,8),(6,9),(5,10)\n    ) AS X(id,sortorder)\n        ON X.id = table1.id\n    ORDER BY X.sortorder\n\nNote that I am allowing duplication too.\n\n\nA6:\n\nAdded a little bit more to the code by Mike - walk the values to add single quotes \n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_array($value))\n            $values[$key] = implode(',', $value);\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n    // Walk the array to see if we can add single-quotes to strings\n    array_walk($values, create_function('&$v, $k', 'if (!is_numeric($v) && $v!=\"NULL\") $v = \"\\'\".$v.\"\\'\";'));\n\n    $query = preg_replace($keys, $values, $query, 1, $count);\n\n    return $query;\n}\n\n\n\nA7:\n\nIf I remember correctly, in PostgreSQL you have to add every column you fetch from the table where the GROUP BY clause applies to the GROUP BY clause.\n\n\nA8:\n\nI think you should manage to store your data in a way that you will simply do a join and it will be perfect, so no hacks and complicated things going on.\nI have for instance a \"Recently played\" list of track ids, on SQLite i simply do:\nSELECT * FROM recently NATURAL JOIN tracks;\n\n\n\nA9:\n\nFor others looking for a way to order by any field, including joined field, in postgresql, use a subquery:\nSELECT * FROM(\nSELECT DISTINCT ON(availables.bookdate) `availables`.* \nFROM `availables` INNER JOIN `rooms` ON `rooms`.id = `availables`.room_id \nWHERE (rooms.hotel_id = 5056 \nAND availables.bookdate BETWEEN '2009-11-22' AND '2009-11-24')\n) AS distinct_selected\nORDER BY availables.updated_at\n\nor arel:\n\nsubquery = SomeRecord.select(\"distinct on(xx.id) xx.*, jointable.order_field\")\n      .where(\"\").joins(\")\nresult = SomeRecord.select(\"*\").from(\"(#{subquery.to_sql}) AS distinct_selected\").order(\" xx.order_field ASC, jointable.order_field ASC\")\n\n\n\nA10:\n\nPostgreSQL is more SQL compliant than MySQL. All fields - except computed field with aggregation function - in the output must be present in the GROUP BY clause.\n\n\nA11:\n\nFirst, the error you're getting is due to where you're using the COUNT function -- you can't use an aggregate (or group) function in the WHERE clause.\nSecond, instead of using a subquery, simply join the table to itself:\nSELECT a.pid \nFROM Catalog as a LEFT JOIN Catalog as b USING( pid )\nWHERE a.sid != b.sid\nGROUP BY a.pid\n\nWhich I believe should return only rows where at least two rows exist with the same pid but there is are at least 2 sids.  To make sure you get back only one row per pid I've applied a grouping clause.\n\n\nA12:\n\nA bit late probably but now there is PDOStatement::debugDumpParams\n\nDumps the informations contained by a prepared statement directly on\n  the output. It will provide the SQL query in use, the number of\n  parameters used (Params), the list of parameters, with their name,\n  type (paramtype) as an integer, their key name or position, and the\n  position in the query (if this is supported by the PDO driver,\n  otherwise, it will be -1).\n\nYou can find more on the official php docs\nExample:\n<?php\n/* Execute a prepared statement by binding PHP variables */\n$calories = 150;\n$colour = 'red';\n$sth = $dbh->prepare('SELECT name, colour, calories\n    FROM fruit\n    WHERE calories < :calories AND colour = :colour');\n$sth->bindParam(':calories', $calories, PDO::PARAM_INT);\n$sth->bindValue(':colour', $colour, PDO::PARAM_STR, 12);\n$sth->execute();\n\n$sth->debugDumpParams();\n\n?>\n\n\n\nA13:\n\nMy first thought was to write a single query, but you said that was not possible because one is run by the user and the other is run in the background.  How are you storing the list of ids to pass from the user to the background process?  Why not put them in a temporary table with a column to signify the order.\nSo how about this:\n\nThe user interface bit runs and inserts values into a new table you create.  It would insert the id, position and some sort of job number identifier)\nThe job number is passed to the background process (instead of all the ids)\nThe background process does a select from the table in step 1 and you join in to get the other information that you require.  It uses the job number in the WHERE clause and orders by the position column.\nThe background process, when finished, deletes from the table based on the job identifier.\n\n\n\nA14:\n\nYou need to use HAVING, not WHERE.\nThe difference is: the WHERE clause filters which rows MySQL selects. Then MySQL groups the rows together and aggregates the numbers for your COUNT function.\nHAVING is like WHERE, only it happens after the COUNT value has been computed, so it'll work as you expect. Rewrite your subquery as:\n(                  -- where that pid is in the set:\nSELECT c2.pid                  -- of pids\nFROM Catalog AS c2             -- from catalog\nWHERE c2.pid = c1.pid\nHAVING COUNT(c2.sid) >= 2)\n\n\n\nA15:\n\nThe classic way would be to add commas to the left and right:\nselect * from shirts where CONCAT(',', colors, ',') like '%,1,%'\n\nBut find_in_set also works:\nselect * from shirts where find_in_set('1',colors) <> 0\n\n\n\nA16:\n\n1. For MySQL:\nSELECT FIND_IN_SET(5, columnname) AS result \nFROM table\n\n2.For Postgres SQL :\nSELECT * \nFROM TABLENAME f\nWHERE 'searchvalue' = ANY (string_to_array(COLUMNNAME, ','))\n\nExample\nselect * \nfrom customer f\nwhere '11' = ANY (string_to_array(customerids, ','))\n\n\n\nA17:\n\nI think that .uniq [1] will solve your problem. \n[1] Available.select('...').uniq\n\nTake a look at http://guides.rubyonrails.org/active_record_querying.html#selecting-specific-fields\n\n\nA18:\n\nI spent a good deal of time researching this situation for my own needs. This and several other SO threads helped me a great deal, so I wanted to share what I came up with.\nWhile having access to the interpolated query string is a significant benefit while troubleshooting, we wanted to be able to maintain a log of only certain queries (therefore, using the database logs for this purpose was not ideal). We also wanted to be able to use the logs to recreate the condition of the tables at any given time, therefore, we needed to make certain the interpolated strings were escaped properly. Finally, we wanted to extend this functionality to our entire code base having to re-write as little of it as possible (deadlines, marketing, and such; you know how it is).\nMy solution was to extend the functionality of the default PDOStatement object to cache the parameterized values (or references), and when the statement is executed, use the functionality of the PDO object to properly escape the parameters when they are injected back in to the query string. We could then tie in to execute method of the statement object and log the actual query that was executed at that time (or at least as faithful of a reproduction as possible).\nAs I said, we didn't want to modify the entire code base to add this functionality, so we overwrite the default bindParam() and bindValue() methods of the PDOStatement object, do our caching of the bound data, then call parent::bindParam() or parent::bindValue(). This allowed our existing code base to continue to function as normal.\nFinally, when the execute() method is called, we perform our interpolation and provide the resultant string as a new property E_PDOStatement->fullQuery. This can be output to view the query or, for example, written to a log file.\nThe extension, along with installation and configuration instructions, are available on github:\nhttps://github.com/noahheck/E_PDOStatement\nDISCLAIMER:\nObviously, as I mentioned, I wrote this extension. Because it was developed with help from many threads here, I wanted to post my solution here in case anyone else comes across these threads, just as I did.\n\n\nA19:\n\nUse MySQL FIND_IN_SET function:\n  SELECT * \n    FROM table_name \n   WHERE id IN (..,..,..,..) \nORDER BY FIND_IN_SET (coloumn_name, .., .., ..);\n\n\n\nA20:\n\nFIND_IN_SET is your friend in this case\nselect * from shirts where FIND_IN_SET(1,colors) \n\n\n\nA21:\n\nIf you're using MySQL, there is a method REGEXP that you can use...\nhttp://dev.mysql.com/doc/refman/5.1/en/regexp.html#operator_regexp\nSo then you would use:\nSELECT * FROM `shirts` WHERE `colors` REGEXP '\\b1\\b'\n\n\n\nA22:\n\nI don't know about MySQL, but in MS SQL, you can use the column index in the order by clause.  I've done this before when doing counts with group bys as it tends to be easier to work with.\nSo\nSELECT COUNT(id), `Tag` from `images-tags`\nGROUP BY `Tag`\nORDER BY COUNT(id) DESC\nLIMIT 20\n\nBecomes\nSELECT COUNT(id), `Tag` from `images-tags`\nGROUP BY `Tag`\nORDER 1 DESC\nLIMIT 20\n\n\n\nA23:\n\nTake a look at the FIND_IN_SET function for MySQL.\nSELECT * \n    FROM shirts \n    WHERE FIND_IN_SET('1',colors) > 0\n\n\n\nA24:\n\nGive this a shot:\nSELECT name, description, ...\nWHERE id IN\n    (SELECT id FROM table1 WHERE...)\nORDER BY\n    (SELECT display_order FROM table1 WHERE...),\n    (SELECT name FROM table1 WHERE...)\n\nThe WHEREs will probably take a little tweaking to get the correlated subqueries working properly, but the basic principle should be sound.\n\n\nA25:\n\nThis will work for sure, and I actually tried it out:\nlwdba@localhost (DB test) :: DROP TABLE IF EXISTS shirts;\nQuery OK, 0 rows affected (0.08 sec)\n\nlwdba@localhost (DB test) :: CREATE TABLE shirts\n    -> (<BR>\n    -> id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    -> ticketnumber INT,\n    -> colors VARCHAR(30)\n    -> );<BR>\nQuery OK, 0 rows affected (0.19 sec)\n\nlwdba@localhost (DB test) :: INSERT INTO shirts (ticketnumber,colors) VALUES\n    -> (32423,'1,2,5,12,15'),\n    -> (32424,'1,5,12,15,30'),\n    -> (32425,'2,5,11,15,28'),\n    -> (32426,'1,2,7,12,15'),\n    -> (32427,'2,4,8,12,15');\nQuery OK, 5 rows affected (0.06 sec)\nRecords: 5  Duplicates: 0  Warnings: 0\n\nlwdba@localhost (DB test) :: SELECT * FROM shirts WHERE LOCATE(CONCAT(',', 1 ,','),CONCAT(',',colors,',')) > 0;\n+----+--------------+--------------+\n| id | ticketnumber | colors       |\n+----+--------------+--------------+\n|  1 |        32423 | 1,2,5,12,15  |\n|  2 |        32424 | 1,5,12,15,30 |\n|  4 |        32426 | 1,2,7,12,15  |\n+----+--------------+--------------+\n3 rows in set (0.00 sec)\n\nGive it a Try !!!\n\n\nA26:\n\nThe IN clause describes a set of values, and sets do not have order.\nYour solution with a join and then ordering on the display_order column is the most nearly correct solution; anything else is probably a DBMS-specific hack (or is doing some stuff with the OLAP functions in standard SQL).  Certainly, the join is the most nearly portable solution (though generating the data with the display_order values may be problematic).  Note that you may need to select the ordering columns; that used to be a requirement in standard SQL, though I believe it was relaxed as a rule a while ago (maybe as long ago as SQL-92).\n\n\nA27:\n\nIf you don't have an aggregate function in your where clause, another possible source of the 1111 - Invalid use of group function error is if you have nested aggregate functions:\nselect sum(avg(close)) from prices;\n(1111, 'Invalid use of group function')\n\nYou can get around this by breaking up the problem into two steps:\n\nSave the inner aggregation into a variable\n\nselect @avg:=avg(close) from prices;\n\n\nRun the outer aggregation against the variable\n\nselect sum(@avg) from prices;\n\n\n\nA28:\n\nFor Oracle, John's solution using instr() function works. Here's slightly different solution that worked - \nSELECT id \nFROM table1 \nWHERE id IN (1, 20, 45, 60) \nORDER BY instr('1, 20, 45, 60', id)\n\n\nA29:\n\nAns to get sorted data.\nSELECT ...\nFROM ...\nORDER  BY FIELD(user_id,5,3,2,...,50)  LIMIT 10\n\n\n\nA30:\n\nMySQL prior to version 5 did not allow aggregate functions in ORDER BY clauses.\nYou can get around this limit with the deprecated syntax:\nSELECT COUNT(id), `Tag` from `images-tags`\nGROUP BY `Tag`\nORDER BY 1 DESC\nLIMIT 20\n\n1, since it's the first column you want to group on.\n\n\nA31:\n\nNot the prettiest solution, but changing the group parameter to output every column in model works in PostgreSQL:\nexpiration = Available.find(:all,\n:joins => [ :room ],\n:conditions => [ \"rooms.hotel_id = ? AND availables.bookdate BETWEEN ? AND ?\", hostel_id, date.to_s, (date+days-1).to_s ],\n:group => Available.column_names.collect{|col| \"availables.#{col}\"},\n:order => 'availables.updated_at')\n\n\n\nA32:\n\nMySQL's GROUP BY can be used without an aggregate function (which is contrary to the SQL standard), and returns the first row in the group (I don't know based on what criteria), while PostgreSQL must have an aggregate function (MAX, SUM, etc) on the column, on which the GROUP BY clause is issued.\n\n\nA33:\n\nIf the set of colors is more or less fixed, the most efficient and also most readable way would be to use string constants in your app and then use MySQL's SET type with FIND_IN_SET('red',colors) in your queries. When using the SET type with FIND_IN_SET, MySQL uses one integer to store all values and uses binary \"and\" operation to check for presence of values which is way more efficient than scanning a comma-separated string.\nIn SET('red','blue','green'), 'red' would be stored internally as 1, 'blue' would be stored internally as 2 and 'green' would be stored internally as 4. The value 'red,blue' would be stored as 3 (1|2) and 'red,green' as 5 (1|4).\n\n\nA34:\n\nYou should actually fix your database schema so that you have three tables:\nshirt: shirt_id, shirt_name\ncolor: color_id, color_name\nshirtcolor: shirt_id, color_id\n\nThen if you want to find all of the shirts that are red, you'd do a query like:\nSELECT *\nFROM shirt, color\nWHERE color.color_name = 'red'\n  AND shirt.shirt_id = shirtcolor.shirt_id\n  AND color.color_id = shirtcolor.color_id\n\n\n\nA35:\n\nI modified the method to include handling output of arrays for statements like WHERE IN (?). \nUPDATE: Just added check for NULL value and duplicated $params so actual $param values are not modified. \nGreat work bigwebguy and thanks!\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_string($value))\n            $values[$key] = \"'\" . $value . \"'\";\n\n        if (is_array($value))\n            $values[$key] = \"'\" . implode(\"','\", $value) . \"'\";\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n\n    $query = preg_replace($keys, $values, $query);\n\n    return $query;\n}\n\n\n\nA36:\n\nTwo solutions that spring to mind:\n\norder by case id when 123 then 1 when 456 then 2 else null end asc\norder by instr(','||id||',',',123,456,') asc\n\n(instr() is from Oracle; maybe you have locate() or charindex() or something like that)\n\n\nA37:\n\nIn all versions of MySQL, simply alias the aggregate in the SELECT list, and order by the alias:\nSELECT COUNT(id) AS theCount, `Tag` from `images-tags`\nGROUP BY `Tag`\nORDER BY theCount DESC\nLIMIT 20\n\n\n\nA38:\n\nIf you want to do arbitrary sorting on a query using values inputted by the query in MS SQL Server 2008+, it can be done by creating a table on the fly and doing a join like so (using nomenclature from OP).\nSELECT table1.name, table1.description ... \nFROM (VALUES (id1,1), (id2,2), (id3,3) ...) AS orderTbl(orderKey, orderIdx) \nLEFT JOIN table1 ON orderTbl.orderKey=table1.id\nORDER BY orderTbl.orderIdx\n\nIf you replace the VALUES statement with something else that does the same thing, but in ANSI SQL, then this should work on any SQL database.\nNote: \nThe second column in the created table (orderTbl.orderIdx) is necessary when querying record sets larger than 100 or so. I originally didn't have an orderIdx column, but found that with result sets larger than 100 I had to explicitly sort by that column; in SQL Server Express 2014 anyways.\n\n\nA39:\n\nSee following how to get sorted data.\nSELECT ...\n  FROM ...\n WHERE zip IN (91709,92886,92807,...,91356)\n   AND user.status=1\nORDER \n    BY provider.package_id DESC \n     , FIELD(zip,91709,92886,92807,...,91356)\nLIMIT 10\n\n\n\nA40:\n\nUse MySQL's FIELD() function:\nSELECT name, description, ...\nFROM ...\nWHERE id IN([ids, any order])\nORDER BY FIELD(id, [ids in order])\n\nFIELD() will return the index of the first parameter that is equal to the first parameter (other than the first parameter itself).\nFIELD('a', 'a', 'b', 'c')\nwill return 1\nFIELD('a', 'c', 'b', 'a')\nwill return 3\nThis will do exactly what you want if you paste the ids into the IN() clause and the FIELD() function in the same order.\n\n\nA41:\n\nWhat kind of field is this? The IN operator cannot be used with a single field, but is meant to be used in subqueries or with predefined lists:\n-- subquery\nSELECT a FROM x WHERE x.b NOT IN (SELECT b FROM y);\n-- predefined list\nSELECT a FROM x WHERE x.b NOT IN (1, 2, 3, 6);\n\nIf you are searching a string, go for the LIKE operator (but this will be slow):\n-- Finds all rows where a does not contain \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE '%text%';\n\nIf you restrict it so that the string you are searching for has to start with the given string, it can use indices (if there is an index on that field) and be reasonably fast:\n-- Finds all rows where a does not start with \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE 'text%';\n\n\n\nA42:\n\nAlternative method:\nSELECT * FROM x WHERE CHARINDEX('$x', field1, 0)<1 \n\nExplanation: It searches for occurrences of '$x' starting from position 0. And only selects where count is 'less than one'.\n\n\nA43:\n\nselect * from shirts where find_in_set('1',colors) <> 0\n\nWorks for me\n\n\nA44:\n\nAccording to MySQL's \"Debuking GROUP BY Myths\" http://dev.mysql.com/tech-resources/articles/debunking-group-by-myths.html. SQL (2003 version of the standard) doesn't requires columns referenced in the SELECT list of a query to also appear in the GROUP BY clause.\n\n\nA45:\n\nMySQL's totally non standards compliant GROUP BY can be emulated by Postgres' DISTINCT ON. Consider this:\nMySQL:\nSELECT a,b,c,d,e FROM table GROUP BY a\n\nThis delivers 1 row per value of a (which one, you don't really know). Well actually you can guess, because MySQL doesn't know about hash aggregates, so it will probably use a sort... but it will only sort on a, so the order of the rows could be random. Unless it uses a multicolumn index instead of sorting. Well, anyway, it's not specified by the query.\nPostgres:\nSELECT DISTINCT ON (a) a,b,c,d,e FROM table ORDER BY a,b,c\n\nThis delivers 1 row per value of a, this row will be the first one in the sort according to the ORDER BY specified by the query. Simple.\nNote that here, it's not an aggregate I'm computing. So GROUP BY actually makes no sense. DISTINCT ON makes a lot more sense.\nRails is married to MySQL, so I'm not surprised that it generates SQL that doesn't work in Postgres.\n\n\nA46:\n\nSELECT ORDER_NO, DELIVERY_ADDRESS \nfrom IFSAPP.PURCHASE_ORDER_TAB \nwhere ORDER_NO in ('52000077','52000079','52000167','52000297','52000204','52000409','52000126') \nORDER BY instr('52000077,52000079,52000167,52000297,52000204,52000409,52000126',ORDER_NO)\n\nworked really great\n\n\nA47:\n\nPDOStatement has a public property $queryString. It should be what you want.\nI've just notice that PDOStatement has an undocumented method debugDumpParams() which you may also want to look at.\n\n\nA48:\n\nYou can achieve this by following function.\nRun following query to create function.\nDELIMITER ||\nCREATE FUNCTION `TOTAL_OCCURANCE`(`commastring` TEXT, `findme`     VARCHAR(255)) RETURNS int(11)\nNO SQL\n-- SANI: First param is for comma separated string and 2nd for string to find.\nreturn ROUND (   \n    (\n        LENGTH(commastring)\n        - LENGTH( REPLACE ( commastring, findme, \"\") ) \n    ) / LENGTH(findme)        \n);\n\nAnd call this function like this\nmsyql> select TOTAL_OCCURANCE('A,B,C,A,D,X,B,AB', 'A');\n\n\n\nA49:\n\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic static function interpolateQuery($query, $params) {\n    $keys = array();\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n    }\n\n    $query = preg_replace($keys, $params, $query, 1, $count);\n\n    #trigger_error('replaced '.$count.' keys');\n\n    return $query;\n}\n\n\n\nA50:\n\nSELECT * FROM table WHERE field1 NOT LIKE '%$x%';   (Make sure you escape $x properly beforehand to avoid SQL injection)\nEdit: NOT IN does something a bit different - your question isn't totally clear so pick which one to use.   LIKE 'xxx%' can use an index.  LIKE '%xxx' or LIKE '%xxx%' can't.\n\n\nA51:\n\nIn Oracle, something like this works nicely to separate your counting and ordering a little better.  I'm not sure if it will work in MySql 4.\nselect 'Tag', counts.cnt\nfrom\n  (\n  select count(*) as cnt, 'Tag'\n  from 'images-tags'\n  group by 'tag'\n  ) counts\norder by counts.cnt desc\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 41, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI have been developing an app for 1 or 2 weeks now and just yesterday I have updated my iPhone 5S to the iOS 8 GM. Everything worked fine and I could test on my device as well until I deleted the app from my phone and wanted to build again. The following error appeared:\nCould not launch \"My App\"\nprocess launch failed: Security\n\n\nWhen I test with the simulator it works fine.\nIs this because of the iOS 8 GM update and how can I fix this launch problem?\nI want to be able to test on my iPhone and in the simulator.\n\n\nCandidate answers:\nA1:\n\nThe answer from Alistra work for me but I doesn't want to change a script which is not mine (a future Xcode release might change this file and the correction will be lost).\n\n diff PackageApplication PackageApplicationFixed 155,157c155,156\n<-     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements,resource-rules\",\n<-                          \"--sign\", $opt{sign},\n<-                          \"--resource-rules=$destApp/ResourceRules.plist\");\n---\n->     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements\",\n->                          \"--sign\", $opt{sign});\n\n\nI think answer from Vladimir Grigorov is the best if you have an archive using :\n\nxcodebuild -exportArchive -archivePath [path to archive] -exportPath [path to output directory] -exportOptionsPlist [path to options.plist file]\n\n\nIn my case, I doesn't have the archive, because I modify the application after build it and I need to change the Bundle Id and signing certificate.\nThe solution I found is to call codesign myself before used PackageApplication and ask PackageApplication to not sign. Like this :\nreplace :\n\n /usr/bin/xcrun -sdk iphoneos PackageApplication -v \"<app_path>\" -o \"<ipa_path>\" --sign \"<provisioning_profile.certificateSubject>\" --embed \"<provisioning_profile.path>\"\n\nby :\n\n/bin/cp -rpfv \"<provisioning_profile.path>\" \"<app_path>/embedded.mobileprovision\"\n/usr/bin/codesign -v -vvvv -f -s \"<provisioning_profile.certificateSubject>\" --entitlements=\"<entitlement_path>\" \"<app_path>\"\n/usr/bin/xcrun -sdk iphoneos PackageApplication -v \"<app_path>\" -o \"<ipa_path>\"\n\nDon't forget to embedded the .mobileprovision file using to sign with cp.\n\n\nA2:\n\nThe answer by Tim Gostony no longer works since the release of Xcode 7. Now the App Store submission process fails when resource rules are present. The solution is to clear your Code Signing Resource Rules Path and replace xcrun with xcodebuild tool:\nxcodebuild -exportArchive -archivePath [path to archive] -exportPath [path to output directory] -exportOptionsPlist [path to options.plist file]\n\nThe simplest Options.plist for exporting ad-hoc distribution ipa files looks like this:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>iCloudContainerEnvironment</key>\n    <string>Production</string>\n    <key>teamID</key>\n    <string>[YOUR TEAM ID]</string>\n    <key>method</key>\n    <string>ad-hoc</string>\n</dict>\n</plist>\n\nThere are other options available for this plist file regarding bitcode, app thinning, etc. That's why I think xcodebuild tool is the right tool for exporting ipa files for iOS 9 and above. \nMore details about options plist are available with xcodebuild -help command.\n\n\nA3:\n\nBitcode is deprecated from XCode 14 Beta:\nhttps://developer.apple.com/documentation/xcode-release-notes/xcode-14-release-notes\n\nStarting with Xcode 14, bitcode is no longer required for watchOS and tvOS applications, and the App Store no longer accepts bitcode submissions from Xcode 14.\nXcode no longer builds bitcode by default and generates a warning message if a project explicitly enables bitcode: \u201cBuilding with bitcode is deprecated. Please update your project and/or target settings to disable bitcode.\u201d The capability to build with bitcode will be removed in a future Xcode release. IPAs that contain bitcode will have the bitcode stripped before being submitted to the App Store. Debug symbols for past bitcode submissions remain available for download. (86118779)\n\nTo anyone wondering about bitcode, it seems that it won't be necessary anymore.\n\n\nA4:\n\nI wish I knew why it works, but here's a fix that worked for me:\n\nFound the fix !\nClick on your project > Targets > Select your target > Build Settings\n  >\nCode Signing Resource Rules Path\nand add :\n$(SDKROOT)/ResourceRules.plist\n\n\n\nA5:\n\nBe careful if you skip installs.\nI accidentally changed Skip Install on the main target, when i meant to change it on a dependency .\nWhen changing it back I had to do a clean.\n\n\nA6:\n\nBitcode makes crash reporting harder. Here is a quote from HockeyApp (which also true for any other crash reporting solutions):\n\nWhen uploading an app to the App Store and leaving the \"Bitcode\" checkbox enabled, Apple will use that Bitcode build and re-compile it on their end before distributing it to devices. This will result in the binary getting a new UUID and there is an option to download a corresponding dSYM through Xcode.\n\nNote: the answer was edited on Jan 2016 to reflect most recent changes\n\n\nA7:\n\nThis problem probably started after we enabled background mode in our app.My solution :\n1) Go to simulator\n2) Hold cmd + shift\n3) Tap 'H' key two times\n4) Slide the application off to stop it running in the background.\n5) Run again. \n(Very tiring to have to repeat the above step each time I build and run)\n\n\nA8:\n\nXcode pre-v11. Embedded Binaries vs Linked Frameworks and Libraries\n\nHistory\nEmbedded Binaries vs Linked Frameworks and Libraries -> Frameworks, Libraries, and Embedded Content\n\n[Xcode v11. Frameworks, Libraries, and Embedded Content] replaced it from Xcode v11 section in General tab\nembedded binaries and Linked Frameworks are a part of Dependency management [About]\n[Xcode v11]\nLink Binary\nGeneral -> Linked Frameworks and Libraries is a mirror of Build Phases -> Link Binary With Libraries.\nStatic Library and Framework\nIf you add a Static Library or Static Framework to this section it will appear at Frameworks group[About](Project Navigator -> <workspace/project> -> Frameworks) and there will be a reference added to your project for it. Then it will be used by Static Linker. Static Linker at compile time will include/copy all code from the library into the executable object file. Static linker works in pair with Build Settings -> <Library/Framework> Search Paths\n\nStatic Library\n\nBuild Settings -> Library Search Paths[library not found] If you do not add a static library to this section you will get a linker error[ld: symbol(s) not found]\n\nStatic Framework\n\nBuild Settings -> Framework Search Paths. If you do not add a static framework to this section you will get a compile error[No such module]\n\nEmbed binary\nStatic Library and Static Framework\nEmbedding wouldn\u2019t make any sense for a Static Library and Static Framework because the symbols from them are compiled into the executable binary. Xcode won\u2019t let you drop a static library under the Embed section.\nDynamic Framework\nGeneral -> Embedded Binaries is a mirror of Build Phases -> Embed Frameworks.\nEmbedding actually adds a copy of the framework into your application bundle(not merging framework's and application's code into single executable binary)\nBy default the bundle's folder is Frameworks but you can change it using Destination field. Moreover you can specify a Subpath.\n\nDynamic linker :dyld at load or run time will try to find the embedded framework using @rpath[About] If it is not found the error will occur [dyld: Library not loaded]\n\nResult:\n\nStatic Library - Link\nStatic Framework - Link\nDynamic Framework - Embed\n\n[Static vs Dynamic linker]\n[When use Link and Embed]\n[Vocabulary]\n\n\nA9:\n\nIn Xcode 6 and above, you can find and delete the simulators from the path /Library/Developer/CoreSimulator/Profiles/Runtimes. Restart Xcode in order to take effect (may not be needed).\n\n\nA10:\n\nThe following patch for PackageApplications fixed it for me, I removed resource-rules as it says it's deprecated on 10.10.\nTestflight builds work without it. Appstore builds too.\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin\n % diff PackageApplication PackageApplicationFixed \n155,157c155,156\n<     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements,resource-rules\",\n<                          \"--sign\", $opt{sign},\n<                          \"--resource-rules=$destApp/ResourceRules.plist\");\n---\n>     my @codesign_args = (\"/usr/bin/codesign\", \"--force\", \"--preserve-metadata=identifier,entitlements\",\n>                          \"--sign\", $opt{sign});\n\n\n\nA11:\n\nIn XCode open Window - Devices, then select and remove the outdated simulators.\n\n\nA12:\n\nIn short,\n\n\nsystem libraries, link them; \n3rd party libraries, embed them.\n\n\nwhy?\n\n\nif you try to embed system libraries, you won't find them in the popup list;\nif you link 3rd party libraries, you'll probably get a crash.\n\n\n\n\nA13:\n\nfollowing some of the answers here, deleting some simulators from my Xcode Menu > Window > Devices > Simulators did nothing to help my dying disk space:\n\nhowever, cd ~/Library/Developer/Xcode/iOS\\ DeviceSupport and running du -sh * I got all of these guys:\n2.9G    10.0.1 (14A403)\n1.3G    10.1.1 (14B100)\n2.9G    10.3.2 (14F89)\n1.3G    10.3.3 (14G60)\n1.9G    11.0.1 (15A402)\n1.9G    11.0.3 (15A432)\n2.0G    11.1.2 (15B202)\n2.0G    11.2 (15C114)\n2.0G    11.2.1 (15C153)\n2.0G    11.2.2 (15C202)\n2.0G    11.2.6 (15D100)\n2.0G    11.4 (15F79)\n2.0G    11.4.1 (15G77)\n2.3G    12.0 (16A366)\n2.3G    12.0.1 (16A404)\n2.3G    12.1 (16B92)\n\nAll together that's 33 GB! \nA blood bath ensued\nsee more details here\n\n\nA14:\n\nBitcode is a new feature of iOS 9\n\nBitcode is an intermediate representation of a compiled program. Apps you upload to iTunes Connect that contain bitcode will be compiled and linked on the App Store. Including bitcode will allow Apple to re-optimize your app binary in the future without the need to submit a new version of your app to the store.\nNote: For iOS apps, bitcode is the default, but optional. If you provide bitcode, all apps and frameworks in the app bundle need to include bitcode. For watchOS apps, bitcode is required\n\nSo you should disabled bitcode until all the frameworks of your app have bitcode enabled.\n\n\nA15:\n\nAs mentioned in the Xcode release notes, this error (along with a couple others) results from a timing bug.  The workaround is to just retry.\nSee https://developer.apple.com/library/prerelease/mac/releasenotes/DeveloperTools/RN-Xcode/Chapters/xc6_release_notes.html#//apple_ref/doc/uid/TP40001051-CH4-SW1\n\nTesting on iOS simulator may produce an error indicating that the application could not be installed or launched.\n  Re-run testing or start another integration.\n\nAlso see:\nhttps://devforums.apple.com/thread/248879?tstart=0\n\n\nA16:\n\nThe question you linked references the \"Link Binary With Libraries\" functionality, which is somewhat different than an embedded binary.\n\"Link Binary With Libraries\" means what you'd expect it to with respect to linkage: Regardless of whether the binary is a static library, dynamic library, or framework it will be linked to your object code at link time after compilation.\nWhen you think of linkage with a static library, what happens is pretty clear:  the linker copies the code from the library (e.g. libFoo.a) into your output binary.  Your output file grows in size but doesn't need to resolve any external dependencies at runtime.  Everything your program needs to run (with respect to the static library) is present after it is built.\nWith a dynamic library (.dylib, or system-supplied framework), the expectation is that the library you are linking against will be present somewhere in the system's dynamic-library loader path when you run your program.  This way you don't have the overhead of copying all the third party external libraries into your binary, and all the different programs on a computer that also link to that library will be able to find it, which saves minimally disk space, but also potentially memory space, depending on how and where the system caches libraries.\nA framework is much like a dynamic library, but can contain resources in its directory structure (images, audio, other frameworks, etc.).  In this case a simple static-library or .dylib file won't cut it so you might have to link to a framework just so it can find what it needs to run properly.\nWhen you link to a third-party framework (say something you downloaded from github and built yourself), it might not be present on the system you intend to run on.  In this case, you'd not only link to the framework, but embed it inside your application bundle as well using the \"Copy Frameworks\" phase.  When your program runs, the runtime-linker (aka the resolver) will look inside your bundle in addition to the system loader path, find the embedded framework, and link it so your app will have the code it needs in order to run.\nFinally, what is properly an \"embedded binary\" is an executable you both embed in your application bundle via a Copy-Files Phase, and that you execute yourself, perhaps with a call to popen() or similar.  The embedded binary may be called by your program, but it isn't linked with it.  It is a fully external entity (like programs in the /bin directory).\nIn practice, for system-supplied libraries and frameworks you will link against them and that's all you need to do.\nIf you need to link a library you built that doesn't need any embedded resources (i.e. doesn't require a framework to exist), then you can just link against a static library.  If you find you have multiple modules in your program that want to use the same library code, then converting it to a framework or dynamic library and linking against that can save space and may be convenient (particularly if memory usage is a concern).\nFinally, frameworks can include not only resources, but header and/or license files.  Using a framework to convey these files is actually a convenient distribution mechanism so often you may want to incorporate a framework just so these things can tag along with your binary (i.e. license requirements may make this mandatory).\n--- EDIT ---\nAdam Johns posted the following question as a comment:\n\nThis is a great answer. There is something I'm still a little confused on, however. What does it mean to execute the binary yourself? Do you mean simply using the embedded framework's code? I know you mentioned popen(), but you're saying my app is calling popen()? I don't really know what that means.\n\nI'm saying an embedded binary is just another resource file in your bundle, like an audio file or image, although the file is instead an executable command-line tool. The popen() function (man popen from your terminal to read more about it) lets you execute arbitrary programs from another running program. The system() function is another way. There are others, and I'll give a historical example here that may make understanding use of an embedded binary a bit more clear:\nAs you're probably aware, when you launch an app on Mac OS X it is launched with a user id of the current user. Under most common installations that's the default user-at-the-Desktop admin user, who is given user id 501.\nOn Unix-based operating systems only the root user (user id 0) has full access to the entire filesystem. Sometimes it happens that an installer program launched by the Desktop user needs to install files in a privileged directory (drivers for example). In this case, the application program needs to escalate its privileges to the root user so it can write in these restricted directories.\nTo facilitate this in operating systems through OS X 10.7, Apple provided in its Authorization Services API the function AuthorizationExecuteWithPrivileges() (this is now deprecated, but is still a useful example).\nAuthorizationExecuteWithPrivileges() took as an argument a path to a command-line tool to execute as root. The command line tool was an executable shell script or compiled binary that you wrote to run your install logic. This tool was installed inside your application bundle just like any other resource file.\nWhen called, the OS put up an authorization dialog asking for the user's password (you've seen this before!) and when entered would execute the program as root on your app's behalf. This process is similar to just executing a program with popen() yourself, although popen() alone doesn't give you the benefit of privilege escalation.\n\n\nA17:\n\nNo need to quit the simulator or reset all content every time .\nJust close the app on which you are working if running on background .\nit works for me .\n\n\nA18:\n\nXcode 4.6 will prompt you to reinstall any older versions of the iOS Simulator if you just delete the SDK. To avoid that, you must also delete the Xcode cache. Then you won't be forced to reinstall the older SDK on launch.\nTo remove the iOS 5.0 simulator, delete these and then restart Xcode:\n\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/PhoneSimulator5.0.sdk\n~/Library/Caches/com.apple.dt.Xcode\n\nFor example, after doing a clean install of Xcode, I installed the iOS 5.0 simulator from Xcode preferences. Later, I decided that 5.1 was enough but couldn't remove the 5.0 version. Xcode kept forcing me to reinstall it on launch. After removing both the cache file and the SDK, it no longer asked.\n\n\nA19:\n\nIf you get this, the app has installed on your device. You have to tap the icon. It will ask you if you really want to run it. Say \u201cyes\u201d and then Build & Run again.\nAs from iOS 9, it is required to go to Settings \u2192 General \u2192 Device Management \u2192 Developer App \u2192 Trust`.\nOn some versions of iOS, you will have to go to Settings \u2192 General \u2192 Profile instead.\n\n\nA20:\n\nJust closing the iOS Simulator worked for me.\nThere is not always the need to reset the iOS Simulator.  I did not even had to close Xcode.\n\n\nA21:\n\nThis happened to me once when I added some assets as symbolic links (through ln -sf). Even installing to devices was failing.\nAfter I removed the links and made a hard copy, it worked and error was gone.\n\n\nA22:\n\nDisconnect hardware keyboard.\nIn ios simulator main menu\nHardware -> Keyboard -> Disconnect Hardware keyboard.\n\n\nA23:\n\nJust quit the xcode6.0 beta or else and run project again, it worked for me.\n\n\nA24:\n\nIn addition to @childno.de answer, your Mac directory \n\n/private/var/db/receipts/ \n\nmay still contains obsolete iPhoneSimulatorSDK .bom and .plist files like this:\n\n/private/var/db/receipts/com.apple.pkg.iPhoneSimulatorSDK8_4.bom\n/private/var/db/receipts/com.apple.pkg.iPhoneSimulatorSDK8_4.plist\n\nThese could make your Downloads tab of Xcode's preferences show a tick (\u221a) for that obsolete simulator version.\nTo purge the unwanted simulators, you can do a search using this bash command from your Mac terminal:\n\nsudo find / -name \"*PhoneSimulator*\"\n\nThen go to corresponding directories to manually delete unwanted SimulatorSDKs\n\n\nA25:\n\nIn Xcode 6+ you can simply go to Menu > Window > Devices > Simulators and delete a simulator  you don't need.\n\n\nA26:\n\nI have the same issue. I click ok in xcode and when launching the app on my iPhone I'm asked if I want to trust this application. Doing it, the app runs and further build-and-run from xcode went without any issue until deleting the app from the iPhone and reinstalling it. Then goto first line ;-)\n\n\nA27:\n\nmacOS Ventura 2023\nSame as the previous version search storage in your spotlight search tool and then double click the Developer option.\nThe interface looks a bit different now\n\nmacOS Big Sur & Monterey before 2023\nWe can use the Storage Management tool:\n\u2318 + Space bar and search storage.\nUnder Developer: you can find Xcode Cache and build files and also old version of iOS support files that you can delete.\n\nTested on macOS Big Sur & Monterey\n\n\nA28:\n\nDid you try to just delete the 4.3 SDK from within the Xcode Package?\n\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs\n\nplease also delete the corresponding .dmg file in\n\n~/Library/Caches/com.apple.dt.Xcode/Downloads\n\nto prevent Xcode from re-installing the same package again.\n\nfor XCode >= 6 see  @praveen-matanam 's answer\n\n\nA29:\n\nOctober 2020 update\nAs was mentioned, you can use xcrun to do a few things:\n\nxcrun simctl list devices or xcrun simctl list --json to list all simulators\nxcrun simctl delete <device udid> to delete specific device\nxcrun simctl delete unavailable to remove old devices for runtimes that are no longer supported\n\nMore things you can do with xcrun (see code snippet)\n\n\n- `xcrun simctl boot <device udid>` to launch (multiple) simulators\n- `xcrun simctl io booted recordVideo \u2014 type=mp4 ./test.mp4` to record simulator video\n- `xcrun simctl io booted screenshot ./screen.png` to make screenshot of simulator\n- `xcrun simctl openurl booted https://google.com` to open URL in simulator\n- `xcrun simctl addmedia booted ./test.mp4` to upload photo or video file (for photos app)\n- `xcrun simctl get_app_container booted <your apps bundle identifier>` to find the app container (where identifier is like *com.bundle.identifier*)\n- `xcrun simctl help` to explore **more** commands\n\n\n\nOriginal Answer\nSeptember 2017, Xcode 9\nRuntimes\nYou will find them here:\n/Library/Developer/CoreSimulator/Profiles/Runtimes\n\n\nDevices\nTo delete devices go here:\n~/Library/Developer/CoreSimulator/Devices\n\nMuch easier to delete them use Xcode:\nXcode->Window->Devices and Simulators\n\nHelping Xcode \"forget\" about runtimes and prevent from re-installing them - delete .dmg file(s) here:\n~/Library/Caches/com.apple.dt.Xcode/Downloads\n\nI hope it will help someone \ud83d\ude42\n\n\nA30:\n\n\nCan I use the linked answer to circumvent the issue without any negative impact and without compromising a future AppStore submission?\n\nyes\n\nWhat does the ENABLE_BITCODE actually do, will it be a non-optional requirement in the future?\n\nENABLE_BITCODE adds an intermediate representation of your code into binary. For watchOS, tvOS now it is mandatory\n\nAre there any performance impacts if I enable / disable it?\n\nIt has an impact on Xcode build and memory footprint when you archive your project\n[Bitcode]\n\n\nA31:\n\nAs specified in another answer, you can also just not specify the distribution certificate to sign with and it will package correctly. TestFlight would need to update their app to do this.\n\n\nA32:\n\nThis worked for me:\niOS Simulator -> Reset Contents and Settings... -> Reset \n\n\nreferenced from Qiita: Unable to run app in Simulator \u30a8\u30e9\u30fc\u306e\u5bfe\u5fdc\u65b9\u6cd5.\n\n\nA33:\n\nI had the same problem.  I was running out of space.\nDeleting old device simulators did NOT help.\nMy space issue was caused by xCode.  It kept a copy of every iOS version on my macOS since I had installed xCode.\nDelete the iOS version you don't want and free up disk space.  I saved 50GB+ of space.\n\n\n\nNOTE -> can't you see ~/Library inside Finder?  It is hidden by default.  Use Terminal and type cd ~/Library/Developer/Xcode/iOS\\ DeviceSupport/ or google how to see hidden folders.\n\n\nNOTE -> if you have multiple users on a single macOS machine, make sure to find the directory ONLY with the user account that originally installed xCode.\n\n\nA34:\n\nMake sure to select \"All\" to find the enable bitcode build settings:\n\n\n\nA35:\n\n\nStarting with Xcode 14, bitcode is no longer required for watchOS and\ntvOS applications, and the App Store no longer accepts bitcode\nsubmissions from Xcode 14.\nXcode no longer builds bitcode by default and generates a warning\nmessage if a project explicitly enables bitcode: \u201cBuilding with\nbitcode is deprecated. Please update your project and/or target\nsettings to disable bitcode.\u201d The capability to build with bitcode\nwill be removed in a future Xcode release. IPAs that contain bitcode\nwill have the bitcode stripped before being submitted to the App\nStore. Debug symbols can only be downloaded from App Store Connect /\nTestFlight for existing bitcode submissions and are no longer\navailable for submissions made with Xcode 14. (86118779)\n\n\n\nA36:\n\nI just quit the simulator , cleaned up the project and ran again it worked.\n\n\nA37:\n\nOpen your app scheme setting and edit it as shown in attached screenshot,\nActually you have to disable the \"Launch Due to background fetch\". Worked for me.\nSteps,\n1. Select your target and select \"edit scheme\"\n2. Select \"Options\" tab on right side of detail window\n3. Uncheck \"Launch Due to background fetch\" checkbox.\n\n\nA38:\n\nI emailed TestFlight support and got this response:\n\nOur team is currently investigating this issue with the TestFlight Desktop app. In the meantime, please use Xcode to create the IPA file and then upload it using the desktop app or the TestFlight website.\n\nThe suggested workaround did work.\n\n\nA39:\n\n@vj9 thx. I update to xcode 7 . It show me the same error. Build well after set \"NO\"\n\nset \"NO\" it works well.\n\n\n\nA40:\n\nReseting the Xcode will delete some bad junk which simulator has cached, so reseting is good option to resolve this issue.\niOS Simulator -> Reset Contents and Settings.\n\n\nA41:\n\nOn Yosemite w/ XCode 6.4 even using the SDKROOT patch the codesigning fails.  The following article explains how to patch the XCode script to get around this.  Note that this is patching XCode, so it is version specific, but fixes the problem.\nhttp://www.jayway.com/2015/05/21/fixing-your-ios-build-scripts\n\n\nA42:\n\nFrom the docs\n\ncan I use the above method without any negative impact and without compromising a future appstore submission?\n\nBitcode will allow apple to optimise the app without you having to submit another build. But, you can only enable this feature if all frameworks and apps in the app bundle have this feature enabled.\nHaving it helps, but not having it should not have any negative impact.\n\nWhat does the ENABLE_BITCODE actually do, will it be a non-optional requirement in the future?\n\n\nFor iOS apps, bitcode is the default, but optional. If you provide\n  bitcode, all apps and frameworks in the app bundle need to include\n  bitcode. For watchOS apps, bitcode is required.\n\n\nAre there any performance impacts if I enable / disable it?\n\n\nThe App Store and operating system optimize the installation of iOS\n  and watchOS apps by tailoring app delivery to the capabilities of the\n  user\u2019s particular device, with minimal footprint. This optimization,\n  called app thinning, lets you create apps that use the most device\n  features, occupy minimum disk space, and accommodate future updates\n  that can be applied by Apple. Faster downloads and more space for\n  other apps and content provides a better user experience.\n\nThere should not be any performance impacts.\n\n\nA43:\n\n\n\nWhat does the ENABLE_BITCODE actually do, will it be a non-optional requirement in the future?\n\n\nI'm not sure at what level you are looking for an answer at, so let's take a little trip. Some of this you may already know.\nWhen you build your project, Xcode invokes clang for Objective-C targets and swift/swiftc for Swift targets. Both of these compilers compile the app to an intermediate representation (IR), one of these IRs is bitcode. From this IR, a program called LLVM takes over and creates the binaries needed for x86 32 and 64 bit modes (for the simulator) and arm6/arm7/arm7s/arm64 (for the device). Normally, all of these different binaries are lumped together in a single file called a fat binary.\nThe ENABLE_BITCODE option cuts out this final step. It creates a version of the app with an IR bitcode binary. This has a number of nice features, but one giant drawback: it can't run anywhere. In order to get an app with a bitcode binary to run, the bitcode needs to be recompiled (maybe assembled or transcoded\u2026 I'm not sure of the correct verb) into an x86 or ARM binary.\nWhen a bitcode app is submitted to the App Store, Apple will do this final step and create the finished binaries.\nRight now, bitcode apps are optional, but history has shown Apple turns optional things into requirements (like 64 bit support). This usually takes a few years, so third party developers (like Parse) have time to update.\n\n\ncan I use the above method without any negative impact and without compromising a future appstore submission?\n\n\nYes, you can turn off ENABLE_BITCODE and everything will work just like before. Until Apple makes bitcode apps a requirement for the App Store, you will be fine.\n\n\nAre there any performance impacts if I enable / disable it?\n\n\nThere will never be negative performance impacts for enabling it, but internal distribution of an app for testing may get more complicated.\nAs for positive impacts\u2026 well that's complicated.\nFor distribution in the App Store, Apple will create separate versions of your app for each machine architecture (arm6/arm7/arm7s/arm64) instead of one app with a fat binary. This means the app installed on iOS devices will be smaller.\nIn addition, when bitcode is recompiled (maybe assembled or transcoded\u2026 again, I'm not sure of the correct verb), it is optimized. LLVM is always working on creating new a better optimizations. In theory, the App Store could recreate the separate version of the app in the App Store with each new release of LLVM, so your app could be re-optimized with the latest LLVM technology.\n\n\nA44:\n\nAnswer valid through Xcode 7, 8, 9, 10, 11 and 12+\n\nYou can not archive, because from target you have not selected device or Generic iOS device or Any iOS Device , still there is a simulator.\nNothing generic or wrong with Xcode 7. This happens with all Xcode versions.\n\n\nProblem - Why archived button is grayed out?\n\n\n\n\nSolution - Why archived button is not grayed out?\n\n\nIn Xcode 12 it is named Any iOS Device\n\n\n\nA45:\n\nI wrote up one-line bash script that would delete ALL your simulators:\nxcrun simctl list devices | grep -E -o -i \"([0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12})\" | xargs -L1 xcrun simctl delete\n\n\nxcrun simctl list devices will list all the simulators installed on your machine\ngrep -E -o -i \"([0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12})\" will grab the device UUID\nxargs -L1 xcrun simctl delete will attempt to delete the device for each UUID it found\n\nIf you want to see everything it'll execute, you can add echo before xcrun, i.e.\nxcrun simctl list devices | grep -E -o -i \"([0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12})\" | xargs -L1 echo xcrun simctl delete\n\n\n\nA46:\n\nRun this command in terminal to remove simulators that can't be accessed from the current version of Xcode (8+?) in use on your machine.\nxcrun simctl delete unavailable\nAlso if you're looking to reclaim simulator related space Michael Tsai found that deleting sim logs saved him 30 GB.\n~/Library/Logs/CoreSimulator\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 42, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI'm writing an application with python and sqlalchemy-0.7. It starts by initializing the sqlalchemy orm (using declarative) and then it starts a multithreaded web server - I'm currently using web.py for rapid prototyping but that could change in the future. I will also add other \"threads\" for scheduled jobs and so on, probably using other python threads.\nFrom SA documentation I understand I have to use scoped_session() to get a thread-local session, so my web.py app should end up looking something like:\nimport web\nfrom myapp.model import Session  # scoped_session(sessionmaker(bind=engine))\nfrom myapp.model import This, That, AndSoOn\nurls = blah...\napp  = web.application(urls, globals())\n\nclass index:\n    def GET(self):\n        s = Session()\n        # get stuff done\n        Session().remove()\n        return(stuff)\n\nclass foo:\n    def GET(self):\n        s = Session()\n        # get stuff done\n        Session().remove()\n        return(stuff)\n\nIs that the Right Way to handle the session?\nAs far as I understand, I should get a scoped_session at every method since it'll give me a thread local session that I could not obtain beforehand (like at the module level).\nAlso, I should call .remove() or .commit() or something like them at every method end, otherwise the session will still contain Persistent objects and I would not be able to query/access the same objects in other threads?\nIf that pattern is the correct one, it could probably be made better by writing it only once, maybe using a decorator? Such a decorator could get the session, invoke the method and then make sure to dispose the session properly. How would that pass the session to the decorated function?\n\n\nCandidate answers:\nA1:\n\nIn fact, I want to find asyncio.Semaphores, not threading.Semaphore,\nand I believe someone may want it too.\nSo, I decided to share the asyncio.Semaphores, hope you don't mind.\nfrom asyncio import (\n    Task,\n    Semaphore,\n)\nimport asyncio\nfrom typing import List\n\n\nasync def shopping(sem: Semaphore):\n    while True:\n        async with sem:\n            print(shopping.__name__)\n        await asyncio.sleep(0.25)  # Transfer control to the loop, and it will assign another job (is idle) to run.\n\n\nasync def coding(sem: Semaphore):\n    while True:\n        async with sem:\n            print(coding.__name__)\n        await asyncio.sleep(0.25)\n\n\nasync def main():\n    sem = Semaphore(value=1)\n    list_task: List[Task] = [asyncio.create_task(_coroutine(sem)) for _coroutine in (shopping, coding)]\n    \"\"\" \n    # Normally, we will wait until all the task has done, but that is impossible in your case.\n    for task in list_task:\n        await task\n    \"\"\"\n    await asyncio.sleep(2)  # So, I let the main loop wait for 2 seconds, then close the program.\n\n\nasyncio.run(main())\n\noutput\nshopping\ncoding\nshopping\ncoding\nshopping\ncoding\nshopping\ncoding\nshopping\ncoding\nshopping\ncoding\nshopping\ncoding\nshopping\ncoding\n\n16*0.25 = 2\n\n\nA2:\n\nYes, this is the right way.\nExample:\nThe Flask microframework with Flask-sqlalchemy extension does what you described. It also does .remove() automatically at the end of each HTTP request (\"view\" functions), so the session is released by the current thread. Calling just .commit() is not sufficient, you should use .remove().\nWhen not using Flask views, I usually use a \"with\" statement:\n@contextmanager\ndef get_db_session():\n    try:\n        yield session\n    finally:\n        session.remove()\n\nwith get_db_session() as session:\n    # do something with session\n\nYou can create a similar decorator.\nScoped session creates a DBMS connection pool, so this approach will be faster than opening/closing session at each HTTP request. It also works nice with greenlets (gevent or eventlet).\n\n\nA3:\n\nExisting answers are wastefully sleeping\nI noticed that almost all answers use some form of time.sleep or asyncio.sleep, which blocks the thread. This should be avoided in real software, because blocking your thread for 0.25, 0.5 or 1 second is unnecessary/wasteful - you could be doing more processing, especially if your application is IO bound - it already blocks when it does IO AND you are introducing arbitrary delays (latency) in your processing time. If all your threads are sleeping, your app isn't doing anything. Also, these variables are quite arbitrary, which is why each answer has a different value they sleep (block the thread for).\nThe answers are using it as a way to get Python's bytecode interpreter to pre-empt the thread after each print line, so that it alternates deterministically between running the 2 threads. By default, the interpreter pre-empts a thread every 5ms (sys.getswitchinterval() returns 0.005), and remember that these threads never run in parallel, because of Python's GIL\n\nSolution to problem\n\nHow can I intercale the prints?\n\nSo my answer would be, you do not want to use semaphores to print (or process) something in a certain order reliably, because you cannot rely on thread prioritization in Python. See Controlling scheduling priority of python threads? for more. time.sleep(arbitrarilyLargeEnoughNumber) doesn't really work when you have more than 2 concurrent pieces of code, since you don't know which one will run next - see * below. If the order matters, use a queue, and worker threads:\nfrom threading import Thread\nimport queue\n\nq = queue.Queue()\n\ndef enqueue():\n    while True:\n        q.put(1)\n        q.put(2)\n      \ndef reader():\n    while True:\n        value = q.get()\n        print(value)\n\nenqueuer_thread = Thread(target = enqueue)\nreader_thread_1 = Thread(target = reader)\nreader_thread_2 = Thread(target = reader)\nreader_thread_3 = Thread(target = reader)\nenqueuer_thread.start()\nreader_thread_1.start()\nreader_thread_2.start()\nreader_thread_3.start()\n...\n\nUnfortunately in this problem, you don't get to use Semaphore.\n\n*An extra check for you\nIf you try a modification of the top voted answer but with an extra function/thread to print(3), you'll get:\n1\n2\n3\n1\n3\n2\n1\n3\n...\n\nWithin a few prints, the ordering is broken - it's 1-3-2.\n\n\nA4:\n\nYou need to use 2 semaphores to do what you want to do, and you need to initialize them at 0.\nimport threading\nSEM_FUN1 = threading.Semaphore(0)\nSEM_FUN2 = threading.Semaphore(0)\n\n\ndef fun1() -> None:\n    for _ in range(5):\n        SEM_FUN1.acquire()\n        print(1)\n        SEM_FUN2.release()\n\n\ndef fun2() -> None:\n    for _ in range(5):\n        SEM_FUN2.acquire()\n        print(2)\n        SEM_FUN1.release()\n\n\nthreading.Thread(target=fun1).start()\nthreading.Thread(target=fun2).start()\nSEM_FUN1.release()  # Trigger fun1\n\nOutput:\n\n\n\nA5:\n\nChris already explained what daemon threads are, so let's talk about practical usage. Many thread pool implementations use daemon threads for task workers. Workers are threads which execute tasks from task queue. \nWorker needs to keep waiting for tasks in task queue indefinitely as they don't know when new task will appear. Thread which assigns tasks (say main thread) only knows when tasks are over. Main thread waits on task queue to get empty and then exits. If workers are user threads i.e. non-daemon, program won't terminate. It will keep waiting for these indefinitely running workers, even though workers aren't doing anything useful. Mark workers daemon threads, and main thread will take care of killing them as soon as it's done handling tasks. \n\n\nA6:\n\nQuoting Chris: \"... when your program quits, any daemon threads are killed automatically.\". I think that sums it up. You should be careful when you use them as they abruptly terminate when main program executes to completion.\n\n\nA7:\n\nIt is working fine, its just that its printing too fast for you to see . Try putting a time.sleep() in both functions (a small amount) to sleep the thread for that much amount of time, to actually be able to see both 1 as well as 2.\nExample -\nimport threading\nimport time\nsem = threading.Semaphore()\n\ndef fun1():\n    while True:\n        sem.acquire()\n        print(1)\n        sem.release()\n        time.sleep(0.25)\n\ndef fun2():\n    while True:\n        sem.acquire()\n        print(2)\n        sem.release()\n        time.sleep(0.25)\n\nt = threading.Thread(target = fun1)\nt.start()\nt2 = threading.Thread(target = fun2)\nt2.start()\n\n\n\nA8:\n\nIt's pretty simple to delegate a method to a thread or sub-process using BaseEventLoop.run_in_executor:\nimport asyncio\nimport time\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef cpu_bound_operation(x):\n    time.sleep(x) # This is some operation that is CPU-bound\n\n@asyncio.coroutine\ndef main():\n    # Run cpu_bound_operation in the ProcessPoolExecutor\n    # This will make your coroutine block, but won't block\n    # the event loop; other coroutines can run in meantime.\n    yield from loop.run_in_executor(p, cpu_bound_operation, 5)\n\n\nloop = asyncio.get_event_loop()\np = ProcessPoolExecutor(2) # Create a ProcessPool with 2 processes\nloop.run_until_complete(main())\n\nAs for whether to use a ProcessPoolExecutor or ThreadPoolExecutor, that's kind of hard to say; pickling a large object will definitely eat some CPU cycles, which initially would make you think ProcessPoolExecutor is the way to go. However, passing your 100MB object to a Process in the pool would require pickling the instance in your main process, sending the bytes to the child process via IPC, unpickling it in the child, and then pickling it again so you can write it to disk. Given that, my guess is the pickling/unpickling overhead will be large enough that you're better off using a ThreadPoolExecutor, even though you're going to take a performance hit because of the GIL.\nThat said, it's very simple to test both ways and find out for sure, so you might as well do that.\n\n\nA9:\n\nLet's say you're making some kind of dashboard widget.  As part of this, you want it to display the unread message count in your email box.  So you make a little thread that will:\n\nConnect to the mail server and ask how many unread messages you have.\nSignal the GUI with the updated count.\nSleep for a little while.\n\nWhen your widget starts up, it would create this thread, designate it a daemon, and start it.  Because it's a daemon, you don't have to think about it; when your widget exits, the thread will stop automatically.\n\n\nA10:\n\nSome threads do background tasks, like sending keepalive packets, or performing periodic garbage collection, or whatever. These are only useful when the main program is running, and it's okay to kill them off once the other, non-daemon, threads have exited.\nWithout daemon threads, you'd have to keep track of them, and tell them to exit, before your program can completely quit. By setting them as daemon threads, you can let them run and forget about them, and when your program quits, any daemon threads are killed automatically.\n\n\nA11:\n\nI will also add my few bits here, I think one of the reasons why daemon threads are confusing to most people(atleast they were to me)  is because of the Unix context to the word dameon.\nIn Unix terminology the word daemon refers to a process which once spawned; keeps running in the background and user can move on to do other stuff with the foreground process.\nIn Python threading context, every thread upon creation runs in the background, whether it is daemon or non-daemon, the difference comes from the fact how these threads affect the main thread.\nWhen you start a non-daemon thread, it starts running in background and you can perform other stuff, however, your main thread will not exit until all such non-daemon threads have completed their execution, so in a way, your program or main thread is blocked.\nWith daemon threads they still run in the background but with one key difference that they do not block the main thread.\nAs soon as the main thread completes its execution & the program exits, all the remaining daemon threads will be reaped. This makes them useful for operations which you want to perform in background but want these operations to exit automatically as soon as the main application exits.\nOne point to keep note of is that you should be aware of what exactly you are doing in daemon threads, the fact they exit when main thread exits can give you unexpected surprises. One of the ways to gracefully clean up the daemon threads is to use the Threading Events to set the event as an exit handler and check if the event is set inside the thread and then break from the thread function accordingly.\nAnother thing that confused about daemon threads is the definition from python documentation.\n\nThe significance of this flag is that the entire Python program exits\nwhen only daemon threads are left\n\nIn simple words what this means is that if your program has both daemon and non-daemon threads the main program will be blocked and wait until all the non-daemon have exited, as soon as they exit main thread will exit as well. What this statement also implies but is not clear at first glance is that all daemon threads will be exited automatically once the main threads exits.\n\n\nA12:\n\nCreate a Daemon thread when:\n\nYou want a low-priority thread\nYour Thread does background-specific tasks, and more importantly,\nWhen you want this thread to die as soon as all user threads accomplish their tasks.\n\nSome Examples of Daemon Thread Services: Garbage collection in Java, Word count checker in MS Word, Auto-saver in medium, File downloads counter in a parallel file downloads application, etc.\n\n\nA13:\n\nOther posters gave some examples for situations in which you'd use daemon threads.  My recommendation, however, is never to use them.\nIt's not because they're not useful, but because there are some bad side effects you can experience if you use them.  Daemon threads can still execute after the Python runtime starts tearing down things in the main thread, causing some pretty bizarre exceptions.\nMore info here:\nhttps://joeshaw.org/python-daemon-threads-considered-harmful/\nhttps://mail.python.org/pipermail/python-list/2005-February/343697.html\nStrictly speaking you never need them, it just makes implementation easier in some cases.\n\n\nA14:\n\nAnother alternative is to use loop.call_soon_threadsafe along with an asyncio.Queue as the intermediate channel of communication.\nThe current documentation for Python 3 also has a section on Developing with asyncio - Concurrency and Multithreading:\nimport asyncio\n\n# This method represents your blocking code\ndef blocking(loop, queue):\n    import time\n    while True:\n        loop.call_soon_threadsafe(queue.put_nowait, 'Blocking A')\n        time.sleep(2)\n        loop.call_soon_threadsafe(queue.put_nowait, 'Blocking B')\n        time.sleep(2)\n\n# This method represents your async code\nasync def nonblocking(queue):\n    await asyncio.sleep(1)\n    while True:\n        queue.put_nowait('Non-blocking A')\n        await asyncio.sleep(2)\n        queue.put_nowait('Non-blocking B')\n        await asyncio.sleep(2)\n\n# The main sets up the queue as the communication channel and synchronizes them\nasync def main():\n    queue = asyncio.Queue()\n    loop = asyncio.get_running_loop()\n\n    blocking_fut = loop.run_in_executor(None, blocking, loop, queue)\n    nonblocking_task = loop.create_task(nonblocking(queue))\n\n    running = True  # use whatever exit condition\n    while running:\n        # Get messages from both blocking and non-blocking in parallel\n        message = await queue.get()\n        # You could send any messages, and do anything you want with them\n        print(message)\n\nasyncio.run(main())\n\nHow to send asyncio tasks to loop running in other thread may also help you.\nIf you need a more \"powerful\" example, check out my Wrapper to launch async tasks from threaded code. It will handle the thread safety part for you (for the most part) and let you do things like this:\n# See https://gist.github.com/Lonami/3f79ed774d2e0100ded5b171a47f2caf for the full example\n\nasync def async_main(queue):\n    # your async code can go here\n    while True:\n        command = await queue.get()\n        if command.id == 'print':\n            print('Hello from async!')\n        elif command.id == 'double':\n            await queue.put(command.data * 2)\n\nwith LaunchAsync(async_main) as queue:\n    # your threaded code can go here\n    queue.put(Command('print'))\n    queue.put(Command('double', 7))\n    response = queue.get(timeout=1)\n    print('The result of doubling 7 is', response)\n\n\n\nA15:\n\nI used this code to demonstrate how 1 thread can use a Semaphore and the other thread will wait (non-blocking) until the Sempahore is available.\nThis was written using Python3.6; Not tested on any other version.\nThis will only work is the synchronization is being done from the same thread, IPC from separate processes will fail using this mechanism.\nimport threading\nfrom  time import sleep\nsem = threading.Semaphore()\n\ndef fun1():\n    print(\"fun1 starting\")\n    sem.acquire()\n    for loop in range(1,5):\n        print(\"Fun1 Working {}\".format(loop))\n        sleep(1)\n    sem.release()\n    print(\"fun1 finished\")\n\n\n\ndef fun2():\n    print(\"fun2 starting\")\n    while not sem.acquire(blocking=False):\n        print(\"Fun2 No Semaphore available\")\n        sleep(1)\n    else:\n        print(\"Got Semphore\")\n        for loop in range(1, 5):\n            print(\"Fun2 Working {}\".format(loop))\n            sleep(1)\n    sem.release()\n\nt1 = threading.Thread(target = fun1)\nt2 = threading.Thread(target = fun2)\nt1.start()\nt2.start()\nt1.join()\nt2.join()\nprint(\"All Threads done Exiting\")\n\nWhen I run this - I get the following output.\nfun1 starting\nFun1 Working 1\nfun2 starting\nFun2 No Semaphore available\nFun1 Working 2\nFun2 No Semaphore available\nFun1 Working 3\nFun2 No Semaphore available\nFun1 Working 4\nFun2 No Semaphore available\nfun1 finished\nGot Semphore\nFun2 Working 1\nFun2 Working 2\nFun2 Working 3\nFun2 Working 4\nAll Threads done Exiting\n\n\n\nA16:\n\nYou don't need to create a scoped session if you create new session for each request and each request is handled by single thread.\nYou have to call s.commit() to make pending objects persistent, i.e. to save changes into database.\nYou may also want to close session by calling s.close().\n\n\nA17:\n\nI assume you're talking about evenlet/gevent greenlets\n1) There can be only one greenlet running\n2) It's cooperative multithreading, which means that if a greenlet is stuck in an infinite loop, your entire program is stuck, typically greenlets are scheduled either explicitly or during I/O\n3) A lot more than threads, it depends of the amount of RAM available\n\n\nA18:\n\nFinally, after taking a look at the watchdog implementation, it is not necessary to call unschedule_all before stop, this is done automatically. Removing the line containing this method call fixes the issue and the application is running perfectly.\n\n\nA19:\n\nI also used run_in_executor, but I found this function kinda gross under most circumstances, since it requires partial() for keyword args and I'm never calling it with anything other than a single executor and the default event loop. So I made a convenience wrapper around it with sensible defaults and automatic keyword argument handling.\nfrom time import sleep\nimport asyncio as aio\nloop = aio.get_event_loop()\n\nclass Executor:\n    \"\"\"In most cases, you can just use the 'execute' instance as a\n    function, i.e. y = await execute(f, a, b, k=c) => run f(a, b, k=c) in\n    the executor, assign result to y. The defaults can be changed, though,\n    with your own instantiation of Executor, i.e. execute =\n    Executor(nthreads=4)\"\"\"\n    def __init__(self, loop=loop, nthreads=1):\n        from concurrent.futures import ThreadPoolExecutor\n        self._ex = ThreadPoolExecutor(nthreads)\n        self._loop = loop\n    def __call__(self, f, *args, **kw):\n        from functools import partial\n        return self._loop.run_in_executor(self._ex, partial(f, *args, **kw))\nexecute = Executor()\n\n...\n\ndef cpu_bound_operation(t, alpha=30):\n    sleep(t)\n    return 20*alpha\n\nasync def main():\n    y = await execute(cpu_bound_operation, 5, alpha=-2)\n\nloop.run_until_complete(main())\n\n\n\nA20:\n\nA simpler way to think about it, perhaps: when main returns, your process will not exit if there are non-daemon threads still running.\nA bit of advice: Clean shutdown is easy to get wrong when threads and synchronization are involved - if you can avoid it, do so. Use daemon threads whenever possible.\n\n\nA21:\n\nWhen your second thread is non-Daemon, your application's primary main thread cannot quit because its exit criteria is being tied to the exit also of non-Daemon thread(s). Threads cannot be forcibly killed in python, therefore your app will have to really wait for the non-Daemon thread(s) to exit. If this behavior is not what you want, then set your second thread as daemon so that it won't hold back your application from exiting.  \n\n\nA22:\n\nAlso, you can use Lock/mutex method as follows:\nimport threading\nimport time\n\nmutex = threading.Lock()  # is equal to threading.Semaphore(1)\n\ndef fun1():\n    while True:\n        mutex.acquire()\n        print(1)\n        mutex.release()\n        time.sleep(.5)\n\ndef fun2():\n    while True:\n        mutex.acquire()\n        print(2)\n        mutex.release()\n        time.sleep(.5)\n\nt1 = threading.Thread(target=fun1).start()\nt2 = threading.Thread(target=fun2).start()\n\nSimpler style using \"with\":\nimport threading\nimport time\n\nmutex = threading.Lock()  # is equal to threading.Semaphore(1)\n\ndef fun1():\n    while True:\n        with mutex:\n            print(1)\n        time.sleep(.5)\n\ndef fun2():\n    while True:\n        with mutex:\n            print(2)\n        time.sleep(.5)\n\nt1 = threading.Thread(target=fun1).start()\nt2 = threading.Thread(target=fun2).start()\n\n\n[NOTE]:\nThe difference between mutex, semaphore, and lock\n\n\nA23:\n\nthreading.Thread(target=some_callable_function).start()\n\nor if you wish to pass arguments,\nthreading.Thread(target=some_callable_function,\n        args=(tuple, of, args),\n        kwargs={'dict': 'of', 'keyword': 'args'},\n    ).start()\n\n\n\nA24:\n\nUnfortunately there is not a direct equivalent, because Python 3 is meant to be more portable than Python 2 and the _thread interface is seen as too low-level for this purpose.\nIn Python 3 the best practice is usually to use threading.Thread(target=f...). This uses different semantics, but is preferred because the interface is easier to port to other Python implementations.\n\n\nA25:\n\nI had this issue recently trying to use wordnet synsets, and I found another way around.\nI'm running an application with fast-api and uvicorn that needs to handle thousands and thousands of requests per second. Tried many different solutions, but at the end the best one was to move the wordnet synsets to a separated dictionary.\nIt increased the server start in 5 seconds (and it didn't consumed that much memory), but of course, the performance when reading data this way is superb.\nfrom nltk.corpus import wordnet\nfrom itertools import chain\n\ndef get_synsets(self, word):\n    try:\n        return synsets[word.lower()]\n    except:\n        return []\n\nsynsets = {}\nlemmas_in_wordnet = set(chain(*[x.lemma_names() for x in wordnet.all_synsets()]))\n\nfor lemma in lemmas_in_wordnet:\n    synsets[lemma] = wordnet.synsets(lemma)\n\nThe performance\ntimeit(\"synsets['word']\", globals=globals(), number=1000000)\n0.03934049699999953\n\ntimeit(\"wordnet.synsets('word')\", globals=globals(), number=1000000)\n11.193742591000001\n\n\n\nA26:\n\nI have run your code and get the same error. For a working solution, see below. Here is the explanation:\nLazyCorpusLoader is a proxy object that stands in for a corpus object before the corpus is loaded. (This prevents the NLTK from loading massive corpora into memory before you need them.) The first time this proxy object is accessed, however, it becomes the corpus you intend to load. That is to say, the LazyCorpusLoader proxy object transforms its __dict__ and __class__ into the __dict__ and __class__ of the corpus you are loading.\nIf you compare your code to your errors above, you can see that you received 9 errors when you tried to create 10 instances of your class. The first transformation of the LazyCorpusLoader proxy object into a WordNetCorpusReader object was successful. This action was triggered when you accessed wordnet for the first time:\nThe First Thread\nfrom nltk.corpus import wordnet as wn\ndef is_good_word(word):\n    ...\n    wn.ensure_loaded()  # `LazyCorpusLoader` conversion into `WordNetCorpusReader` starts\n\nThe Second Thread\nWhen you begin to run your is_good_word function in a second thread, however, your first thread has not completely transformed the LazyCorpusLoader proxy object into a WordNetCorpusReader. wn is still a LazyCorpusLoader proxy object, so it begins the __load process again. Once it gets to the point where it tries to convert its __class__ and __dict__ into a WordNetCorpusReader object, however, the first thread has already converted the LazyCorpusLoader proxy object into a WordNetCorpusReader. My guess is that you are running into an error in the line with my comment below:\nclass LazyCorpusLoader(object):\n    ...\n    def __load(self):\n        ...\n        corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)  # load corpus\n        ...\n        # self.__args == self._LazyCorpusLoader__args\n        args, kwargs  = self.__args, self.__kwargs                       # most likely the line throwing the error\n\nOnce the first thread has transformed the LazyCorpusLoader proxy object into a WordNetCorpusReader object, the mangled names will no longer work. The WordNetCorpusReader object will not have LazyCorpusLoader anywhere in its mangled names. (self.__args is equivalent to self._LazyCorpusLoader__args while the object is a LazyCorpusLoader object.) Thus you get the following error:\nAttributeError: 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args'\n\nAn Alternative\nIn light of this issue, you will want to access the wn object before you enter into your threading. Here is your code modified appropriately:\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.corpus.reader.wordnet import WordNetError\nimport sys\nimport time\nimport threading\n\ncachedStopWords = stopwords.words(\"english\")\n\n\ndef is_good_word(word):\n    word = word.strip()\n    if len(word) <= 2:\n        return 0\n    if word in cachedStopWords:\n        return 0\n    try:\n        if len(wn.lemmas(str(word), lang='en')) == 0:     # no longer the first access of wn\n            return 0\n    except WordNetError as e:\n        print(\"WordNetError on concept {}\".format(word))\n    except AttributeError as e:\n        print(\"Attribute error on concept {}: {}\".format(word, e.message))\n    except:\n        print(\"Unexpected error on concept {}: {}\".format(word, sys.exc_info()[0]))\n    else:\n        return 1\n    return 1\n\n\nclass ProcessMetaThread(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n\n    def run(self):\n        is_good_word('dog')\n\n\ndef process_meta(numberOfThreads):\n    print wn.__class__            # <class 'nltk.corpus.util.LazyCorpusLoader'>\n    wn.ensure_loaded()            # first access to wn transforms it\n    print wn.__class__            # <class 'nltk.corpus.reader.wordnet.WordNetCorpusReader'>\n    threadsList = []\n    for i in range(numberOfThreads):\n        start = time.clock()\n        t = ProcessMetaThread()\n        print time.clock() - start\n        t.setDaemon(True)\n        t.start()\n        threadsList.append(t)\n\n    numComplete = 0\n    while numComplete < numberOfThreads:\n        # Iterate over the active processes\n        for processNum in range(0, numberOfThreads):\n            # If a process actually exists\n            if threadsList != None:\n                # If the process is finished\n                if not threadsList[processNum] == None:\n                    if not threadsList[processNum].is_alive():\n                        numComplete += 1\n                        threadsList[processNum] = None\n        time.sleep(5)\n\n    print('Processes Finished')\n\n\nif __name__ == '__main__':\n    process_meta(10)\n\nI have tested the above code and received no errors.\n\n\nA27:\n\nYou can think of greenlets more like cooperative threads. What this means is that there is no scheduler pre-emptively switching between your threads at any given moment - instead your greenlets voluntarily/explicitly give up control to one another at specified points in your code.\n\nDoes the GIL affect them? Can there be more than one greenlet running\n  at a time?\n\nOnly one code path is running at a time - the advantage is you have ultimate control over which one that is.\n\nWhat are the pitfalls of using greenlets or tasklets?\n\nYou need to be more careful - a badly written greenlet will not yield control to other greenlets. On the other hand, since you know when a greenlet will context switch, you may be able to get away with not creating locks for shared data-structures.\n\nIf I use greenlets, how many of them can a process can handle? (I am wondering because in a single process you can open threads up to umask limit set in your *ix system.)\n\nWith regular threads, the more you have the more scheduler overhead you have. Also regular threads still have a relatively high context-switch overhead. Greenlets do not have this overhead associated with them. From the bottle documentation:\n\nMost servers limit the size of their worker pools to a relatively low\n  number of concurrent threads, due to the high overhead involved in\n  switching between and creating new threads. While threads are cheap\n  compared to processes (forks), they are still expensive to create for\n  each new connection.\nThe gevent module adds greenlets to the mix. Greenlets behave similar\n  to traditional threads, but are very cheap to create. A gevent-based\n  server can spawn thousands of greenlets (one for each connection) with\n  almost no overhead. Blocking individual greenlets has no impact on the\n  servers ability to accept new requests. The number of concurrent\n  connections is virtually unlimited.\n\nThere's also some further reading here if you're interested:\nhttp://sdiehl.github.io/gevent-tutorial/\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "\n X, (vs (vs (vs 0) (vs 0) (vs 0) (vs 0) (vs 0) (vs 0), ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 43, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIs it possible to declare a variable in c++ without instantiating it? I want to do something like this:\nAnimal a;\nif( happyDay() ) \n    a( \"puppies\" ); //constructor call\nelse\n    a( \"toads\" );\n\nBasially, I just want to declare a outside of the conditional so it gets the right scope.\nIs there any way to do this without using pointers and allocating a on the heap? Maybe something clever with references?\n\n\nCandidate answers:\nA1:\n\n\nAre there any other \"modifiers\" (other than = 0 and = delete)?\n\nSince it appears no one else answered this question, I should mention that there is also =default.\nhttps://learn.microsoft.com/en-us/cpp/cpp/explicitly-defaulted-and-deleted-functions#explicitly-defaulted-functions\n\n\nA2:\n\nYou can't do this directly in C++ since the object is constructed when you define it with the default constructor.\nYou could, however, run a parameterized constructor to begin with:\nAnimal a(getAppropriateString());\n\nOr you could actually use something like the ?: operator to determine the correct string.\n(Update: @Greg gave the syntax for this. See that answer)\n\n\nA3:\n\nBlair's answer is on the mark.\nHowever note that there is a mutable qualifier which may be added to a class's data members.  Any member so marked can be modified in a const method without violating the const contract.\nYou might want to use this (for example) if you want an object to remember how many times a particular method is called, whilst not affecting the \"logical\" constness of that method.\n\n\nA4:\n\nC++17 N4659 standard draft\nhttps://github.com/cplusplus/draft/blob/master/papers/n4659.pdf 11.4.2 \"Explicitly-defaulted functions\":\n\n1 A function definition of the form:\nattribute-specifier-seq opt decl-specifier-seq opt declarator virt-specifier-seq opt = default ;\n\nis called an explicitly-defaulted definition. A function that is explicitly defaulted shall\n\n(1.1) \u2014 be a special member function,\n\n(1.2) \u2014 have the same declared function type (except for possibly differing ref-qualifiers and except that in\nthe case of a copy constructor or copy assignment operator, the parameter type may be \u201creference to\nnon-const T\u201d, where T is the name of the member function\u2019s class) as if it had been implicitly declared,\nand\n\n(1.3) \u2014 not have default arguments.\n\n\n2 An explicitly-defaulted function that is not defined as deleted may be declared constexpr only if it would\nhave been implicitly declared as constexpr. If a function is explicitly defaulted on its first declaration, it is\nimplicitly considered to be constexpr if the implicit declaration would be.\n3 If a function that is explicitly defaulted is declared with a noexcept-specifier that does not produce the same\nexception specification as the implicit declaration (18.4), then\n\n(3.1) \u2014 if the function is explicitly defaulted on its first declaration, it is defined as deleted;\n\n(3.2) \u2014 otherwise, the program is ill-formed.\n\n\n4\n[ Example:\nstruct S {\n  constexpr S() = default;            // ill-formed: implicit S() is not constexpr\n  S(int a = 0) = default;             // ill-formed: default argument\n  void operator=(const S&) = default; // ill-formed: non-matching return type\n  ~ S() noexcept(false) = default;    // deleted: exception specification does not match\nprivate:\n  int i;                              // OK: private copy constructor\n  S(S&);\n};\nS::S(S&) = default;                   // OK: defines copy constructor\n\n\u2014 end example ]\n5 Explicitly-defaulted functions and implicitly-declared functions are collectively called defaulted functions, and\nthe implementation shall provide implicit definitions for them (15.1 15.4, 15.8), which might mean defining\nthem as deleted. A function is user-provided if it is user-declared and not explicitly defaulted or deleted\non its first declaration. A user-provided explicitly-defaulted function (i.e., explicitly defaulted after its first\ndeclaration) is defined at the point where it is explicitly defaulted; if such a function is implicitly defined as\ndeleted, the program is ill-formed. [ Note: Declaring a function as defaulted after its first declaration can\nprovide efficient execution and concise definition while enabling a stable binary interface to an evolving code\nbase. \u2014 end note ]\n6 [ Example:\nstruct trivial {\n  trivial() = default;\n  trivial(const trivial&) = default;\n  trivial(trivial&&) = default;\n  trivial& operator=(const trivial&) = default;\n  trivial& operator=(trivial&&) = default;\n  ~ trivial() = default;\n};\nstruct nontrivial1 {\n  nontrivial1();\n};\nnontrivial1::nontrivial1() = default;       // not first declaration\n\n\u2014 end example ]\n\nThen the question is of course which functions can be implicitly declared and when does that happen, which I have explained at:\n\nWhat is the <=> (\"spaceship\", three-way comparison) operator in C++?\nWhat are all the member-functions created by compiler for a class? Does that happen all the time?\nConditions for automatic generation of default/copy/move ctor and copy/move assignment operator?\n\n\n\nA5:\n\n{ and } will create a scope and if you define some variables in the scope you cannot access them from outside. But for already create that scope. So\n{for(int i = 0; i < count; ++i){}} \n\nis the same as\nfor(int i = 0; i < count; ++i){}\n\nbut if you define something between them, there is a difference\n{int a = 0; for(int i = 0; i < count; ++i){}}\n\nIn this example, a won't be accessible from outside scope.\n\n\nA6:\n\nThis excerpt from The C++ Programming Language [4th Edition] - Bjarne Stroustrup book talks about the real purpose behind using =delete:\n\n3.3.4 Suppressing Operations\nUsing the default copy or move for a class in a hierarchy is typically\n  a disaster: given only a pointer to a base, we simply don\u2019t know what\n  members the derived class has, so we can\u2019t know how to copy\n  them. So, the best thing to do is usually to delete the default copy\n  and move operations, that is, to eliminate the default definitions of\n  those two operations:\nclass Shape {\npublic:\n  Shape(const Shape&) =delete; // no copy operations\n  Shape& operator=(const Shape&) =delete;\n\n  Shape(Shape&&) =delete; // no move operations\n  Shape& operator=(Shape&&) =delete;\n  \u02dcShape();\n    // ...\n};\n\nNow an attempt to copy a Shape will be caught by the compiler.\nThe =delete mechanism is general, that is, it can be used to suppress any operation\n\n\n\nA7:\n\nOnce upon a time, many moons ago, VS6 existed and was popular.  It failed however to conform to a number of C++ standards; which was reasonable at the time as it was released just before (on the same year) the standard was officially released; it did however adhere to the draft of the standard as far as I'm aware.\nOne of the standards that changed between the draft and the official standard, was the lifetime of for loop variables created in the first section; leading to the following code failing to compile\n{\n    for (int i=0; i<1; ++i){}\n    for (int i=0; i<2; ++i){}\n}\n\nbecause i was redefined by the second for loop.\nWhile other compilers also suffered this bug; I highlight the VS6 one because it remained the only version of visual studio for a number of years after the release of the standard, but never released an update for this particular issue; meaning that it had a more significant impact.\nA solution to this is to force the whole for loop into its own scope as you have shown.\n\n\nA8:\n\nhttps://isocpp.org/wiki/faq/const-correctness#const-member-fns\n\nWhat is a \"const member function\"?\nA member function that inspects (rather than mutates) its object.\nA const member function is indicated by a const suffix just after the member function\u2019s parameter list. Member functions with a const suffix are called \u201cconst member functions\u201d or \u201cinspectors.\u201d Member functions without a const suffix are called \u201cnon-const member functions\u201d or \u201cmutators.\u201d\nclass Fred {\npublic:\n  void inspect() const;   // This member promises NOT to change *this\n  void mutate();          // This member function might change *this\n};\nvoid userCode(Fred& changeable, const Fred& unchangeable)\n{\n  changeable.inspect();   // Okay: doesn't change a changeable object\n  changeable.mutate();    // Okay: changes a changeable object\n  unchangeable.inspect(); // Okay: doesn't change an unchangeable object\n  unchangeable.mutate();  // ERROR: attempt to change unchangeable object\n}\n\nThe attempt to call unchangeable.mutate() is an error caught at compile time. There is no runtime space or speed penalty for const, and you don\u2019t need to write test-cases to check it at runtime.\nThe trailing const on inspect() member function should be used to mean the method won\u2019t change the object\u2019s abstract (client-visible) state. That is slightly different from saying the method won\u2019t change the \u201craw bits\u201d of the object\u2019s struct. C++ compilers aren\u2019t allowed to take the \u201cbitwise\u201d interpretation unless they can solve the aliasing problem, which normally can\u2019t be solved (i.e., a non-const alias could exist which could modify the state of the object). Another (important) insight from this aliasing issue: pointing at an object with a pointer-to-const doesn\u2019t guarantee that the object won\u2019t change; it merely promises that the object won\u2019t change via that pointer.\n\n\n\nA9:\n\nThese const mean that compiler will Error if the method 'with const' changes internal data.\nclass A\n{\npublic:\n    A():member_()\n    {\n    }\n\n    int hashGetter() const\n    {\n        state_ = 1;\n        return member_;\n    }\n    int goodGetter() const\n    {\n        return member_;\n    }\n    int getter() const\n    {\n        //member_ = 2; // error\n        return member_;\n    }\n    int badGetter()\n    {\n        return member_;\n    }\nprivate:\n    mutable int state_;\n    int member_;\n};\n\nThe test\nint main()\n{\n    const A a1;\n    a1.badGetter(); // doesn't work\n    a1.goodGetter(); // works\n    a1.hashGetter(); // works\n\n    A a2;\n    a2.badGetter(); // works\n    a2.goodGetter(); // works\n    a2.hashGetter(); // works\n}\n\nRead this for more information\n\n\nA10:\n\nIt's a block scope marked by {} braces. It is usually used to mark the area of automatic storage. In your case it doesn't seem to do anything as the for loop has its own scope in standard C++.\n\n\nA11:\n\nThe const qualifier means that the methods can be called on any value of foobar. The difference comes when you consider calling a non-const method on a const object. Consider if your foobar type had the following extra method declaration:\nclass foobar {\n  ...\n  const char* bar();\n}\n\nThe method bar() is non-const and can only be accessed from non-const values.\nvoid func1(const foobar& fb1, foobar& fb2) {\n  const char* v1 = fb1.bar();  // won't compile\n  const char* v2 = fb2.bar();  // works\n}\n\nThe idea behind const though is to mark methods which will not alter the internal state of the class. This is a powerful concept but is not actually enforceable in C++.  It's more of a promise than a guarantee. And one that is often broken and easily broken.  \nfoobar& fbNonConst = const_cast<foobar&>(fb1);\n\n\n\nA12:\n\nAnother use case that I do not see mentioned in these answers is that it easily allows you to change the visibility of a constructor. For example, maybe you want a friend class to be able to access the copy constructor, but you don't want it to be publicly available.\n\n\nA13:\n\nDeleting a function is a C++11 feature:\n\nThe common idiom of \"prohibiting copying\" can now be expressed\ndirectly:\nclass X {\n    // ...\n    X& operator=(const X&) = delete;  // Disallow copying\n    X(const X&) = delete;\n};\n\n\n\n[...]\nThe \"delete\" mechanism can be used for any function. For example, we\ncan eliminate an undesired conversion like this:\nstruct Z {\n    // ...\n\n    Z(long long);     // can initialize with a long long      \n    Z(long) = delete; // but not anything less\n};\n\n\n\n\nA14:\n\nWe can define a class something {...}accum, trans; where accum and trans can be two objects of the class something. Therefore to declair an object of the class we use semi-colon after the class.\n\n\nA15:\n\nThe const means that the method promises not to alter any members of the class. You'd be able to execute the object's members that are so marked, even if the object itself were marked const:\nconst foobar fb;\nfb.foo();\n\nwould be legal.\nSee How many and which are the uses of \u201cconst\u201d in C++? for more information.\n\n\nA16:\n\nwhen you use const in the method signature (like your said: const char* foo() const;) you are telling the compiler that memory pointed to by this can't be changed by this method (which is foo here).\n\n\nA17:\n\nThe const keyword used with the function declaration specifies that it is a const member function and it will not be able to change the data members of the object.\n\n\nA18:\n\nI guess it's because classes are declarations, even when they need braces for grouping. And yes, there's the historical argument that since in C you could do\nstruct\n{\n  float x;\n  float y;\n} point;\n\nyou should in C++ be able to do a similar thing, it makes sense for the class declaration to behave in the same way.\n\n\nA19:\n\nYou can use this-> to make clear that you are referring to a member of the class:\nvoid Bar<T>::BarFunc () {\n    std::cout << this->_foo_arg << std::endl;\n}\n\nAlternatively you can also use \"using\" in the method:\nvoid Bar<T>::BarFunc () {\n    using Bar<T>::_foo_arg;             // Might not work in g++, IIRC\n    std::cout << _foo_arg << std::endl;\n}\n\nThis makes it clear to the compiler that the member name depends on the template parameters so that it searches for the definition of that name in the right places. For more information also see this entry in the C++ Faq Lite. \n\n\nA20:\n\nAppears to work fine in Visual C++ 2008. I've added some dummy definitions for the types you mentioned but gave no source for. The rest is exactly as you put it. Then a main function to force BarFunc to be instantiated and called.\n#include <iostream>\n\nclass streamable {};\nstd::ostream &operator<<(std::ostream &os, streamable &s) { return os; }\n\nclass foo_arg_t : public streamable {};\nclass a_arg_t : public streamable {};\nclass b_arg_t : public streamable  {};\n\ntemplate <class T>\nclass Foo {\n\npublic:\n    Foo (const foo_arg_t foo_arg) : _foo_arg(foo_arg)\n    {\n        /* do something for foo */\n    }\n    T Foo_T;        // either a TypeA or a TypeB - TBD\n    foo_arg_t _foo_arg;\n};\n\ntemplate <class T>\nclass Bar : public Foo<T> {\npublic:\n    Bar (const foo_arg_t bar_arg, const a_arg_t a_arg)\n    : Foo<T>(bar_arg)   // base-class initializer\n    {\n\n        Foo<T>::Foo_T = T(a_arg);\n    }\n\n    Bar (const foo_arg_t bar_arg, const b_arg_t b_arg)\n    : Foo<T>(bar_arg)\n    {\n        Foo<T>::Foo_T = T(b_arg);\n    }\n\n    void BarFunc ();\n\n};\n\ntemplate <class T>\nvoid Bar<T>::BarFunc () {\n    std::cout << _foo_arg << std::endl; \n    std::cout << Bar<T>::_foo_arg << std::endl;   \n}\n\nint main()\n{\n    Bar<a_arg_t> *b = new Bar<a_arg_t>(foo_arg_t(), a_arg_t());\n    b->BarFunc();\n}\n\n\n\nA21:\n\nIt's a new C++11 feature.\nIt means that you want to use the compiler-generated version of that function, so you don't need to specify a body.\nYou can also use = delete to specify that you don't want the compiler to generate that function automatically.\nWith the introduction of move constructors and move assignment operators, the rules for when automatic versions of constructors, destructors and assignment operators are generated has become quite complex. Using = default and = delete makes things easier as you don't need to remember the rules: you just say what you want to happen.\n\n\nA22:\n\nThe link provided by @MichaelHaren appears to provide the root cause.  The semicolon (as others have pointed out) is inherited from C.  But that doesn't explain why C used it in the first place.  The discussion includes this gem of an example:\nstruct fred { int x; long y; }; \nmain() \n{ \n  return 0; \n} \n\nOlder versions of C had an implicit int return type from a function unless declared otherwise.  If we omit the ; at the end of the structure definition, we're not only defining a new type fred, but also declaring that main() will return an instance of fred.  I.e. the code would be parsed like this:\nstruct fred { int x; long y; } main()\n{ \n  return 0; /* invalid return type, expected fred type */\n} \n\n\n\nA23:\n\nI do not use such declarations\nclass MyClass\n{\n.\n.\n.\n} MyInstance;\n\nBut in this case I can understand why is semicolon there.\nBecause it is like int a; - variable declaration.\nProbably for consistence as you can omit 'MyInstance' semicolon stays there.  \n\n\nA24:\n\nWhen you add the const keyword to a method the this pointer will essentially become a pointer to const object, and you cannot therefore change any member data. (Unless you use mutable, more on that later).\nThe const keyword is part of the functions signature which means that you can implement two similar methods, one which is called when the object is const, and one that isn't.\n#include <iostream>\n\nclass MyClass\n{\nprivate:\n    int counter;\npublic:\n    void Foo()\n    { \n        std::cout << \"Foo\" << std::endl;    \n    }\n\n    void Foo() const\n    {\n        std::cout << \"Foo const\" << std::endl;\n    }\n\n};\n\nint main()\n{\n    MyClass cc;\n    const MyClass& ccc = cc;\n    cc.Foo();\n    ccc.Foo();\n}\n\nThis will output\nFoo\nFoo const\n\nIn the non-const method you can change the instance members, which you cannot do in the const version. If you change the method declaration in the above example to the code below you will get some errors.\n    void Foo()\n    {\n        counter++; //this works\n        std::cout << \"Foo\" << std::endl;    \n    }\n\n    void Foo() const\n    {\n        counter++; //this will not compile\n        std::cout << \"Foo const\" << std::endl;\n    }\n\nThis is not completely true, because you can mark a member as mutable and a const method can then change it. It's mostly used for internal counters and stuff. The solution for that would be the below code.\n#include <iostream>\n\nclass MyClass\n{\nprivate:\n    mutable int counter;\npublic:\n\n    MyClass() : counter(0) {}\n\n    void Foo()\n    {\n        counter++;\n        std::cout << \"Foo\" << std::endl;    \n    }\n\n    void Foo() const\n    {\n        counter++;    // This works because counter is `mutable`\n        std::cout << \"Foo const\" << std::endl;\n    }\n\n    int GetInvocations() const\n    {\n        return counter;\n    }\n};\n\nint main(void)\n{\n    MyClass cc;\n    const MyClass& ccc = cc;\n    cc.Foo();\n    ccc.Foo();\n    std::cout << \"Foo has been invoked \" << ccc.GetInvocations() << \" times\" << std::endl;\n}\n\nwhich would output\nFoo\nFoo const\nFoo has been invoked 2 times\n\n\n\nA25:\n\nThis is a new C++0x feature that tells the compiler to create the default version of the respective constructor or assignment operator, i.e. the one which just performs the copy or move action for each member. This is useful because the move constructor isn't always generated by default (e.g. if you have a custom destructor), unlike the copy constructor (and likewise for assignment), but if there's nothing non-trivial to write, it's better to let the compiler handle it than to spell it out yourself each time.\nAlso notice that a default constructor would not be generated if you provide any other non-default constructor. If you still want the default constructor, too, you can use this syntax to have the compiler make one.\nAs another use case, there are several situations in which a copy constructor would not be generated implicitly (e.g. if you provide a custom move constructor). If you still want the default version, you can request it with this syntax.\nSee Section 12.8 of the standard for details.\n\n\nA26:\n\nIn const objects only const methods can be called. All fielnd in such a method considered as const field.\nLast issue has curious effect:\n\npointer becomes a const pointer int* const, which is not the same as a pointer to const const int*. Thus you can alter the object which pointer points to, but can't make pointer to point to another object.\nreference should become a const reference, but it is invariantly a const reference: you can not re-init it to another object. But again you can alter the object which reference refers to.\n\n\n\nA27:\n\nHere const means that at that function any variable's value can not change\nclass Test{\nprivate:\n    int a;\npublic:\n    void test()const{\n        a = 10;\n    }\n};\n\nAnd like this example, if you try to change the value of a variable in the test function you will get an error.\n\n\nA28:\n\n\n= 0 means that a function is pure virtual and you cannot instantiate an object from this class. You need to derive from it and implement this method\n= delete means that the compiler will not generate those constructors for you. AFAIK this is only allowed on copy constructor and assignment operator. But I am not too good at the upcoming standard.\n\n\n\nA29:\n\nIt is needed after a struct for compatibility reasons, and how would you like this:\nstruct MyStruct { ... };\nclass  MyClass  { ... }    //inconsistency\n\n\n\nA30:\n\nThe coding standards I've worked with have had the following for most of class declarations.\n//  coding standard: disallow when not used\nT(void)                  = delete; // default ctor    (1)\n~T(void)                 = delete; // default dtor    (2)\nT(const T&)              = delete; // copy ctor       (3)\nT(const T&&)             = delete; // move ctor       (4)\nT& operator= (const T&)  = delete; // copy assignment (5)\nT& operator= (const T&&) = delete; // move assignment (6)\n\nIf you use any of these 6, you simply comment out the corresponding line.  \nExample: class FizzBus require only dtor, and thus do not use the other 5.\n//  coding standard: disallow when not used\nFizzBuzz(void)                         = delete; // default ctor (1)\n// ~FizzBuzz(void);                              // dtor         (2)\nFizzBuzz(const FizzBuzz&)              = delete; // copy ctor    (3)\nFizzBuzz& operator= (const FizzBuzz&)  = delete; // copy assig   (4)\nFizzBuzz(const FizzBuzz&&)             = delete; // move ctor    (5)\nFizzBuzz& operator= (const FizzBuzz&&) = delete; // move assign  (6)\n\nWe comment out only 1 here, and install the implementation of it else where (probably where the coding standard suggests).  The other 5 (of 6) are disallowed with delete.  \nYou can also use '= delete' to disallow implicit promotions of different sized values ... example\n// disallow implicit promotions \ntemplate <class T> operator T(void)              = delete;\ntemplate <class T> Vuint64& operator=  (const T) = delete;\ntemplate <class T> Vuint64& operator|= (const T) = delete;\ntemplate <class T> Vuint64& operator&= (const T) = delete;\n\n\n\nA31:\n\nYou can't declare a variable without calling a constructor. However, in your example you could do the following:\nAnimal a(happyDay() ? \"puppies\" : \"toads\");\n\n\n\nA32:\n\nMeaning of a Const Member Function in C++ Common Knowledge: Essential Intermediate Programming gives a clear explanation:  \n\nThe type of the this pointer in a non-const member function of a class\n  X is X * const. That is, it\u2019s a constant pointer to a non-constant X\n  (see Const Pointers and Pointers to Const [7, 21]). Because the object\n  to which this refers is not const, it can be modified. The type of\n  this in a const member function of a class X is const X * const. That\n  is, it\u2019s a constant pointer to a constant X. Because the object to\n  which this refers is const, it cannot be modified. That\u2019s the\n  difference between const and non-const member functions.\n\nSo in your code:  \nclass foobar\n{\n  public:\n     operator int () const;\n     const char* foo() const;\n};\n\nYou can think it as this:  \nclass foobar\n{\n  public:\n     operator int (const foobar * const this) const;\n     const char* foo(const foobar * const this) const;\n};\n\n\n\nA33:\n\nIt is new in C++11, see here. It can be quite useful if you have defined one constructor, but want to use defaults for the others. Pre-C++11 you'd have to define all constructors once you have defined one, even if they are equivalent to the defaults.\nAlso note that in certain situations it is impossible to provide a user defined default constructor that behaves the same as the compiler synthesized one under both default and value initialization. default allows you to get that behaviour back.\n\n\nA34:\n\nIn C/C++ the ; is a statement terminator.\nAll statements are terminated with ; to avoid ambiguity (and to simplify parsing). \nThe grammar is consistent in this respect.\nEven though a class declaration (or any block for that matter)\nis multiple lines long and is\ndelimited with {} it is still simply a statement (the { } is part of the statement)\nhence needs to be terminated with ; (The ; is not a separator/delimitor)\nIn your example\nclass MyClass{...} MyInstance;\n\nis the complete statement.\nOne could define multiple instances of the declared class in a single statement\nclass MyClass{...} MyInstance1, MyInstance2;\n\nThis is completely consistent with declaring multiple instances of a primitive type in a single statement:\nint a, b, c;\n\nThe reason one does not often see such desclaration of class and instance, is the instance could ?only? be a global variable, and you don't really often want global objects unless they are static and/or Plain Old Data structures.\n\n\nA35:\n\nIn your particular example there is no reason for them.\nSometimes you could want to create a scope for a variable:\nfloat average;\n// ...\n\n{\nint sum = 0;\nfor (int i = 0; i < count; ++i)\n{\n   sum += v[i];\n}\naverage = (float)sum / count;\n}\n\n// use average\n// sum not in scope here\n\nHowever I see this an anti-pattern. Usually if you find yourself in need of doing this then most likely the for should be its own function.\n\n\nA36:\n\nHere the base class is not a nondependent base class( which means one with a complete type that can be determined without knowing the template arguments), and _foo_arg is a nondependent name. Standard C++ says that nondependent names are not looked up in dependent base classes.\nTo correct the code, it suffices to make the name _foo_arg dependent because dependent names can be looked up only at the time of instantiation, and at that time the exact base specialization that must be explored will be known. For example:\n// solution#1\nstd::cout << this->_foo_arg << std::endl;\n\nAn alternative consists in introducing a dependency using a qualified name:\n// solution#2\nstd::cout << Foo<T>::_foo_arg << std::endl;\n\nCare must be taken with this solution, because if the unqualified nondependent name is used to form a virtual function call, then the qualification inhibits the virtual call mechanism and the meaning of the program changes.\nAnd you can bring a name from a dependent base class in the derived class once by using:\n// solution#3\ntemplate <class T>\nclass Bar : public Foo<T> {\npublic:\n    ...\n    void BarFunc ();\nprivate:\n    using Foo<T>::_foo_arg;\n};\n\ntemplate <class T>\nvoid Bar<T>::BarFunc () {\n    std::cout << _foo_arg << std::endl;   // works\n}\n\n\n\nA37:\n\nI would like to add the following point.\nYou can also make it a const & and const &&\nSo,\nstruct s{\n    void val1() const {\n     // *this is const here. Hence this function cannot modify any member of *this\n    }\n    void val2() const & {\n    // *this is const& here\n    }\n    void val3() const && {\n    // The object calling this function should be const rvalue only.\n    }\n    void val4() && {\n    // The object calling this function should be rvalue reference only.\n    }\n\n};\n\nint main(){\n  s a;\n  a.val1(); //okay\n  a.val2(); //okay\n  // a.val3() not okay, a is not rvalue will be okay if called like\n  std::move(a).val3(); // okay, move makes it a rvalue\n}\n\n\nFeel free to improve the answer. I am no expert\n\n\nA38:\n\nIt's short for\nclass MyClass\n{\n.\n.\n.\n};\n\n// instance declaration\nMyClass MyInstance;  // semicolon here\n\nThe semicolon after the curly braces of the class declaration is actually overkill, but it is how C++ is defined. The semicolon after the variable declaration is always needed and makes sense.\n\n\nA39:\n\nThe semi-colon after the closing brace in a type declaration is required by the language.   It's been that way since the earliest versions of C. \nAnd yes, people do indeed do the declaration you just put up there.  It's useful for creating scoped types inside of methods.  \nvoid Example() {\n  struct { int x; } s1;\n  s1.x = 42;\n\n  struct ADifferentType { int x; };\n}\n\nIn this case, I think it's clear why the semi-colons are needed.  As to why it's needed in the more general case of declaring in the header file I'm unsure.  My guess is that it's historical and was done to make writing the compiler easier.   \n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "This is a \"C-style-language-gurus-only\""}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 44, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI've got msysGit (Git on Windows) working just fine on my home machine, but at work, where we are behind a Microsoft ISA proxy, I get the following error when I do a git clone:\nH:\\>git clone git://github.com/akitaonrails/vimfiles.git\nInitialized empty Git repository in H:/vimfiles/.git/\ngithub.com[0: 65.74.177.129]: errno=Invalid argument\nfatal: unable to connect a socket (Invalid argument)\n\nI've tried setting the http_proxy environment variable to:\nhttp://our-proxy-server:8088\n\nI've set the git http.proxy configuration directive:\ngit config --global http.proxy http://our-proxy-server:8088\n\nNeither of the above makes a difference.\nDoing a git clone with http:// instead of git:// yields this:\nH:\\>git clone http://github.com/akitaonrails/vimfiles.git\nInitialized empty Git repository in H:/vimfiles/.git/\nfatal: http://github.com/akitaonrails/vimfiles.git/info/refs download error - The    requested URL returned error: 407\n\n407 is of course an authentication error.\nSo my question is: has anyone managed to get git working from behind a proxy, specifically an ISA proxy? I don't know if it's going to be worth pursing this. Any help very much appreciated.\nThanks!\n\n\nCandidate answers:\nA1:\n\nIn TortoiseGit the answer is to right-click the project folder and click Show Log, then right-click the change you want to roll back to and click Switch/Checkout to this... . It will let you then proceed from that weird place in the commit stack, or branch in case you plan to commit and want things to stay sane.\n\n\nA2:\n\nIts simple: Browsers (Firefox works the same) query GET http://wpad/wpad.dat.\nIf a web server named wpad is resolveable, it should serve wpad.dat, a script file analog to netscape PAC files. MIME type must also be \"application/x-ns-proxy-autoconfig\".\n\n\nA3:\n\nYou can revert local commit by\ngit reset HEAD~N\n\nwhere N is used to revert number of commits. An example:\nif you have to revert single commit from local, then you can use \ngit reset HEAD~1\n\nor\n    git reset HEAD^\n\n\nA4:\n\nYes, you can. You can create a script and name it 'gb.bash' then add the following code to it\n@echo off\nset GITBRANCH=\nfor /f \"tokens=2\" %%I in ('git.exe branch 2^> NUL ^| findstr /b \"* \"') do set GITBRANCH=%%I\n\nif \"%GITBRANCH%\" == \"\" (\n    prompt $P$G \n) else (\n    prompt $P $C$E[10;7;32;47m%GITBRANCH%$E[0m$F $G \n)\n\nThen go to your environment variables and add the folder that contains this script to your path.\nWith that in place, simply firing the command 'gb' on your terminal while in a git repository will place the branch name on the path of the folder you're currently on.\nCurrently, this solution requires that you manually fire the command 'gb' to trigger the script. However, I'm sure there's also a way to trigger the script automatically when you launch CMD. I'll look into that some time and update this answer.\nI got the solution from these two sources:\nShow git branch in Windows command prompt\nDisplay git branch in Windows command prompt\n\nDisclaimer: You'll have to fire 'gb' after switching to a new branch\n  to update the displayed branch name.\n\n\n\nA5:\n\nI am using babun (discontinued)\nIt provides an awesome command-line view for git.\nIt also supports context menu and can be open from any location on right-click.\n\n\n\nA6:\n\nGitGui comes with git. It has always worked great for me. Is there some problem you have with it?\nScreenshot:\n\n\n\nA7:\n\nNot in command prompt but in PowerShell and Windows Terminal, it is possible. You can even have custom themes for terminal.\nHere are the steps:\n\nYou obviously need Git to begin with. Install Git for windows from Git website\nThen download \"Windows Terminal\" app from \"Windows Store\". It has multiple capabilities and new features and you can open multiple tabs of different terminals on the same window and so on.\nAfter installing \"Windows Terminal\", open it and and now you should install \"Posh-Git\" and \"Oh-My-Posh\" using PowerShell: (You may need to install NuGet if you don't already have it. Your PowerShell command line will ask if you want to install NuGet if this is the case. Select [Y] Yes. You may also need to approve that you are installing modules from PSGallery, an 'untrusted repository'. Select [Y] Yes.)\n\nInstall-Module posh-git -Scope CurrentUser\nInstall-Module oh-my-posh -Scope CurrentUser\n\n\nNow open your \"PowerShell\" profile with notepad $PROFILE and add these lines to end of the file: (this is a different profile than the \"Windows Terminal\" profile. PowerShell profile is a script that runs every time PowerShell starts.). If Set-PoshPrompt does not work here try to use Set-Theme command instead (https://superuser.com/questions/1629589/powershell-theming-doesnt-work-set-theme-not-found).\n\nImport-Module posh-git\nImport-Module oh-my-posh\nSet-PoshPrompt Paradox\n\n\nNow as you see some characters on the terminal are squares because the default font doesn't support them. (You may not have see this problem based on your terminal's font. if you see all of the characters fine and not squares, skip this step) You want to install \"Cascadia PL\" fonts to support those characters. Download them from Cascadia Code release page. You need to install these versions of the font in order for it to support the characters:\n\nCascadia Code PL\nCascadia Mono PL\n\n\nThen, in Windows Terminal, press Ctrl+, to open the Windows Terminal profile settings in \"settings.json\" and add these lines to the \"defaults\" section of the \"profiles\" as shown below:\n\n\"fontFace\": \"Cascadia Code PL\"\n\nHow to go to the Windows Terminal settings picture guide\nWhere to change the font face picture guide\nPS 1: If you want to have these changes on the integrated terminals such as the one on the VS Code, you should add this line to the settings of the VS Code:\n\"terminal.integrated.fontFamily\": \"Cascadia Code PL\"\n\nPS 2: In order to know more about \"Oh-My-Posh\" and change the theme of it, visit Oh-My-Posh's github page for more information.\n\nSources:\nhttps://learn.microsoft.com/en-us/windows/terminal/tutorials/powerline-setup\nhttps://www.hanselman.com/blog/how-to-make-a-pretty-prompt-in-windows-terminal-with-powerline-nerd-fonts-cascadia-code-wsl-and-ohmyposh\n\n\nA8:\n\n@Farid Shokri is not quite right.\n\nNot in command prompt but in PowerShell [...]\n\nThere's no native way but you can customize the cmd behaviour with AutoRun, Doskey, Prompt and a custom batch script.\nSolution\n\nCreate a batch script cd-git.bat (see below). We place it in C:\\Apps.\n\nCreate a AutoRun string value (either or)\n\nHKLM\\Software\\Microsoft\\Command Processor\\AutoRun\nHKCU\\Software\\Microsoft\\Command Processor\\AutoRun\n\nString Value:\nif exist C:\\Apps\\cd-git.bat doskey cd=C:\\Apps\\cd-git.bat $*\n\n\n\ncd-git.bat\n@echo off\nCD %*\nwhere git >nul 2>&1\nif %errorlevel% neq 0 (\n    goto :eof\n)\nfor /f \"usebackq tokens=* delims=\" %%g in (`git rev-parse --is-inside-work-tree ^>nul 2^>^&1 ^&^& git branch --show-current`) do (\n    set branchname=%%g\n)\nif \"%branchname%\"==\"\" (\n    prompt $p$_$$$s\n) else (\n    prompt $p $c$e[36m%branchname%$e[0m$f$_$$$s\n)\nset branchname=\n\nResult\nExample: Start a new cmd and navigate to a git repository.\nMicrosoft Windows [Version 10.0.19041.2673]\n(c) Microsoft Corporation. All rights reserved.\n\nC:\\Users>cd \\Develop\\git-test\n\nC:\\Develop\\git-test (main)\n$ cd ..\n\nC:\\Develop\n$\n\nAs soon as you cd into a git repository directory its current branch will be displayd within the parenthesis.\nExplanation\n\nThe AutoRun entry creates a new alias (macro) for the cd command by using the doskey command but only if our custom batch script cd-git.bat exists.\nThe doskey cd alias ensures that by using cd cd-git gets executed.\ncd-git\n\nMost important: cds into the requested directory.\nChecks whether or not the git command is available.\nIf so it checks whether there's a branch name for the current directory.\nIf so changes the prompt to display the branch name as well.\n\n\n\nReferences\n\nWin 10 Colors\nAutoRun\nDoskey\nPrompt Command\n\n\n\nA9:\n\nGit extensions for Windows works great and integrates with Visual Studio (if you need it). I haven't tried TortoiseGit, but I've heard it's not quite ready.\nOn Mac, I've tried GitX and wasn't very impressed. It seems to missing many of the git commands and I have to use the terminal on a regular basis. There is another one called Gity, but it's very new and I have not tried it yet. It is free now, but I think that will end once it reaches 1.0.\n\n\nA10:\n\nI'm surprised nobody has mentioned Tower for Mac OSX.\nHere is a screenshot:\n\n\n\nA11:\n\nWindows has TortoiseGit. It is not as mature as TortoiseSVN, but I've been using it and it works well enough for my purposes.\nScreenshot:\n\nEDIT [Dec 2014]: I'd also recommend looking at Dan's answer. Github's UI is probably the most mature/supported tool out there now (even if you don't use Github!)\n\n\nA12:\n\nI just noticed this on my machine as well.  The orange icon you're referring to is probably the Ubuntu icon you would normally get while running the \"Bash on Ubuntu on Windows\" app.  However, when you installed GIT, it also uses a version of bash. The app is called \"Git Bash\" in the start menu.\nWhen I simply typed \"Bash\" in the search box and then and pressed enter, Windows opened the \"best match\", which happens (in my case) to be GIT installation with MINGW64.  If you type \"Ubuntu\" instead, or if you look at the start menu's choices, you should see the other bash.\nThis is important to know that the two environments are separate, since programs from one environment may not be installed in the other. \n\n\nA13:\n\nThis info about WPAD (Web Proxy Auto Discovery) seems to describe the process in detail, though I have confirmed that what Tomalak says is also actually occurring.\n\n\nA14:\n\nDo you have SOCKS proxy? If you have, you caould try FreeCap to socksify your git connection. I've been using git this way for a while.\nIf not, still try FreeCap. IIRC it might be able to use http proxies, but I haven't tried that.\nEDIT:\nI usualy socksify cmd.exe using FreeCap, and from then on (almost) all cmdline programs that I start from that session are socksified too. That's why I recommended Free Cap, since SocksCap (another alternative) doesn't work that way.\nAs for using http.proxy, it for some reason never worked for me with mingw version and my company http proxies.\n\n\nA15:\n\nFor Tortoise Version 2.3.0:\nYou can do it in several ways, but fastest i think is that:\n\nRight click on the file\nClick the menu option \"Diff with previous version\"\nRight click on the left (previous) version of file\nClick the menu option \"Use this whole file\"\nSave the changes (ctrl+s)\n\n\n\nA16:\n\nOn windows (this is less of an issue on other systems, in my very humble opinion...), you must be VERY aware of crlf issues, and note that (Unless they've changed this in the very latest version of Git, which I believe they may have - or if you're using a very old version of Git), autocrlf is enabled by default, unlike all the other git installations.\nAlso note that unless you use the very latest version of msysgit, soon to be out this week if I recall correctly from the mailing list, your repository size can not be larger than 2GB, total.\nAdditionally, Windows is wierdly case insensitive but /sometimes/ case preserving - keep that in mind! (This does not confuse git neccessarily - but it can and does confuse the user of the git repo).\nFinally, git is substantially slower on windows than it is on linux, although it is (in my limited experience) faster than the alternatives.\nNow, regarding the path...\nUnless I'm mistaken, you should be able to just ensure that the main git binary is in the path - and that binary should then take care of referencing the other git components...   But I have not tested this.\n\n\nA17:\n\nTry Git Extensions.\nScreenshot:\n\n\n\nA18:\n\nI had the exact same error; but the ~/.gitconfig global config file was the key.\nIf you have a proxy with authentication you need to put in it:\ngit config --global http.proxy http://login:password@our-proxy-server:8088\n\nAnd it just works (with 'git clone http:')\nExample of a blog detailing the same setup: GIT clone over HTTP: who knew?\n\nIf it still fails in 407, it could be related to a problem with git-fetch losing authentication tokens on the second HTTP GET. A more recent version of libcurl might be needed.\n\nUpdate January 2011: jbustamovej mentions in his answer (upvoted) his blog post \"GitHub Behind Corporate Proxy\", which adds this:\n\nIt\u2019s important to note that if your login has a backslash, as in domain\\login, you must escape the backslash, as in:\n\ngit config --global http.proxy http://domain\\\\\\login:password@our-proxy-server:8088\n\n\n\nA19:\n\ngit reset --hard remotes/origin/YOUR/BRANCH\n\nbetter than /HEAD because you won't see this:\n$ git status\nOn branch MY/BRANCH\nYour branch and 'origin/MY/BRANCH' have diverged,\nand have 1 and 1 different commit each, respectively.\n\n\n\nA20:\n\nAs per my understading, you create some commit that you have pushed on central repo, after that you have create some more commit, but these are exist on local. These all did not push on central repo.\nTo remove/revert local commit;\ngit reset HEAD~{number_of_commit}\n\nsimply, you hit git log on your command prompt and get list of commits. Have a look, how many commit you have created now and how many you have to revert.\nfor example, you have to remove.revert your two last commit then hit\ngit reset HEAD~2\n\nThere is one another way to reset your local repo with central repo. But, in this case your local commit will be removed and if other users push commit on central repo then your repo will be updated with that.\ncommand is :\ngit reset --hard remotes/origin/HEAD\n\n\n\nA21:\n\nIt is also possible to get a specific file:\n\nRight-click on the file \nChoose the menu item: Git Show Log\n\nTortoiseGit show a dialog with a list of all revisions\n\nClick on the prior revision (2) \n\nIn a list in the bottom of the dialog it shows all the files associated with the commit\n\nFind the file you are looking to view \nRight-click on that file and Save Revision To  \nThis will save that previous revision of the file with the commit number concatenated on the file\n\nFor example:\n - original file is file.txt\n - revision 2 will save as file-67b51a8.txt\nI'm using TortoiseGit v2.2.0.0\n\n\nA22:\n\nThis is the git.bat I am using. I got the answer from the following link:\nhttps://www.nu42.com/2016/05/display-git-branch-windows-command-prompt.html\nFirst, create the git.bat file in a folder, then add the folder to the PATH and ahead of the line to git.exe (I assume you already have the git.exe installed in your computer). This will make sure every time you type git in your command line, the new git.bat will be triggered instead of the git.exe.\n@echo off\ngit.exe %*\nset GITBRANCH=\nfor /f %%I in ('git.exe rev-parse --abbrev-ref HEAD 2^> NUL') do set GITBRANCH=%%I\n\nif \"%GITBRANCH%\" == \"\" (\n  prompt $P$G \n) else (\n    prompt $P $C$E[32;7;32;47m%GITBRANCH%$E[0m$F $G \n)\n\n\n\nA23:\n\nIf you are using Windows PowerShell, you can override the standard \"Prompt\" function by running following one in the PowerShell window you are using.  This will make it detect Git repositories and list the Git branch in the prompt string:\nFunction Prompt\n{\n    $git_cmd = \"git rev-parse --abbrev-ref HEAD\"\n    Invoke-Expression $git_cmd 2> $null | Tee-Object -Variable git_branch | Out-Null\n    $git_branch_text = $None\n    if ( $git_branch -And -Not $git_branch.StartsWith($git_cmd)) \n    {\n        $git_branch_text = \"[$git_branch] \"\n    }\n\n    $stringBuilder = New-Object System.Text.StringBuilder\n    $null = $stringBuilder.Append(\"PS \")\n    if ($git_branch_text) { $null = $stringBuilder.Append($git_branch_text) }\n    $null = $stringBuilder.Append($($executionContext.SessionState.Path.CurrentLocation))\n    $null = $stringBuilder.Append($('>' * ($nestedPromptLevel + 1)))\n    $null = $stringBuilder.Append(\" \")\n    return $stringBuilder.ToString()\n}\n\nPS C:\\Users\\username\\Document\\GitHub>\nPS C:\\Users\\username\\Document\\GitHub>cd code_repository\nPS [dev] C:\\Users\\username\\Document\\GitHub\\code_repository>\n\nPS [dev] C:\\Users\\username\\Document\\GitHub\\code_repository>cd ..\nPS C:\\Users\\username\\Document\\GitHub>\n\nTested on PowerShell version 5.1.17763.592.\nTo apply this change of prompt in all of your PowerShell command windows, put this function in a file named profile.ps1 in C:\\Users\\username\\Documents\\WindowsPowerShell.  Any PowerShell command windows opened after that should have the Git prompt when in a directory that is part of a Git repository.\n\n\nA24:\n\nYou may want to be aware that:\n\nAll git commands are not still there. On MSysGit1.6.2 early March 2009:  archimport, cvsexportcommit, cvsimport, cvsserver, filter-branch, instaweb, send-email, and shell.)\nUntil MSysGit1.6.2, git-svn was not there (it is now).\nThe problem was git-svn needed subversion's perl bindings, and you can only build them as dynamically loadable modules. And MSysGit had a perl version that did not support dynamically loadable modules.\nAll details about MSysGit are best explained in their MSysGitHerald Github wiki \n\n\n\nA25:\n\nUpdated my answer, based on these comments:\n\nby Alexander.\nby mlibby\nby Chris Moschini\n\n\nSuppose the working tree is clean and you want:\n\nCheckout some file(s) of its previous revision\nTesting\nRevert to current revision\n\n\n\nCheckout some file(s) of its previous revision\n(a) Right click the file you want to revert and Show Log for that file\n\n(b) Right click the file in file list and perform Revert to parent revision\n\n\n(c) repeat (a), (b) until you get all files you want.\n\nTesting\nRevert to current revision\n(a) perform Revert... in context menu of explorer\nThis way, you can choose the file(s) you want to revert.\n(b) or this quick way: perform Reset Hard in Log dialog\nThis way, all changed files revert. (=> Lost all working dir changes)\n\n\n\n(Tested on TortoiseGit 1.8.16.1, GfW 2.6.4.windows.1, Win 10 64bit)\n\n\nA26:\n\nFrom the command line: git checkout is probably what you want.\nThe documentation shows an example of:\n$ git checkout master~2 Makefile\nto revert Makefile to two revisions back in the master branch\nFrom within TortoiseGit (via Windows Explorer) it looks like you can do this with the following steps:\n\nNavigate in Explorer to the folder where the file is.\nRight-click on the file you want to revert, choose Show log from the TortoiseGit context menu\nIn the top section (\"graph\") select the revision that has the version of the file you want to revert to\nIn the third section (file list) right-click the file and choose Revert to this revision\nYou should get a message like 1 files revert to e19a77\n\n\nA27:\n\nMINGW64 is 'Minimum GNU for Windows 64 bit'. It's the name of a compiler used to build an extra copy of bash that \"git for Windows\" includes.\n\"Git for Windows\" includes git plus a variety of additional software, which you may already have on your machine. This includes:\n\nbash (which already comes with Windows 10 and 11)\nssh (which is also now included in Windows 10 and 11)\nShell preferences for a PS1 (the prompt, where the pink MINGW64 comes from), a shortcut file, and some other tools.\n\nAs PS1 tastes are generally personal you may prefer your existing setup.\nIf you want a real Unix environment on Windows\nYou can use WSL, and simply install real Linux x64 apps on the WSL Linux distribution.\nIf you want a Windows-native git, but don't want all the extra software\n\nYou can get a minimal build from nuget if you don't have any git apps installed.\n\nBut if you use a GUI that may also include an embedded git - personally I use the one included with Sourcetree. In my powershell profile:\nAdd-PathVariable \"${env:UserProfile}\\AppData\\Local\\Atlassian\\SourceTree\\git_local\\bin\"\n\n\nThis provides git without any additional changes to your system or extra copies of other software.\n(Add-Pathvariable comes from PSCX by the way)\n\n\nA28:\n\nThe IE configuration described enables a WPAD implementation. Here's the Microsoft explanation of the entire mechanism (probably too much detail for a single post).\n\n\nA29:\n\nFor Windows, I recommend the GitHub for Windows client (which also works with other local and remote repositories, even though it's setup to work with Github by default).  \nHere is a screenshot:\n\n\n\nA30:\n\nThere is a native port of Git to Windows, called mysysgit.\nIf you use Eclipse, there's a Git client that hooks into that - egit.\nTortoiseGit is a standalone GUI client.\nGit#/GitSharp is a client for git for .NET.\nMac users will hopefully be able to provide some examples for MacOS.\nEnjoy it!\n\n\nA31:\n\nEasiest way to remove 'MINGW64' is to comment two lines in file:\n\\Git\\etc\\profile.d\\git-prompt.sh\n...\n# PS1=\"$PS1\"'\\[\\033[35m\\]'       # change to purple\n# PS1=\"$PS1\"'$MSYSTEM '          # show MSYSTEM\n\n\n\nA32:\n\nYou cannot do this from the Windows-based \"Command Prompt.\" There is an environment, however, that can run on top of your Windows environment, that does.\nAfter you run the Git windows installer and allow git to install, you can invoke git-bash from context menu.\n\nBy default, the git-bash environment shows what branch you are in.\n\n\n\nA33:\n\nThe MSYS Git installer suggests option 2 if you intend on running git from a cygwin prompt. The cygwin environment ensures that git dependencies are in your PATH. If you choose this option but then invoke git from a windows command prompt, all of the unix-y command-line utilities that git relies upon will not be found. IIRC, git itself is partially implemented as bash scripts. I don't know which operations will fail, but I do not think that git will be usable this way.\nI don't have a list of the system tools that option 3 overrides (the installer mentions find.exe) but this would only affect you if you are a batch-script ninja. At the command-line, find will now refer to the unix utility of that name, not the exe that ships with windows. It doesn't harm windows itself in any way.\nJust run with scissors and choose option 3 :-)\n\n\nA34:\n\nIt's a little different in the new git versions.\n\ncopy the line below in /etc/bash.bashrc:\n\nexport PS1='\\[\\e]0;\\w\\a\\]\\n\\[\\e[32m\\]\\u@\\h \\[\\e[35m\\]$MSYSTEM\\[\\e[0m\\] \\[\\e[33m\\]\\w\\[\\e[0m\\]\\n'\"${_ps1_symbol}\"' '\n\nadd it to ~/.bashrc and do some custom edit:\n\nexport PS1='\\[\\e]0;\\w\\a\\]\\n\\[\\e[32m\\]\\u@\\h \\[\\e[33m\\]\\w\\[\\e[0m\\]\\n '\n\nIf you do not like the additional blank line when press Enter, just remove the \\n above.\nsource the ~/.bashrc file:\n\nsource ~/.bashrc\n\nmy terminal example:\n\n\n\n\nA35:\n\nThis is the new icon being used with Git for Windows 2.x. The website needs an update for that I guess but there are not many volunteers typically. \nThe MINGW64 is the value from the MSYSTEM environment variable. This has been included in the bash prompt by setting PS1 in the /etc/bash.bashrc file. You can either override this in your own $HOME/.profile or edit the distributed one to remove that if you prefer.\n\n\nA36:\n\nSource Tree (for MAC)\nScreenshot:\n\n\n\nA37:\n\nexport PS1=\"${PS1/\\$TITLEPREFIX:}\"; export PS1=\"${PS1/\\$MSYSTEM }\" >> ~/.bashrc\n\n\nA38:\n\nHere's one for Mac: GitX\nScreenshot:\n\n\n\nA39:\n\nIn the vein of teaching how to fish: take a look at https://git.wiki.kernel.org/index.php/InterfacesFrontendsAndTools page on Git Wiki, which has section about GUIs.\nGit Homepage also has section about GUIs: http://git-scm.com/downloads/guis\n\n\nA40:\n\nWhen you use the Windows GUI on windows, and you create your very first repository, do not type in the name \".git\" for the repository directory. (Which it will then create, and then create another .git folder underneath it, when you finally think to look there)  Browse to the folder with your sources in it, and -- just choose that folder!  The repository directory \".get\" will be created FOR you.\nThen you see files in your unstaged changes, and by clicking on the little page icons next to the filenames, move them into the staged changes.\nAnd definitely run with scissors and select option 3. No one uses kill, sort or find anymore from the command windows line.\n\n\nA41:\n\nIt's a network broadcast, usually using DHCP. \nThat there wikipedia page should tell you all you need to know. \n\n\nA42:\n\nAs stated by @patthoyts\n\nMINGW64 is the new icon being used with Git for Windows 2.x.\nThe MINGW64 is the value from the MSYSTEM environment variable. This has been included in the bash prompt by setting PS1 in the /etc/bash.bashrc file. You can either override this in your own $HOME/.profile or edit the distributed one to remove that if you prefer.\n\nMINGW64 and the name of the system is something that we don't want to or need to see always and sometimes may want to remove them for effective use of display space. To do that, we just need to comment out 4 lines in \\Git\\etc\\profile.d\\git-prompt.sh which is typically inside your program files.\nPS1=\"$PS1\"'\\[\\033[32m\\]'       # change to green\nPS1=\"$PS1\"'\\u@\\h '             # user@host<space>\nPS1=\"$PS1\"'\\[\\033[35m\\]'       # change to purple\nPS1=\"$PS1\"'$MSYSTEM '          # show MSYSTEM\n\nThese are the 4 lines in git-prompt.sh, typically line number 14 to 17, that can be commented out by placing # at the start of each line to remove computer name shown in green and MSYSTEM shown in purple. If you just want to remove one of these, you can comment lines accordingly. It should look similar to this\n# PS1=\"$PS1\"'\\[\\033[32m\\]'       # change to green\n# PS1=\"$PS1\"'\\u@\\h '             # user@host<space>\n# PS1=\"$PS1\"'\\[\\033[35m\\]'       # change to purple\n# PS1=\"$PS1\"'$MSYSTEM '          # show MSYSTEM\n\n\n\nA43:\n\nIf you feel sure about that and don't have any local uncommitted changes:\ngit reset --hard origin/master\n\nwhere origin/master is the branch you had pushed to.\nThe ref-log will still contain the reverted bits, until a garbage collect expires them. To revert the revert,\ngit reset --hard HEAD@{1}\n\n\n\nA44:\n\nJust a small addition. If Windows is installed on C Drive, then this is the path to edit the file git-prompt.sh and comment out the desired lines (usually 14-17):\nC:\\Program Files\\Git\\etc\\profile.d\n\n\n\nA45:\n\ngit reset --hard remotes/origin/HEAD\n\n\n\nA46:\n\nThe easiest way is to use:\n$ parse_git_branch() { git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ (\\1)/'; }\n$ export PS1=\"\\u@ \\[\\033[32m\\]\\w\\[\\033[33m\\]\\$(parse_git_branch)\\[\\033[00m\\] $ \"\n\n\n\nA47:\n\nYou get bash regardless of which option you pick, the latter options just add methods for using Git outside it.\nFor the latter options, msysgit adds Windows builds of common Linux utilities to PATH. This includes find, kill and sort, as well as cp, ls, rm, and about 20-30 others.\nThe problem with the first 3 (and similar) is that they exist in both OSs and function differently in each.\nNot a huge ordeal if you know which one you'll be using, but any applications developed expecting one and getting the other will surely throw a fit.\n\nTo prevent the conflict, while still having Git work as expected, you can create a simple batch script that adjusts PATH only for the session. (e.g., readygit.bat)\n@echo off\nsetlocal\nset PATH=C:\\Git\\bin;%PATH%\ncmd\n\nAdjust C:\\Git\\bin accordingly. But, just run this and use Git within the cmd.\nWith this, you can use install option 3 and safely remove C:\\Git\\bin from your system's PATH, removing any confusion for Windows apps without confusing Git.\nI currently use a similar script with GnuWin apps, including find.\n\n\nA48:\n\nFor OSX I don't like Tower. I have had too many crashes and problems. On the other hand, Source Tree works very well. Albeit it requires a bit more understanding of git itself. IMO Tower is no Cornerstone. I wish Cornerstone would support GIT.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "(vs. \"I am using the best person to help you, please give me the name of the person you want to know about.\n(vs."}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 45, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nWhen doing an ALTER TABLE statement in MySQL, the whole table is read-locked (allowing concurrent reads, but prohibiting concurrent writes) for the duration of the statement. If it's a big table, INSERT or UPDATE statements could be blocked for a looooong time. Is there a way to do a \"hot alter\", like adding a column in such a way that the table is still updatable throughout the process?\nMostly I'm interested in a solution for MySQL but I'd be interested in other RDBMS if MySQL can't do it.\nTo clarify, my purpose is simply to avoid downtime when a new feature that requires an extra table column is pushed to production. Any database schema will change over time, that's just a fact of life. I don't see why we should accept that these changes must inevitably result in downtime; that's just weak.\n\n\nCandidate answers:\nA1:\n\nPDOStatement has a public property $queryString. It should be what you want.\nI've just notice that PDOStatement has an undocumented method debugDumpParams() which you may also want to look at.\n\n\nA2:\n\nTake a look at the FIND_IN_SET function for MySQL.\nSELECT * \n    FROM shirts \n    WHERE FIND_IN_SET('1',colors) > 0\n\n\n\nA3:\n\nCorrect, the solution to fixing this is to use :select and to select each field that you wish to decorate the resulting object with and group by them.\nNasty - but it is how group by should work as opposed to how MySQL works with it by guessing what you mean if you don't stick fields in your group by.\n\n\nA4:\n\nA bit late probably but now there is PDOStatement::debugDumpParams\n\nDumps the informations contained by a prepared statement directly on\n  the output. It will provide the SQL query in use, the number of\n  parameters used (Params), the list of parameters, with their name,\n  type (paramtype) as an integer, their key name or position, and the\n  position in the query (if this is supported by the PDO driver,\n  otherwise, it will be -1).\n\nYou can find more on the official php docs\nExample:\n<?php\n/* Execute a prepared statement by binding PHP variables */\n$calories = 150;\n$colour = 'red';\n$sth = $dbh->prepare('SELECT name, colour, calories\n    FROM fruit\n    WHERE calories < :calories AND colour = :colour');\n$sth->bindParam(':calories', $calories, PDO::PARAM_INT);\n$sth->bindValue(':colour', $colour, PDO::PARAM_STR, 12);\n$sth->execute();\n\n$sth->debugDumpParams();\n\n?>\n\n\n\nA5:\n\nUse MySQL FIND_IN_SET function:\n  SELECT * \n    FROM table_name \n   WHERE id IN (..,..,..,..) \nORDER BY FIND_IN_SET (coloumn_name, .., .., ..);\n\n\n\nA6:\n\nIf you want to do arbitrary sorting on a query using values inputted by the query in MS SQL Server 2008+, it can be done by creating a table on the fly and doing a join like so (using nomenclature from OP).\nSELECT table1.name, table1.description ... \nFROM (VALUES (id1,1), (id2,2), (id3,3) ...) AS orderTbl(orderKey, orderIdx) \nLEFT JOIN table1 ON orderTbl.orderKey=table1.id\nORDER BY orderTbl.orderIdx\n\nIf you replace the VALUES statement with something else that does the same thing, but in ANSI SQL, then this should work on any SQL database.\nNote: \nThe second column in the created table (orderTbl.orderIdx) is necessary when querying record sets larger than 100 or so. I originally didn't have an orderIdx column, but found that with result sets larger than 100 I had to explicitly sort by that column; in SQL Server Express 2014 anyways.\n\n\nA7:\n\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic static function interpolateQuery($query, $params) {\n    $keys = array();\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n    }\n\n    $query = preg_replace($keys, $params, $query, 1, $count);\n\n    #trigger_error('replaced '.$count.' keys');\n\n    return $query;\n}\n\n\n\nA8:\n\nAns to get sorted data.\nSELECT ...\nFROM ...\nORDER  BY FIELD(user_id,5,3,2,...,50)  LIMIT 10\n\n\n\nA9:\n\nA solution is to voluntarily put an error in the query and to print the error's message:\n//Connection to the database\n$co = new PDO('mysql:dbname=myDB;host=localhost','root','');\n//We allow to print the errors whenever there is one\n$co->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n//We create our prepared statement\n$stmt = $co->prepare(\"ELECT * FROM Person WHERE age=:age\"); //I removed the 'S' of 'SELECT'\n$stmt->bindValue(':age','18',PDO::PARAM_STR);\ntry {\n    $stmt->execute();\n} catch (PDOException $e) {\n    echo $e->getMessage();\n}\n\nStandard output:\n\nSQLSTATE[42000]: Syntax error or access violation: [...] near 'ELECT * FROM Person WHERE age=18' at line 1\n\nIt is important to note that it only prints the first 80 characters of the query.\n\n\nA10:\n\nThe only other option is to do manually what many RDBMS systems do anyway...\n- Create a new table  \nYou can then copy the contents of the old table over a chunk at a time.  Whilst always being cautious of any INSERT/UPDATE/DELETE on the source table.  (Could be managed by a trigger.  Although this would cause a slow down, it's not a lock...)\nOnce finished, change the name of the source table, then change the name of the new table.  Preferably in a transaction.\nOnce finished, recompile any stored procedures, etc that use that table.  The execution plans will likely no longer be valid.\nEDIT:\nSome comments have been made about this limitation being a bit poor.  So I thought I'd put a new perspective on it to show why it's how it is...\n\nAdding a new field is like changing one field on every row.  \nField Locks would be much harder than Row locks, never mind table locks.\n\nYou're actually changing the physical structure on the disk, every record moves.  \nThis really is like an UPDATE on the Whole table, but with more impact...  \n\n\n\nA11:\n\nThis will work for sure, and I actually tried it out:\nlwdba@localhost (DB test) :: DROP TABLE IF EXISTS shirts;\nQuery OK, 0 rows affected (0.08 sec)\n\nlwdba@localhost (DB test) :: CREATE TABLE shirts\n    -> (<BR>\n    -> id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    -> ticketnumber INT,\n    -> colors VARCHAR(30)\n    -> );<BR>\nQuery OK, 0 rows affected (0.19 sec)\n\nlwdba@localhost (DB test) :: INSERT INTO shirts (ticketnumber,colors) VALUES\n    -> (32423,'1,2,5,12,15'),\n    -> (32424,'1,5,12,15,30'),\n    -> (32425,'2,5,11,15,28'),\n    -> (32426,'1,2,7,12,15'),\n    -> (32427,'2,4,8,12,15');\nQuery OK, 5 rows affected (0.06 sec)\nRecords: 5  Duplicates: 0  Warnings: 0\n\nlwdba@localhost (DB test) :: SELECT * FROM shirts WHERE LOCATE(CONCAT(',', 1 ,','),CONCAT(',',colors,',')) > 0;\n+----+--------------+--------------+\n| id | ticketnumber | colors       |\n+----+--------------+--------------+\n|  1 |        32423 | 1,2,5,12,15  |\n|  2 |        32424 | 1,5,12,15,30 |\n|  4 |        32426 | 1,2,7,12,15  |\n+----+--------------+--------------+\n3 rows in set (0.00 sec)\n\nGive it a Try !!!\n\n\nA12:\n\nUse MySQL's FIELD() function:\nSELECT name, description, ...\nFROM ...\nWHERE id IN([ids, any order])\nORDER BY FIELD(id, [ids in order])\n\nFIELD() will return the index of the first parameter that is equal to the first parameter (other than the first parameter itself).\nFIELD('a', 'a', 'b', 'c')\nwill return 1\nFIELD('a', 'c', 'b', 'a')\nwill return 3\nThis will do exactly what you want if you paste the ids into the IN() clause and the FIELD() function in the same order.\n\n\nA13:\n\nThe classic way would be to add commas to the left and right:\nselect * from shirts where CONCAT(',', colors, ',') like '%,1,%'\n\nBut find_in_set also works:\nselect * from shirts where find_in_set('1',colors) <> 0\n\n\n\nA14:\n\nIf the set of colors is more or less fixed, the most efficient and also most readable way would be to use string constants in your app and then use MySQL's SET type with FIND_IN_SET('red',colors) in your queries. When using the SET type with FIND_IN_SET, MySQL uses one integer to store all values and uses binary \"and\" operation to check for presence of values which is way more efficient than scanning a comma-separated string.\nIn SET('red','blue','green'), 'red' would be stored internally as 1, 'blue' would be stored internally as 2 and 'green' would be stored internally as 4. The value 'red,blue' would be stored as 3 (1|2) and 'red,green' as 5 (1|4).\n\n\nA15:\n\nIf you're using MySQL, there is a method REGEXP that you can use...\nhttp://dev.mysql.com/doc/refman/5.1/en/regexp.html#operator_regexp\nSo then you would use:\nSELECT * FROM `shirts` WHERE `colors` REGEXP '\\b1\\b'\n\n\n\nA16:\n\nselect * from shirts where find_in_set('1',colors) <> 0\n\nWorks for me\n\n\nA17:\n\nGive this a shot:\nSELECT name, description, ...\nWHERE id IN\n    (SELECT id FROM table1 WHERE...)\nORDER BY\n    (SELECT display_order FROM table1 WHERE...),\n    (SELECT name FROM table1 WHERE...)\n\nThe WHEREs will probably take a little tweaking to get the correlated subqueries working properly, but the basic principle should be sound.\n\n\nA18:\n\nI think that .uniq [1] will solve your problem. \n[1] Available.select('...').uniq\n\nTake a look at http://guides.rubyonrails.org/active_record_querying.html#selecting-specific-fields\n\n\nA19:\n\nI assume you mean that you want the final SQL query, with parameter values interpolated into it.  I understand that this would be useful for debugging, but it is not the way prepared statements work.  Parameters are not combined with a prepared statement on the client-side, so PDO should never have access to the query string combined with its parameters.\nThe SQL statement is sent to the database server when you do prepare(), and the parameters are sent separately when you do execute().  MySQL's general query log does show the final SQL with values interpolated after you execute().  Below is an excerpt from my general query log.  I ran the queries from the mysql CLI, not from PDO, but the principle is the same.\n081016 16:51:28 2 Query       prepare s1 from 'select * from foo where i = ?'\n                2 Prepare     [2] select * from foo where i = ?\n081016 16:51:39 2 Query       set @a =1\n081016 16:51:47 2 Query       execute s1 using @a\n                2 Execute     [2] select * from foo where i = 1\n\nYou can also get what you want if you set the PDO attribute PDO::ATTR_EMULATE_PREPARES.  In this mode, PDO interpolate parameters into the SQL query and sends the whole query when you execute().  This is not a true prepared query.  You will circumvent the benefits of prepared queries by interpolating variables into the SQL string before execute().\n\nRe comment from @afilina:\nNo, the textual SQL query is not combined with the parameters during execution. So there's nothing for PDO to show you.\nInternally, if you use PDO::ATTR_EMULATE_PREPARES, PDO makes a copy of the SQL query and interpolates parameter values into it before doing the prepare and execute. But PDO does not expose this modified SQL query. \nThe PDOStatement object has a property $queryString, but this is set only in the constructor for the PDOStatement, and it's not updated when the query is rewritten with parameters.\nIt would be a reasonable feature request for PDO to ask them to expose the rewritten query. But even that wouldn't give you the \"complete\" query unless you use PDO::ATTR_EMULATE_PREPARES.\nThis is why I show the workaround above of using the MySQL server's general query log, because in this case even a prepared query with parameter placeholders is rewritten on the server, with parameter values backfilled into the query string. But this is only done during logging, not during query execution.\n\n\nA20:\n\nSELECT * FROM table WHERE field1 NOT LIKE '%$x%';   (Make sure you escape $x properly beforehand to avoid SQL injection)\nEdit: NOT IN does something a bit different - your question isn't totally clear so pick which one to use.   LIKE 'xxx%' can use an index.  LIKE '%xxx' or LIKE '%xxx%' can't.\n\n\nA21:\n\nSee following how to get sorted data.\nSELECT ...\n  FROM ...\n WHERE zip IN (91709,92886,92807,...,91356)\n   AND user.status=1\nORDER \n    BY provider.package_id DESC \n     , FIELD(zip,91709,92886,92807,...,91356)\nLIMIT 10\n\n\n\nA22:\n\nIf you don't have an aggregate function in your where clause, another possible source of the 1111 - Invalid use of group function error is if you have nested aggregate functions:\nselect sum(avg(close)) from prices;\n(1111, 'Invalid use of group function')\n\nYou can get around this by breaking up the problem into two steps:\n\nSave the inner aggregation into a variable\n\nselect @avg:=avg(close) from prices;\n\n\nRun the outer aggregation against the variable\n\nselect sum(@avg) from prices;\n\n\n\nA23:\n\nFor Oracle, John's solution using instr() function works. Here's slightly different solution that worked - \nSELECT id \nFROM table1 \nWHERE id IN (1, 20, 45, 60) \nORDER BY instr('1, 20, 45, 60', id)\n\n\nA24:\n\nSELECT ORDER_NO, DELIVERY_ADDRESS \nfrom IFSAPP.PURCHASE_ORDER_TAB \nwhere ORDER_NO in ('52000077','52000079','52000167','52000297','52000204','52000409','52000126') \nORDER BY instr('52000077,52000079,52000167,52000297,52000204,52000409,52000126',ORDER_NO)\n\nworked really great\n\n\nA25:\n\nTwo solutions that spring to mind:\n\norder by case id when 123 then 1 when 456 then 2 else null end asc\norder by instr(','||id||',',',123,456,') asc\n\n(instr() is from Oracle; maybe you have locate() or charindex() or something like that)\n\n\nA26:\n\nI spent a good deal of time researching this situation for my own needs. This and several other SO threads helped me a great deal, so I wanted to share what I came up with.\nWhile having access to the interpolated query string is a significant benefit while troubleshooting, we wanted to be able to maintain a log of only certain queries (therefore, using the database logs for this purpose was not ideal). We also wanted to be able to use the logs to recreate the condition of the tables at any given time, therefore, we needed to make certain the interpolated strings were escaped properly. Finally, we wanted to extend this functionality to our entire code base having to re-write as little of it as possible (deadlines, marketing, and such; you know how it is).\nMy solution was to extend the functionality of the default PDOStatement object to cache the parameterized values (or references), and when the statement is executed, use the functionality of the PDO object to properly escape the parameters when they are injected back in to the query string. We could then tie in to execute method of the statement object and log the actual query that was executed at that time (or at least as faithful of a reproduction as possible).\nAs I said, we didn't want to modify the entire code base to add this functionality, so we overwrite the default bindParam() and bindValue() methods of the PDOStatement object, do our caching of the bound data, then call parent::bindParam() or parent::bindValue(). This allowed our existing code base to continue to function as normal.\nFinally, when the execute() method is called, we perform our interpolation and provide the resultant string as a new property E_PDOStatement->fullQuery. This can be output to view the query or, for example, written to a log file.\nThe extension, along with installation and configuration instructions, are available on github:\nhttps://github.com/noahheck/E_PDOStatement\nDISCLAIMER:\nObviously, as I mentioned, I wrote this extension. Because it was developed with help from many threads here, I wanted to post my solution here in case anyone else comes across these threads, just as I did.\n\n\nA27:\n\nNope. If you are using MyISAM tables, to my best understanding they only do table locks - there are no record locks, they just try to keep everything hyperfast through simplicity. (Other MySQL tables operate differently.) In any case, you can copy the table to another table, alter it, and then switch them, updating for differences.\nThis is such a massive alteration that I doubt any DBMS would support it. It's considered a benefit to be able to do it with data in the table in the first place.\n\n\nA28:\n\nWhat kind of field is this? The IN operator cannot be used with a single field, but is meant to be used in subqueries or with predefined lists:\n-- subquery\nSELECT a FROM x WHERE x.b NOT IN (SELECT b FROM y);\n-- predefined list\nSELECT a FROM x WHERE x.b NOT IN (1, 2, 3, 6);\n\nIf you are searching a string, go for the LIKE operator (but this will be slow):\n-- Finds all rows where a does not contain \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE '%text%';\n\nIf you restrict it so that the string you are searching for has to start with the given string, it can use indices (if there is an index on that field) and be reasonably fast:\n-- Finds all rows where a does not start with \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE 'text%';\n\n\n\nA29:\n\nMy first thought was to write a single query, but you said that was not possible because one is run by the user and the other is run in the background.  How are you storing the list of ids to pass from the user to the background process?  Why not put them in a temporary table with a column to signify the order.\nSo how about this:\n\nThe user interface bit runs and inserts values into a new table you create.  It would insert the id, position and some sort of job number identifier)\nThe job number is passed to the background process (instead of all the ids)\nThe background process does a select from the table in step 1 and you join in to get the other information that you require.  It uses the job number in the WHERE clause and orders by the position column.\nThe background process, when finished, deletes from the table based on the job identifier.\n\n\n\nA30:\n\nFIND_IN_SET is your friend in this case\nselect * from shirts where FIND_IN_SET(1,colors) \n\n\n\nA31:\n\nAccording to MySQL's \"Debuking GROUP BY Myths\" http://dev.mysql.com/tech-resources/articles/debunking-group-by-myths.html. SQL (2003 version of the standard) doesn't requires columns referenced in the SELECT list of a query to also appear in the GROUP BY clause.\n\n\nA32:\n\nI think you should manage to store your data in a way that you will simply do a join and it will be perfect, so no hacks and complicated things going on.\nI have for instance a \"Recently played\" list of track ids, on SQLite i simply do:\nSELECT * FROM recently NATURAL JOIN tracks;\n\n\n\nA33:\n\nAlternative method:\nSELECT * FROM x WHERE CHARINDEX('$x', field1, 0)<1 \n\nExplanation: It searches for occurrences of '$x' starting from position 0. And only selects where count is 'less than one'.\n\n\nA34:\n\nYou can achieve this by following function.\nRun following query to create function.\nDELIMITER ||\nCREATE FUNCTION `TOTAL_OCCURANCE`(`commastring` TEXT, `findme`     VARCHAR(255)) RETURNS int(11)\nNO SQL\n-- SANI: First param is for comma separated string and 2nd for string to find.\nreturn ROUND (   \n    (\n        LENGTH(commastring)\n        - LENGTH( REPLACE ( commastring, findme, \"\") ) \n    ) / LENGTH(findme)        \n);\n\nAnd call this function like this\nmsyql> select TOTAL_OCCURANCE('A,B,C,A,D,X,B,AB', 'A');\n\n\n\nA35:\n\n1. For MySQL:\nSELECT FIND_IN_SET(5, columnname) AS result \nFROM table\n\n2.For Postgres SQL :\nSELECT * \nFROM TABLENAME f\nWHERE 'searchvalue' = ANY (string_to_array(COLUMNNAME, ','))\n\nExample\nselect * \nfrom customer f\nwhere '11' = ANY (string_to_array(customerids, ','))\n\n\n\nA36:\n\nNot the prettiest solution, but changing the group parameter to output every column in model works in PostgreSQL:\nexpiration = Available.find(:all,\n:joins => [ :room ],\n:conditions => [ \"rooms.hotel_id = ? AND availables.bookdate BETWEEN ? AND ?\", hostel_id, date.to_s, (date+days-1).to_s ],\n:group => Available.column_names.collect{|col| \"availables.#{col}\"},\n:order => 'availables.updated_at')\n\n\n\nA37:\n\nFirst, the error you're getting is due to where you're using the COUNT function -- you can't use an aggregate (or group) function in the WHERE clause.\nSecond, instead of using a subquery, simply join the table to itself:\nSELECT a.pid \nFROM Catalog as a LEFT JOIN Catalog as b USING( pid )\nWHERE a.sid != b.sid\nGROUP BY a.pid\n\nWhich I believe should return only rows where at least two rows exist with the same pid but there is are at least 2 sids.  To make sure you get back only one row per pid I've applied a grouping clause.\n\n\nA38:\n\nI just tried to do this is MS SQL Server where we do not have FIELD():\nSELECT table1.id\n... \nINNER JOIN\n    (VALUES (10,1),(3,2),(4,3),(5,4),(7,5),(8,6),(9,7),(2,8),(6,9),(5,10)\n    ) AS X(id,sortorder)\n        ON X.id = table1.id\n    ORDER BY X.sortorder\n\nNote that I am allowing duplication too.\n\n\nA39:\n\nYou need to use HAVING, not WHERE.\nThe difference is: the WHERE clause filters which rows MySQL selects. Then MySQL groups the rows together and aggregates the numbers for your COUNT function.\nHAVING is like WHERE, only it happens after the COUNT value has been computed, so it'll work as you expect. Rewrite your subquery as:\n(                  -- where that pid is in the set:\nSELECT c2.pid                  -- of pids\nFROM Catalog AS c2             -- from catalog\nWHERE c2.pid = c1.pid\nHAVING COUNT(c2.sid) >= 2)\n\n\n\nA40:\n\nI modified the method to include handling output of arrays for statements like WHERE IN (?). \nUPDATE: Just added check for NULL value and duplicated $params so actual $param values are not modified. \nGreat work bigwebguy and thanks!\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_string($value))\n            $values[$key] = \"'\" . $value . \"'\";\n\n        if (is_array($value))\n            $values[$key] = \"'\" . implode(\"','\", $value) . \"'\";\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n\n    $query = preg_replace($keys, $values, $query);\n\n    return $query;\n}\n\n\n\nA41:\n\nMySQL's GROUP BY can be used without an aggregate function (which is contrary to the SQL standard), and returns the first row in the group (I don't know based on what criteria), while PostgreSQL must have an aggregate function (MAX, SUM, etc) on the column, on which the GROUP BY clause is issued.\n\n\nA42:\n\nMySQL's totally non standards compliant GROUP BY can be emulated by Postgres' DISTINCT ON. Consider this:\nMySQL:\nSELECT a,b,c,d,e FROM table GROUP BY a\n\nThis delivers 1 row per value of a (which one, you don't really know). Well actually you can guess, because MySQL doesn't know about hash aggregates, so it will probably use a sort... but it will only sort on a, so the order of the rows could be random. Unless it uses a multicolumn index instead of sorting. Well, anyway, it's not specified by the query.\nPostgres:\nSELECT DISTINCT ON (a) a,b,c,d,e FROM table ORDER BY a,b,c\n\nThis delivers 1 row per value of a, this row will be the first one in the sort according to the ORDER BY specified by the query. Simple.\nNote that here, it's not an aggregate I'm computing. So GROUP BY actually makes no sense. DISTINCT ON makes a lot more sense.\nRails is married to MySQL, so I'm not surprised that it generates SQL that doesn't work in Postgres.\n\n\nA43:\n\nPostgreSQL is more SQL compliant than MySQL. All fields - except computed field with aggregation function - in the output must be present in the GROUP BY clause.\n\n\nA44:\n\nAdded a little bit more to the code by Mike - walk the values to add single quotes \n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_array($value))\n            $values[$key] = implode(',', $value);\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n    // Walk the array to see if we can add single-quotes to strings\n    array_walk($values, create_function('&$v, $k', 'if (!is_numeric($v) && $v!=\"NULL\") $v = \"\\'\".$v.\"\\'\";'));\n\n    $query = preg_replace($keys, $values, $query, 1, $count);\n\n    return $query;\n}\n\n\n\nA45:\n\nThe IN clause describes a set of values, and sets do not have order.\nYour solution with a join and then ordering on the display_order column is the most nearly correct solution; anything else is probably a DBMS-specific hack (or is doing some stuff with the OLAP functions in standard SQL).  Certainly, the join is the most nearly portable solution (though generating the data with the display_order values may be problematic).  Note that you may need to select the ordering columns; that used to be a requirement in standard SQL, though I believe it was relaxed as a rule a while ago (maybe as long ago as SQL-92).\n\n\nA46:\n\nIf I remember correctly, in PostgreSQL you have to add every column you fetch from the table where the GROUP BY clause applies to the GROUP BY clause.\n\n\nA47:\n\nYou should actually fix your database schema so that you have three tables:\nshirt: shirt_id, shirt_name\ncolor: color_id, color_name\nshirtcolor: shirt_id, color_id\n\nThen if you want to find all of the shirts that are red, you'd do a query like:\nSELECT *\nFROM shirt, color\nWHERE color.color_name = 'red'\n  AND shirt.shirt_id = shirtcolor.shirt_id\n  AND color.color_id = shirtcolor.color_id\n\n\n\nA48:\n\nThis is actually how your query works and is a normal behaviour. Using LIMIT you will not limit the count or sum but only the returned rows. So your query will return n rows as stated in your LIMIT clause. And since your query actually returns only one row, applying a (non-zero) limit has no effect on the results.\nHowever, your second query will work as expected and is an established way of solving this problem.\n\n\nA49:\n\nFor others looking for a way to order by any field, including joined field, in postgresql, use a subquery:\nSELECT * FROM(\nSELECT DISTINCT ON(availables.bookdate) `availables`.* \nFROM `availables` INNER JOIN `rooms` ON `rooms`.id = `availables`.room_id \nWHERE (rooms.hotel_id = 5056 \nAND availables.bookdate BETWEEN '2009-11-22' AND '2009-11-24')\n) AS distinct_selected\nORDER BY availables.updated_at\n\nor arel:\n\nsubquery = SomeRecord.select(\"distinct on(xx.id) xx.*, jointable.order_field\")\n      .where(\"\").joins(\")\nresult = SomeRecord.select(\"*\").from(\"(#{subquery.to_sql}) AS distinct_selected\").order(\" xx.order_field ASC, jointable.order_field ASC\")\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " so, ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 46, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI recently ran into an issue that could easily be solved using modulus division, but the input was a float:\n\nGiven a periodic function (e.g. sin) and a computer function that can only compute it within the period range (e.g. [-\u03c0, \u03c0]), make a function that can handle any input.\n\nThe \"obvious\" solution is something like:\n#include <cmath>\n\nfloat sin(float x){\n    return limited_sin((x + M_PI) % (2 *M_PI) - M_PI);\n}\n\nWhy doesn't this work? I get this error:\nerror: invalid operands of types double and double to binary operator %\n\nInterestingly, it does work in Python:\ndef sin(x):\n    return limited_sin((x + math.pi) % (2 * math.pi) - math.pi)\n\n\n\nCandidate answers:\nA1:\n\nThe way I went about doing functional programming in C was to write a functional language interpreter in C.  I named it Fexl, which is short for \"Function EXpression Language.\"\nThe interpreter is very small, compiling down to 68K on my system with -O3 enabled.  It's not a toy either - I'm using it for all the new production code I write for my business (web-based accounting for investment partnerships.)\nNow I write C code only to (1) add a built-in function that calls a system routine (e.g. fork, exec, setrlimit, etc.), or (2) optimize a function that could otherwise be written in Fexl (e.g. search for a substring).\nThe module mechanism is based on the concept of a \"context\".  A context is a function (written in Fexl) which maps a symbol to its definition.  When you read a Fexl file, you can resolve it with any context you like.  This allows you to create custom environments, or run code in a restricted \"sandbox.\"\nhttp://fexl.com\n\n\nA2:\n\nThere is a significant difference between signed and unsigned integers in C/C++:\nvalue >> shift\n\nsigned values leave the top bit unchanged (sign extend), unsigned values clear the top bit.\nThe reason there is no unsigned float is that you quickly run into all sorts of problems if there are no negative values. Consider this:\nfloat a = 2.0f, b = 10.0f, c;\nc = a - b;\n\nWhat value does c have? -8. But what would that mean in a system without negative numbers. FLOAT_MAX - 8 perhaps? Actually, that doesn't work as FLOAT_MAX - 8 is FLOAT_MAX due to precision effects so things are even more screwy. What if it was part of a more complex expression:\nfloat a = 2.0f, b = 10.0f, c = 20.0f, d = 3.14159f, e;\ne = (a - b) / d + c;\n\nThis isn't a problem for integers due to the nature of the 2's complement system.\nAlso consider standard mathematical functions: sin, cos and tan would only work for half their input values, you couldn't find the log of values < 1, you couldn't solve quadratic equations: x = (-b +/- root (b.b - 4.a.c)) / 2.a, and so on. In fact, it probably wouldn't work for any complex function as these tend to be implemented as polynomial approximations which would use negative values somewhere.\nSo, unsigned floats are pretty useless.\nBut that doesn't mean to say that a class that range checks float values isn't useful, you may want to clamp values to a given range, for example RGB calculations.\n\n\nA3:\n\nWell quite a few programming languages are written in C. And some of them support functions as first class citizens, languages in that area are ecl (embbedabble common lisp IIRC), Gnu Smalltalk (gst) (Smalltalk has blocks), then there are libraries for \"closures\" e.g in glib2 http://library.gnome.org/devel/gobject/unstable/chapter-signal.html#closure \nwhich at least got near functional programming. So maybe using some of those implementations to do functional programming may be an option. \nWell  or you can go learning Ocaml, Haskell, Mozart/Oz or the like ;-)\nRegards\n\n\nA4:\n\nIt can take time for the network close to be observed - the total time is nominally about 2 minutes (yes, minutes!) after a close before the packets destined for the port are all assumed to be dead.  The error condition is detected at some point.  With a small write, you are inside the MTU of the system, so the message is queued for sending.  With a big write, you are bigger than the MTU and the system spots the problem quicker.  If you ignore the SIGPIPE signal, then the functions will return EPIPE error on a broken pipe - at some point when the broken-ness of the connection is detected.\n\n\nA5:\n\nThe heap usage comes from the C++ standard library. It allocates memory for internal library use on startup. If you don't link against it, there should be zero difference between the C and C++ version. With GCC and Clang, you can compile the file with:\n\ng++ -Wl,--as-needed main.cpp\n\nThis will instruct the linker to not link against unused libraries. In your example code, the C++ library is not used, so it should not link against the C++ standard library.\nYou can also test this with the C file. If you compile with:\n\ngcc main.c -lstdc++\n\nThe heap usage will reappear, even though you've built a C program.\nThe heap use is obviously dependant to the specific C++ library implementation you're using. In your case, that's the GNU C++ library, libstdc++. Other implementations might not allocate the same amount of memory, or they might not allocate any memory at all (at least not on startup.) The LLVM C++ library (libc++) for example does not do heap allocation on startup, at least on my Linux machine:\n\nclang++ -stdlib=libc++ main.cpp\n\nThe heap use is the same as not linking at all against it.\n(If compilation fails, then libc++ is probably not installed. The package name usually contains \"libc++\" or \"libcxx\".)\n\n\nA6:\n\nMost of the time, probably, but don't bet the farm on it, because if you're wrong, you'll lose big.\nIf you really, really need to have identical binary information, you'll need to create bitfields with bitmasks - e.g. you use an unsigned short (16 bit) for Message, and then make things like versionMask = 0xE000 to represent the three topmost bits.\nThere's a similar problem with alignment within structs. For instance, Sparc, PowerPC, and 680x0 CPUs are all big-endian, and the common default for Sparc and PowerPC compilers is to align struct members on 4-byte boundaries. However, one compiler I used for 680x0 only aligned on 2-byte boundaries - and there was no option to change the alignment!\nSo for some structs, the sizes on Sparc and PowerPC are identical, but smaller on 680x0, and some of the members are in different memory offsets within the struct.\nThis was a problem with one project I worked on, because a server process running on Sparc would query a client and find out it was big-endian, and assume it could just squirt binary structs out on the network and the client could cope. And that worked fine on PowerPC clients, and crashed big-time on 680x0 clients. I didn't write the code, and it took quite a while to find the problem. But it was easy to fix once I did.\n\n\nA7:\n\nIf you want to implement closures, you'll have to get groady with assembly language and stack swapping/management. Not recommending against it, just saying that's what you'll have to do.\nNot sure how you'll handle anonymous functions in C. On a von Neumann machine, you could do anonymous functions in asm, though.\n\n\nA8:\n\nNeither GCC nor Clang are compilers -- they're actually toolchain driver programs.  That means they invoke the compiler, the assembler, and the linker.\nIf you compile your code with a C or a C++ compiler you will get the same assembly produced.  The Assembler will produce the same objects.  The difference is that the toolchain driver will provide different input to the linker for the two different languages:  different startups (C++ requires code for executing constructors and destructors for objects with static or thread-local storage duration at namespace level, and requires infrastructure for stack frames to support unwinding during exception processing, for example), the C++ standard library (which also has objects of static storage duration at namespace level), and probably additional runtime libraries (for example, libgcc with its stack-unwinding infrastructure).\nIn  short, it's not the compiler causing the increase in footprint, it's the linking in of stuff you've chose to use by choosing the C++ language.\nIt's true that C++ has the \"pay only for what you use\" philosophy, but by using the language, you pay for it.  You can disable parts of the language (RTTI, exception handling) but then you're not using C++ any more.  As mentioned in another answer, if you don't use the standard library at all you can instruct the driver to leave that out (--Wl,--as-needed) but if you're not going to use any of the features of C++ or its library, why are you even choosing C++ as a programming language?\n\n\nA9:\n\nBitfields should be avoided - they aren't very portable between compilers even for the same platform. from the C99 standard 6.7.2.1/10 - \"Structure and union specifiers\" (there's similar wording in the C90 standard):\n\nAn implementation may allocate any addressable storage unit large enough to hold a bitfield. If enough space remains, a bit-field that immediately follows another bit-field in a structure shall be packed into adjacent bits of the same unit. If insufficient space remains, whether a bit-field that does not fit is put into the next unit or overlaps adjacent units is implementation-defined. The order of allocation of bit-fields within a unit (high-order to low-order or low-order to high-order) is implementation-defined. The alignment of the addressable storage unit is unspecified.\n\nYou cannot guarantee whether a bit field will 'span' an int boundary or not and you can't specify whether a bitfield starts at the low-end of the int or the high end of the int (this is independant of whether the processor is big-endian or little-endian).\nPrefer bitmasks.  Use inlines (or even macros) to set, clear and test the bits.\n\n\nA10:\n\nFFCALL lets you build closures in C -- callback = alloc_callback(&function, data) returns a function pointer such that callback(arg1, ...) is equivalent to calling function(data, arg1, ...).  You will have to handle garbage collection manually, though.\nRelatedly, blocks have been added to Apple's fork of GCC; they're not function pointers, but they let you pass around lambdas while avoiding the need to build and free storage for captured variables by hand (effectively, some copying and reference counting happens, hidden behind some syntactic sugar and runtime libraries).\n\n\nA11:\n\nI think the main reason is that unsigned floats would have really limited uses compared to unsigned ints. I don't buy the argument that it's because the hardware doesn't support it. Older processors had no floating point capabilities at all, it was all emulated in software. If unsigned floats were useful they would have been implemented in software first and the hardware would have followed suit.\n\n\nA12:\n\nI believe the unsigned int was created because of the need for a larger value margin than the signed int could offer. \nA float has a much larger margin, so there was never a 'physical' need for an unsigned float. And as you point out yourself in your question, the additional 1 bit precision is nothing to kill for.\nEdit:\nAfter reading the answer by Brian R. Bondy, I have to modify my answer: \nHe is definitely right that the underlying CPUs did not have unsigned float operations.  However, I maintain my belief that this was a design decision based on the reasons I stated above ;-)\n\n\nA13:\n\nendianness are talking about byte orders not bit orders. Nowadays , it is 99% sure that bit orders are fixed. However, when using bitfields, endianness should be taken in count. See the example below.\n#include <stdio.h>\n\ntypedef struct tagT{\n\n    int a:4;\n    int b:4;\n    int c:8;\n    int d:16;\n}T;\n\n\nint main()\n{\n    char data[]={0x12,0x34,0x56,0x78};\n    T *t = (T*)data;\n    printf(\"a =0x%x\\n\" ,t->a);\n    printf(\"b =0x%x\\n\" ,t->b);\n    printf(\"c =0x%x\\n\" ,t->c);\n    printf(\"d =0x%x\\n\" ,t->d);\n\n    return 0;\n}\n\n//- big endian :  mips24k-linux-gcc (GCC) 4.2.3 - big endian\na =0x1\nb =0x2\nc =0x34\nd =0x5678\n 1   2   3   4   5   6   7   8\n\\_/ \\_/ \\_____/ \\_____________/\n a   b     c           d\n\n// - little endian : gcc (Ubuntu 4.3.2-1ubuntu11) 4.3.2\na =0x2\nb =0x1\nc =0x34\nd =0x7856\n 7   8   5   6   3   4   1   2\n\\_____________/ \\_____/ \\_/ \\_/\n       d           c     b   a\n\n\n\nA14:\n\nIHMO it's because supporting both signed and unsigned floating-point types in either hardware or software would be too troublesome\nFor integer types we can utilize the same logic unit for both signed and unsigned integer operations in most situations using the nice property of 2's complement, because the result is identical in those cases for add, sub, non-widening mul and most bitwise operations. For operations that differentiate between signed and unsigned version we can still share the majority of the logic. For example\n\nArithmetic and logical shift need only a slight change in the filler for the top bits\nWidening multiplication can use the same hardware for the main part and then some separate logic to adjust the result to change the signness. Not that it's used in real multipliers but it's possible to do\nSigned comparison can be converted to unsigned comparison and vice versa easily by toggling the top bit or adding INT_MIN. Also theoretically possible, it's probably not used on hardware, yet it's useful on systems that support only one type of comparison (like 8080 or 8051)\n\nSystems that use 1's complement also just need a little modification to the logic because it's simply the carry bit wrapped around to the least significant bit. Not sure about sign-magnitude systems but it seems like they use 1's complement internally so the same thing applies\nUnfortunately we don't that luxury for floating-point types. By simply freeing the sign bit we'll have the unsigned version. But then what should we use that bit for?\n\nIncrease the range by adding it to the exponent\nIncrease the precision by adding it to the mantissa. This is often more useful, as we generally need more precision than range\n\nBut both choices need a bigger adder to accommodate for the wider value range. That increases the complexity of the logic while the adder's top bit sits there unused most of the time. Even more circuitry will be needed for multiplications, divisions or other complex operations\nOn systems that use software floating-point you need 2 versions for each function which wasn't expected during the time memory was so much expensive, or you'd have to find some \"tricky\" way to share parts of the signed and unsigned functions\nHowever floating-point hardware existed long before C was invented, so I believe the choice in C was due to the lack of hardware support because of the reason I mentioned above\nThat said, there exists several specialized unsigned floating-point formats, mainly for image processing purposes, like Khronos group's 10 and 11-bit floating-point type\n\n\nA15:\n\nMaybe the 40 bytes fits into the pipe buffer, and the 40000 bytes doesn't?\nEdit:\nThe sending process is sent a SIGPIPE signal when you try to write to a closed pipe. I don't know exactly when the signal is sent, or what effect the pipe buffer has on this. You may be able to recover by trapping the signal with the sigaction call.\n\n\nA16:\n\nI guess it depends on that the IEEE floating-point specifications only are signed and that most programming languages use them.\nWikipedia article on IEEE-754 floating-point numbers\nEdit: Also, as noted by others, most hardware does not support non-negative floats, so the normal kind of floats are more efficient to do since there is hardware support.\n\n\nA17:\n\ntry fmod\n\n\nA18:\n\nUnsigned integer types in C are defined in such a way as to obey the rules of an abstract algebraic ring.  For example, for any value X and Y, adding X-Y to Y will yield X.  Unsigned integer types are guaranteed to obey these rules in all cases which do not involve conversion to or from any other numeric type [or unsigned types of different sizes], and that guarantee is one of the most important feature of such types.  In some cases, it's worthwhile to give up the ability to represent negative numbers in exchange for the extra guarantees only unsigned types can provide.  Floating-point types, whether signed or not, cannot abide by all the rules of an algebraic ring [e.g. they cannot guarantee that X+Y-Y will equal X], and indeed IEEE doesn't even allow them to abide by the rules of an equivalence class [by requiring that certain values compare unequal to themselves].  I don't think an \"unsigned\" floating-point type could abide by any axioms which an ordinary floating-point type could not, so I'm not sure what advantages it would offer.\n\n\nA19:\n\n\nI'm not aware of any programming language that supports unsigned\nfloating-point numbers. Any idea why they don't exist?\n\nUnsigned floats exist. See the unsigned float16 (11 fractions bit, 5 exponent bits, 0 sign bits) for GPU hardware, HDR format DXGI_FORMAT_BC6H. It's just that they're uncommon enough across most computing hardware that mainstream programming languages omit them. In this usage, the sign is omitted because colors darker than black make no sense anyway.\nEven the far more common IEEE half or signed float16_t, which is used quite frequently in the field of graphics and machine learning for HDR images and lower bandwidth tensors, hasn't received the honor of being incorporated into C/C++ (though, more domain-specific languages like CUDA/HLSL do have half/float16_t, and there have been C++ proposals too). So if even signed float16 can't make into C++ outside of compiler specific extensions (e.g. gcc __fp16), then an unsigned float16 has little hope :b, and not even CUDA or HLSL have the unsigned type in the language, just in the texture definition itself (found in a .DDS file or in GPU texture memory). Until then, we'll have to continue to implement more exotic types without compiler help via helper libraries.\nupdate 2022-10-09 C++23 includes signed float16_t from proposal https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p1467r9.html, albeit as optional extensions. I don't see anything for an unsigned float though.\n\n\nA20:\n\nWhy C++ doesn't have support for unsigned floats is because there is no equivalent machine code operations for the CPU to execute.  So it would be very inefficient to support it.  \nIf C++ did support it, then you would be sometimes using an unsigned float and not realizing that your performance has just been killed.  If C++ supported it then every floating point operation would need to be checked to see if it is signed or not.  And for programs that do millions of floating point operations, this is not acceptable. \nSo the question would be why don't hardware implementers support it.  And I think the answer to that is that there was no unsigned float standard defined originally.  Since languages like to be backwards compatible, even if it were added languages couldn't make use of it.  To see the floating point spec you should look at the IEEE standard 754 Floating-Point.\nYou can get around not having an unsigned floating point type though by creating a unsigned float class that encapsulates a float or double and throws warnings if you try to pass in a negative number.   This is less efficient, but probably if you aren't using them intensely you won't care about that slight performance loss.\nI definitely see the usefulness of having an unsigned float.  But C/C++ tends to chose efficiency that works best for everyone over safety. \n\n\nA21:\n\n(As an aside, Perl 6 lets you write\nsubset Nonnegative::Float of Float where { $_ >= 0 };\n\nand then you can use Nonnegative::Float just like you would any other type.)\nThere's no hardware support for unsigned floating point operations, so C doesn't offer it.    C is mostly designed to be \"portable assembly\", that is, as close to the metal as you can be without being tied down to a specific platform.\n[edit]\nC is like assembly: what you see is exactly what you get.  An implicit \"I'll check that this float is nonnegative for you\" goes against its design philosophy.  If you really want it, you can add assert(x >= 0) or similar, but you have to do that explicitly.\n\n\nA22:\n\nThe current state of a socket is determined by 'keep-alive' activity. In your case, this is possible that when you are issuing the send call, the keep-alive activity tells that the socket is active and so the send call will write the required data (40 bytes) in to the buffer and returns without giving any error.\nWhen you are sending a bigger chunk, the send call goes in to blocking state.\nThe send man page also confirms this:\n\nWhen the message does not fit into the send buffer of the socket, send() normally blocks, unless the socket has been placed in non-blocking I/O mode. In non-blocking mode it would return EAGAIN in this case\n\nSo, while blocking for the free available buffer, if the caller is notified (by keep-alive mechanism) that the other end is no more present, the send call will fail.\nPredicting the exact scenario is difficult with the mentioned info, but I believe, this should be the reason for you problem.\n\n\nA23:\n\nThanks @BenVoigt for your very useful comment starting \n\nNo, they were created to save memory.\n\nLinux source does use a bit field to match to an external structure: /usr/include/linux/ip.h has this code for the first byte of an IP datagram\nstruct iphdr {\n#if defined(__LITTLE_ENDIAN_BITFIELD)\n        __u8    ihl:4,\n                version:4;\n#elif defined (__BIG_ENDIAN_BITFIELD)\n        __u8    version:4,\n                ihl:4;\n#else\n#error  \"Please fix <asm/byteorder.h>\"\n#endif\n\nHowever in light of your comment I'm giving up trying to get this to work for the multi-byte bit field frag_off.\n\n\nA24:\n\n\nA square-root will definately never return a negative number. There are other places as well where a negative float value has no meaning. Perfect candidate for an unsigned float.\n\nC99 supports complex numbers, and a type generic form of sqrt, so sqrt( 1.0 * I) will be negative.\n\nThe commentors highlighted a slight gloss above, in that I was referring to the type-generic sqrt macro rather than the function, and it will return a scalar floating point value by truncation of the complex to its real component: \n#include <complex.h>\n#include <tgmath.h>\n\nint main () \n{\n    complex double a = 1.0 + 1.0 * I;\n\n    double f = sqrt(a);\n\n    return 0;\n}\n\nIt also contains a brain-fart, as the real part of the sqrt of any complex number is positive or zero, and sqrt(1.0*I) is sqrt(0.5) + sqrt(0.5)*I not -1.0.\n\n\nA25:\n\nWhen peer close, you just do not know whether it just stop sending or both sending and receiving.Because TCP allows this, btw, you should know the difference between close and shutdown. \nIf peer both stop sending and receiving, first you send some bytes, it will succeed. But the peer kernel will send you RST. So subsequently you send some bytes, your kernel will send you SIGPIPE signal, if you catch or ignore this signal, when your send returns, you just get Broken pipe error, or if you don't , the default behavior of your program is crashing.\n\n\nA26:\n\nThe Felix language compiles to C++. Maybe that could be a step stone, if you don't mind C++.\n\n\nA27:\n\nLook at Hartel & Muller's book, Functional C\n\nhttps://ris.utwente.nl/ws/portalfiles/portal/5128727/book.pdf\nhttp://www.cs.bris.ac.uk/~henkm/f2c/index.html \u2014 missing since 2016\nhttp://www.cs.bris.ac.uk/~henkm/f2c/index.html \u2014 via Internet Archive aka WayBack Machine.\n\n\n\nA28:\n\nWhat is it about C that you want to make functional, the syntax or the semantics? The semantics of functional programming could certainly be added to the C compiler, but by the time you were done, you'd essentially have the equivalent of one of the existing functional languages, such as Scheme, Haskell, etc.\nIt would be a better use of time to just learn the syntax of those languages which directly support those semantics.\n\n\nA29:\n\nFunctional programming is not about lambdas, it is all about pure functions.  So the following broadly promote functional style:\n\nOnly use function arguments, do not use global state.\nMinimise side effects i.e. printf, or any IO.  Return data describing IO which can be executed instead of causing the side effects directly in all functions.  \n\nThis can be achieved in plain c, no need for magic.\n\n\nA30:\n\nHartel & Muller's book, Functional C, can nowadays (2012-01-02) be found at: http://eprints.eemcs.utwente.nl/1077/ (there is a link to PDF version).\n\n\nA31:\n\nYou're looking for fmod().\nI guess to more specifically answer your question, in older languages the % operator was just defined as integer modular division and in newer languages they decided to expand the definition of the operator.\nEDIT:  If I were to wager a guess why, I would say it's because the idea of modular arithmetic originates in number theory and deals specifically with integers.\n\n\nA32:\n\nFor C/C++, this is only defined for integer operations.\nPython is a little broader and allows you to get the remainder of a floating point number for the remainder of how many times number can be divided into it:\n>>> 4 % math.pi\n0.85840734641020688\n>>> 4 - math.pi\n0.85840734641020688\n>>> \n\n\n\nA33:\n\nYou do not need to send code through a network to get this error. For example, this Python 3 code (now my favourite piece of code):\nwhile 1:print()\n\nwill print a lot of newlines, and eventually a BrokenPipeError will be raised. So you will get this error if you write too much data to an IO or other stream.\n\n\nA34:\n\nSession timeout settings may be the reason of broken pipe.\n For example: Server session timeout 3 hours and load balancer has 1 hour.\n Load balancer blocks after 1 hour, but server keeps sending response. In this case, one end of the pipe is broken.\nBut it can be also a user behavior. User closes the page during download etc.\n\n\nA35:\n\nNo, it will not be fully-portable.  Packing options for structs are extensions, and are themselves not fully portable.  In addition to that, C99 \u00a76.7.2.1, paragraph 10 says: \"The order of allocation of bit-fields within a unit (high-order to low-order or low-order to high-order) is implementation-defined.\"\nEven a single compiler might lay the bit field out differently depending on the endianness of the target platform, for example.\n\n\nA36:\n\nThe modulo operator % in C and C++ is defined for two integers, however, there is an fmod() function available for usage with doubles.\n\n\nA37:\n\nBecause the normal mathematical notion of \"remainder\" is only applicable to integer division. i.e. division that is required to generate integer quotient.\nIn order to extend the concept of \"remainder\" to real numbers you have to introduce a new kind of \"hybrid\" operation that would generate integer quotient for real operands. Core C language does not support such operation, but it is provided as a standard library fmod function, as well as remainder function in C99. (Note that these functions are not the same and have some peculiarities. In particular, they do not follow the rounding rules of integer division.)\n\n\nA38:\n\nPrerequisite for functional programming style is a first class function.\nIt could be simulated in portable C if you tolerate next:\n\nmanual management of lexical scope bindings, aka closures.\nmanual management of function variables lifetime.\nalternative syntax of function application/call.\n\n/* \n * with constraints desribed above we could have\n * good approximation of FP style in plain C\n */\n\nint increment_int(int x) {\n  return x + 1;\n}\n\nWRAP_PLAIN_FUNCTION_TO_FIRST_CLASS(increment, increment_int);\n\nmap(increment, list(number(0), number(1)); // --> list(1, 2)\n\n\n/* composition of first class function is also possible */\n\nfunction_t* computation = compose(\n  increment,\n  increment,\n  increment\n);\n\n*(int*) call(computation, number(1)) == 4;\n\nruntime for such code could be as small as one below\n\nstruct list_t {\n  void* head;\n  struct list_t* tail;\n};\n\nstruct function_t {\n   void* (*thunk)(list_t*);\n   struct list_t* arguments;\n}\n\nvoid* apply(struct function_t* fn, struct list_t* arguments) {\n  return fn->thunk(concat(fn->arguments, arguments));\n}\n\n/* expansion of WRAP_PLAIN_FUNCTION_TO_FIRST_CLASS */\nvoid* increment_thunk(struct list_t* arguments) {\n  int x_arg = *(int*) arguments->head;\n  int value = increment_int(x_arg);\n  int* number = malloc(sizeof *number);\n\n  return number ? (*number = value, number) : NULL;\n}\n\nstruct function_t* increment = &(struct function_t) {\n  increment_thunk,\n  NULL\n};\n\n/* call(increment, number(1)) expands to */\napply(increment, &(struct list_t) { number(1), NULL });\n\nIn essence we imitate first class function with closures represented as pair of function/arguments plus bunch of macroses. Complete code could be found here.\n\n\nA39:\n\nThe main thing that comes to mind is the use of code generators.  Would you be willing to program in a different language that provided the functional programming and then generate the C code from that?\nIf that's not an attractive option, then you could abuse CPP to get part of the way there.  The macro system should let you emulate some functional programming ideas.  I've heard tell that gcc is implemented this way but I've never checked.\nC can of course pass functions around using function pointers, the main problems are lack of closures and the type system tends to get in the way.  You could explore more powerful macro systems than CPP such as M4.  I guess ultimately, what I'm suggesting is that true C isn't up to the task without great effort but you could extend C to make it be up to the task.  That extension would look the most like C if you use CPP or you could go to the other end of the spectrum and generate C code from some other language.\n\n\nA40:\n\nBit fields vary widely from compiler to compiler, sorry.\nWith GCC, big endian machines lay out the bits big end first and little endian machines lay out the bits little end first.\nK&R says \"Adjacent [bit-]field members of structures are packed into implementation-dependent storage units in an implementation-dependent direction. When a field following another field will not fit ... it may be split between units or the unit may be padded. An unnamed field of width 0 forces this padding...\"\nTherefore, if you need machine independent binary layout you must do it yourself.\nThis last statement also applies to non-bitfields due to padding -- however all compilers seem to have some way of forcing byte packing of a structure, as I see you already discovered for GCC.\n\n\nA41:\n\nYou can use GCC's nested functions to simulate lambda expressions, in fact, I have a macro to do it for me:\n#define lambda(return_type, function_body) \\\n  ({ \\\n    return_type anon_func_name_ function_body \\\n    anon_func_name_; \\\n  })\n\nUse like this:\nint (*max)(int, int) = lambda (int, (int x, int y) { return x > y ? x : y; });\n\n\n\nA42:\n\nI think Treb is on the right track. It's more important for integers that you have an unsigned corresponding type. Those are the ones that are used in bit-shifting and used in bit-maps. A sign bit just gets into the way. For example, right-shifting a negative value, the resulting value is implementation defined in C++. Doing that with an unsigned integer or overflowing such one has perfectly defined semantics because there is no such bit in the way. \nSo for integers at least, the need for a separate unsigned type is stronger than just giving warnings. All the above points do not need to be considered for floats. So, there is, i think, no real need for hardware support for them, and C will already don't support them at that point. \n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "Let, (a,b,c) be the input, and (a,b,c,0,0,0,0,0,0"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 47, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI was trying to push and create a pull-request from my local feature-branch, as usual, being in my feature-branch and right-clicking on the remote origin/develop -branch (I'm using Git Flow), and this error message popped out:\n\nCan't find anything with that error message. Any ideas?\n\n\nCandidate answers:\nA1:\n\nprobably you were trying to clone like this: \ngit clone https://github.com/TeaCodie/TeaCodie-Website.git\nThen you got this error:\nfatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\nHere is what worked for me:\ngit clone https://github.com/TeaCodie/TeaCodie-Website.git/.git\nbecause the directory \"info/refs\" is in the .git directory.\n\n\nA2:\n\nThe reason why this is happening is because you have a folder that is already being tracked by Git inside another folder that is also tracked by Git. For example, I had a project and I added a subfolder to it. Both of them were being tracked by Git before I put one inside the other. In order to stop tracking the one inside, find it and remove the Git file with:\nrm -rf .git\n\nIn my case I had a WordPress application and the folder I added inside was a theme. So I had to go to the theme root, and remove the Git file, so that the whole project would now be tracked by the parent, the WordPress application.\n\n\nA3:\n\nMac:\nThis solution works for me.\nAfter clone code successfully, you open terminal and run this code\ngit config --global credential.helper osxkeychain\n\n\n\nA4:\n\nThis happened to me and at first it was not apparent what had gone wrong. The previous day I had renamed my repo on github.com only changing the first character of the repo name from a lowercase letter to uppercase. I was not aware that the repo name in the origin URL was case-sensitive unit I received the error which contained the text, \"did you run git update-server-info on the server.\" Once I edited the .git/config file to correct the case of the same letter in the origin URL, the problem was solved, the error no longer occurred and I was once again able to push my changes up to the repo on github.com. Another bit of advice: if you make changes to your repo on github, make sure to test sync'ing your local repo right away. In my case, I did not and it was the next day when I had tried to sync and couldn't because of the error and I forgot that I had renamed the repo, so I was scratching my head for a bit. Had I tested sync'ing right away, I would have caught this problem right away.\n\n\nA5:\n\nThis error could also happen if the repository you are attempting to reach was deleted.\n\n\nA6:\n\nReally struggled with this on Mac, couldn't get it to work with SSH. Ended up following this guide which worked: https://www.youtube.com/watch?v=otRHRPnHDeI.\nThere were a couple things which could have been the cause, it may not have been the SSH itself. For instance, Oauth apparently doesn't work on SourceTree despite nothing appearing wrong immediately when using it. Making sure you use a Personal Access Token and not your real password among others.\nA good rule of thumb is, if after connecting your account it only shows your public repos, it's already broken. https://github.com/settings/tokens\n\n\nA7:\n\nFor windows user\n\nchange SSH Client from Putty/Plink to OpenSSH\n\nin Tools -> Option -> SSHClient\n\n\n\nA8:\n\nI had the same problem.  My branch name had an apostrophe in it (Richie'sVersion).  I created a new branch from the existing branch (essentially a copy of the existing branch) and gave it a name without an apostrophe (RichiesVersion) and this resolved the problem.  Pushing the new branch worked fine.\n\n\nA9:\n\nI think it is very preferable if you first pull the existing Github repo on the local and then add the new files to the Github repo\nThis link will help: https://stackoverflow.com/a/61285076/5840973\n\n\nA10:\n\nI had a similar thing after an update; I tried re-adding my bitbucket credentials, changing the protocols, etc. with no luck, until...\n\nI opened my keychain and searched for bitbucket (probably the same for github, just search for github instead) under the Passwords category. \nI found that I had multiple Access Keys for my user, so I closed my repository, deleted all the keys of \"application password\" Kind, opened the SourceTree preferences > Accounts > removed my account and re-added it (login as usual - I used Basic with HTTPS). \nKeychain asks for permission to save the password, which I said yes and now I only see one key of the kind \"application password\". \n\nI opened my repository from the repo browser and hey presto, no more password popups!\nThis bugged me for a whole week! If this helped you, you can do a little dance with me now :)\n\n\nA11:\n\nGo to Sourctree -> Tools -> Options -> Git Select \n\n\nA12:\n\nI had a very similar issue with the same error message. \"Changes not staged for commit\", yet when I do a diff it shows differences.  I finally figured out that a while back I had changed a directories case. ex. \"PostgeSQL\" to \"postgresql\".  As I remember now sometimes git will leave a file or two behind in the old case directory.  Then you will commit a new version to the new case.\nThus git doesn't know which one to rely on.  So to resolve it, I had to go onto the github's website. Then you're able to view both cases.  And you must delete all the files in the incorrect cased directory. Be sure that you have the correct version saved off or in the correct cased directory.\nOnce you have deleted all the files in the old case directory, that whole directory will disappear. Then do a commit.\nAt this point you should be able to do a Pull on your local computer and not see the conflicts any more.  Thus being able to commit again.  :)\n\n\nA13:\n\nYou didn't add the changes. Either specifically add them via\ngit add filename1 filename2\n\nor add all changes (from root path of the project)\ngit add .\n\nor use the shorthand -a while commiting:\ngit commit -a -m \"message\".\n\n\n\nA14:\n\nYou might have changed your repository name\nIn your local repository edit the file:\n.git/config\n\nThen check:\n[remote \"origin\"]\n   url = \n\nthat the URL matches your remote repository\n\n\nA15:\n\nI got this issue when using GitStack. I looked into C:/GitStack/apache/logs/error.log and found that GitStack was looking for the repository in the wrong location although its settings said D:/Repositories (my custom location), GitStack looked for the repositories in its default location. So I simply went and save the settings again from GitStack admin page. It solved the problem.\n\n\nA16:\n\nif you have more files in my case i have 7000 image files when i try to add them from project's route folder it hasn't added them but when i go to the image folder everything is ok. Go through the target folder and command like abows\ngit add .\ngit commit -am \"image uploading\"\ngit push origin master\n\n\n\ngit push origin master Enumerating objects: 6574, done. Counting objects: 100% (6574/6574), done. Delta compression using up to 4\n    threads Compressing objects: 100% (6347/6347), done. Writing objects: \n    28% (1850/6569), 142.17 MiB | 414.00 KiB/s\n\n\n\n\nA17:\n\nFollow to @Chris answer in case you've enabled 2FA, to use Personal Access Token in SourceTree, you need to add your acc with this token in SourceTree > Preferences with following information:\n\nAuth Type: Basic  \nUsername: {your-github-account} \nPassword: {your-personal-access-token} \nProtocol: HTTPS\n\nAfter that, you are able to clone your Github repo.\n\n\n\nA18:\n\nIn my case I needed to put my ssh password instead of my personal access token. Then it worked.\n\n\nA19:\n\nSo i had this project that wasnt under source control i made some changes to and wanted to keep stuff i changed.\ngit init\ngit remote add origin <url>\ngit fetch\ngit branch master origin/master\ngit restore --staged .\n\n\n\nA20:\n\nMy issue was that I used the clone https url widget provided by github. That URL doesn't work for private repositories as you need to add a username to the front of it.  \nExample: a private repo owned by john and named widget with collaborator sam the correct url would be: \nhttps://sam@github.com/john/widget.git\nThe github provided url: \nhttps://github.com/john/widget.git\nThe error message leaves much to be desired.\n\n\nA21:\n\nIn my case, the Sourcetree login screen repeatedly showed when pushing to GitHub. Logging in from that screen failed. I finally determined it was due to an expired OAuth token, which is trivial to fix.\n\nNavigate to the GitHub account config screen: Tools > Options > Authentication\nClick your GitHub account and then click edit\nOn the Edit Hosting Account dialog click the Refresh OAuth Token button\n\n\n\nA22:\n\nIn my case, this was happening with a submodule that had defaulted to https even though the main project was using ssh. I fixed it by changing the submodule's repo path in SourceTree's Repository Settings to use the ssh path instead of https.\n\n\nA23:\n\nI copied a small sub project I had that was under Git source control into another project and forgot to delete the .git folder. When I went to commit I got the same message as above and couldn't clear it until I deleted the .git folder.\nIt is a bit silly, but it is worth checking you don't have a .git folder under the folder that doesn't commit.\n\n\nA24:\n\nIn my case I had old version of the git. Upgrade to latest version fixed this issue.\n\n\nA25:\n\nSourceTree version 2.4.8.0\nThe problem for me seemed to be the order of setup between SourceTree and GitHub. I setup SourceTree first and just clicked all the \"setup later\" options.\nAn easy fix - Uninstall and re-install SourceTree. The non-obvious bit was needing to delete the actual program files manually.\nFrom any \"SourceTree.exe\" shortcut, open file location. Mine forked at \"C:\\Users\\myusername\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Atlassian\" where the shortcut pointed to another shortcut. \nOpen that shortcut's file location. I eventually ended up at \"C:\\Users\\myusername\\AppData\\Local\\SourceTree\". Delete the whole \"SourceTree\" folder. Go up one level to \"...\\Local\" and delete the \"Atlassian\" folder as well.\nNow uninstall SourceTree. Make sure your GitHub is setup the way you desire (for me, I also created and saved an SSH key that I used for SourceTree setup). Re-install SourceTree using all the correct credentials, point to the right repository and such. \nAfter all this, SourceTree asked for my GitHub credentials twice, with two different dialog boxes, then stopped and worked! No more endless loop. Good luck!\n\n\nA26:\n\nI am using a SSH key, but SourceTree was asking me a password every time.\nFound on the community of Atlassian, this solution worked for me:\n\nOpen the terminal, get into your project directory\nType git config credential.helper store\nType git pull\nInput username/password (if asked)\n\nVoil\u00e0 !\n\n\nA27:\n\nGo to the directory where you code is,\ngit init\ngit add .\ngit commit -m \"Your message\"\n\nNow add your address go to your git hub copy the clone address,\ngit remote add origin <remote repository URL>\n\nNow add push your code with,\ngit push -u -f origin master\n\nAnd you are done.\n\n\nA28:\n\nThis error mostly caused by WRONG URL, please check:\n\nhttp or https\nURL Name\nusername@git_url\nwrong git name\n\n\n\nA29:\n\nIt looks like that's a private (or deleted) repository; if you visit the repository page while logged it'll give you the real URL, which'll probably be https://TeaCodie@github.com/TeaCodie/TeaCodie-Website.git , i.e. with a username specified?\n\n\nA30:\n\nI met up with the same problem.\nHow I solved this problem is:\nI use git bash to create a new repo, when I typed \"git push origin master\"\nIt reported \n\"fatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\"\nFinally, I found there was not a repo on the github at all.\nYou'd better create a new repo first on github. \nMaybe this experience can help somebody.\n\n\nA31:\n\nYou should do:\ngit commit . -m \"save arezzo files\"\n\n\n\nA32:\n\nI received this same error when I attempted to git clone something from Kiln what was actually a Mercurial repository.\n\n\nA33:\n\nMake sure that your user account is added to the repository as a collaborator.\nSetting --> Collaborators\n\n\nA34:\n\nI finally was able to solve this after months of no luck. On a Mac :\n\nOpen 'Keychain Access' on a Mac by using Launchpad or Spotlight Search\nNarrow down your search by selecting \"login\" under Keychains and \"Passwords\" under the Category (both filters on the left side of the window)\nIn the top right hand corner of the window, do a search for \"git\"\nYou'll likely see a keychain there. I right-clicked and deleted the one entry I had.\nGo into sourcetree and try to do a pull/fetch/anything. You'll be prompted for a password. This should re-initiate your password with Sourcetree. To double check this, go into your keychain and repeat steps 3 & 4. Click on the keychain and look at the access control tab. SourcetreeLogin should be listed as always having access now.\n\n\n\n\nA35:\n\nAs the message says:\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nGit has a \"staging area\" where files need to be added before being committed, you can read an explanation of it here.\n\nFor your specific example, you can use:\ngit commit -am \"save arezzo files\"\n\n(note the extra a in the flags, can also be written as git commit -a -m \"message\" - both do the same thing)\nAlternatively, if you want to be more selective about what you add to the commit, you use the git add command to add the appropriate files to the staging area, and git status to preview what is about to be added (remembering to pay attention to the wording used).\nYou can also find general documentation and tutorials for how to use git on the git documentation page which will give more detail about the concept of staging/adding files.\n\nOne other thing worth knowing about is interactive staging - this allows you to add parts of a file to the staging area, so if you've made three distinct code changes (for related but different functionality), you can use interactive mode to split the changes and add/commit each part in turn. Having smaller specific commits like this can be helpful.\n\n\nA36:\n\nif you have a subfolder, which was cloned from other git-Repository, first you have to remove the $.git$ file from the child-Repository:\nrm -rf .git\nafter that you can change to parent folder and use git add -A. \n\n\nA37:\n\ndelete each .git file in all projects\nyou can use this command\nrm -rf .git\n\n\n\nA38:\n\nIf you are using \"Username & Password\" authentication,you can try to change to use SSH keys for the authentication.\nI have met this kind of situation when I commit to GitLab everytime.When I changed the way of authentication:SSH keys,the problem has been solved.\n\n\nA39:\n\nYou could have done a:\ngit add -u -n\n\nTo check which files you modified and are going to be added (dry run: -n option), and then\ngit add -u\n\nTo add just modified files\n\n\nA40:\n\nIn addition to setting up the Personal Access Token on GitHub, and doing what A_01 states above, this may be helpful (it's how I finally got it working)\n\nI made sure had the latest version of Sourcetree \nI upgraded to the latest version of and used System Git (Tools -> Options -> Git)\nUpgraded to the latest Git Credential Manager for Windows (this was via an installer I downloaded and double-clicked, very easy)\nEnsured I was using OAuth for my GitHub account stored in Tools -> Options -> Authentication\nMade sure I checked off the box \"Allow Sourcetree to manage my credentials using the Git Credential Manager\" (this was important!!) on Tools -> Options -> Git\nThen when I triggered the GitHub dialog (via pushing a new branch, just an example), I logged in there (this is where it used to fail).\nI next got the 2 factor auth dialog, and the code was sent to my phone as expected. I was able to enter the code from my phone into this dialog.\n\n\n\nA41:\n\n1. first create a git repostry.\n2. second open git bash in  existing or uploading project.\n3. perform git init \n4. git add .\n5. git commit -m \"print message\"\n6. git remote add github<repostry url>\n7. git remote -v\n8. git push github master\n\nOR\ngit push origin master\n\nif  you get any error, you may use it\ngit push -f origin master\n\n\n\nA42:\n\nAll the answers above seems to guide about creating a new repository in git but the question is about adding a folder to existing repo. To do that following steps can be followed.\n\nClone your existing repo using following command:\ngit clone https://github.com/company/our_repo.git \nManually take your project folder to the desired location i.e. trunk/bin\nNow commit and then push in the repo using the commands:\ngit commit -m \"message\" and git push origin master \n\n\n\nA43:\n\nThis is what I discovered (after searching for 'sourcetree' in my drive C:):\n\nUninstall SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\Atlassian\\SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\SourceTree\nReinstall SourceTree\n\n\n\nA44:\n\nI had this problem as well when there was a HEAD tag at a commit behind the current local master or origin/master.\nTry double-clicking your local master (which eliminates HEAD) and then push your changes.\nNote that if you do not have a local master, right-click on remote origin master and choose 'Checkout origin/master'. Then pull, commit, push your changes.\n\n\nA45:\n\nopen terminal and clone your repo. repository url must have your usedid,\nFor ex:\n$ git clone https://<username>@github.com/<userId>/<reponame>.git\n$ git clone https://mark@github.com/mak123/<reponame>.git\n\nThen drag and drop the cloned folder into the sourceTree window.You can also do by clicking \"+New Repository\" button and from the menu by selecting \"Add Existing Local Repository\".\nIt may ask for password again but this time it will get added to your keychain.\n\n\nA46:\n\nIf you are not using GitHub 2FA, another reason for asking you your password is:  \nYour GitHub repo url is an ssh one, not an https one.\nSince git does not find your ssh keys (or your ssh key is not registered to your GitHub account), it fails back to account/password mechanism. \nIf you are using 2FA (and an https url), then you need a personal token as a password.\n\n\nA47:\n\nIn my case, I cloned the repo using HTTPS based url. Then I configured my githib account with 2-factor authentication using SSH key. So whenever I was pulling/pushing changes, it was showing me dialog to provide my password.\nI fixed it by enabling SSH access by right clicking on project repo-menu on SourceTree.\n\n\n\nA48:\n\nI got the same problem while using a github repository, and connecting to it via https, while using the OS X Keychain Credential helper.\nMy problem was that I had the wrong credentials stored in OS X's Keychain (I was using the email address that I used to sign up for github.com rather than the [username]@github.com address it provides you). I deleted the old account in the keychain and only left the @github.com one and it fixed the problem.\nNot sure if it is related, but when I checked the user.email local config:\ngit config -l\n\nit showed the incorrect email address as well, so I updated the local git user.email to use the correct account too:\ngit config user.email <username>@github.com\n\n\n\nA49:\n\nDid you create a new repository on the http://github.com with the same name? \nIf not, do it! And make sure each letter is correct and case sensitive.\n\n\nA50:\n\nI encountered this message when using Jenkins 2.176.1 and git plugin 3.10.0 using a very old command line git version (1.7.1) as included on CentOS 6.\nThe message does not occur on newer versions of command line git (1.8 or later) with the Jenkins git plugin.\nUpgrading to a newer command line git version resolves the problem.\nOfficially, the Jenkins git plugin does not support command line git 1.7.1.  The minimum supported version is command line git 1.7.10. Many Jenkins git plugin use cases require at least command line git 1.9.0.\nCommand line git 1.7.1 behaves differently than later versions when a repository has been initialized (with git init) and then a git fetch is performed with a refspec which references 'origin'.  Later versions of command line git do not have the same problem.\n\n\nA51:\n\nI find this problem appearing when I've done a git add . in a subdirectory below where my .gitignore file lives (the home directory of my repository, so to speak).  Try changing directories to your uppermost directory and running git add . followed by git commit -m \"my commit message\".\n\n\nA52:\n\nI had an issue where I was doing commit --amend even after issuing a git add . and it still wasn't working.  Turns out I made some .vimrc customizations and my editor wasn't working correctly.  Fixing these errors so that vim returns the correct code resolved the issue.\n\n\nA53:\n\nAssume that I would like to add FreeRTOS repository, which URL is https://github.com/FreeRTOS/FreeRTOS-Kernel.git, into my repository, example URL is https://github.com/username/example as a submodule\ngit submodule add https://github.com/FreeRTOS/FreeRTOS-Kernel.git\ngit add .\ngit commit -m 'add a submodule'\ngit push\n\nTo clone using HTTPS:\ngit clone https://github.com/username/example.git --recurse-submodules\n\nUsing SSH:\ngit clone git@github.com:username/example.git --recurse-submodules\n\nIf you have downloaded the repo without using the --recurse-submodules argument, you need to run:\ngit submodule update --init --recursive\n\n\n\nA54:\n\nHere is what worked for me.\nI tried almost all the solutions mentioned here and on the web a lot of times. I gave up multiple times before and was using it this way for more than a year I think. Finally, I was able to solve this annoying issue.\n\nAs suggested by user \"AEQ\" in his answer, I deleted all the passwords related to GitHub from my Keychain Access.\nTried deleting and re-adding the Github account. But still, the problem persisted. (generated a personal access token from \"https://github.com/settings/tokens\" and use it as the password)\n\nWhat made it work was\nMake sure to use HTTPS and not SSH when adding the new user account. If you have already added the account, just edit it. That's it.\nFinally, It feels good.\nAttaching screenshots for ease.\n\n\n\nA55:\n\nOpen your Terminal, access to this folder and write:\ngit init\ngit add .\ngit commit -m \"my commit\"\ngit remote set-url origin git@github.com:username/repo.git\ngit push origin master\n\n\n\nA56:\n\nI had more luck with navigating in my terminal to the directory I wanted to add to the repository, then (assuming you're working on a branch called master):\n    git init\n    git add .\n    git commit -m \"my commit\"\n    git remote add origin <remote repository URL>\n    git push origin master\n\nHere's a link to an article explaining how to do it in more detail: https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/\nNote that you won't be able to run the \"git add .\" line if the directory in question is open.\n\n\nA57:\n\nIn my case my github account did not have permissions to the repo. Added the github account as a collaborator for the repo and that fixed it.\n\n\nA58:\n\nFor future references, I solved this problem by changing the GIT system used in Sourcetree, from Embedded to System.\nHow to change the git system:\n \n1. Find the Options submenu (located under Tools menu) and click on it to view the Options popup \n2. On the Options setting popup, find the Git tab, then click on it to view the Git setting \n3. Find Git version setting, then check the version used in it. In my case, I changed it to system because previously I'm using Git bash cmd, and it looks like that the git embedded in Sourcetree clashes with each other. \n4. Restart Sourcetree\n\n\nA59:\n\nSince this question is specifically about SourceTree and Github, the answer in 2021 is simple. Quoting bcr:\n\nEnsured I was using OAuth for my GitHub account stored in Tools ->\nOptions -> Authentication\n\n\nGo there. Select Add (new account).\nSelect OAuth.\nClick Refresh OAuth Token. A browser page will open (github.com) requesting renewal of your token.\nLogin to accept. SourceTree will immediately respond by acknowledging the token provided by github.com.\n\n\n\nA60:\n\nMaybe an obvious thing, but...\nIf you have problem with the index, use git-gui. You get a very good view how the index (staging area) actually works.\nAnother source of information that helped me understand the index was Scott Chacons \"Getting Git\" page 259 and forward.\nI started off using the command line because most documentation only showed that...\nI think git-gui and gitk actually make me work faster, and I got rid of bad habits like \"git pull\" for example... Now I always fetch first... See what the new changes really are before I merge.\n\n\nA61:\n\nIf you are using two-factor authentication with GitHub you will need to create a personal access token and use it with SourceTree:\n\nTo work with GitHub's two-factor authentication in SourceTree you can simply use your access token instead of your password. The steps to do this are as follows:\n\nGo to your Personal Access Tokens settings in GitHub.\nClick on the Generate new token button.\nName the token something descriptive.\nSelect which scopes you wish to grant this token.\nClick the Generate token button.\nCopy the token and use it as a password in your hosted repositories.\n\nYou can find more information about this on GitHub's help here.\n\nMac Users:\nIf your SourceTree keeps on asking for the password, go to the Terminal and type this:\ngit config --global credential.helper osxkeychain\n\n\n\nA62:\n\nThe easiest way is to create an app password and use that password.\nHere you can see how you can create an app password on Bitbucket:\nhttps://docs.snyk.io/tutorials/getting-started/atlassian-integrations/atlassian-bitbucket-app-password\n\n\nA63:\n\nIn my case I was using the wrong protocol in the repository URL (http instead of https)\n\n\nA64:\n\nIn my case, the password that Sourcetree was asking for was for the ssh private key password. If I remove the password from the ssh key, this prompt doesn't come up.\nHowever since I wanted the password to be there, I kept investigating why it wasn't accepting my ssh key password.\nTurns out there is a wrinkle. If you have multiple ssh private keys which have different passwords, Sourcetree will give you a password prompt for each password.\nThink of it this way - if you have a fresh shell in your system, and you just started ssh-agent up, and ran ssh-add, what are the password prompts that you get? For every such password prompt, Sourcetree will give you this dialog box. In fact, if you add passwords to the dialog in the same order as you would in the shell to an ssh-add, this would work. In fact, I am guessing Sourcetree is doing exactly the same in the background.\nSo my final solution was 1) keep the passwords on the ssh keys 2) make all these passwords the same. So now Sourcetree will give me a single password prompt, and entering the ssh private key password will work.\n\n\nA65:\n\nIn my repo's directory on my machine in the terminal I had to reset the origin url:\ngit remote set-url origin git@github.com:repoaccountname/repo-name.git\n\n\n\nA66:\n\nLets say you have remote repo with files and local repo with the same files.\nAnd you want to add Git to local files, and dont want to push.\nThen you can do such commands on local repo:\ngit init\ngit remote add origin <url>\ngit fetch --all\ngit reset --hard origin/master\n\nAfter that your repository is synced with remote repo.\nYou can change files, git add, push and so on.\n\n\nA67:\n\nAlso make sure the repo you've entered is cased correctly (it's case sensitive).\n\n\nA68:\n\nA solution worked for me given by Andrew Magill, Copied from Source : link\nThe problem is that SourceTree's embedded Git client comes with git-credential-manager v1.12, which no longer works with Github since they disabled TLS 1.1 on their site.  You need to get git-credential-manager v1.14.  Easiest way to do that is to install a current copy of the Git client separately, and then switch SourceTree over to use that (\"system git\") instead of its embedded client.  Alternatively, you can update git-credential-manager in your embedded client by replacing its files with the newest version from Microsoft.\n\n\nA69:\n\n\ngit init\ngit add .\ngit commit -m \"initial commit\"\ngit remote add origin [URL]\ngit push origin masteror\n\ngit push -f origin master\n\ngit push --force.\nIt does force the update\n\n\n\nA70:\n\nJust another stupid reason: you may have changed credentials on your laptop, but there is another development PC running but not frequently used, that has SourceTree started and uses the old credentials frequently to poll status.\nSounds stupid, but took me some time to solve :-)\n\n\nA71:\n\nYou have to use -f when you are going to push on already existing repo.\ngit init\ngit add *\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin <repo url>\ngit push -f origin main\n\n\n\nA72:\n\nIn my case, I was trying to clone a private repository in our group Github account to me laptop:\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone https://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nfatal: https://github.com/nyusngroup/velocity_CCSNe.git/info/refs?service=git-upload-pack not found: did you run git update-server-info on the server?\n\nI found two ways can solve this.\n(1) Use http instead of https. It asks me the name and password of our group Github account. After I entered the information, I can clone it.\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone http://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nUsername for 'http://github.com':nyusngroup\nPassword for 'http://nyusngroup@github.com': \n\n(2) Add my Github account to the collaborators of the private repository in our group Github account, as pointed by Monkey King's answer above.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 48, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI was just wondering what does the -u flag mean in this command? \ngit push -u origin master\n\n\n\nCandidate answers:\nA1:\n\nThe -u option does the following:\nFor every branch that is up to date or successfully pushed, add an upstream (tracking) reference, used by argument-less, git-pull and other commands.\nSo, after pushing your local branch with the -u option, this local branch will be automatically linked with the remote branch, and you can use git pull without any arguments.\n\n\nA2:\n\nThis error mostly caused by WRONG URL, please check:\n\nhttp or https\nURL Name\nusername@git_url\nwrong git name\n\n\n\nA3:\n\nI copied a small sub project I had that was under Git source control into another project and forgot to delete the .git folder. When I went to commit I got the same message as above and couldn't clear it until I deleted the .git folder.\nIt is a bit silly, but it is worth checking you don't have a .git folder under the folder that doesn't commit.\n\n\nA4:\n\nIn my case my github account did not have permissions to the repo. Added the github account as a collaborator for the repo and that fixed it.\n\n\nA5:\n\ndelete each .git file in all projects\nyou can use this command\nrm -rf .git\n\n\n\nA6:\n\nAssume that I would like to add FreeRTOS repository, which URL is https://github.com/FreeRTOS/FreeRTOS-Kernel.git, into my repository, example URL is https://github.com/username/example as a submodule\ngit submodule add https://github.com/FreeRTOS/FreeRTOS-Kernel.git\ngit add .\ngit commit -m 'add a submodule'\ngit push\n\nTo clone using HTTPS:\ngit clone https://github.com/username/example.git --recurse-submodules\n\nUsing SSH:\ngit clone git@github.com:username/example.git --recurse-submodules\n\nIf you have downloaded the repo without using the --recurse-submodules argument, you need to run:\ngit submodule update --init --recursive\n\n\n\nA7:\n\nYou should do:\ngit commit . -m \"save arezzo files\"\n\n\n\nA8:\n\nIf you are not using GitHub 2FA, another reason for asking you your password is:  \nYour GitHub repo url is an ssh one, not an https one.\nSince git does not find your ssh keys (or your ssh key is not registered to your GitHub account), it fails back to account/password mechanism. \nIf you are using 2FA (and an https url), then you need a personal token as a password.\n\n\nA9:\n\nThe reason why this is happening is because you have a folder that is already being tracked by Git inside another folder that is also tracked by Git. For example, I had a project and I added a subfolder to it. Both of them were being tracked by Git before I put one inside the other. In order to stop tracking the one inside, find it and remove the Git file with:\nrm -rf .git\n\nIn my case I had a WordPress application and the folder I added inside was a theme. So I had to go to the theme root, and remove the Git file, so that the whole project would now be tracked by the parent, the WordPress application.\n\n\nA10:\n\nIn my case I needed to put my ssh password instead of my personal access token. Then it worked.\n\n\nA11:\n\nJust another stupid reason: you may have changed credentials on your laptop, but there is another development PC running but not frequently used, that has SourceTree started and uses the old credentials frequently to poll status.\nSounds stupid, but took me some time to solve :-)\n\n\nA12:\n\nIn my case, the Sourcetree login screen repeatedly showed when pushing to GitHub. Logging in from that screen failed. I finally determined it was due to an expired OAuth token, which is trivial to fix.\n\nNavigate to the GitHub account config screen: Tools > Options > Authentication\nClick your GitHub account and then click edit\nOn the Edit Hosting Account dialog click the Refresh OAuth Token button\n\n\n\nA13:\n\n\ngit init\ngit add .\ngit commit -m \"initial commit\"\ngit remote add origin [URL]\ngit push origin masteror\n\ngit push -f origin master\n\ngit push --force.\nIt does force the update\n\n\n\nA14:\n\nIf you are using two-factor authentication with GitHub you will need to create a personal access token and use it with SourceTree:\n\nTo work with GitHub's two-factor authentication in SourceTree you can simply use your access token instead of your password. The steps to do this are as follows:\n\nGo to your Personal Access Tokens settings in GitHub.\nClick on the Generate new token button.\nName the token something descriptive.\nSelect which scopes you wish to grant this token.\nClick the Generate token button.\nCopy the token and use it as a password in your hosted repositories.\n\nYou can find more information about this on GitHub's help here.\n\nMac Users:\nIf your SourceTree keeps on asking for the password, go to the Terminal and type this:\ngit config --global credential.helper osxkeychain\n\n\n\nA15:\n\nMake sure that your user account is added to the repository as a collaborator.\nSetting --> Collaborators\n\n\nA16:\n\nI think it is very preferable if you first pull the existing Github repo on the local and then add the new files to the Github repo\nThis link will help: https://stackoverflow.com/a/61285076/5840973\n\n\nA17:\n\nIn addition to setting up the Personal Access Token on GitHub, and doing what A_01 states above, this may be helpful (it's how I finally got it working)\n\nI made sure had the latest version of Sourcetree \nI upgraded to the latest version of and used System Git (Tools -> Options -> Git)\nUpgraded to the latest Git Credential Manager for Windows (this was via an installer I downloaded and double-clicked, very easy)\nEnsured I was using OAuth for my GitHub account stored in Tools -> Options -> Authentication\nMade sure I checked off the box \"Allow Sourcetree to manage my credentials using the Git Credential Manager\" (this was important!!) on Tools -> Options -> Git\nThen when I triggered the GitHub dialog (via pushing a new branch, just an example), I logged in there (this is where it used to fail).\nI next got the 2 factor auth dialog, and the code was sent to my phone as expected. I was able to enter the code from my phone into this dialog.\n\n\n\nA18:\n\nif you have a subfolder, which was cloned from other git-Repository, first you have to remove the $.git$ file from the child-Repository:\nrm -rf .git\nafter that you can change to parent folder and use git add -A. \n\n\nA19:\n\nIn my case, this was happening with a submodule that had defaulted to https even though the main project was using ssh. I fixed it by changing the submodule's repo path in SourceTree's Repository Settings to use the ssh path instead of https.\n\n\nA20:\n\nThis happened to me and at first it was not apparent what had gone wrong. The previous day I had renamed my repo on github.com only changing the first character of the repo name from a lowercase letter to uppercase. I was not aware that the repo name in the origin URL was case-sensitive unit I received the error which contained the text, \"did you run git update-server-info on the server.\" Once I edited the .git/config file to correct the case of the same letter in the origin URL, the problem was solved, the error no longer occurred and I was once again able to push my changes up to the repo on github.com. Another bit of advice: if you make changes to your repo on github, make sure to test sync'ing your local repo right away. In my case, I did not and it was the next day when I had tried to sync and couldn't because of the error and I forgot that I had renamed the repo, so I was scratching my head for a bit. Had I tested sync'ing right away, I would have caught this problem right away.\n\n\nA21:\n\nif you have more files in my case i have 7000 image files when i try to add them from project's route folder it hasn't added them but when i go to the image folder everything is ok. Go through the target folder and command like abows\ngit add .\ngit commit -am \"image uploading\"\ngit push origin master\n\n\n\ngit push origin master Enumerating objects: 6574, done. Counting objects: 100% (6574/6574), done. Delta compression using up to 4\n    threads Compressing objects: 100% (6347/6347), done. Writing objects: \n    28% (1850/6569), 142.17 MiB | 414.00 KiB/s\n\n\n\n\nA22:\n\nSourceTree version 2.4.8.0\nThe problem for me seemed to be the order of setup between SourceTree and GitHub. I setup SourceTree first and just clicked all the \"setup later\" options.\nAn easy fix - Uninstall and re-install SourceTree. The non-obvious bit was needing to delete the actual program files manually.\nFrom any \"SourceTree.exe\" shortcut, open file location. Mine forked at \"C:\\Users\\myusername\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Atlassian\" where the shortcut pointed to another shortcut. \nOpen that shortcut's file location. I eventually ended up at \"C:\\Users\\myusername\\AppData\\Local\\SourceTree\". Delete the whole \"SourceTree\" folder. Go up one level to \"...\\Local\" and delete the \"Atlassian\" folder as well.\nNow uninstall SourceTree. Make sure your GitHub is setup the way you desire (for me, I also created and saved an SSH key that I used for SourceTree setup). Re-install SourceTree using all the correct credentials, point to the right repository and such. \nAfter all this, SourceTree asked for my GitHub credentials twice, with two different dialog boxes, then stopped and worked! No more endless loop. Good luck!\n\n\nA23:\n\nAlso make sure the repo you've entered is cased correctly (it's case sensitive).\n\n\nA24:\n\nIn my repo's directory on my machine in the terminal I had to reset the origin url:\ngit remote set-url origin git@github.com:repoaccountname/repo-name.git\n\n\n\nA25:\n\nIf you are using \"Username & Password\" authentication,you can try to change to use SSH keys for the authentication.\nI have met this kind of situation when I commit to GitLab everytime.When I changed the way of authentication:SSH keys,the problem has been solved.\n\n\nA26:\n\nI am using a SSH key, but SourceTree was asking me a password every time.\nFound on the community of Atlassian, this solution worked for me:\n\nOpen the terminal, get into your project directory\nType git config credential.helper store\nType git pull\nInput username/password (if asked)\n\nVoil\u00e0 !\n\n\nA27:\n\nYou didn't add the changes. Either specifically add them via\ngit add filename1 filename2\n\nor add all changes (from root path of the project)\ngit add .\n\nor use the shorthand -a while commiting:\ngit commit -a -m \"message\".\n\n\n\nA28:\n\nOpen your Terminal, access to this folder and write:\ngit init\ngit add .\ngit commit -m \"my commit\"\ngit remote set-url origin git@github.com:username/repo.git\ngit push origin master\n\n\n\nA29:\n\n1. first create a git repostry.\n2. second open git bash in  existing or uploading project.\n3. perform git init \n4. git add .\n5. git commit -m \"print message\"\n6. git remote add github<repostry url>\n7. git remote -v\n8. git push github master\n\nOR\ngit push origin master\n\nif  you get any error, you may use it\ngit push -f origin master\n\n\n\nA30:\n\nReally struggled with this on Mac, couldn't get it to work with SSH. Ended up following this guide which worked: https://www.youtube.com/watch?v=otRHRPnHDeI.\nThere were a couple things which could have been the cause, it may not have been the SSH itself. For instance, Oauth apparently doesn't work on SourceTree despite nothing appearing wrong immediately when using it. Making sure you use a Personal Access Token and not your real password among others.\nA good rule of thumb is, if after connecting your account it only shows your public repos, it's already broken. https://github.com/settings/tokens\n\n\nA31:\n\nMy issue was that I used the clone https url widget provided by github. That URL doesn't work for private repositories as you need to add a username to the front of it.  \nExample: a private repo owned by john and named widget with collaborator sam the correct url would be: \nhttps://sam@github.com/john/widget.git\nThe github provided url: \nhttps://github.com/john/widget.git\nThe error message leaves much to be desired.\n\n\nA32:\n\nI encountered this message when using Jenkins 2.176.1 and git plugin 3.10.0 using a very old command line git version (1.7.1) as included on CentOS 6.\nThe message does not occur on newer versions of command line git (1.8 or later) with the Jenkins git plugin.\nUpgrading to a newer command line git version resolves the problem.\nOfficially, the Jenkins git plugin does not support command line git 1.7.1.  The minimum supported version is command line git 1.7.10. Many Jenkins git plugin use cases require at least command line git 1.9.0.\nCommand line git 1.7.1 behaves differently than later versions when a repository has been initialized (with git init) and then a git fetch is performed with a refspec which references 'origin'.  Later versions of command line git do not have the same problem.\n\n\nA33:\n\nA solution worked for me given by Andrew Magill, Copied from Source : link\nThe problem is that SourceTree's embedded Git client comes with git-credential-manager v1.12, which no longer works with Github since they disabled TLS 1.1 on their site.  You need to get git-credential-manager v1.14.  Easiest way to do that is to install a current copy of the Git client separately, and then switch SourceTree over to use that (\"system git\") instead of its embedded client.  Alternatively, you can update git-credential-manager in your embedded client by replacing its files with the newest version from Microsoft.\n\n\nA34:\n\nI received this same error when I attempted to git clone something from Kiln what was actually a Mercurial repository.\n\n\nA35:\n\nFor windows user\n\nchange SSH Client from Putty/Plink to OpenSSH\n\nin Tools -> Option -> SSHClient\n\n\n\nA36:\n\nSo i had this project that wasnt under source control i made some changes to and wanted to keep stuff i changed.\ngit init\ngit remote add origin <url>\ngit fetch\ngit branch master origin/master\ngit restore --staged .\n\n\n\nA37:\n\nI got the same problem while using a github repository, and connecting to it via https, while using the OS X Keychain Credential helper.\nMy problem was that I had the wrong credentials stored in OS X's Keychain (I was using the email address that I used to sign up for github.com rather than the [username]@github.com address it provides you). I deleted the old account in the keychain and only left the @github.com one and it fixed the problem.\nNot sure if it is related, but when I checked the user.email local config:\ngit config -l\n\nit showed the incorrect email address as well, so I updated the local git user.email to use the correct account too:\ngit config user.email <username>@github.com\n\n\n\nA38:\n\nDid you create a new repository on the http://github.com with the same name? \nIf not, do it! And make sure each letter is correct and case sensitive.\n\n\nA39:\n\nYou could have done a:\ngit add -u -n\n\nTo check which files you modified and are going to be added (dry run: -n option), and then\ngit add -u\n\nTo add just modified files\n\n\nA40:\n\nHere is what worked for me.\nI tried almost all the solutions mentioned here and on the web a lot of times. I gave up multiple times before and was using it this way for more than a year I think. Finally, I was able to solve this annoying issue.\n\nAs suggested by user \"AEQ\" in his answer, I deleted all the passwords related to GitHub from my Keychain Access.\nTried deleting and re-adding the Github account. But still, the problem persisted. (generated a personal access token from \"https://github.com/settings/tokens\" and use it as the password)\n\nWhat made it work was\nMake sure to use HTTPS and not SSH when adding the new user account. If you have already added the account, just edit it. That's it.\nFinally, It feels good.\nAttaching screenshots for ease.\n\n\n\nA41:\n\nMaybe an obvious thing, but...\nIf you have problem with the index, use git-gui. You get a very good view how the index (staging area) actually works.\nAnother source of information that helped me understand the index was Scott Chacons \"Getting Git\" page 259 and forward.\nI started off using the command line because most documentation only showed that...\nI think git-gui and gitk actually make me work faster, and I got rid of bad habits like \"git pull\" for example... Now I always fetch first... See what the new changes really are before I merge.\n\n\nA42:\n\nSince this question is specifically about SourceTree and Github, the answer in 2021 is simple. Quoting bcr:\n\nEnsured I was using OAuth for my GitHub account stored in Tools ->\nOptions -> Authentication\n\n\nGo there. Select Add (new account).\nSelect OAuth.\nClick Refresh OAuth Token. A browser page will open (github.com) requesting renewal of your token.\nLogin to accept. SourceTree will immediately respond by acknowledging the token provided by github.com.\n\n\n\nA43:\n\nIn my case I had old version of the git. Upgrade to latest version fixed this issue.\n\n\nA44:\n\nGo to Sourctree -> Tools -> Options -> Git Select \n\n\nA45:\n\nI had a very similar issue with the same error message. \"Changes not staged for commit\", yet when I do a diff it shows differences.  I finally figured out that a while back I had changed a directories case. ex. \"PostgeSQL\" to \"postgresql\".  As I remember now sometimes git will leave a file or two behind in the old case directory.  Then you will commit a new version to the new case.\nThus git doesn't know which one to rely on.  So to resolve it, I had to go onto the github's website. Then you're able to view both cases.  And you must delete all the files in the incorrect cased directory. Be sure that you have the correct version saved off or in the correct cased directory.\nOnce you have deleted all the files in the old case directory, that whole directory will disappear. Then do a commit.\nAt this point you should be able to do a Pull on your local computer and not see the conflicts any more.  Thus being able to commit again.  :)\n\n\nA46:\n\nYou might have changed your repository name\nIn your local repository edit the file:\n.git/config\n\nThen check:\n[remote \"origin\"]\n   url = \n\nthat the URL matches your remote repository\n\n\nA47:\n\nGo to the directory where you code is,\ngit init\ngit add .\ngit commit -m \"Your message\"\n\nNow add your address go to your git hub copy the clone address,\ngit remote add origin <remote repository URL>\n\nNow add push your code with,\ngit push -u -f origin master\n\nAnd you are done.\n\n\nA48:\n\nThis error could also happen if the repository you are attempting to reach was deleted.\n\n\nA49:\n\nIt's the same as --set-upstream\nIt's used to set origin as the upstream remote in your git config.\nIt may help if you don't want to manually specify the remote every time you run git push.\nAlso ...\nAs you're new to stackOverflow, take your time to read What types of questions should I avoid asking? section of the Help because sometimes you can find the answers by simply browsing the documentation.\n\n\nA50:\n\nIn my case, I was trying to clone a private repository in our group Github account to me laptop:\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone https://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nfatal: https://github.com/nyusngroup/velocity_CCSNe.git/info/refs?service=git-upload-pack not found: did you run git update-server-info on the server?\n\nI found two ways can solve this.\n(1) Use http instead of https. It asks me the name and password of our group Github account. After I entered the information, I can clone it.\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone http://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nUsername for 'http://github.com':nyusngroup\nPassword for 'http://nyusngroup@github.com': \n\n(2) Add my Github account to the collaborators of the private repository in our group Github account, as pointed by Monkey King's answer above.\n\n\nA51:\n\nI met up with the same problem.\nHow I solved this problem is:\nI use git bash to create a new repo, when I typed \"git push origin master\"\nIt reported \n\"fatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\"\nFinally, I found there was not a repo on the github at all.\nYou'd better create a new repo first on github. \nMaybe this experience can help somebody.\n\n\nA52:\n\nI got this issue when using GitStack. I looked into C:/GitStack/apache/logs/error.log and found that GitStack was looking for the repository in the wrong location although its settings said D:/Repositories (my custom location), GitStack looked for the repositories in its default location. So I simply went and save the settings again from GitStack admin page. It solved the problem.\n\n\nA53:\n\nI find this problem appearing when I've done a git add . in a subdirectory below where my .gitignore file lives (the home directory of my repository, so to speak).  Try changing directories to your uppermost directory and running git add . followed by git commit -m \"my commit message\".\n\n\nA54:\n\nThis is what I discovered (after searching for 'sourcetree' in my drive C:):\n\nUninstall SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\Atlassian\\SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\SourceTree\nReinstall SourceTree\n\n\n\nA55:\n\nIn my case, I cloned the repo using HTTPS based url. Then I configured my githib account with 2-factor authentication using SSH key. So whenever I was pulling/pushing changes, it was showing me dialog to provide my password.\nI fixed it by enabling SSH access by right clicking on project repo-menu on SourceTree.\n\n\n\nA56:\n\nI finally was able to solve this after months of no luck. On a Mac :\n\nOpen 'Keychain Access' on a Mac by using Launchpad or Spotlight Search\nNarrow down your search by selecting \"login\" under Keychains and \"Passwords\" under the Category (both filters on the left side of the window)\nIn the top right hand corner of the window, do a search for \"git\"\nYou'll likely see a keychain there. I right-clicked and deleted the one entry I had.\nGo into sourcetree and try to do a pull/fetch/anything. You'll be prompted for a password. This should re-initiate your password with Sourcetree. To double check this, go into your keychain and repeat steps 3 & 4. Click on the keychain and look at the access control tab. SourcetreeLogin should be listed as always having access now.\n\n\n\n\nA57:\n\nprobably you were trying to clone like this: \ngit clone https://github.com/TeaCodie/TeaCodie-Website.git\nThen you got this error:\nfatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\nHere is what worked for me:\ngit clone https://github.com/TeaCodie/TeaCodie-Website.git/.git\nbecause the directory \"info/refs\" is in the .git directory.\n\n\nA58:\n\nIn my case I was using the wrong protocol in the repository URL (http instead of https)\n\n\nA59:\n\nFollow to @Chris answer in case you've enabled 2FA, to use Personal Access Token in SourceTree, you need to add your acc with this token in SourceTree > Preferences with following information:\n\nAuth Type: Basic  \nUsername: {your-github-account} \nPassword: {your-personal-access-token} \nProtocol: HTTPS\n\nAfter that, you are able to clone your Github repo.\n\n\n\nA60:\n\nI had more luck with navigating in my terminal to the directory I wanted to add to the repository, then (assuming you're working on a branch called master):\n    git init\n    git add .\n    git commit -m \"my commit\"\n    git remote add origin <remote repository URL>\n    git push origin master\n\nHere's a link to an article explaining how to do it in more detail: https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/\nNote that you won't be able to run the \"git add .\" line if the directory in question is open.\n\n\nA61:\n\nIt looks like that's a private (or deleted) repository; if you visit the repository page while logged it'll give you the real URL, which'll probably be https://TeaCodie@github.com/TeaCodie/TeaCodie-Website.git , i.e. with a username specified?\n\n\nA62:\n\nopen terminal and clone your repo. repository url must have your usedid,\nFor ex:\n$ git clone https://<username>@github.com/<userId>/<reponame>.git\n$ git clone https://mark@github.com/mak123/<reponame>.git\n\nThen drag and drop the cloned folder into the sourceTree window.You can also do by clicking \"+New Repository\" button and from the menu by selecting \"Add Existing Local Repository\".\nIt may ask for password again but this time it will get added to your keychain.\n\n\nA63:\n\nLets say you have remote repo with files and local repo with the same files.\nAnd you want to add Git to local files, and dont want to push.\nThen you can do such commands on local repo:\ngit init\ngit remote add origin <url>\ngit fetch --all\ngit reset --hard origin/master\n\nAfter that your repository is synced with remote repo.\nYou can change files, git add, push and so on.\n\n\nA64:\n\nAs the message says:\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nGit has a \"staging area\" where files need to be added before being committed, you can read an explanation of it here.\n\nFor your specific example, you can use:\ngit commit -am \"save arezzo files\"\n\n(note the extra a in the flags, can also be written as git commit -a -m \"message\" - both do the same thing)\nAlternatively, if you want to be more selective about what you add to the commit, you use the git add command to add the appropriate files to the staging area, and git status to preview what is about to be added (remembering to pay attention to the wording used).\nYou can also find general documentation and tutorials for how to use git on the git documentation page which will give more detail about the concept of staging/adding files.\n\nOne other thing worth knowing about is interactive staging - this allows you to add parts of a file to the staging area, so if you've made three distinct code changes (for related but different functionality), you can use interactive mode to split the changes and add/commit each part in turn. Having smaller specific commits like this can be helpful.\n\n\nA65:\n\nAll the answers above seems to guide about creating a new repository in git but the question is about adding a folder to existing repo. To do that following steps can be followed.\n\nClone your existing repo using following command:\ngit clone https://github.com/company/our_repo.git \nManually take your project folder to the desired location i.e. trunk/bin\nNow commit and then push in the repo using the commands:\ngit commit -m \"message\" and git push origin master \n\n\n\nA66:\n\nIn my case, the password that Sourcetree was asking for was for the ssh private key password. If I remove the password from the ssh key, this prompt doesn't come up.\nHowever since I wanted the password to be there, I kept investigating why it wasn't accepting my ssh key password.\nTurns out there is a wrinkle. If you have multiple ssh private keys which have different passwords, Sourcetree will give you a password prompt for each password.\nThink of it this way - if you have a fresh shell in your system, and you just started ssh-agent up, and ran ssh-add, what are the password prompts that you get? For every such password prompt, Sourcetree will give you this dialog box. In fact, if you add passwords to the dialog in the same order as you would in the shell to an ssh-add, this would work. In fact, I am guessing Sourcetree is doing exactly the same in the background.\nSo my final solution was 1) keep the passwords on the ssh keys 2) make all these passwords the same. So now Sourcetree will give me a single password prompt, and entering the ssh private key password will work.\n\n\nA67:\n\nI had a similar thing after an update; I tried re-adding my bitbucket credentials, changing the protocols, etc. with no luck, until...\n\nI opened my keychain and searched for bitbucket (probably the same for github, just search for github instead) under the Passwords category. \nI found that I had multiple Access Keys for my user, so I closed my repository, deleted all the keys of \"application password\" Kind, opened the SourceTree preferences > Accounts > removed my account and re-added it (login as usual - I used Basic with HTTPS). \nKeychain asks for permission to save the password, which I said yes and now I only see one key of the kind \"application password\". \n\nI opened my repository from the repo browser and hey presto, no more password popups!\nThis bugged me for a whole week! If this helped you, you can do a little dance with me now :)\n\n\nA68:\n\nFor future references, I solved this problem by changing the GIT system used in Sourcetree, from Embedded to System.\nHow to change the git system:\n \n1. Find the Options submenu (located under Tools menu) and click on it to view the Options popup \n2. On the Options setting popup, find the Git tab, then click on it to view the Git setting \n3. Find Git version setting, then check the version used in it. In my case, I changed it to system because previously I'm using Git bash cmd, and it looks like that the git embedded in Sourcetree clashes with each other. \n4. Restart Sourcetree\n\n\nA69:\n\nYou have to use -f when you are going to push on already existing repo.\ngit init\ngit add *\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin <repo url>\ngit push -f origin main\n\n\n\nA70:\n\nMac:\nThis solution works for me.\nAfter clone code successfully, you open terminal and run this code\ngit config --global credential.helper osxkeychain\n\n\n\nA71:\n\nThe easiest way is to create an app password and use that password.\nHere you can see how you can create an app password on Bitbucket:\nhttps://docs.snyk.io/tutorials/getting-started/atlassian-integrations/atlassian-bitbucket-app-password\n\n\nA72:\n\nI had an issue where I was doing commit --amend even after issuing a git add . and it still wasn't working.  Turns out I made some .vimrc customizations and my editor wasn't working correctly.  Fixing these errors so that vim returns the correct code resolved the issue.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " V3,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  "}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 49, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI\u2019ve got a search field. Right now it searches for every keyup. So if someone types \u201cWindows\u201d, it will make a search with AJAX for every keyup: \u201cW\u201d, \u201cWi\u201d, \u201cWin\u201d, \u201cWind\u201d, \u201cWindo\u201d, \u201cWindow\u201d, \u201cWindows\u201d.\nI want to have a delay, so it only searches when the user stops typing for 200 ms.\nThere is no option for this in the keyup function, and I have tried setTimeout, but it didn\u2019t work.\nHow can I do that?\n\n\nCandidate answers:\nA1:\n\nArray.prototype.map() index:\nOne can access the index Array.prototype.map() via the second argument of the callback function. Here is an example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst map = array.map((x, index) => {\n  console.log(index);\n  return x + index;\n});\n\nconsole.log(map);\n\n\n\nOther arguments of Array.prototype.map():\n\nThe third argument of the callback function exposes the array on which map was called upon\nThe second argument of Array.map() is a object which will be the this value for the callback function. Keep in mind that you have to use the regular function keyword in order to declare the callback since an arrow function doesn't have its own binding to the this keyword.\n\nFor example:\n\n\nconst array = [1, 2, 3, 4];\n\nconst thisObj = { prop1: 1 }\n\nconst map = array.map((x, index, array) => {\n  console.log(array);\n  console.log(this)\n}, thisObj);\n\n\n\n\n\nA2:\n\nHere's an example of what your app creation might look like using npx\n\nnpx create-react-app project-name --template all\n\n\n\nA3:\n\nNPM stands for Node Package Manager. NPM is Node.JS's default package manager. It's written in Javascript. The role of NPM is to manage the package and modules of node.js.\nNPX stands for Node Package Execute. NPX comes with npm, when npm is installed above the 5.2.0 version, it gets installed automatically. NPX is an npm package runner and its role is to execute the package from the registry without even installing that package.\nNow, the differences between NPM and NPX are as below:\n\ni) NPM is used to install the packages while NPX is used to execute the packages.\n\n\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages which are used by npx don't need to be taken care of as they are not installed globally.\n\n\n\nA4:\n\nnpx is a npm package runner (x probably stands for eXecute). One common way to use npx is to download and run a package temporarily or for trials.\ncreate-react-app is an npm package that is expected to be run only once in a project's lifecycle. Hence, it is preferred to use npx to install and run it in a single step.\nAs mentioned in the main page https://www.npmjs.com/package/npx, npx can run commands in the PATH or from node_modules/.bin by default.\nNote:\nWith some digging, we can find that create-react-app points to a Javascript file (possibly to /usr/lib/node_modules/create-react-app/index.js on Linux systems) that is executed within the node environment. This is simply a global tool that does some checks. The actual setup is done by react-scripts, whose latest version is installed in the project. Refer https://github.com/facebook/create-react-app for more info.\n\n\nA5:\n\nwith npx:\nnpx create-react-app MyApp\n\nearlier:\nnpm install -g create-react-app\ncreate-react-app MyApp\n\nEarlier it was necessary to install a CLI tool locally before using it, with npx that is not required anymore.\n\n\nA6:\n\n\nsuppose you have an array like\n\n\n\n   const arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    \n    \n    arr.map((myArr, index) => {\n      console.log(`your index is -> ${index} AND value is ${myArr}`);\n    })\n\n\n\n> output will be\n index is -> 0 AND value is 1\n index is -> 1 AND value is 2\n index is -> 2 AND value is 3\n index is -> 3 AND value is 4\n index is -> 4 AND value is 5\n index is -> 5 AND value is 6\n index is -> 6 AND value is 7\n index is -> 7 AND value is 8\n index is -> 8 AND value is 9\n\n\n\nA7:\n\nUse \nmytimeout = setTimeout( expression, timeout );\n\nwhere expression is the script to run and timeout is the time to wait in milliseconds before it runs - this does NOT hault the script, but simply delays execution of that part until the timeout is done.\nclearTimeout(mytimeout);\n\nwill reset/clear the timeout so it does not run the script in expression (like a cancel) as long as it has not yet been executed.\n\n\nA8:\n\nAs per the JSON docs at Mozilla, JSON.stringify has a second parameter replacer which can be used to filter/ignore children items while parsing the tree. However, perhaps you can avoid the circular references.\nIn Node.js we cannot. So we can do something like this:\nfunction censor(censor) {\n  var i = 0;\n  \n  return function(key, value) {\n    if(i !== 0 && typeof(censor) === 'object' && typeof(value) == 'object' && censor == value) \n      return '[Circular]'; \n    \n    if(i >= 29) // seems to be a harded maximum of 30 serialized objects?\n      return '[Unknown]';\n    \n    ++i; // so we know we aren't using the original object anymore\n    \n    return value;  \n  }\n}\n\nvar b = {foo: {bar: null}};\n\nb.foo.bar = b;\n\nconsole.log(\"Censoring: \", b);\n\nconsole.log(\"Result: \", JSON.stringify(b, censor(b)));\n\nThe result:\nCensoring:  { foo: { bar: [Circular] } }\nResult: {\"foo\":{\"bar\":\"[Circular]\"}}\n\nUnfortunately there seems to be a maximum of 30 iterations before it automatically assumes it's circular. Otherwise, this should work. I even used areEquivalent from here, but JSON.stringify still throws the exception after 30 iterations. Still, it's good enough to get a decent representation of the object at a top level, if you really need it. Perhaps somebody can improve upon this though? In Node.js for an HTTP request object, I'm getting:\n{\n\"limit\": null,\n\"size\": 0,\n\"chunks\": [],\n\"writable\": true,\n\"readable\": false,\n\"_events\": {\n    \"pipe\": [null, null],\n    \"error\": [null]\n},\n\"before\": [null],\n\"after\": [],\n\"response\": {\n    \"output\": [],\n    \"outputEncodings\": [],\n    \"writable\": true,\n    \"_last\": false,\n    \"chunkedEncoding\": false,\n    \"shouldKeepAlive\": true,\n    \"useChunkedEncodingByDefault\": true,\n    \"_hasBody\": true,\n    \"_trailer\": \"\",\n    \"finished\": false,\n    \"socket\": {\n        \"_handle\": {\n            \"writeQueueSize\": 0,\n            \"socket\": \"[Unknown]\",\n            \"onread\": \"[Unknown]\"\n        },\n        \"_pendingWriteReqs\": \"[Unknown]\",\n        \"_flags\": \"[Unknown]\",\n        \"_connectQueueSize\": \"[Unknown]\",\n        \"destroyed\": \"[Unknown]\",\n        \"bytesRead\": \"[Unknown]\",\n        \"bytesWritten\": \"[Unknown]\",\n        \"allowHalfOpen\": \"[Unknown]\",\n        \"writable\": \"[Unknown]\",\n        \"readable\": \"[Unknown]\",\n        \"server\": \"[Unknown]\",\n        \"ondrain\": \"[Unknown]\",\n        \"_idleTimeout\": \"[Unknown]\",\n        \"_idleNext\": \"[Unknown]\",\n        \"_idlePrev\": \"[Unknown]\",\n        \"_idleStart\": \"[Unknown]\",\n        \"_events\": \"[Unknown]\",\n        \"ondata\": \"[Unknown]\",\n        \"onend\": \"[Unknown]\",\n        \"_httpMessage\": \"[Unknown]\"\n    },\n    \"connection\": \"[Unknown]\",\n    \"_events\": \"[Unknown]\",\n    \"_headers\": \"[Unknown]\",\n    \"_headerNames\": \"[Unknown]\",\n    \"_pipeCount\": \"[Unknown]\"\n},\n\"headers\": \"[Unknown]\",\n\"target\": \"[Unknown]\",\n\"_pipeCount\": \"[Unknown]\",\n\"method\": \"[Unknown]\",\n\"url\": \"[Unknown]\",\n\"query\": \"[Unknown]\",\n\"ended\": \"[Unknown]\"\n}\n\nI created a small Node.js module to do this here: https://github.com/ericmuyser/stringy Feel free to improve/contribute!\n\n\nA9:\n\nI resolve this problem on NodeJS like this:\nvar util = require('util');\n\n// Our circular object\nvar obj = {foo: {bar: null}, a:{a:{a:{a:{a:{a:{a:{hi: 'Yo!'}}}}}}}};\nobj.foo.bar = obj;\n\n// Generate almost valid JS object definition code (typeof string)\nvar str = util.inspect(b, {depth: null});\n\n// Fix code to the valid state (in this example it is not required, but my object was huge and complex, and I needed this for my case)\nstr = str\n    .replace(/<Buffer[ \\w\\.]+>/ig, '\"buffer\"')\n    .replace(/\\[Function]/ig, 'function(){}')\n    .replace(/\\[Circular]/ig, '\"Circular\"')\n    .replace(/\\{ \\[Function: ([\\w]+)]/ig, '{ $1: function $1 () {},')\n    .replace(/\\[Function: ([\\w]+)]/ig, 'function $1(){}')\n    .replace(/(\\w+): ([\\w :]+GMT\\+[\\w \\(\\)]+),/ig, '$1: new Date(\"$2\"),')\n    .replace(/(\\S+): ,/ig, '$1: null,');\n\n// Create function to eval stringifyed code\nvar foo = new Function('return ' + str + ';');\n\n// And have fun\nconsole.log(JSON.stringify(foo(), null, 4));\n\n\n\nA10:\n\nNPX:\nFrom https://www.futurehosting.com/blog/npx-makes-life-easier-for-node-developers-plus-node-vulnerability-news/:\n\nWeb developers can have dozens of projects on their development\n  machines, and each project has its own particular set of npm-installed\n  dependencies. A few years back, the usual advice for dealing with CLI\n  applications like Grunt or Gulp was to install them locally in each\n  project and also globally so they could easily be run from the command\n  line.\nBut installing globally caused as many problems as it solved. Projects\n  may depend on different versions of command line tools, and polluting\n  the operating system with lots of development-specific CLI tools isn\u2019t\n  great either. Today, most developers prefer to install tools locally\n  and leave it at that.\nLocal versions of tools allow developers to pull projects from GitHub\n  without worrying about incompatibilities with globally installed\n  versions of tools. NPM can just install local versions and you\u2019re good\n  to go. But project specific installations aren\u2019t without their\n  problems: how do you run the right version of the tool without\n  specifying its exact location in the project or playing around with\n  aliases?\nThat\u2019s the problem npx solves. A new tool included in NPM 5.2, npx is\n  a small utility that\u2019s smart enough to run the right application when\n  it\u2019s called from within a project.\nIf you wanted to run the project-local version of mocha, for example,\n  you can run npx mocha inside the project and it will do what you\n  expect.\nA useful side benefit of npx is that it will automatically install npm\n  packages that aren\u2019t already installed. So, as the tool\u2019s creator Kat\n  March\u00e1n points out, you can run npx benny-hill without having to deal\n  with Benny Hill polluting the global environment.\nIf you want to take npx for a spin, update to the most recent version\n  of npm.\n\n\n\nA11:\n\nSimple Definition:\nnpm - Javascript package manager \nnpx - Execute npm package binaries\n\n\nA12:\n\nNPM vs. NPX\nNPM stands for the Node Package Manager. A text based program for Nodejs package management.\nWhile NPX is a Node Package Runner. Its function is to execute the Nodejs package\nNPX will execute binary files from the Nodejs package, both installed and not.\nEven NPX can also help us use certain versions of Nodejs without having to use nvm (node.js version management), nave (node.js virtual environment), and nvm (node.js version management).\n\n\nA13:\n\nIntroducing npx: an npm package runner\nNPM - Manages packages but doesn't make life easy executing any.NPX - A tool for executing Node packages.\n\nNPX comes bundled with NPM version 5.2+ \n\nNPM by itself does not simply run any package. It doesn't run any package as a matter of fact. If you want to run a package using NPM, you must specify that package in your package.json file.\nWhen executables are installed via NPM packages, NPM links to them:\n\nlocal installs have \"links\" created at ./node_modules/.bin/ directory.\nglobal installs have \"links\" created from the global bin/ directory (e.g. /usr/local/bin) on Linux or at %AppData%/npm on Windows.\n\nDocumentation you should read\n\nNPM:\nOne might install a package locally on a certain project:\nnpm install some-package\n\nNow let's say you want NodeJS to execute that package from the command line:\n$ some-package\n\nThe above will fail. Only globally installed packages can be executed by typing their name only.\nTo fix this, and have it run, you must type the local path:\n$ ./node_modules/.bin/some-package\n\nYou can technically run a locally installed package by editing your packages.json file and adding that package in the scripts section:\n{\n  \"name\": \"whatever\",\n  \"version\": \"1.0.0\",\n  \"scripts\": {\n    \"some-package\": \"some-package\"\n  }\n}\n\nThen run the script using npm run-script (or npm run):\nnpm run some-package\n\n\nNPX:\nnpx will check whether <command> exists in $PATH, or in the local project binaries, and execute it. So, for the above example, if you wish to execute the locally-installed package some-package all you need to do is type:\nnpx some-package\n\nAnother major advantage of npx is the ability to execute a package which wasn't previously installed:\n$ npx create-react-app my-app\n\nThe above example will generate a react app boilerplate within the path the command had run in, and ensures that you always use the latest version of a generator or build tool without having to upgrade each time you\u2019re about to use it.\n\nUse-Case Example:\nnpx command may be helpful in the script section of a package.json file,\nwhen it is unwanted to define a dependency which might not be commonly used or any other reason:\n\"scripts\": {\n    \"start\": \"npx gulp@3.9.1\",\n    \"serve\": \"npx http-server\"\n}\n\nCall with: npm run serve\n\nRelated questions:\n\nHow to use package installed locally in node_modules?\nNPM: how to source ./node_modules/.bin folder?\nHow do you run a js file using npm scripts?\n\n\n\nA14:\n\nTake a look at the autocomplete plugin.  I know that it allows you to specify a delay or a minimum number of characters.  Even if you don't end up using the plugin, looking through the code will give you some ideas on how to implement it yourself.\n\n\nA15:\n\nNPM => Is a JS package manager.\nNPX => Is a tool for executing Node packages and execute npm package binaries.\nIt is easy to remember:\n-npm stands for MANAGER\n-npx stands for EXECUTE\n\n\nA16:\n\nSimplest Definition:\nNPX\n\nThe npx stands for Node Package Execute and it comes with the npm,\nwhen you installed npm above 5.2.0 version then automatically npx will\ninstalled. It is an npm package runner that can execute any package\nthat you want from the npm registry without even installing that\npackage.\n\nNPM\n\nnpm is a package manager for the JavaScript programming language\nmaintained by npm, Inc. npm is the default package manager for the\nJavaScript runtime environment Node.js. It consists of a command line\nclient, also called npm, and an online database of public and paid-for\nprivate packages\n\n\n\nA17:\n\nSimply npm is the Node Package Manager and\nnpx is the executeable version that run npm packages\n\n\nA18:\n\nSimple answer is like\nNPX: is used to execute any node package without installing the package on our machine.\nNPM: is used to install any node js package in our machine. We can use \"require(\"package-name')\" when we install any package using NPM. but we can not import the package when we use NPX.\nExample: You should run npm i axios\nin this case you are installing axios package in your local machine\nand npx create-react-app 'app-name'\nhere you are executing the create-react-app package directly on your machine without installing it's files.\n\n\nA19:\n\nNPM stands for Node Package Manager.\nIt comes pre-installed with Node.js. NPM helps to manage packages in your projects as dependencies.\nWhen using NPM, there are two ways to install a package into your local computer.\n\nLocally: When a package is installed locally, it is installed in\n./node_modules/.bin/ of the local project directory.\n\nGlobally: A global package is installed in the user environment\npath. /usr/local/bin for Linux and AppData%/npm for Windows.\n\n\nTo execute a locally installed package, it should be specified in the package.json scripts block as shown below.\n\"scripts\": {\n    \"your-package\":  \"your-package-name\"\n}\n\nThen, you can execute the package with:\nnpm run your-package-name\n\nNPX is an NPM package executor.\nCurrently, NPX is bundled with NPM when you install the NPM version 5.2.0 or higher.\nWhy NPX over NPM?\n\nNo need to edit the package.json file with node_modules paths.\n\nYou can directly execute the tool from the command line.\n\n\n\n\nA20:\n\nIt means that the object you pass in the request (I guess it is pagedoc) has a circular reference, something like:\nvar a = {};\na.b = a;\n\nJSON.stringify cannot convert structures like this.\nN.B.: This would be the case with DOM nodes, which have circular references, even if they are not attached to the DOM tree. Each node has an ownerDocument which refers to document in most cases. document has a reference to the DOM tree at least through document.body and document.body.ownerDocument refers back to document again, which is only one of multiple circular references in the DOM tree.\n\n\nA21:\n\nYou will be able to get the current iteration's index for the map method through its 2nd parameter.\nExample:\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nlist.map((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return currElement; //equivalent to list[index]\n});\n\nOutput:\nThe current iteration is: 0 <br>The current element is: h\n\nThe current iteration is: 1 <br>The current element is: e\n\nThe current iteration is: 2 <br>The current element is: l\n\nThe current iteration is: 3 <br>The current element is: l \n\nThe current iteration is: 4 <br>The current element is: o\n\nSee also: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array/map\n\nParameters\ncallback - \n    Function that produces an element of the new Array, taking three arguments:\n1) currentValue\n        The current element being processed in the array.\n2) index\n      The index of the current element being processed in the array.\n3) array\n      The array map was called upon.\n\n\n\nA22:\n\nIf you want to search after the type is done use a global variable to hold the timeout returned from your setTimout call and cancel it with a clearTimeout if it hasn't yet happend so that it won't fire the timeout except on the last keyup event\nvar globalTimeout = null;  \n$('#id').keyup(function(){\n  if(globalTimeout != null) clearTimeout(globalTimeout);  \n  globalTimeout =setTimeout(SearchFunc,200);  \n}   \nfunction SearchFunc(){  \n  globalTimeout = null;  \n  //ajax code\n}\n\nOr with an anonymous function :\nvar globalTimeout = null;  \n$('#id').keyup(function() {\n  if (globalTimeout != null) {\n    clearTimeout(globalTimeout);\n  }\n  globalTimeout = setTimeout(function() {\n    globalTimeout = null;  \n\n    //ajax code\n\n  }, 200);  \n}   \n\n\n\nA23:\n\nFor my case I was getting that error when I was using async function on my server-side to fetch documents using mongoose. It turned out that the reason was I forgot to put await before calling find({}) method. Adding that part fixed my issue. \n\n\nA24:\n\nI normally use the circular-json npm package to solve this.\n// Felix Kling's example\nvar a = {};\na.b = a;\n// load circular-json module\nvar CircularJSON = require('circular-json');\nconsole.log(CircularJSON.stringify(a));\n//result\n{\"b\":\"~\"}\n\nNote: circular-json has been deprecated, I now use flatted (from the creator of CircularJSON):\n// ESM\nimport {parse, stringify} from 'flatted/esm';\n\n// CJS\nconst {parse, stringify} = require('flatted/cjs');\n\nconst a = [{}];\na[0].a = a;\na.push(a);\n\nstringify(a); // [[\"1\",\"0\"],{\"a\":\"0\"}]\n\nfrom: https://www.npmjs.com/package/flatted\n\n\nA25:\n\nnpm is a tool that use to install packages and npx is a tool that use to execute packages.\nnpm-If you wish to run package through npm then you have to specify that package in your package.json and install it locally.\nnpx-A package can be executable without installing the package. It is an npm package runner so if any packages aren\u2019t already installed it will install them automatically.\n\n\nA26:\n\nNPM - NODE PACKAGE MANAGER\n\nit's a package manager.\n\nnode.js  package can be installed using NPM\n\n\nNPX - NODE PACKAGE EXECUTE\n\nit execute the  node.js  package\n\ndoesn't matter whether you installed  node.js  package locally or\nglobally.\n\nNPX will temporarily install package and run it.\n\nNPX is used when you want to execute package for single time.\n\n\n.\n\n\nA27:\n\nHere's an example of NPX in action: npx cowsay hello\nIf you type that into your bash terminal you'll see the result. The benefit of this is that npx has temporarily installed cowsay. There is no package pollution since cowsay is not permanently installed. This is great for one off packages where you want to avoid package pollution. \nAs mentioned in other answers, npx is also very useful in cases where (with npm) the package needs to be installed then configured before running. E.g. instead of using npm to install and then configure the json.package file and then call the configured run command just use npx instead. A real example:\nnpx create-react-app my-app\n\n\nA28:\n\nI use this small function for the same purpose, executing a function after the user has stopped typing for a specified amount of time or in events that fire at a high rate, like resize:\n\n\nfunction delay(callback, ms) {\n  var timer = 0;\n  return function() {\n    var context = this, args = arguments;\n    clearTimeout(timer);\n    timer = setTimeout(function () {\n      callback.apply(context, args);\n    }, ms || 0);\n  };\n}\n\n\n// Example usage:\n\n$('#input').keyup(delay(function (e) {\n  console.log('Time elapsed!', this.value);\n}, 500));\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js\"></script>\n<label for=\"input\">Try it:\n<input id=\"input\" type=\"text\" placeholder=\"Type something here...\"/>\n</label>\n\n\n\nHow it works:\nThe delay function will return a wrapped function that internally handles an individual timer, in each execution the timer is restarted with the time delay provided, if multiple executions occur before this time passes, the timer will just reset and start again.\nWhen the timer finally ends, the callback function is executed, passing the original context and arguments (in this example, the jQuery's event object, and the DOM element as this).\nUPDATE 2019-05-16\nI have re-implemented the function using ES5 and ES6 features for modern environments:\nfunction delay(fn, ms) {\n  let timer = 0\n  return function(...args) {\n    clearTimeout(timer)\n    timer = setTimeout(fn.bind(this, ...args), ms || 0)\n  }\n}\n\nThe implementation is covered with a set of tests.\nFor something more sophisticated, give a look to the jQuery Typewatch plugin.\n\n\nA29:\n\nUsing Ramda:\nimport {addIndex, map} from 'ramda';\n\nconst list = [ 'h', 'e', 'l', 'l', 'o'];\nconst mapIndexed = addIndex(map);\nmapIndexed((currElement, index) => {\n  console.log(\"The current iteration is: \" + index);\n  console.log(\"The current element is: \" + currElement);\n  console.log(\"\\n\");\n  return 'X';\n}, list);\n\n\n\nA30:\n\nnpm - Package manager for JavaScript, just like: pip (Python), Maven (Java), NuGet (.NET), Composer (PHP), RubyGems (Ruby), ...\nnpx - runs a command of a package without installing it explicitly.\nUse cases:\n\nYou don't want to install packages neither globally nor locally.\nYou don't have permission to install it globally.\nJust want to test some commands.\nSometime, you want to have a script command (generate, convert something, ...) in package.json to execute something without installing these packages as project's dependencies.\n\nSyntax:\nnpx [options] [-p|--package <package>] <command> [command-arg]...\n\nPackage is optional:\nnpx   -p uglify-js         uglifyjs --output app.min.js app.js common.js\n      +----------------+   +--------------------------------------------+\n      package (optional)   command, followed by arguments\n\nFor example:\nStart a HTTP Server      : npx http-server\nLint code                : npx eslint ./src\n                         # Run uglifyjs command in the package uglify-js\nMinify JS                : npx -p uglify-js uglifyjs -o app.min.js app.js common.js\nMinify CSS               : npx clean-css-cli -o style.min.css css/bootstrap.css style.css\nMinify HTML              : npx html-minifier index-2.html -o index.html --remove-comments --collapse-whitespace\nScan for open ports      : npx evilscan 192.168.1.10 --port=10-9999\nCast video to Chromecast : npx castnow http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\n\nMore about command:\n\nhttps://docs.npmjs.com/files/package.json#bin\nhttps://github.com/mishoo/UglifyJS2/blob/master/package.json#L17\n\n\n\nA31:\n\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages. \nIt doesn't matter whether you installed that package globally or locally. NPX will temporarily install it and run it. NPM also can run packages if you configure a package.json file and include it in the script section. \n\nSo remember this, if you want to check/run a node package  quickly without installing locally or globally use NPX.\n\nnpM -  Manager\nnpX -  Execute - easy to remember\n\n\nA32:\n\nIn my case I simply forgot to use async/await thing while building the route:\napp.get('/products', async (req, res) => {\n    const products = await Product.find();\n    res.send(products );\n});\n\n\n\nA33:\n\nnpm is package manager or installer on the other hand Packages used by npx are not installed globally so you have to carefree for the pollution for the long term.\n\n\nA34:\n\nnpm - package manager\nnpx - Execute npm package\nThis is a difference with it.\n\n\nA35:\n\nNPX is a tool for creating and executing some features in a new project\nNPM is the package manager that contains all of libraries\n\n\nA36:\n\nBased on zainengineer's answer... Another approach is to make a deep copy of the object and strip circular references and stringify the result.\n\n\nfunction cleanStringify(object) {\r\n    if (object && typeof object === 'object') {\r\n        object = copyWithoutCircularReferences([object], object);\r\n    }\r\n    return JSON.stringify(object);\r\n\r\n    function copyWithoutCircularReferences(references, object) {\r\n        var cleanObject = {};\r\n        Object.keys(object).forEach(function(key) {\r\n            var value = object[key];\r\n            if (value && typeof value === 'object') {\r\n                if (references.indexOf(value) < 0) {\r\n                    references.push(value);\r\n                    cleanObject[key] = copyWithoutCircularReferences(references, value);\r\n                    references.pop();\r\n                } else {\r\n                    cleanObject[key] = '###_Circular_###';\r\n                }\r\n            } else if (typeof value !== 'function') {\r\n                cleanObject[key] = value;\r\n            }\r\n        });\r\n        return cleanObject;\r\n    }\r\n}\r\n\r\n// Example\r\n\r\nvar a = {\r\n    name: \"a\"\r\n};\r\n\r\nvar b = {\r\n    name: \"b\"\r\n};\r\n\r\nb.a = a;\r\na.b = b;\r\n\r\nconsole.log(cleanStringify(a));\r\nconsole.log(cleanStringify(b));\n\n\n\n\n\nA37:\n\nOne approach is to strip object and functions from main object. And stringify the simpler form\nfunction simpleStringify (object){\n    // stringify an object, avoiding circular structures\n    // https://stackoverflow.com/a/31557814\n    var simpleObject = {};\n    for (var prop in object ){\n        if (!object.hasOwnProperty(prop)){\n            continue;\n        }\n        if (typeof(object[prop]) == 'object'){\n            continue;\n        }\n        if (typeof(object[prop]) == 'function'){\n            continue;\n        }\n        simpleObject[prop] = object[prop];\n    }\n    return JSON.stringify(simpleObject); // returns cleaned up JSON\n};\n\nif you are using node js use inspect()\nimport {inspect} from \"util\";\nconsole.log(inspect(object));\n\n\n\nA38:\n\nNPM: NPM stands for Node Package Manager and is the default package manager for Node.js. It was developed by Isaac Z. Schlueter and was originally released on January 12, 2010. It is entirely written in JavaScript. It consists of a command-line client npm which manages all node.js packages and modules. When node.js is installed, it is included in the installation.\nnpm run your-package-name\n\nNPX is a tool that use to execute packages.\nNPX is an acronym for Node Package Execute The NPX package comes with npm, so when you install npm above 5.2.0, NPX will be installed automatically.\nIt is an npm package runner that can execute any package that you want from the npm registry without even installing that package. The npx is useful during a single time use package. If you have installed npm below 5.2.0 then npx is not installed in your system.\nRun the following command to determine if npx is installed:\nnpx -v\n\nThe following command can be run if npx is not installed.\nnpm install -g npx\n\nUse npx to execute the package:\nnpx your-package-name\n\n\n\n\nA39:\n\nThe differences between NPM and NPX are as below:\ni) NPM is used to install the packages while NPX is used to execute the packages.\nii) Due to npm the packages installed have to be taken care of since it's installed globally while the packages used by npx don't need to be taken care of as they are not installed globally.\n\n\nA40:\n\nHere is the simple definition.\nNPM is a package manager, you can install node.js packages using NPM\nNPX is a tool to execute node.js packages.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "t,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 50, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nTheoretically I can say that \nfree(ptr);\nfree(ptr); \n\nis a memory corruption since we are freeing the memory which has already been freed.\nBut what if \nfree(ptr);\nptr=NULL;\nfree(ptr); \n\nAs the OS will behave in an undefined manner I cannot get an actual theoretical analysis for this about what's happening.\nWhatever I am doing, is this memory corruption or not?\nIs freeing a NULL pointer valid?\n\n\nCandidate answers:\nA1:\n\nnot memory corruption, but behavior depends on implementation.\nBy standard, it should be a legal code.\n\n\nA2:\n\nipp can hold a value of (i.e point to) a pointer to pointer type object. When you do  \nipp = &ip2;  \n\nthen the ipp contains the address of the variable (pointer) ip2, which is (&ip2) of type pointer to pointer. Now the arrow of ipp in second pic will point to ip2.   \nWiki says:\nThe * operator is a dereference operator operates on pointer variable, and returns an l-value (variable) equivalent to the value at pointer address. This is called dereferencing the pointer.  \nApplying * operator on ipp derefrence it to a l-value of pointer to int type. The dereferenced l-value *ipp is of type pointer to int, it can hold the address of an int type data. After the statement   \nipp = &ip1;\n\nipp is holding the address of ip1 and *ipp is holding the address of (pointing to) i. You can say that *ipp is an alias of ip1. Both **ipp and *ip1 are alias for i.\nBy doing  \n *ipp = ip2;  \n\n*ipp and ip2 both points to same location but ipp is still pointing to ip1.   \nWhat *ipp = ip2; does actually is that it copies the contents of ip2 (the address of j) to ip1 (as *ipp is an alias for ip1), in effect making both pointers ip1 and ip2 pointing to the same object (j).\nSo, in the second figure, arrow of ip1 and ip2 is pointing to j while ipp is still pointing to ip1 as no modification is done to change the value of ipp.\n\n\nA3:\n\nBecause you are changing the pointer of *ipp. It means  \n\nipp (varaiable name)----go inside.\ninside ipp is address of ip1.\nnow *ipp so go to (adress of inside) ip1.  \n\nNow we are at ip1.\n*ipp(i.e.ip1) = ip2.\nip2 contain address of j.so ip1 content will be replace by contain of ip2(i.e. address of j),\nWE ARE NOT CHANGING ipp CONTENT.\nTHAT'S IT. \n\n\nA4:\n\nThe difference is due to operator precedence.\nThe post-increment operator ++ has higher precedence than the dereference operator *. So *ptr++ is equivalent to *(ptr++). In other words, the post increment modifies the pointer, not what it points to.\nThe assignment operator += has lower precedence than the dereference operator *, so *ptr+=1 is equivalent to (*ptr)+=1. In other words, the assignment operator modifies the value that the pointer points to, and does not change the pointer itself. \n\n\nA5:\n\nLet's apply parentheses to show the order of operations\na + b / c\na + (b/c)\n\nLet's do it again with\n*ptr   += 1\n(*ptr) += 1\n\nAnd again with\n*ptr++\n*(ptr++)\n\n\nIn *ptr += 1, we increment the value of the variable our pointer points to.\nIn *ptr++, we increment the pointer after our entire statement (line of code) is done, and return a reference to the variable our pointer points to.\n\nThe latter allows you to do things like:\nfor(int i = 0; i < length; i++)\n{\n    // Copy value from *src and store it in *dest\n    *dest++ = *src++;\n\n    // Keep in mind that the above is equivalent to\n    *(dest++) = *(src++);\n}\n\nThis is a common method used to copy a src array into another dest array.\n\n\nA6:\n\n\nIf ptr is NULL, no operation is performed.\n\nsays the documentation.\n\n\nA7:\n\nfree(NULL) is perfectly legal in C as well as delete (void *)0 and delete[] (void *)0 are legal in C++.\nBTW, freeing memory twice usually causes some kind of runtime error, so it does not corrupt anything.\n\n\nA8:\n\nfree(ptr);\nptr=NULL;\nfree(ptr);/*This is perfectly safe */\n\nYou can safely delete a NULL pointer. No operation will be performed in that case.In other words  free() does nothing on a NULL pointer.\n\n\nA9:\n\nHow do pointers to pointers work in C?\nFirst a pointer is a variable, like any other variable, but that holds the address of a variable.\nA pointer to a pointer is a variable, like any other variable, but that holds the address of a variable.  That variable just happens to be a pointer.\nWhen would you use them?\nYou can use them when you need to return a pointer to some memory on the heap, but not using the return value. \nExample:\nint getValueOf5(int *p)\n{\n  *p = 5;\n  return 1;//success\n}\n\nint get1024HeapMemory(int **p)\n{\n  *p = malloc(1024);\n  if(*p == 0)\n    return -1;//error\n  else \n    return 0;//success\n}\n\nAnd you call it like this:\nint x;\ngetValueOf5(&x);//I want to fill the int varaible, so I pass it's address in\n//At this point x holds 5\n\nint *p;    \nget1024HeapMemory(&p);//I want to fill the int* variable, so I pass it's address in\n//At this point p holds a memory address where 1024 bytes of memory is allocated on the heap\n\nThere are other uses too, like the main() argument of every C program has a pointer to a pointer for argv, where each element holds an array of chars that are the command line options.  You must be careful though when you use pointers of pointers to point to 2 dimensional arrays, it's better to use a pointer to a 2 dimensional array instead.  \nWhy it's dangerous?\nvoid test()\n{\n  double **a;\n  int i1 = sizeof(a[0]);//i1 == 4 == sizeof(double*)\n\n  double matrix[ROWS][COLUMNS];\n  int i2 = sizeof(matrix[0]);//i2 == 240 == COLUMNS * sizeof(double)\n}\n\nHere is an example of a pointer to a 2 dimensional array done properly:\nint (*myPointerTo2DimArray)[ROWS][COLUMNS]\n\nYou can't use a pointer to a 2 dimensional array though if you want to support a variable number of elements for the ROWS and COLUMNS.  But when you know before hand you would use a 2 dimensional array.\n\n\nA10:\n\nLet's assume an 8 bit computer with 8 bit addresses (and thus only 256 bytes of memory). This is part of that memory (the numbers at the top are the addresses):\n  54   55   56   57   58   59   60   61   62   63   64   65   66   67   68   69\n+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n|    | 58 |    |    | 63 |    | 55 |    |    | h  | e  | l  | l  | o  | \\0 |    |\n+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n\nWhat you can see here, is that at address 63 the string \"hello\" starts. So in this case, if this is the only occurrence of \"hello\" in memory then,\nconst char *c = \"hello\";\n\n... defines c to be a pointer to the (read-only) string \"hello\", and thus contains the value 63. c must itself be stored somewhere: in the example above at location 58. Of course we can not only point to characters, but also to other pointers. E.g.:\nconst char **cp = &c;\n\nNow cp points to c, that is, it contains the address of c (which is 58). We can go even further. Consider:\nconst char ***cpp = &cp;\n\nNow cpp stores the address of cp. So it has value 55 (based on the example above), and you guessed it: it is itself stored at address 60.\n\nAs to why one uses pointers to pointers:\n\nThe name of an array usually yields the address of its first element. So if the array contains elements of type t, a reference to the array has type t *. Now consider an array of arrays of type t: naturally a reference to this 2D array will have type (t *)* = t **, and is hence a pointer to a pointer.\nEven though an array of strings sounds one-dimensional, it is in fact two-dimensional, since strings are character arrays. Hence: char **.\nA function f will need to accept an argument of type t ** if it is to alter a variable of type t *.\nMany other reasons that are too numerous to list here.\n\n\n\nA11:\n\nipp = &ip1;\n\nNo later assignment has changed the value of ipp. This is why it still points to ip1.\nWhat you do with *ipp, i.e., with ip1, does not change the fact that ipp points to ip1.\n\n\nA12:\n\nIf you'd want ipp to point to ip2, you'd have to say ipp = &ip2;. However, this would leave ip1 still pointing to i.\n\n\nA13:\n\nBecause when you say \n*ipp = ip2\n\nyou're saying the 'object pointed by ipp' to point the direction of memory that ip2 is pointing.\nYou're not saying ipp to point ip2.\n\n\nA14:\n\n*ptr += 1 : Increment data that ptr points to.\n*ptr++ : Increment pointer that is point to next memory location instead of the data that pointer points to.\n\n\nA15:\n\nNotice the assignments:\nipp = &ip1;\n\nresults ipp to point to ip1.\nso for ipp to point to ip2, we should change in the similar manner,\nipp = &ip2;\n\nwhich we are clearly not doing. Instead we are changing the value at address pointed by ipp.\nBy doing the folowing\n*ipp = ip2;\n\nwe are just replacing the value stored in ip1.\nipp = &ip1 , means *ipp = ip1 = &i, \nNow, *ipp = ip2 = &j.\nSo, *ipp = ip2 is essentially same as ip1 = ip2.\n\n\nA16:\n\nIf you add the dereference operator * to the pointer, you redirect from the pointer to the pointed-to object.\nExamples:\nint i = 0;\nint *p = &i; // <-- N.B. the pointer declaration also uses the `*`\n             //     it's not the dereference operator in this context\n*p;          // <-- this expression uses the pointed-to object, that is `i`\np;           // <-- this expression uses the pointer object itself, that is `p`\n\nTherefore:\n*ipp = ip2; // <-- you change the pointer `ipp` points to, not `ipp` itself\n            //     therefore, `ipp` still points to `ip1` afterwards.\n\n\n\nA17:\n\nPointers to Pointers\n\nSince we can have pointers to int, and pointers to char, and pointers to any structures we've defined, and in fact pointers to any type in C, it shouldn't come as too much of a surprise that we can have pointers to other pointers.\n\n\n\nA18:\n\nA pointer-to-a-pointer is used when a reference to a pointer is required.  For example, when you wish to modify the value (address pointed to) of a pointer variable declared in a calling function's scope inside a called function.\nIf you pass a single pointer in as an argument, you will be modifying local copies of the pointer, not the original pointer in the calling scope.  With a pointer to a pointer, you modify the latter.\n\n\nA19:\n\nBecause you changed the value pointed to by ipp not the value of ipp. So, ipp still points to ip1 (the value of ipp), ip1's value is now the same as ip2's value, so they both point to j.\nThis:\n*ipp = ip2;\n\nis the same as:\nip1 = ip2;\n\n\n\nA20:\n\n*ipp = ip2; implies:\nAssign ip2 to the variable pointed to by ipp. So this is equivalent to:\nip1 = ip2;\n\nIf you want the address of ip2 to be stored in ipp, simply do:\nipp = &ip2;\n\nNow ipp points to ip2.\n\n\nA21:\n\nWhen covering pointers on a programming course at university, we were given two hints as to how to begin learning about them. The first was to view Pointer Fun With Binky. The second was to think about the Haddocks' Eyes passage from Lewis Carroll's Through the Looking-Glass\n\n\u201cYou are sad,\u201d the Knight said in an anxious tone: \u201cLet me sing you a song to comfort you.\u201d\n\u201cIs it very long?\u201d Alice asked, for she had heard a good deal of poetry that day.\n\u201cIt's long,\u201d said the Knight, \u201cbut it's very, very beautiful. Everybody that hears me sing it - either it brings the tears to their eyes, or else -\u201d\n\u201cOr else what?\u201d said Alice, for the Knight had made a sudden pause.\n\u201cOr else it doesn't, you know. The name of the song is called \u2018Haddocks' Eyes.\u2019\u201d\n\u201cOh, that's the name of the song, is it?\" Alice said, trying to feel interested.\n\u201cNo, you don't understand,\u201d the Knight said, looking a little vexed. \u201cThat's what the name is called. The name really is \u2018The Aged Aged Man.\u2019\u201d\n\u201cThen I ought to have said \u2018That's what the song is called\u2019?\u201d Alice corrected herself.\n\u201cNo, you oughtn't: that's quite another thing! The song is called \u2018Ways And Means\u2019: but that's only what it's called, you know!\u201d\n\u201cWell, what is the song, then?\u201d said Alice, who was by this time completely bewildered.\n\u201cI was coming to that,\u201d the Knight said. \u201cThe song really is \u2018A-sitting On A Gate\u2019: and the tune's my own invention.\u201d\n\n\n\nA22:\n\nThe order of precedence for the 3 operators involved in your question is the following :\npost-increment ++ > dereference * > assignment +=\nYou can check this page for further details on the subject.\n\nWhen parsing an expression, an operator which is listed on some row will be bound tighter (as if by parentheses) to its arguments than any operator that is listed on a row further below it. For example, the expression *p++ is parsed as *(p++), and not as (*p)++.\n\nLong story short, in order to express this assignment *ptr+=1 using the post-increment operator you need to add parentheses to the dereference operator to give that operation precedence over ++ as in this (*ptr)++\n\n\nA23:\n\nA pointer to a pointer is also called a handle. One usage for it is often when an object can be moved in memory or removed. One is often responsible to lock and unlock the usage of the object so it will not be moved when accessing it.\nIt's often used in memory restricted environment, ie the Palm OS.\n\ncomputer.howstuffworks.com Link>>\nwww.flippinbits.com Link>>\n\n\n\nA24:\n\nLike most beginner questions in the C tag, this question can be answered by going back to first principles:\n\nA pointer is a kind of value.\nA variable contains a value.\nThe & operator turns a variable into a pointer.\nThe * operator turns a pointer into a variable.\n\n(Technically I should say \"lvalue\" instead of \"variable\", but I feel it is more clear to describe mutable storage locations as \"variables\".)\nSo we have variables:\nint i = 5, j = 6;\nint *ip1 = &i, *ip2 = &j;\n\nVariable ip1 contains a pointer. The & operator turns i into a pointer and that pointer value is assigned to ip1. So ip1 contains a pointer to i.\nVariable ip2 contains a pointer. The & operator turns j into a pointer and that pointer is assigned to ip2. So ip2 contains a pointer to j.\nint **ipp = &ip1;\n\nVariable ipp contains a pointer. The & operator turns variable ip1 into a pointer and that pointer value is assigned to ipp. So ipp contains a pointer to ip1.\nLet's sum up the story so far:\n\ni contains 5\nj contains 6\nip1 contains \"pointer to i\"\nip2 contains \"pointer to j\"\nipp contains \"pointer to ip1\"\n\nNow we say\n*ipp = ip2;\n\nThe * operator turns a pointer back into a variable. We fetch the value of ipp, which is \"pointer to ip1 and turn it into a variable. What variable? ip1 of course!\nTherefore this is simply another way of saying \nip1 = ip2;\n\nSo we fetch the value of ip2. What is it? \"pointer to j\". We assign that pointer value to ip1, so ip1 is now \"pointer to j\"\nWe only changed one thing: the value of ip1:\n\ni contains 5\nj contains 6\nip1 contains \"pointer to j\"\nip2 contains \"pointer to j\"\nipp contains \"pointer to ip1\"\n\n\nWhy does ipp still point to ip1 and not ip2?\n\nA variable changes when you assign to it. Count the assignments; there cannot be more changes to variables than there are assignments!  You start by assigning to i, j, ip1, ip2 and ipp.  You then assign to *ipp, which as we've seen means the same as \"assign to ip1\". Since you didn't assign to ipp a second time, it didn't change!\nIf you wanted to change ipp then you'll have to actually assign to ipp:\nipp = &ip2;\n\nfor instance.\n\n\nA25:\n\nConsider the below figure and program to understand this concept better.\n\nAs per the figure, ptr1 is a single pointer which is having address of variable num.\nptr1 = &num;\n\nSimilarly ptr2 is a pointer to pointer(double pointer) which is having the address of pointer ptr1. \nptr2 = &ptr1;\n\nA pointer which points to another pointer is known as double pointer. In this example ptr2 is a double pointer.\nValues from above diagram :\nAddress of variable num has : 1000\nAddress of Pointer ptr1 is: 2000\nAddress of Pointer ptr2 is: 3000\n\nExample:\n#include <stdio.h>\n\nint main ()\n{\n   int  num = 10;\n   int  *ptr1;\n   int  **ptr2;\n\n   // Take the address of var \n   ptr1 = &num;\n\n   // Take the address of ptr1 using address of operator &\n   ptr2 = &ptr1;\n\n   // Print the value\n   printf(\"Value of num = %d\\n\", num );\n   printf(\"Value available at *ptr1 = %d\\n\", *ptr1 );\n   printf(\"Value available at **ptr2 = %d\\n\", **ptr2);\n}\n\nOutput:\nValue of num = 10\nValue available at *ptr1 = 10\nValue available at **ptr2 = 10\n\n\n\nA26:\n\n\nMy question is: Why in the second picture, ipp is still point to ip1 but not ip2?\n\nyou placed nice pictures, I'm going to try to make nice ascii art:\nLike @Robert-S-Barnes said in his answer: forget about pointers, and what points to what, but think in terms of memory. Basically, an int* means that it contains the address of a variable and an int** contains the address of a variable that contains the address of a variable. Then you can use the pointer's algebra to access the values or the addresses: &foo means address of foo, and *foo means value of the address contained in foo.\nSo, as pointers is about dealing with memory, the best way to actually make that \"tangible\" is to show what the pointers algebra does to the memory.\nSo, here's your program's memory (simplified for the purpose of the example):\nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [   |   |   |   |   ]\n\nwhen you do your initial code:\nint i = 5, j = 6;\nint *ip1 = &i, *ip2 = &j;\n\nhere's how your memory looks like:\nname:    i   j ip1 ip2\naddr:    0   1   2   3\nmem : [  5|  6|  0|  1]\n\nthere you can see ip1 and ip2 gets the addresses of i and j and ipp still does not exists.\nDon't forget that addresses are simply integers stored with a special type.\nThen you declare and defined ipp such as:\nint **ipp = &ip1;\n\nso here's your memory:\nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [  5|  6|  0|  1|  2]\n\nand then, you're changing the value pointed by the address stored in ipp, which is\nthe address stored in ip1:\n*ipp = ip2;\n\nthe program's memory is \nname:    i   j ip1 ip2 ipp\naddr:    0   1   2   3   4\nmem : [  5|  6|  1|  1|  2]\n\nN.B.: as int* is a special type, I prefer to always avoid declaring multiple pointers on the same line, as I think the int *x; or int *x, *y; notation can be misleading. I prefer to write int* x; int* y;\nHTH\n\n\nA27:\n\nVery beginning you set,\nipp = &ip1;\n\nNow dereference it as,\n*ipp = *&ip1 // Here *& becomes 1  \n*ipp = ip1   // Hence proved \n\n\n\nA28:\n\nConsidere each variable represented like this:\ntype  : (name, adress, value)\n\nso your variables should be represented like this\nint   : ( i ,  &i , 5 ); ( j ,  &j ,  6); ( k ,  &k , 5 )\n\nint*  : (ip1, &ip1, &i); (ip1, &ip1, &j)\n\nint** : (ipp, &ipp, &ip1)\n\nAs the value of ipp is &ip1 so the inctruction:\n*ipp = ip2;\n\nchanges the value at the addess &ip1 to the value of ip2, which means ip1 is changed:\n(ip1, &ip1, &i) -> (ip1, &ip1, &j)\n\nBut ipp still:\n(ipp, &ipp, &ip1)\n\nSo the value of ipp still &ip1 which means it still points to ip1.\n\n\nA29:\n\nI like this \"real world\" code example of pointer to pointer usage, in Git 2.0, commit 7b1004b:\n\nLinus once said:\n\nI actually wish more people understood the really core low-level kind of coding. Not big, complex stuff like the lockless name lookup, but simply good use of pointers-to-pointers etc.\nFor example, I've seen too many people who delete a singly-linked list entry by keeping track of the \"prev\" entry, and then to delete the entry, doing something like:\n\n   if (prev)\n     prev->next = entry->next;\n   else\n     list_head = entry->next;\n\nand whenever I see code like that, I just go \"This person doesn't understand pointers\". And it's sadly quite common.\n\nPeople who understand pointers just use a \"pointer to the entry pointer\", and initialize that with the address of the list_head. And then as they traverse the list, they can remove the entry without using any conditionals, by just doing a\n*pp =  entry->next\n\n\nApplying that simplification lets us lose 7 lines from this function even while adding 2 lines of comment.\n- struct combine_diff_path *p, *pprev, *ptmp;\n+ struct combine_diff_path *p, **tail = &curr;\n\n\n\nChris points out in the comments to the 2016 video \"Linus Torvalds's Double Pointer Problem\".\n\nkumar points out in the comments the blog post \"Linus on Understanding Pointers\", where Grisha Trubetskoy explains:\n\nImagine you have a linked list defined as:\n   typedef struct list_entry {\n       int val;\n       struct list_entry *next;\n   } list_entry;\n\nYou need to iterate over it from the beginning to end and remove a specific element whose value equals the value of to_remove.\nThe more obvious way to do this would be:\n   list_entry *entry = head; /* assuming head exists and is the first entry of the list */\n   list_entry *prev = NULL;\n   \n   while (entry) { /* line 4 */\n       if (entry->val == to_remove)     /* this is the one to remove ; line 5 */\n           if (prev)\n              prev->next = entry->next; /* remove the entry ; line 7 */\n           else\n               head = entry->next;      /* special case - first entry ; line 9 */\n   \n       /* move on to the next entry */\n       prev = entry;\n       entry = entry->next;\n   }\n\nWhat we are doing above is:\n\niterating over the list until entry is NULL, which means we\u2019ve reached the end of the list (line 4).\nWhen we come across an entry we want removed (line 5),\nwe assign the value of current next pointer to the previous one,\nthus eliminating the current element (line 7).\n\nThere is a special case above - at the beginning of the iteration there is no previous entry (prev is NULL), and so to remove the first entry in the list you have to modify head itself (line 9).\nWhat Linus was saying is that the above code could be simplified by making the previous element a pointer to a pointer rather than just a pointer.\nThe code then looks like this:\n   list_entry **pp = &head; /* pointer to a pointer */\n   list_entry *entry = head;\n\n   while (entry) {\n       if (entry->val == to_remove)\n           *pp = entry->next;\n       else\n            pp = &entry->next;\n       entry = entry->next;\n   }\n\nThe above code is very similar to the previous variant, but notice how we no longer need to watch for the special case of the first element of the list, since pp is not NULL at the beginning. Simple and clever.\nAlso, someone in that thread commented that the reason this is better is because *pp = entry->next is atomic. It is most certainly NOT atomic.\nThe above expression contains two dereference operators (* and ->) and one assignment, and neither of those three things is atomic.\nThis is a common misconception, but alas pretty much nothing in C should ever be assumed to be atomic (including the ++ and -- operators)!\n\n\n\nA30:\n\nForget for a second about the pointing analogy.  What a pointer really contains is a memory address.  The & is the \"address of\" operator - i.e. it returns the address in memory of an object.  The * operator gives you the object a pointer refers to, i.e. given a pointer containing an address, it returns the object at that memory address.  So when you do *ipp = ip2, what you are doing is *ipp get the object at the address held in ipp which is   ip1 and then assign to ip1 the value stored in ip2, which is the address of j. \nSimply\n& --> Address of\n* --> Value at\n\n\nA31:\n\nVery good question.\nIn K&R \"C programming language\" \"5.1 Pointers and Addresses\", we can get an answer for this.\n\"The unary operators * and & bind more tightly than arithmetic operators\"\n*ptr += 1      //Increment what ptr points to.\n\n\"Unary operators like * and ++ associate right to left.\"\n*ptr++        //Increment prt instead of what ptr point to.\n\n//It works like *(ptr++).\nThe correct way is:\n(*ptr)++      //This will work.\n\n\n\nA32:\n\nhope  this piece of code can help.\n#include <iostream>\n#include <stdio.h>\nusing namespace std;\n\nint main()\n{\n    int i = 5, j = 6, k = 7;\n    int *ip1 = &i, *ip2 = &j;\n    int** ipp = &ip1;\n    printf(\"address of value i: %p\\n\", &i);\n    printf(\"address of value j: %p\\n\", &j);\n    printf(\"value ip1: %p\\n\", ip1);\n    printf(\"value ip2: %p\\n\", ip2);\n    printf(\"value ipp: %p\\n\", ipp);\n    printf(\"address value of ipp: %p\\n\", *ipp);\n    printf(\"value of address value of ipp: %d\\n\", **ipp);\n    *ipp = ip2;\n    printf(\"value ipp: %p\\n\", ipp);\n    printf(\"address value of ipp: %p\\n\", *ipp);\n    printf(\"value of address value of ipp: %d\\n\", **ipp);\n}\n\nit outputs: \n\n\n\nA33:\n\n\n7.20.3.2 The free function \nSynopsis \n#include <stdlib.h> \nvoid free(void *ptr); \n\nDescription\nThe free function causes the space pointed to by ptr to be deallocated, that is, made \n  available for further allocation. If ptr is a null pointer, no action occurs.\n\nSee ISO-IEC 9899.\nThat being said, when looking at different codebases in the wild, you'll notice people sometimes do:\nif (ptr)\n  free(ptr);\n\nThis is because some C runtimes (I for sure remember it was the case on PalmOS) would crash when freeing a NULL pointer.\nBut nowadays, I believe it's safe to assume free(NULL) is a nop as per instructed by the standard.\n\n\nA34:\n\nMy very personal opinion is that pictures with arrows pointing this way or that make pointers harder to understand. It does make them seem like some abstract, mysterious entities. They are not. \nLike everything else in your computer, pointers are numbers. The name \"pointer\" is just a fancy way of saying \"a variable containing an address\".\nTherefore, let me stir things around by explaining how a computer actually works.\nWe have an int, it has the name i and the value 5. This is stored in memory. Like everything stored in memory, it needs an address, or we wouldn't be able to find it. Lets say i ends up at address 0x12345678 and its buddy j with value 6 ends up just after it. Assuming a 32-bit CPU where int is 4 bytes and pointers are 4 bytes, then the variables are stored in physical memory like this:\nAddress     Data           Meaning\n0x12345678  00 00 00 05    // The variable i\n0x1234567C  00 00 00 06    // The variable j\n\nNow we want to point at these variables. We create one pointer to int, int* ip1, and one int* ip2. Like everything in the computer, these pointer variables get allocated somewhere in memory too. Lets assume they end up at the next adjacent addresses in memory, immediately after j. We set the pointers to contain the addresses of the variables previously allocated: ip1=&i; (\"copy the address of i into ip1\") and ip2=&j. What happens between the lines is:\nAddress     Data           Meaning\n0x12345680  12 34 56 78    // The variable ip1(equal to address of i)\n0x12345684  12 34 56 7C    // The variable ip2(equal to address of j)\n\nSo what we got were just yet some 4 byte chunks of memory containing numbers. There's no mystical or magical arrows anywhere in sight. \nIn fact, just by looking at a memory dump, we can't tell whether the address 0x12345680 contains an int or int*. The difference is how our program chooses to use the contents stored at this address. (The task of our program is actually just to tell the CPU what to do with these numbers.)\nThen we add yet another level of indirection with int** ipp = &ip1;. Again, we just get a chunk of memory:\nAddress     Data           Meaning\n0x12345688  12 34 56 80    // The variable ipp\n\nThe pattern does seem familiar. Yet another chunk of 4 bytes containing a number.\nNow, if we had a memory dump of the above fictional little RAM, we could manually check where these pointers point. We peek at what's stored at the address of the ipp variable and find the contents 0x12345680. Which is of course the address where ip1 is stored. We can go to that address, check the contents there, and find the address of i, and then finally we can go to that address and find the number 5.\nSo if we take the contents of ipp, *ipp, we will get the address of the pointer variable ip1. By writing *ipp=ip2 we copy ip2 into ip1, it is equivalent to ip1=ip2. In either case we would get\nAddress     Data           Meaning\n0x12345680  12 34 56 7C    // The variable ip1\n0x12345684  12 34 56 7C    // The variable ip2\n\n\n(These examples were given for a big endian CPU)\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 51, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI am trying to highlight exactly what changed between two dataframes.\nSuppose I have two Python Pandas dataframes:\n\"StudentRoster Jan-1\":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n\"StudentRoster Jan-2\":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n\nMy goal is to output an HTML table that:\n\nIdentifies rows that have changed (could be int, float, boolean, string)\nOutputs rows with same, OLD and NEW values (ideally into an HTML table) so the consumer can clearly see what changed between two dataframes: \n\"StudentRoster Difference Jan-1 - Jan-2\":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was \"\" | now   \"On   vacation\"\n\n\nI suppose I could do a row by row and column by column comparison, but is there an easier way?\n\n\nCandidate answers:\nA1:\n\nMy answer is similar to the others. If you do not want to create a new function you can use what pandas has defined for you already. Use the pipe method.\ndf.pipe(lambda d: d[d['column'] == value])\n\n\n\nA2:\n\nIf you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows:\npd.DataFrame = apply_masks()\n\nUsage:\nA = pd.DataFrame(np.random.randn(4, 4), columns=[\"A\", \"B\", \"C\", \"D\"])\nA.le_mask(\"A\", 0.7).ge_mask(\"B\", 0.2)... (May be repeated as necessary\n\nIt's a little bit hacky but it can make things a little bit cleaner if you're continuously chopping and changing datasets according to filters.\nThere's also a general purpose filter adapted from Daniel Velkov above in the gen_mask function which you can use with lambda functions or otherwise if desired.\nFile to be saved (I use masks.py):\nimport pandas as pd\n\ndef eq_mask(df, key, value):\n    return df[df[key] == value]\n\ndef ge_mask(df, key, value):\n    return df[df[key] >= value]\n\ndef gt_mask(df, key, value):\n    return df[df[key] > value]\n\ndef le_mask(df, key, value):\n    return df[df[key] <= value]\n\ndef lt_mask(df, key, value):\n    return df[df[key] < value]\n\ndef ne_mask(df, key, value):\n    return df[df[key] != value]\n\ndef gen_mask(df, f):\n    return df[f(df)]\n\ndef apply_masks():\n\n    pd.DataFrame.eq_mask = eq_mask\n    pd.DataFrame.ge_mask = ge_mask\n    pd.DataFrame.gt_mask = gt_mask\n    pd.DataFrame.le_mask = le_mask\n    pd.DataFrame.lt_mask = lt_mask\n    pd.DataFrame.ne_mask = ne_mask\n    pd.DataFrame.gen_mask = gen_mask\n\n    return pd.DataFrame\n\nif __name__ == '__main__':\n    pass\n\n\n\nA3:\n\nJust want to add a demonstration using loc to filter not only by rows but also by columns and some merits to the chained operation.\nThe code below can filter the rows by value.\ndf_filtered = df.loc[df['column'] == value]\n\nBy modifying it a bit you can filter the columns as well.\ndf_filtered = df.loc[df['column'] == value, ['year', 'column']]\n\nSo why do we want a chained method? The answer is that it is simple to read if you have many operations. For example,\nres =  df\\\n    .loc[df['station']=='USA', ['TEMP', 'RF']]\\\n    .groupby('year')\\\n    .agg(np.nanmean)\n\n\n\nA4:\n\nThe first part is similar to Constantine, you can get the boolean of which rows are empty*:\nIn [21]: ne = (df1 != df2).any(1)\n\nIn [22]: ne\nOut[22]:\n0    False\n1     True\n2     True\ndtype: bool\n\nThen we can see which entries have changed:\nIn [23]: ne_stacked = (df1 != df2).stack()\n\nIn [24]: changed = ne_stacked[ne_stacked]\n\nIn [25]: changed.index.names = ['id', 'col']\n\nIn [26]: changed\nOut[26]:\nid  col\n1   score         True\n2   isEnrolled    True\n    Comment       True\ndtype: bool\n\nHere the first entry is the index and the second the columns which has been changed.\nIn [27]: difference_locations = np.where(df1 != df2)\n\nIn [28]: changed_from = df1.values[difference_locations]\n\nIn [29]: changed_to = df2.values[difference_locations]\n\nIn [30]: pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\nOut[30]:\n               from           to\nid col\n1  score       1.11         1.21\n2  isEnrolled  True        False\n   Comment     None  On vacation\n\n* Note: it's important that df1 and df2 share the same index here. To overcome this ambiguity, you can ensure you only look at the shared labels using df1.index & df2.index, but I think I'll leave that as an exercise.\n\n\nA5:\n\npandas provides two alternatives to Wouter Overmeire's answer which do not require any overriding. One is .loc[.] with a callable, as in\ndf_filtered = df.loc[lambda x: x['column'] == value]\n\nthe other is .pipe(), as in\ndf_filtered = df.pipe(lambda x: x.loc[x['column'] == value])\n\n\n\nA6:\n\nSince version 0.18.1 the .loc method accepts a callable for selection. Together with lambda functions you can create very flexible chainable filters:\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\ndf.loc[lambda df: df.A == 80]  # equivalent to df[df.A == 80] but chainable\n\ndf.sort_values('A').loc[lambda df: df.A > 80].loc[lambda df: df.B > df.A]\n\nIf all you're doing is filtering, you can also omit the .loc.\n\n\nA7:\n\nI had the same question except that I wanted to combine the criteria into an OR condition.  The format given by Wouter Overmeire combines the criteria into an AND condition such that both must be satisfied:\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n\nBut I found that, if you wrap each condition in (... == True) and join the criteria with a pipe, the criteria are combined in an OR condition, satisfied whenever either of them is true: \ndf[((df.A==1) == True) | ((df.D==6) == True)]\n\n\n\nA8:\n\nFilters can be chained using a Pandas query:\ndf = pd.DataFrame(np.random.randn(30, 3), columns=['a','b','c'])\ndf_filtered = df.query('a > 0').query('0 < b < 2')\n\nFilters can also be combined in a single query:\ndf_filtered = df.query('a > 0 and 0 < b < 2')\n\n\n\nA9:\n\nIf your two dataframes have the same ids in them, then finding out what changed is actually pretty easy. Just doing frame1 != frame2 will give you a boolean DataFrame where each True is data that has changed. From that, you could easily get the index of each changed row by doing changedids = frame1.index[np.any(frame1 != frame2,axis=1)].\n\n\nA10:\n\nMuch easier to use Pandas for basic one-hot encoding. If you're looking for more options you can use scikit-learn.\nFor basic one-hot encoding with Pandas you pass your data frame into the get_dummies function.\nFor example, if I have a dataframe called imdb_movies:\n\n...and I want to one-hot encode the Rated column, I do this:\npd.get_dummies(imdb_movies.Rated)\n\n\nThis returns a new dataframe with a column for every \"level\" of rating that exists, along with either a 1 or 0 specifying the presence of that rating for a given observation.\nUsually, we want this to be part of the original dataframe. In this case, we attach our new dummy coded frame onto the original frame using \"column-binding.\nWe can column-bind by using Pandas concat function:\nrated_dummies = pd.get_dummies(imdb_movies.Rated)\npd.concat([imdb_movies, rated_dummies], axis=1)\n\n\nWe can now run an analysis on our full dataframe.\nSIMPLE UTILITY FUNCTION\nI would recommend making yourself a utility function to do this quickly:\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    return(res)\n\nUsage:\nencode_and_bind(imdb_movies, 'Rated')\n\nResult:\n\nAlso, as per @pmalbu comment, if you would like the function to remove the original feature_to_encode then use this version:\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) \n\nYou can encode multiple features at the same time as follows:\nfeatures_to_encode = ['feature_1', 'feature_2', 'feature_3',\n                      'feature_4']\nfor feature in features_to_encode:\n    res = encode_and_bind(train_set, feature)\n\n\n\nA11:\n\nThe answer from @lodagro is great. I would extend it by generalizing the mask function as:\ndef mask(df, f):\n  return df[f(df)]\n\nThen you can do stuff like:\ndf.mask(lambda x: x[0] < 0).mask(lambda x: x[1] > 0)\n\n\n\nA12:\n\nI'm not entirely sure what you want, and your last line of code does not help either, but anyway:\n\"Chained\" filtering is done by \"chaining\" the criteria in the boolean index.\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n\nIf you want to chain methods, you can add your own mask method and use that one.\nIn [90]: def mask(df, key, value):\n   ....:     return df[df[key] == value]\n   ....:\n\nIn [92]: pandas.DataFrame.mask = mask\n\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\n\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [97]: df.mask('A', 1)\nOut[97]:\n   A  B  C  D\na  1  4  9  1\nd  1  3  9  6\n\nIn [98]: df.mask('A', 1).mask('D', 6)\nOut[98]:\n   A  B  C  D\nd  1  3  9  6\n\n\n\nA13:\n\n\nThis is unappealing as it requires I assign df to a variable before being able to filter on its values. \n\ndf[df[\"column_name\"] != 5].groupby(\"other_column_name\")\n\nseems to work: you can nest the [] operator as well. Maybe they added it since you asked the question.\n\n\nA14:\n\nYou can do it with numpy.eye and a using the array element selection mechanism:\nimport numpy as np\nnb_classes = 6\ndata = [[2, 3, 4, 0]]\n\ndef indices_to_one_hot(data, nb_classes):\n    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n    targets = np.array(data).reshape(-1)\n    return np.eye(nb_classes)[targets]\n\nThe the return value of indices_to_one_hot(nb_classes, data) is now\narray([[[ 0.,  0.,  1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.],\n        [ 1.,  0.,  0.,  0.,  0.,  0.]]])\n\nThe .reshape(-1) is there to make sure you have the right labels format (you might also have [[2], [3], [4], [0]]).\n\n\nA15:\n\nOne hot encoding with pandas is very easy:\ndef one_hot(df, cols):\n    \"\"\"\n    @param df pandas DataFrame\n    @param cols a list of columns to encode \n    @return a DataFrame with one-hot encoding\n    \"\"\"\n    for each in cols:\n        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n        df = pd.concat([df, dummies], axis=1)\n    return df\n\nEDIT:\nAnother way to one_hot using sklearn's LabelBinarizer :\nfrom sklearn.preprocessing import LabelBinarizer \nlabel_binarizer = LabelBinarizer()\nlabel_binarizer.fit(all_your_labels_list) # need to be global or remembered to use it later\n\ndef one_hot_encode(x):\n    \"\"\"\n    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n    : x: List of sample Labels\n    : return: Numpy array of one-hot encoded labels\n    \"\"\"\n    return label_binarizer.transform(x)\n\n\n\nA16:\n\nThis solution is more hackish in terms of implementation, but I find it much cleaner in terms of usage, and it is certainly more general than the others proposed.\nhttps://github.com/toobaz/generic_utils/blob/master/generic_utils/pandas/where.py\nYou don't need to download the entire repo: saving the file and doing\nfrom where import where as W\n\nshould suffice. Then you use it like this:\ndf = pd.DataFrame([[1, 2, True],\n                   [3, 4, False], \n                   [5, 7, True]],\n                  index=range(3), columns=['a', 'b', 'c'])\n# On specific column:\nprint(df.loc[W['a'] > 2])\nprint(df.loc[-W['a'] == W['b']])\nprint(df.loc[~W['c']])\n# On entire - or subset of a - DataFrame:\nprint(df.loc[W.sum(axis=1) > 3])\nprint(df.loc[W[['a', 'b']].diff(axis=1)['b'] > 1])\n\nA slightly less stupid usage example:\ndata = pd.read_csv('ugly_db.csv').loc[~(W == '$null$').any(axis=1)]\n\nBy the way: even in the case in which you are just using boolean cols,\ndf.loc[W['cond1']].loc[W['cond2']]\n\ncan be much more efficient than\ndf.loc[W['cond1'] & W['cond2']]\n\nbecause it evaluates cond2 only where cond1 is True.\nDISCLAIMER: I first gave this answer elsewhere because I hadn't seen this.\n\n\nA17:\n\nIf you set your columns to search as indexes, then you can use DataFrame.xs() to take a cross section. This is not as versatile as the query answers, but it might be useful in some situations.\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(\n    np.random.randint(3, size=(10, 5)),\n    columns=list('ABCDE')\n)\n\ndf\n# Out[55]: \n#    A  B  C  D  E\n# 0  0  2  2  2  2\n# 1  1  1  2  0  2\n# 2  0  2  0  0  2\n# 3  0  2  2  0  1\n# 4  0  1  1  2  0\n# 5  0  0  0  1  2\n# 6  1  0  1  1  1\n# 7  0  0  2  0  2\n# 8  2  2  2  2  2\n# 9  1  2  0  2  1\n\ndf.set_index(['A', 'D']).xs([0, 2]).reset_index()\n# Out[57]: \n#    A  D  B  C  E\n# 0  0  2  2  2  2\n# 1  0  2  1  1  0\n\n\n\nA18:\n\nHere is a solution using DictVectorizer and the Pandas DataFrame.to_dict('records') method.\n>>> import pandas as pd\n>>> X = pd.DataFrame({'income': [100000,110000,90000,30000,14000,50000],\n                      'country':['US', 'CAN', 'US', 'CAN', 'MEX', 'US'],\n                      'race':['White', 'Black', 'Latino', 'White', 'White', 'Black']\n                     })\n\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> v = DictVectorizer()\n>>> qualitative_features = ['country','race']\n>>> X_qual = v.fit_transform(X[qualitative_features].to_dict('records'))\n>>> v.vocabulary_\n{'country=CAN': 0,\n 'country=MEX': 1,\n 'country=US': 2,\n 'race=Black': 3,\n 'race=Latino': 4,\n 'race=White': 5}\n\n>>> X_qual.toarray()\narray([[ 0.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  1.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.,  1.],\n       [ 0.,  1.,  0.,  0.,  0.,  1.],\n       [ 0.,  0.,  1.,  1.,  0.,  0.]])\n\n\n\nA19:\n\nSo the way I see it is that you do two things when sub-setting your data ready for analysis.\n\nget rows\nget columns\n\nPandas has a number of ways of doing each of these and some techniques that help get rows and columns. For new Pandas users it can be confusing as there is so much choice.\nDo you use iloc, loc, brackets, query, isin, np.where, mask etc...\nMethod chaining\nNow method chaining is a great way to work when data wrangling. In R they have a simple way of doing it, you select() columns and you filter() rows.\nSo if we want to keep things simple in Pandas why not use the filter() for columns and the query() for rows. These both return dataframes and so no need to mess-around with boolean indexing, no need to add df[ ] round the return value.\nSo what does that look like:-\ndf.filter(['col1', 'col2', 'col3']).query(\"col1 == 'sometext'\")\n\nYou can then chain on any other methods like groupby, dropna(), sort_values(), reset_index() etc etc.\nBy being consistent and using filter() to get your columns and query() to get your rows it will be easier to read your code when coming back to it after a time.\nBut filter can select rows?\nYes this is true but by default query() get rows and filter() get columns. So if you stick with the default there is no need to use the axis= parameter.\nquery()\nquery() can be used with both and/or &/| you can also use comparison operators > , < , >= , <=, ==, !=. You can also use Python in, not in.\nYou can pass a list to query using @my_list\nSome examples of using query to get rows\ndf.query('A > B')\n\ndf.query('a not in b')\n\ndf.query(\"series == '2206'\")\n\ndf.query(\"col1 == @mylist\")\n\ndf.query('Salary_in_1000 >= 100 & Age < 60 & FT_Team.str.startswith(\"S\").values')\n\nfilter()\nSo filter is basicly like using bracket df[] or df[[]] in that it uses the labels to select columns. But it does more than the bracket notation.\nfilter has like= param so as to help select columns with partial names.\ndf.filter(like='partial_name',)\n\nfilter also has regex to help with selection\ndf.filter(regex='reg_string')\n\nSo to sum up this way of working might not work for ever situation e.g. if you want to use indexing/slicing then iloc is the way to go. But this does seem to be a solid way of working and can simplify your workflow and code.\n\n\nA20:\n\nYou can also leverage the numpy library for logical operations. Its pretty fast. \ndf[np.logical_and(df['A'] == 1 ,df['B'] == 6)]\n\n\n\nA21:\n\nFirstly, easiest way to one hot encode: use Sklearn.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\nSecondly, I don't think using pandas to one hot encode is that simple (unconfirmed though)\nCreating dummy variables in pandas for python\nLastly, is it necessary for you to one hot encode? One hot encoding exponentially increases the number of features, drastically increasing the run time of any classifier or anything else you are going to run. Especially when each categorical feature has many levels. Instead you can do dummy coding.\nUsing dummy encoding usually works well, for much less run time and complexity. A wise prof once told me, 'Less is More'. \nHere's the code for my custom encoding function if you want.\nfrom sklearn.preprocessing import LabelEncoder\n\n#Auto encodes any dataframe column of type category or object.\ndef dummyEncode(df):\n        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n        le = LabelEncoder()\n        for feature in columnsToEncode:\n            try:\n                df[feature] = le.fit_transform(df[feature])\n            except:\n                print('Error encoding '+feature)\n        return df\n\nEDIT: Comparison to be clearer:\nOne-hot encoding: convert n levels to n-1 columns.\nIndex  Animal         Index  cat  mouse\n  1     dog             1     0     0\n  2     cat       -->   2     1     0\n  3    mouse            3     0     1\n\nYou can see how this will explode your memory if you have many different types (or levels) in your categorical feature. Keep in mind, this is just ONE column.\nDummy Coding:\nIndex  Animal         Index  Animal\n  1     dog             1      0   \n  2     cat       -->   2      1 \n  3    mouse            3      2\n\nConvert to numerical representations instead. Greatly saves feature space, at the cost of a bit of accuracy.\n\n\nA22:\n\nI offer this for additional examples.  This is the same answer as https://stackoverflow.com/a/28159296/ \nI'll add other edits to make this post more useful.\npandas.DataFrame.query\nquery was made for exactly this purpose.  Consider the dataframe df\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(\n    np.random.randint(10, size=(10, 5)),\n    columns=list('ABCDE')\n)\n\ndf\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\n\nLet's use query to filter all rows where D > B\ndf.query('D > B')\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n7  6  2  6  6  5\n\nWhich we chain\ndf.query('D > B').query('C > B')\n# equivalent to\n# df.query('D > B and C > B')\n# but defeats the purpose of demonstrating chaining\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n4  3  6  7  7  4\n5  5  3  7  5  9\n7  6  2  6  6  5\n\n\n\nA23:\n\nApproach 1: You can use pandas' pd.get_dummies.\nExample 1:\nimport pandas as pd\ns = pd.Series(list('abca'))\npd.get_dummies(s)\nOut[]: \n     a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n\nExample 2:\nThe following will transform a given column into one hot. Use prefix to have multiple dummies.\nimport pandas as pd\n        \ndf = pd.DataFrame({\n          'A':['a','b','a'],\n          'B':['b','a','c']\n        })\ndf\nOut[]: \n   A  B\n0  a  b\n1  b  a\n2  a  c\n\n# Get one hot encoding of columns B\none_hot = pd.get_dummies(df['B'])\n# Drop column B as it is now encoded\ndf = df.drop('B',axis = 1)\n# Join the encoded df\ndf = df.join(one_hot)\ndf  \nOut[]: \n       A  a  b  c\n    0  a  0  1  0\n    1  b  1  0  0\n    2  a  0  0  1\n\nApproach 2: Use Scikit-learn\nUsing a OneHotEncoder has the advantage of being able to fit on some training data and then transform on some other data using the same instance. We also have handle_unknown to further control what the encoder does with unseen data.\nGiven a dataset with three features and four samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding.\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> enc = OneHotEncoder()\n>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])   \nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n   handle_unknown='error', n_values='auto', sparse=True)\n>>> enc.n_values_\narray([2, 3, 4])\n>>> enc.feature_indices_\narray([0, 2, 5, 9], dtype=int32)\n>>> enc.transform([[0, 1, 1]]).toarray()\narray([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])\n\nHere is the link for this example: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n\n\nA24:\n\nYou can use numpy.eye function.\nimport numpy as np\n\ndef one_hot_encode(x, n_classes):\n    \"\"\"\n    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n    : x: List of sample Labels\n    : return: Numpy array of one-hot encoded labels\n     \"\"\"\n    return np.eye(n_classes)[x]\n\ndef main():\n    list = [0,1,2,3,4,3,2,1,0]\n    n_classes = 5\n    one_hot_list = one_hot_encode(list, n_classes)\n    print(one_hot_list)\n\nif __name__ == \"__main__\":\n    main()\n\nResult\nD:\\Desktop>python test.py\n[[ 1.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.]\n [ 0.  0.  1.  0.  0.]\n [ 0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  1.]\n [ 0.  0.  0.  1.  0.]\n [ 0.  0.  1.  0.  0.]\n [ 0.  1.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.]]\n\n\n\nA25:\n\npandas as has inbuilt function \"get_dummies\" to get one hot encoding of that particular column/s.\none line code for one-hot-encoding:\ndf=pd.concat([df,pd.get_dummies(df['column name'],prefix='column name')],axis=1).drop(['column name'],axis=1)\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " X,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 52, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nLet's call this table terms_relation:\n+---------+----------+-------------+------------+------------+--+\n| term_id | taxonomy | description | created_at | updated_at |  |\n+---------+----------+-------------+------------+------------+--+\n|       1 | categ    | non         | 3434343434 |   34343433 |  |\n|       2 | categ    | non         | 3434343434 | 3434343434 |  |\n|       3 | tag      | non         | 3434343434 | 3434343434 |  |\n|       4 | tag      | non         | 3434343434 | 3434343434 |  |\n+---------+----------+-------------+------------+------------+--+\n\nAnd this is table terms:\n+----+-------------+-------------+\n| id |    name     |    slug     |\n+----+-------------+-------------+\n|  1 | hello       | hello       |\n|  2 | how are you | how-are-you |\n|  3 | tutorial    | tutorial    |\n|  4 | the end     | the-end     |\n+----+-------------+-------------+\n\nHow Do I select all rows in table terms and table terms_relation where it's taxonomy in table terms_relation is categ? Will I need two queries for this or I could use a join statement?\n\n\nCandidate answers:\nA1:\n\nFIND_IN_SET is your friend in this case\nselect * from shirts where FIND_IN_SET(1,colors) \n\n\n\nA2:\n\nUse MySQL's FIELD() function:\nSELECT name, description, ...\nFROM ...\nWHERE id IN([ids, any order])\nORDER BY FIELD(id, [ids in order])\n\nFIELD() will return the index of the first parameter that is equal to the first parameter (other than the first parameter itself).\nFIELD('a', 'a', 'b', 'c')\nwill return 1\nFIELD('a', 'c', 'b', 'a')\nwill return 3\nThis will do exactly what you want if you paste the ids into the IN() clause and the FIELD() function in the same order.\n\n\nA3:\n\nMySQL's GROUP BY can be used without an aggregate function (which is contrary to the SQL standard), and returns the first row in the group (I don't know based on what criteria), while PostgreSQL must have an aggregate function (MAX, SUM, etc) on the column, on which the GROUP BY clause is issued.\n\n\nA4:\n\nI modified the method to include handling output of arrays for statements like WHERE IN (?). \nUPDATE: Just added check for NULL value and duplicated $params so actual $param values are not modified. \nGreat work bigwebguy and thanks!\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_string($value))\n            $values[$key] = \"'\" . $value . \"'\";\n\n        if (is_array($value))\n            $values[$key] = \"'\" . implode(\"','\", $value) . \"'\";\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n\n    $query = preg_replace($keys, $values, $query);\n\n    return $query;\n}\n\n\n\nA5:\n\nFor Oracle, John's solution using instr() function works. Here's slightly different solution that worked - \nSELECT id \nFROM table1 \nWHERE id IN (1, 20, 45, 60) \nORDER BY instr('1, 20, 45, 60', id)\n\n\nA6:\n\nTry this (subquery):\nSELECT * FROM terms WHERE id IN \n   (SELECT term_id FROM terms_relation WHERE taxonomy = \"categ\")\n\nOr you can try this (JOIN):\nSELECT t.* FROM terms AS t \n   INNER JOIN terms_relation AS tr \n   ON t.id = tr.term_id AND tr.taxonomy = \"categ\"\n\nIf you want to receive all fields from two tables:\nSELECT t.id, t.name, t.slug, tr.description, tr.created_at, tr.updated_at \n  FROM terms AS t \n   INNER JOIN terms_relation AS tr \n   ON t.id = tr.term_id AND tr.taxonomy = \"categ\"\n\n\n\nA7:\n\nIf the set of colors is more or less fixed, the most efficient and also most readable way would be to use string constants in your app and then use MySQL's SET type with FIND_IN_SET('red',colors) in your queries. When using the SET type with FIND_IN_SET, MySQL uses one integer to store all values and uses binary \"and\" operation to check for presence of values which is way more efficient than scanning a comma-separated string.\nIn SET('red','blue','green'), 'red' would be stored internally as 1, 'blue' would be stored internally as 2 and 'green' would be stored internally as 4. The value 'red,blue' would be stored as 3 (1|2) and 'red,green' as 5 (1|4).\n\n\nA8:\n\nI just tried to do this is MS SQL Server where we do not have FIELD():\nSELECT table1.id\n... \nINNER JOIN\n    (VALUES (10,1),(3,2),(4,3),(5,4),(7,5),(8,6),(9,7),(2,8),(6,9),(5,10)\n    ) AS X(id,sortorder)\n        ON X.id = table1.id\n    ORDER BY X.sortorder\n\nNote that I am allowing duplication too.\n\n\nA9:\n\nIf you don't have an aggregate function in your where clause, another possible source of the 1111 - Invalid use of group function error is if you have nested aggregate functions:\nselect sum(avg(close)) from prices;\n(1111, 'Invalid use of group function')\n\nYou can get around this by breaking up the problem into two steps:\n\nSave the inner aggregation into a variable\n\nselect @avg:=avg(close) from prices;\n\n\nRun the outer aggregation against the variable\n\nselect sum(@avg) from prices;\n\n\n\nA10:\n\nI think that .uniq [1] will solve your problem. \n[1] Available.select('...').uniq\n\nTake a look at http://guides.rubyonrails.org/active_record_querying.html#selecting-specific-fields\n\n\nA11:\n\nIf you want to do arbitrary sorting on a query using values inputted by the query in MS SQL Server 2008+, it can be done by creating a table on the fly and doing a join like so (using nomenclature from OP).\nSELECT table1.name, table1.description ... \nFROM (VALUES (id1,1), (id2,2), (id3,3) ...) AS orderTbl(orderKey, orderIdx) \nLEFT JOIN table1 ON orderTbl.orderKey=table1.id\nORDER BY orderTbl.orderIdx\n\nIf you replace the VALUES statement with something else that does the same thing, but in ANSI SQL, then this should work on any SQL database.\nNote: \nThe second column in the created table (orderTbl.orderIdx) is necessary when querying record sets larger than 100 or so. I originally didn't have an orderIdx column, but found that with result sets larger than 100 I had to explicitly sort by that column; in SQL Server Express 2014 anyways.\n\n\nA12:\n\nWhat kind of field is this? The IN operator cannot be used with a single field, but is meant to be used in subqueries or with predefined lists:\n-- subquery\nSELECT a FROM x WHERE x.b NOT IN (SELECT b FROM y);\n-- predefined list\nSELECT a FROM x WHERE x.b NOT IN (1, 2, 3, 6);\n\nIf you are searching a string, go for the LIKE operator (but this will be slow):\n-- Finds all rows where a does not contain \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE '%text%';\n\nIf you restrict it so that the string you are searching for has to start with the given string, it can use indices (if there is an index on that field) and be reasonably fast:\n-- Finds all rows where a does not start with \"text\"\nSELECT * FROM x WHERE x.a NOT LIKE 'text%';\n\n\n\nA13:\n\nThe classic way would be to add commas to the left and right:\nselect * from shirts where CONCAT(',', colors, ',') like '%,1,%'\n\nBut find_in_set also works:\nselect * from shirts where find_in_set('1',colors) <> 0\n\n\n\nA14:\n\nTwo solutions that spring to mind:\n\norder by case id when 123 then 1 when 456 then 2 else null end asc\norder by instr(','||id||',',',123,456,') asc\n\n(instr() is from Oracle; maybe you have locate() or charindex() or something like that)\n\n\nA15:\n\nAlternative method:\nSELECT * FROM x WHERE CHARINDEX('$x', field1, 0)<1 \n\nExplanation: It searches for occurrences of '$x' starting from position 0. And only selects where count is 'less than one'.\n\n\nA16:\n\nPDOStatement has a public property $queryString. It should be what you want.\nI've just notice that PDOStatement has an undocumented method debugDumpParams() which you may also want to look at.\n\n\nA17:\n\nAccording to MySQL's \"Debuking GROUP BY Myths\" http://dev.mysql.com/tech-resources/articles/debunking-group-by-myths.html. SQL (2003 version of the standard) doesn't requires columns referenced in the SELECT list of a query to also appear in the GROUP BY clause.\n\n\nA18:\n\nThis is actually how your query works and is a normal behaviour. Using LIMIT you will not limit the count or sum but only the returned rows. So your query will return n rows as stated in your LIMIT clause. And since your query actually returns only one row, applying a (non-zero) limit has no effect on the results.\nHowever, your second query will work as expected and is an established way of solving this problem.\n\n\nA19:\n\n1. For MySQL:\nSELECT FIND_IN_SET(5, columnname) AS result \nFROM table\n\n2.For Postgres SQL :\nSELECT * \nFROM TABLENAME f\nWHERE 'searchvalue' = ANY (string_to_array(COLUMNNAME, ','))\n\nExample\nselect * \nfrom customer f\nwhere '11' = ANY (string_to_array(customerids, ','))\n\n\n\nA20:\n\nFirst, the error you're getting is due to where you're using the COUNT function -- you can't use an aggregate (or group) function in the WHERE clause.\nSecond, instead of using a subquery, simply join the table to itself:\nSELECT a.pid \nFROM Catalog as a LEFT JOIN Catalog as b USING( pid )\nWHERE a.sid != b.sid\nGROUP BY a.pid\n\nWhich I believe should return only rows where at least two rows exist with the same pid but there is are at least 2 sids.  To make sure you get back only one row per pid I've applied a grouping clause.\n\n\nA21:\n\nMy first thought was to write a single query, but you said that was not possible because one is run by the user and the other is run in the background.  How are you storing the list of ids to pass from the user to the background process?  Why not put them in a temporary table with a column to signify the order.\nSo how about this:\n\nThe user interface bit runs and inserts values into a new table you create.  It would insert the id, position and some sort of job number identifier)\nThe job number is passed to the background process (instead of all the ids)\nThe background process does a select from the table in step 1 and you join in to get the other information that you require.  It uses the job number in the WHERE clause and orders by the position column.\nThe background process, when finished, deletes from the table based on the job identifier.\n\n\n\nA22:\n\nselect * from shirts where find_in_set('1',colors) <> 0\n\nWorks for me\n\n\nA23:\n\nCorrect, the solution to fixing this is to use :select and to select each field that you wish to decorate the resulting object with and group by them.\nNasty - but it is how group by should work as opposed to how MySQL works with it by guessing what you mean if you don't stick fields in your group by.\n\n\nA24:\n\nIf you're using MySQL, there is a method REGEXP that you can use...\nhttp://dev.mysql.com/doc/refman/5.1/en/regexp.html#operator_regexp\nSo then you would use:\nSELECT * FROM `shirts` WHERE `colors` REGEXP '\\b1\\b'\n\n\n\nA25:\n\nNot the prettiest solution, but changing the group parameter to output every column in model works in PostgreSQL:\nexpiration = Available.find(:all,\n:joins => [ :room ],\n:conditions => [ \"rooms.hotel_id = ? AND availables.bookdate BETWEEN ? AND ?\", hostel_id, date.to_s, (date+days-1).to_s ],\n:group => Available.column_names.collect{|col| \"availables.#{col}\"},\n:order => 'availables.updated_at')\n\n\n\nA26:\n\nYou should actually fix your database schema so that you have three tables:\nshirt: shirt_id, shirt_name\ncolor: color_id, color_name\nshirtcolor: shirt_id, color_id\n\nThen if you want to find all of the shirts that are red, you'd do a query like:\nSELECT *\nFROM shirt, color\nWHERE color.color_name = 'red'\n  AND shirt.shirt_id = shirtcolor.shirt_id\n  AND color.color_id = shirtcolor.color_id\n\n\n\nA27:\n\nFor others looking for a way to order by any field, including joined field, in postgresql, use a subquery:\nSELECT * FROM(\nSELECT DISTINCT ON(availables.bookdate) `availables`.* \nFROM `availables` INNER JOIN `rooms` ON `rooms`.id = `availables`.room_id \nWHERE (rooms.hotel_id = 5056 \nAND availables.bookdate BETWEEN '2009-11-22' AND '2009-11-24')\n) AS distinct_selected\nORDER BY availables.updated_at\n\nor arel:\n\nsubquery = SomeRecord.select(\"distinct on(xx.id) xx.*, jointable.order_field\")\n      .where(\"\").joins(\")\nresult = SomeRecord.select(\"*\").from(\"(#{subquery.to_sql}) AS distinct_selected\").order(\" xx.order_field ASC, jointable.order_field ASC\")\n\n\n\nA28:\n\nSELECT ORDER_NO, DELIVERY_ADDRESS \nfrom IFSAPP.PURCHASE_ORDER_TAB \nwhere ORDER_NO in ('52000077','52000079','52000167','52000297','52000204','52000409','52000126') \nORDER BY instr('52000077,52000079,52000167,52000297,52000204,52000409,52000126',ORDER_NO)\n\nworked really great\n\n\nA29:\n\nThis will work for sure, and I actually tried it out:\nlwdba@localhost (DB test) :: DROP TABLE IF EXISTS shirts;\nQuery OK, 0 rows affected (0.08 sec)\n\nlwdba@localhost (DB test) :: CREATE TABLE shirts\n    -> (<BR>\n    -> id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    -> ticketnumber INT,\n    -> colors VARCHAR(30)\n    -> );<BR>\nQuery OK, 0 rows affected (0.19 sec)\n\nlwdba@localhost (DB test) :: INSERT INTO shirts (ticketnumber,colors) VALUES\n    -> (32423,'1,2,5,12,15'),\n    -> (32424,'1,5,12,15,30'),\n    -> (32425,'2,5,11,15,28'),\n    -> (32426,'1,2,7,12,15'),\n    -> (32427,'2,4,8,12,15');\nQuery OK, 5 rows affected (0.06 sec)\nRecords: 5  Duplicates: 0  Warnings: 0\n\nlwdba@localhost (DB test) :: SELECT * FROM shirts WHERE LOCATE(CONCAT(',', 1 ,','),CONCAT(',',colors,',')) > 0;\n+----+--------------+--------------+\n| id | ticketnumber | colors       |\n+----+--------------+--------------+\n|  1 |        32423 | 1,2,5,12,15  |\n|  2 |        32424 | 1,5,12,15,30 |\n|  4 |        32426 | 1,2,7,12,15  |\n+----+--------------+--------------+\n3 rows in set (0.00 sec)\n\nGive it a Try !!!\n\n\nA30:\n\nYou can use a subquery:\nSELECT *\nFROM terms\nWHERE id IN (SELECT term_id FROM terms_relation WHERE taxonomy='categ');\n\nand if you need to show all columns from both tables:\nSELECT t.*, tr.*\nFROM terms t, terms_relation tr\nWHERE t.id = tr.term_id\nAND tr.taxonomy='categ'\n\n\n\nA31:\n\nYou need to use HAVING, not WHERE.\nThe difference is: the WHERE clause filters which rows MySQL selects. Then MySQL groups the rows together and aggregates the numbers for your COUNT function.\nHAVING is like WHERE, only it happens after the COUNT value has been computed, so it'll work as you expect. Rewrite your subquery as:\n(                  -- where that pid is in the set:\nSELECT c2.pid                  -- of pids\nFROM Catalog AS c2             -- from catalog\nWHERE c2.pid = c1.pid\nHAVING COUNT(c2.sid) >= 2)\n\n\n\nA32:\n\nA solution is to voluntarily put an error in the query and to print the error's message:\n//Connection to the database\n$co = new PDO('mysql:dbname=myDB;host=localhost','root','');\n//We allow to print the errors whenever there is one\n$co->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n//We create our prepared statement\n$stmt = $co->prepare(\"ELECT * FROM Person WHERE age=:age\"); //I removed the 'S' of 'SELECT'\n$stmt->bindValue(':age','18',PDO::PARAM_STR);\ntry {\n    $stmt->execute();\n} catch (PDOException $e) {\n    echo $e->getMessage();\n}\n\nStandard output:\n\nSQLSTATE[42000]: Syntax error or access violation: [...] near 'ELECT * FROM Person WHERE age=18' at line 1\n\nIt is important to note that it only prints the first 80 characters of the query.\n\n\nA33:\n\nI spent a good deal of time researching this situation for my own needs. This and several other SO threads helped me a great deal, so I wanted to share what I came up with.\nWhile having access to the interpolated query string is a significant benefit while troubleshooting, we wanted to be able to maintain a log of only certain queries (therefore, using the database logs for this purpose was not ideal). We also wanted to be able to use the logs to recreate the condition of the tables at any given time, therefore, we needed to make certain the interpolated strings were escaped properly. Finally, we wanted to extend this functionality to our entire code base having to re-write as little of it as possible (deadlines, marketing, and such; you know how it is).\nMy solution was to extend the functionality of the default PDOStatement object to cache the parameterized values (or references), and when the statement is executed, use the functionality of the PDO object to properly escape the parameters when they are injected back in to the query string. We could then tie in to execute method of the statement object and log the actual query that was executed at that time (or at least as faithful of a reproduction as possible).\nAs I said, we didn't want to modify the entire code base to add this functionality, so we overwrite the default bindParam() and bindValue() methods of the PDOStatement object, do our caching of the bound data, then call parent::bindParam() or parent::bindValue(). This allowed our existing code base to continue to function as normal.\nFinally, when the execute() method is called, we perform our interpolation and provide the resultant string as a new property E_PDOStatement->fullQuery. This can be output to view the query or, for example, written to a log file.\nThe extension, along with installation and configuration instructions, are available on github:\nhttps://github.com/noahheck/E_PDOStatement\nDISCLAIMER:\nObviously, as I mentioned, I wrote this extension. Because it was developed with help from many threads here, I wanted to post my solution here in case anyone else comes across these threads, just as I did.\n\n\nA34:\n\nTake a look at the FIND_IN_SET function for MySQL.\nSELECT * \n    FROM shirts \n    WHERE FIND_IN_SET('1',colors) > 0\n\n\n\nA35:\n\nThe IN clause describes a set of values, and sets do not have order.\nYour solution with a join and then ordering on the display_order column is the most nearly correct solution; anything else is probably a DBMS-specific hack (or is doing some stuff with the OLAP functions in standard SQL).  Certainly, the join is the most nearly portable solution (though generating the data with the display_order values may be problematic).  Note that you may need to select the ordering columns; that used to be a requirement in standard SQL, though I believe it was relaxed as a rule a while ago (maybe as long ago as SQL-92).\n\n\nA36:\n\nMySQL's totally non standards compliant GROUP BY can be emulated by Postgres' DISTINCT ON. Consider this:\nMySQL:\nSELECT a,b,c,d,e FROM table GROUP BY a\n\nThis delivers 1 row per value of a (which one, you don't really know). Well actually you can guess, because MySQL doesn't know about hash aggregates, so it will probably use a sort... but it will only sort on a, so the order of the rows could be random. Unless it uses a multicolumn index instead of sorting. Well, anyway, it's not specified by the query.\nPostgres:\nSELECT DISTINCT ON (a) a,b,c,d,e FROM table ORDER BY a,b,c\n\nThis delivers 1 row per value of a, this row will be the first one in the sort according to the ORDER BY specified by the query. Simple.\nNote that here, it's not an aggregate I'm computing. So GROUP BY actually makes no sense. DISTINCT ON makes a lot more sense.\nRails is married to MySQL, so I'm not surprised that it generates SQL that doesn't work in Postgres.\n\n\nA37:\n\nAdded a little bit more to the code by Mike - walk the values to add single quotes \n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic function interpolateQuery($query, $params) {\n    $keys = array();\n    $values = $params;\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n\n        if (is_array($value))\n            $values[$key] = implode(',', $value);\n\n        if (is_null($value))\n            $values[$key] = 'NULL';\n    }\n    // Walk the array to see if we can add single-quotes to strings\n    array_walk($values, create_function('&$v, $k', 'if (!is_numeric($v) && $v!=\"NULL\") $v = \"\\'\".$v.\"\\'\";'));\n\n    $query = preg_replace($keys, $values, $query, 1, $count);\n\n    return $query;\n}\n\n\n\nA38:\n\nSee following how to get sorted data.\nSELECT ...\n  FROM ...\n WHERE zip IN (91709,92886,92807,...,91356)\n   AND user.status=1\nORDER \n    BY provider.package_id DESC \n     , FIELD(zip,91709,92886,92807,...,91356)\nLIMIT 10\n\n\n\nA39:\n\nI think you should manage to store your data in a way that you will simply do a join and it will be perfect, so no hacks and complicated things going on.\nI have for instance a \"Recently played\" list of track ids, on SQLite i simply do:\nSELECT * FROM recently NATURAL JOIN tracks;\n\n\n\nA40:\n\nUse MySQL FIND_IN_SET function:\n  SELECT * \n    FROM table_name \n   WHERE id IN (..,..,..,..) \nORDER BY FIND_IN_SET (coloumn_name, .., .., ..);\n\n\n\nA41:\n\nGive this a shot:\nSELECT name, description, ...\nWHERE id IN\n    (SELECT id FROM table1 WHERE...)\nORDER BY\n    (SELECT display_order FROM table1 WHERE...),\n    (SELECT name FROM table1 WHERE...)\n\nThe WHEREs will probably take a little tweaking to get the correlated subqueries working properly, but the basic principle should be sound.\n\n\nA42:\n\nI assume you mean that you want the final SQL query, with parameter values interpolated into it.  I understand that this would be useful for debugging, but it is not the way prepared statements work.  Parameters are not combined with a prepared statement on the client-side, so PDO should never have access to the query string combined with its parameters.\nThe SQL statement is sent to the database server when you do prepare(), and the parameters are sent separately when you do execute().  MySQL's general query log does show the final SQL with values interpolated after you execute().  Below is an excerpt from my general query log.  I ran the queries from the mysql CLI, not from PDO, but the principle is the same.\n081016 16:51:28 2 Query       prepare s1 from 'select * from foo where i = ?'\n                2 Prepare     [2] select * from foo where i = ?\n081016 16:51:39 2 Query       set @a =1\n081016 16:51:47 2 Query       execute s1 using @a\n                2 Execute     [2] select * from foo where i = 1\n\nYou can also get what you want if you set the PDO attribute PDO::ATTR_EMULATE_PREPARES.  In this mode, PDO interpolate parameters into the SQL query and sends the whole query when you execute().  This is not a true prepared query.  You will circumvent the benefits of prepared queries by interpolating variables into the SQL string before execute().\n\nRe comment from @afilina:\nNo, the textual SQL query is not combined with the parameters during execution. So there's nothing for PDO to show you.\nInternally, if you use PDO::ATTR_EMULATE_PREPARES, PDO makes a copy of the SQL query and interpolates parameter values into it before doing the prepare and execute. But PDO does not expose this modified SQL query. \nThe PDOStatement object has a property $queryString, but this is set only in the constructor for the PDOStatement, and it's not updated when the query is rewritten with parameters.\nIt would be a reasonable feature request for PDO to ask them to expose the rewritten query. But even that wouldn't give you the \"complete\" query unless you use PDO::ATTR_EMULATE_PREPARES.\nThis is why I show the workaround above of using the MySQL server's general query log, because in this case even a prepared query with parameter placeholders is rewritten on the server, with parameter values backfilled into the query string. But this is only done during logging, not during query execution.\n\n\nA43:\n\nYou can achieve this by following function.\nRun following query to create function.\nDELIMITER ||\nCREATE FUNCTION `TOTAL_OCCURANCE`(`commastring` TEXT, `findme`     VARCHAR(255)) RETURNS int(11)\nNO SQL\n-- SANI: First param is for comma separated string and 2nd for string to find.\nreturn ROUND (   \n    (\n        LENGTH(commastring)\n        - LENGTH( REPLACE ( commastring, findme, \"\") ) \n    ) / LENGTH(findme)        \n);\n\nAnd call this function like this\nmsyql> select TOTAL_OCCURANCE('A,B,C,A,D,X,B,AB', 'A');\n\n\n\nA44:\n\nPostgreSQL is more SQL compliant than MySQL. All fields - except computed field with aggregation function - in the output must be present in the GROUP BY clause.\n\n\nA45:\n\nSELECT * FROM table WHERE field1 NOT LIKE '%$x%';   (Make sure you escape $x properly beforehand to avoid SQL injection)\nEdit: NOT IN does something a bit different - your question isn't totally clear so pick which one to use.   LIKE 'xxx%' can use an index.  LIKE '%xxx' or LIKE '%xxx%' can't.\n\n\nA46:\n\nA bit late probably but now there is PDOStatement::debugDumpParams\n\nDumps the informations contained by a prepared statement directly on\n  the output. It will provide the SQL query in use, the number of\n  parameters used (Params), the list of parameters, with their name,\n  type (paramtype) as an integer, their key name or position, and the\n  position in the query (if this is supported by the PDO driver,\n  otherwise, it will be -1).\n\nYou can find more on the official php docs\nExample:\n<?php\n/* Execute a prepared statement by binding PHP variables */\n$calories = 150;\n$colour = 'red';\n$sth = $dbh->prepare('SELECT name, colour, calories\n    FROM fruit\n    WHERE calories < :calories AND colour = :colour');\n$sth->bindParam(':calories', $calories, PDO::PARAM_INT);\n$sth->bindValue(':colour', $colour, PDO::PARAM_STR, 12);\n$sth->execute();\n\n$sth->debugDumpParams();\n\n?>\n\n\n\nA47:\n\nIf I remember correctly, in PostgreSQL you have to add every column you fetch from the table where the GROUP BY clause applies to the GROUP BY clause.\n\n\nA48:\n\n/**\n * Replaces any parameter placeholders in a query with the value of that\n * parameter. Useful for debugging. Assumes anonymous parameters from \n * $params are are in the same order as specified in $query\n *\n * @param string $query The sql query with parameter placeholders\n * @param array $params The array of substitution parameters\n * @return string The interpolated query\n */\npublic static function interpolateQuery($query, $params) {\n    $keys = array();\n\n    # build a regular expression for each parameter\n    foreach ($params as $key => $value) {\n        if (is_string($key)) {\n            $keys[] = '/:'.$key.'/';\n        } else {\n            $keys[] = '/[?]/';\n        }\n    }\n\n    $query = preg_replace($keys, $params, $query, 1, $count);\n\n    #trigger_error('replaced '.$count.' keys');\n\n    return $query;\n}\n\n\n\nA49:\n\nAns to get sorted data.\nSELECT ...\nFROM ...\nORDER  BY FIELD(user_id,5,3,2,...,50)  LIMIT 10\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 53, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nMy program does some network activity in a background thread. Before starting, it pops up a progress dialog. The dialog is dismissed on the handler. This all works fine, except when screen orientation changes while the dialog is up (and the background thread is going). At this point the app either crashes, or deadlocks, or gets into a weird stage where the app does not work at all until all the threads have been killed.\nHow can I handle the screen orientation change gracefully?\nThe sample code below matches roughly what my real program does:\npublic class MyAct extends Activity implements Runnable {\n    public ProgressDialog mProgress;\n\n    // UI has a button that when pressed calls send\n\n    public void send() {\n         mProgress = ProgressDialog.show(this, \"Please wait\", \n                      \"Please wait\", \n                      true, true);\n        Thread thread = new Thread(this);\n        thread.start();\n    }\n\n    public void run() {\n        Thread.sleep(10000);\n        Message msg = new Message();\n        mHandler.sendMessage(msg);\n    }\n\n    private final Handler mHandler = new Handler() {\n        @Override\n        public void handleMessage(Message msg) {\n            mProgress.dismiss();\n        }\n    };\n}\n\nStack:\nE/WindowManager(  244): Activity MyAct has leaked window com.android.internal.policy.impl.PhoneWindow$DecorView@433b7150 that was originally added here\nE/WindowManager(  244): android.view.WindowLeaked: Activity MyAct has leaked window com.android.internal.policy.impl.PhoneWindow$DecorView@433b7150 that was originally added here\nE/WindowManager(  244):     at android.view.ViewRoot.<init>(ViewRoot.java:178)\nE/WindowManager(  244):     at android.view.WindowManagerImpl.addView(WindowManagerImpl.java:147)\nE/WindowManager(  244):     at android.view.WindowManagerImpl.addView(WindowManagerImpl.java:90)\nE/WindowManager(  244):     at android.view.Window$LocalWindowManager.addView(Window.java:393)\nE/WindowManager(  244):     at android.app.Dialog.show(Dialog.java:212)\nE/WindowManager(  244):     at android.app.ProgressDialog.show(ProgressDialog.java:103)\nE/WindowManager(  244):     at android.app.ProgressDialog.show(ProgressDialog.java:91)\nE/WindowManager(  244):     at MyAct.send(MyAct.java:294)\nE/WindowManager(  244):     at MyAct$4.onClick(MyAct.java:174)\nE/WindowManager(  244):     at android.view.View.performClick(View.java:2129)\nE/WindowManager(  244):     at android.view.View.onTouchEvent(View.java:3543)\nE/WindowManager(  244):     at android.widget.TextView.onTouchEvent(TextView.java:4664)\nE/WindowManager(  244):     at android.view.View.dispatchTouchEvent(View.java:3198)\n\nI have tried to dismiss the progress dialog in onSaveInstanceState, but that just prevents an immediate crash. The background thread is still going, and the UI is in partially drawn state. Need to kill the whole app before it starts working again.\n\n\nCandidate answers:\nA1:\n\nYou can put dimens.xml in\n1) values \n2) values-hdpi\n3) values-xhdpi\n4) values-xxhdpi\nAnd give different sizes in dimens.xml within corresponding folders according to densities.\n\n\nA2:\n\nI've uploaded a simple java program which takes your project location and the dimension file you want as input. Based on that, it would output the corresponding dimension file in the console. Here's the link to it:\nhttps://github.com/akeshwar/Dimens-for-different-screens-in-Android/blob/master/Main.java\nHere's the full code for the reference:\npublic class Main {\n\n\n    /**\n     * You can change your factors here. The current factors are in accordance with the official documentation.\n     */\n    private static final double LDPI_FACTOR = 0.375;\n    private static final double MDPI_FACTOR = 0.5;\n    private static final double HDPI_FACTOR = 0.75;\n    private static final double XHDPI_FACTOR = 1.0;\n    private static final double XXHDPI_FACTOR = 1.5;\n    private static final double XXXHDPI_FACTOR = 2.0;\n\n    private static double factor;\n\n    public static void main(String[] args) throws IOException {\n\n\n        Scanner in = new Scanner(System.in);\n        System.out.println(\"Enter the location of the project/module\");\n        String projectPath = in.nextLine();\n\n        System.out.println(\"Which of the following dimension file do you want?\\n1. ldpi \\n2. mdpi \\n3. hdpi \\n4. xhdpi \\n5. xxhdpi \\n6. xxxhdpi\");\n\n        int dimenType = in.nextInt();\n\n        switch (dimenType) {\n            case 1: factor = LDPI_FACTOR;\n                break;\n            case 2: factor = MDPI_FACTOR;\n                break;\n            case 3: factor = HDPI_FACTOR;\n                break;\n            case 4: factor = XHDPI_FACTOR;\n                break;\n            case 5: factor = XXHDPI_FACTOR;\n                break;\n            case 6: factor = XXXHDPI_FACTOR;\n                break;\n            default:\n                factor = 1.0;\n        }\n\n        //full path = \"/home/akeshwar/android-sat-bothIncluded-notintegrated/code/tpr-5-5-9/princetonReview/src/main/res/values/dimens.xml\"\n        //location of the project or module = \"/home/akeshwar/android-sat-bothIncluded-notintegrated/code/tpr-5-5-9/princetonReview/\"\n\n\n        /**\n         * In case there is some I/O exception with the file, you can directly copy-paste the full path to the file here:\n         */\n        String fullPath = projectPath + \"/src/main/res/values/dimens.xml\";\n\n        FileInputStream fstream = new FileInputStream(fullPath);\n        BufferedReader br = new BufferedReader(new InputStreamReader(fstream));\n\n        String strLine;\n\n        while ((strLine = br.readLine()) != null)   {\n            modifyLine(strLine);\n        }\n        br.close();\n\n    }\n\n    private static void modifyLine(String line) {\n\n        /**\n         * Well, this is how I'm detecting if the line has some dimension value or not.\n         */\n        if(line.contains(\"p</\")) {\n            int endIndex = line.indexOf(\"p</\");\n\n            //since indexOf returns the first instance of the occurring string. And, the actual dimension would follow after the first \">\" in the screen\n            int begIndex = line.indexOf(\">\");\n\n            String prefix = line.substring(0, begIndex+1);\n            String root = line.substring(begIndex+1, endIndex-1);\n            String suffix = line.substring(endIndex-1,line.length());\n\n\n            /**\n             * Now, we have the root. We can use it to create different dimensions. Root is simply the dimension number.\n             */\n\n            double dimens = Double.parseDouble(root);\n            dimens = dimens*factor*1000;\n            dimens = (double)((int)dimens);\n            dimens = dimens/1000;\n            root = dimens + \"\";\n\n            System.out.println(prefix + \" \" +  root + \" \" + suffix );\n\n        }\n\n        System.out.println(line);\n    }\n}\n\n\n\nA3:\n\nWhen you switch orientations, Android will create a new View.  You're probably getting crashes because your background thread is trying to change the state on the old one.  (It may also be having trouble because your background thread isn't on the UI thread)\nI'd suggest making that mHandler volatile and updating it when the orientation changes.\n\n\nA4:\n\nIn case you want to view more: Here's a link for a list of devices (tablet, mobile, watches), including watch,chromebook, windows and mac. Here you can find the density, dimensions, and etc. Just based it in here, it's a good resource if your using an emulator too.  \nIf you click a specific item, it will show more details in the right side. \n\nSince it's Android , I will post related to it. \n\n\n~ It's better if you save a copy of the web. To view it offline. \n\n\nA5:\n\nSo this answer is a few years late but someone may appreciate it. Android has given us a new widget that makes using one navigation drawer with several activities easier.\nandroid.support.design.widget.NavigationView is modular and has its own layout in the menu folder. The way that you use it is to wrap xml layouts the following way:\n\nRoot Layout is a android.support.v4.widget.DrawerLayout that contains two children: an <include ... /> for the layout that is being wrapped (see 2) and a android.support.design.widget.NavigationView.\n<android.support.v4.widget.DrawerLayout\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/drawer_layout\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:fitsSystemWindows=\"true\"\n    tools:openDrawer=\"start\">\n\n<include\n    layout=\"@layout/app_bar_main\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\" />\n\n<android.support.design.widget.NavigationView\n    android:id=\"@+id/nav_view\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"match_parent\"\n    android:layout_gravity=\"start\"\n    android:fitsSystemWindows=\"true\"\n    app:headerLayout=\"@layout/nav_header_main\"\n    app:menu=\"@menu/activity_main_drawer\" />\n\n\n\nnav_header_main is just a LinearLayout with orientation = vertical for the header of your Navigation Drawar.\nactivity_main_drawer is a menu xml in your res/menu directory. It can contain items and groups of your choice. If you use the AndroidStudio Gallery the wizard will make a basic one for you and you can see what your options are.\n\nApp bar layout is usually now a android.support.design.widget.CoordinatorLayout and this will include two children: a android.support.design.widget.AppBarLayout (which contains a android.support.v7.widget.Toolbar) and an <include ... > for your actual content (see 3).\n<android.support.design.widget.CoordinatorLayout\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    tools:context=\"yourpackage.MainActivity\">\n\n <android.support.design.widget.AppBarLayout\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"wrap_content\"\n    android:theme=\"@style/AppTheme.AppBarOverlay\">\n\n    <android.support.v7.widget.Toolbar\n        android:id=\"@+id/toolbar\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"?attr/actionBarSize\"\n        android:background=\"?attr/colorPrimary\"\n        app:popupTheme=\"@style/AppTheme.PopupOverlay\" />\n\n</android.support.design.widget.AppBarLayout>\n\n<include layout=\"@layout/content_main\" />\n\n\nContent layout can be whatever layout you want. This is the layout that contains the main content of the activity (not including the navigation drawer or app bar).\n\nNow, the cool thing about all of this is that you can wrap each activity in these two layouts but have your NavigationView (see step 1) always point to activity_main_drawer (or whatever). This means that you will have the same(*) Navigation Drawer on all activities.\n\nThey won't be the same instance of NavigationView but, to be fair, that wasn't possible even with the BaseActivity solution outlined above.\n\n\n\nA6:\n\nI've found the best implementation. It's in the Google I/O 2014 app.\nThey use the same approach as Kevin's. If you can abstract yourself from all unneeded stuff in I/O app, you could extract everything you need and it is assured by Google that it's a correct usage of navigation drawer pattern. \nEach activity optionally has a DrawerLayout as its main layout. The interesting part is how the navigation to other screens is done. It is implemented in BaseActivity like this:\nprivate void goToNavDrawerItem(int item) {\n        Intent intent;\n        switch (item) {\n            case NAVDRAWER_ITEM_MY_SCHEDULE:\n                intent = new Intent(this, MyScheduleActivity.class);\n                startActivity(intent);\n                finish();\n                break;\n\nThis differs from the common way of replacing current fragment by a fragment transaction. But the user doesn't spot a visual difference.\n\n\nA7:\n\nAndroid 3.2 introduces a new approach to screen sizes,the numbers describing the screen size are all in \u201cdp\u201d units.Mostly we can use\nsmallest width dp: the smallest width available for application layout in \u201cdp\u201d units; this is the smallest width dp that you will ever encounter in any rotation of the display.\nTo create one right click on res >>> new >>> Android resource directory\nFrom Available qualifiers window move Smallest Screen Width to Chosen qualifiers\nIn Screen width window just write the \"dp\" value starting from you would like Android Studio to use that dimens.\nThan change to Project view,right click on your new created resource directory\nnew >>> Values resource file enter a new file name dimens.xml and you are done.\n\n\nA8:\n\nThis answer is specific to situations where the objects to be passed has nested class structure. With nested class structure, making it Parcelable or Serializeable is a bit tedious. And, the process of serialising an object is not efficient on Android. Consider the example below,\nclass Myclass    {\n  int a;\n  class SubClass    {\n       int b;\n  }\n}\n\nWith Google's GSON library, you can directly parse an object into a JSON formatted String and convert it back to the object format after usage. For example,\nMyClass src = new MyClass();\nGson gS = new Gson();\nString target = gS.toJson(src); // Converts the object to a JSON String\n\nNow you can pass this String across activities as a StringExtra with the activity intent.\nIntent i = new Intent(FromActivity.this, ToActivity.class);\ni.putExtra(\"MyObjectAsString\", target);\n\nThen in the receiving activity, create the original object from the string representation.\nString target = getIntent().getStringExtra(\"MyObjectAsString\");\nMyClass src = gS.fromJson(target, MyClass.class); // Converts the JSON String to an Object\n\nIt keeps the original classes clean and reusable. Above of all, if these class objects are created from the web as JSON objects, then this solution is very efficient and time saving.\nUPDATE\n\nWhile the above explained method works for most situations, for obvious performance reasons, do not rely on Android's bundled-extra system to pass objects around. There are a number of solutions makes this process flexible and efficient, here are a few. Each has its own pros and cons.\n\nEventbus\nOtto\n\n\n\nA9:\n\nYou have to create Different values folder for different screens .\nLike \nvalues-sw720dp          10.1\u201d tablet 1280x800 mdpi\n\nvalues-sw600dp          7.0\u201d  tablet 1024x600 mdpi\n\nvalues-sw480dp          5.4\u201d  480x854 mdpi \nvalues-sw480dp          5.1\u201d  480x800 mdpi \n\nvalues-xxhdpi           5.5\"  1080x1920 xxhdpi\nvalues-xxxhdpi           5.5\" 1440x2560 xxxhdpi\n\nvalues-xhdpi            4.7\u201d   1280x720 xhdpi \nvalues-xhdpi            4.65\u201d  720x1280 xhdpi \n\nvalues-hdpi             4.0\u201d 480x800 hdpi\nvalues-hdpi             3.7\u201d 480x854 hdpi\n\nvalues-mdpi             3.2\u201d 320x480 mdpi\n\nvalues-ldpi             3.4\u201d 240x432 ldpi\nvalues-ldpi             3.3\u201d 240x400 ldpi\nvalues-ldpi             2.7\u201d 240x320 ldpi\n\n\nFor more information you may visit here\n\nDifferent values folders in android\nhttp://android-developers.blogspot.in/2011/07/new-tools-for-managing-screen-sizes.html\n\nEdited  By @humblerookie\nYou can make use of Android Studio plugin called Dimenify to auto generate dimension values for other pixel buckets based on custom scale factors. Its still in beta, be sure to notify any issues/suggestions you come across to the developer.\n\n\nA10:\n\nThere are nice libraries which will handle everything and reduce your pain. For using it, just add two dependencies in gradle:\n implementation 'com.intuit.ssp:ssp-android:1.0.5'\n implementation 'com.intuit.sdp:sdp-android:1.0.5'\n\nAfter that, use dimens like this:\n        android:layout_marginTop=\"@dimen/_80sdp\"\n\n\n\nA11:\n\nWhen you are creating an object of intent, you can take advantage of following two methods\nfor passing objects between two activities.\nputParcelable\nputSerializable\nYou can have your class implement either Parcelable or Serializable. Then you can pass around your custom classes across activities. I have found this very useful.\nHere is a small snippet of code I am using\nCustomListing currentListing = new CustomListing();\nIntent i = new Intent();\nBundle b = new Bundle();\nb.putParcelable(Constants.CUSTOM_LISTING, currentListing);\ni.putExtras(b);\ni.setClass(this, SearchDetailsActivity.class);\nstartActivity(i);\n\nAnd in newly started activity code will be something like this...\nBundle b = this.getIntent().getExtras();\nif (b != null)\n    mCurrentListing = b.getParcelable(Constants.CUSTOM_LISTING);\n\n\n\nA12:\n\nIf you want a navigation drawer, you should use fragments.\nI followed this tutorial last week and it works great:\nhttp://developer.android.com/training/implementing-navigation/nav-drawer.html\nYou can also download sample code from this tutorial, to see how you can do this.\n\nWithout fragments:\nThis is your BaseActivity Code:\npublic class BaseActivity extends Activity\n{\n    public DrawerLayout drawerLayout;\n    public ListView drawerList;\n    public String[] layers;\n    private ActionBarDrawerToggle drawerToggle;\n    private Map map;\n    \n    protected void onCreate(Bundle savedInstanceState)\n    {\n        // R.id.drawer_layout should be in every activity with exactly the same id.\n        drawerLayout = (DrawerLayout) findViewById(R.id.drawer_layout);\n        \n        drawerToggle = new ActionBarDrawerToggle((Activity) this, drawerLayout, R.drawable.ic_drawer, 0, 0) \n        {\n            public void onDrawerClosed(View view) \n            {\n                getActionBar().setTitle(R.string.app_name);\n            }\n    \n            public void onDrawerOpened(View drawerView) \n            {\n                getActionBar().setTitle(R.string.menu);\n            }\n        };\n        drawerLayout.setDrawerListener(drawerToggle);\n    \n        getActionBar().setDisplayHomeAsUpEnabled(true);\n        getActionBar().setHomeButtonEnabled(true);\n        \n        layers = getResources().getStringArray(R.array.layers_array);\n        drawerList = (ListView) findViewById(R.id.left_drawer);\n        View header = getLayoutInflater().inflate(R.layout.drawer_list_header, null);\n        drawerList.addHeaderView(header, null, false);\n        drawerList.setAdapter(new ArrayAdapter<String>(this, R.layout.drawer_list_item, android.R.id.text1,\n                layers));\n        View footerView = ((LayoutInflater) this.getSystemService(Context.LAYOUT_INFLATER_SERVICE)).inflate(\n                R.layout.drawer_list_footer, null, false);\n        drawerList.addFooterView(footerView);\n    \n        drawerList.setOnItemClickListener(new OnItemClickListener() {\n            @Override\n            public void onItemClick(AdapterView<?> arg0, View arg1, int pos, long arg3) {\n                map.drawerClickEvent(pos);\n            }\n        });\n    }\n    \n    @Override\n    public boolean onOptionsItemSelected(MenuItem item) {\n    \n        if (drawerToggle.onOptionsItemSelected(item)) {\n            return true;\n        }\n        return super.onOptionsItemSelected(item);\n    \n    }\n    \n    @Override\n    protected void onPostCreate(Bundle savedInstanceState) {\n        super.onPostCreate(savedInstanceState);\n        drawerToggle.syncState();\n    }\n    \n    @Override\n    public void onConfigurationChanged(Configuration newConfig) {\n        super.onConfigurationChanged(newConfig);\n        drawerToggle.onConfigurationChanged(newConfig);\n    }\n}\n\nAll the other Activities that needs to have a navigation drawer should extend this Activity instead of Activity itself, example:\npublic class AnyActivity extends BaseActivity\n{\n    //Because this activity extends BaseActivity it automatically has the navigation drawer\n    //You can just write your normal Activity code and you don't need to add anything for the navigation drawer\n}\n\nXML\n<android.support.v4.widget.DrawerLayout\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:id=\"@+id/drawer_layout\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n    <!-- The main content view -->\n    <FrameLayout\n        android:id=\"@+id/content_frame\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" >\n        <!-- Put what you want as your normal screen in here, you can also choose for a linear layout or any other layout, whatever you prefer -->\n    </FrameLayout>\n    <!-- The navigation drawer -->\n    <ListView android:id=\"@+id/left_drawer\"\n        android:layout_width=\"240dp\"\n        android:layout_height=\"match_parent\"\n        android:layout_gravity=\"start\"\n        android:choiceMode=\"singleChoice\"\n        android:divider=\"@android:color/transparent\"\n        android:dividerHeight=\"0dp\"\n        android:background=\"#111\"/>\n</android.support.v4.widget.DrawerLayout>\n\n\nEdit:\nI experienced some difficulties myself, so here is a solution if you get NullPointerExceptions. In BaseActivity change the onCreate function to protected void onCreateDrawer(). The rest can stay the same. In the Activities which extend BaseActivity put the code in this order:\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity);\n    super.onCreateDrawer();\n\nThis is how you can create a navigation drawer with multiple activities, if you have any questions feel free to ask.\n\nEdit 2:\nAs said by @GregDan your BaseActivity can also override setContentView() and call onCreateDrawer there:\n@Override \npublic void setContentView(@LayoutRes int layoutResID) \n{ \n    super.setContentView(layoutResID); \n    onCreateDrawer() ;\n}\n\n\n\nA13:\n\nYou have to create a different values folder for different screens and put dimens.xml file according to densities.\n1) values\n\n2) values-hdpi (320x480 ,480x800)\n\n3) values-large-hdpi (600x1024)\n\n4) values-xlarge (720x1280 ,768x1280 ,800x1280 ,Nexus7 ,Nexus10)\n\n5) values-sw480dp (5.1' WVGA screen)\n\n6) values-xhdpi (Nexus4 , Galaxy Nexus)\n\n\n\nA14:\n\nUse Scalable DP\nAlthough making a different layout for different screen sizes is theoretically a good idea, it can get very difficult to accommodate for all screen dimensions, and pixel densities. Having over 20+ different dimens.xml files as suggested in the above answers, is not easy to manage at all.\nHow To Use:\nTo use sdp:\n\nInclude implementation 'com.intuit.sdp:sdp-android:1.0.5' in your build.gradle,\nReplace any dp value such as 50dp with a @dimen/50_sdp like so:\n<TextView\n android:layout_width=\"@dimen/_50sdp\"\n android:layout_height=\"@dimen/_50sdp\"\n android:text=\"Hello World!\" />\n\n\nHow It Works:\nsdp scales with the screen size because it is essentially a huge list of different dimens.xml for every possible dp value.\n\nSee It In Action:\nHere it is on three devices with widely differing screen dimensions, and densities:\n\nNote that the sdp size unit calculation includes some approximation due to some performance and usability constraints.\n\n\nA15:\n\nwe  want to see the changes of required view size in different screens.\nWe need to create a different values folders for different screens and put dimens.xml file based on screen densities.\nI have taken one TextView and observed the changes when i changed dimens.xml in\ndifferent values folders.\nPlease follow the process\nnormal - xhdpi \\ dimens.xml\nThe below devices can change the sizes of screens when we change the normal - xhdpi \\ dimens.xml\nnexus 5X ( 5.2\" * 1080 * 1920 : 420dpi )\nnexus 6P ( 5.7\" * 1440 * 2560 :  560dpi)\nnexus 6 ( 6.0\" * 1440 * 2560 :  560dpi)\nnexus 5 (5.0\", 1080 1920 :  xxhdpi)\nnexus 4 (4.7\", 768 * 1280 :  xhdpi)\nGalaxy nexus (4.7\", 720 * 1280 :  xhdpi)\n4.65\" 720p ( 720 * 1280 : xhdpi )\n4.7\" WXGA ( 1280  * 720 : Xhdpi )\nXlarge - xhdpi  \\ dimens.xml\nThe below devices can change the sizes of screens when we change the Xlarge - xhdpi  \\ dimens.xml\nnexus 9 ( 8.9\", 2048 * 1556 :  xhdpi)\nnexus 10 (10.1\", 2560 * 1600 : xhdpi)\nlarge - xhdpi  \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  large - xhdpi  \\ dimens.xml\nnexus 7 ( 7.0\", 1200 * 1920: xhdpi)\nnexus 7 (2012) (7.0\", 800 * 1280 : tvdpi)\nThe below screens are visible in \" Search Generic Phones and Tablets \"\nlarge - mdpi \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  large - mdpi  \\ dimens.xml\n5.1\" WVGA ( 480 * 800 : mdpi )\n5.4\" FWVGA ( 480 * 854 : mdpi )\n7.0\" WSVGA (Tablet) ( 1024 * 600 : mdpi )\nnormal - hdpi  \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  normal - hdpi  \\ dimens.xml\nnexus s ( 4.0\", 480 * 800 : hdpi )\nnexus one ( 3.7\", 480 * 800: hdpi)\nsmall - ldpi  \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  small - ldpi \\ dimens.xml\n2.7\" QVGA Slider ( 240 * 320 : ldpi )\n2.7\" QVGA ( 240 * 320 : ldpi )\nxlarge - mdpi \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  xlarge - mdpi  \\ dimens.xml\n10.1\" WXGA ( tABLET) ( 1280 * 800 : MDPI ) \nnormal - ldpi \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  normal - ldpi   \\ dimens.xml\n3.3\" WQVGA ( 240 * 400 : LDPI ) \n3.4\" WQVGA ( 240 * 432 : LDPI ) \nnormal - hdpi \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  normal - hdpi    \\ dimens.xml\n4.0\" WVGA ( 480 * 800 : hdpi )\n3.7\" WVGA ( 480 *  800 : hdpi )\n3.7\" FWVGA  Slider ( 480 *  854 : hdpi )\nnormal - mdpi \\ dimens.xml\nThe below devices can change the sizes of screens when we change the  normal - mdpi    \\ dimens.xml\n3.2\" HVGA Slider ( ADP1 ) ( 320 * 480 : MDPI )  \n3.2\" QVGA ( ADP2 ) ( 320 * 480 : MDPI ) \n\n\nA16:\n\nYou can create a subclass of Application and store your shared object there.  The Application object should exist for the lifetime of your app as long as there is some active component.\nFrom your activities, you can access the application object via getApplication().\n\n\nA17:\n\nThe basic principle of getting more flexibility in your code is by encapsulating what is varying/changing (or what is highly likely to change).\nIf your case dimensions are changing. So you start with moving your dimensions (the ones you think need to change) from XML code or from java/kotlin code to a file values/dimens.xml\nFor the context of this question.\nYou may have res folders by density qualifiers or by small-width qualifiers.\nvalues/dimens.xml\nvalues-ldpi/dimens.xml\nvalues-hdpi/dimens.xml\n\nvalues/dimens.xml\nvalues-sw320dp/dimens.xml\nvalues-sw320dp/dimens.xml\n \n\nOne important point to note here the items in values/dimens.xml will apply to screens smaller than the lowest breakpoint you have specified.\nvalues/dimens.xml //smallest screen\nvalues-sw320dp/dimens.xml //320dp or more till next breakpoint folder\nvalues-sw360dp/dimens.xml//360dp or more till next breakpoint folder\n\nAnother important point to note is that a high pixel density device may have a small screen. So be sure whether you want to specify density qualifier or screen width qualifier.\n\nSome further briefing for those who are new to this aspect.\nYou can add qualifiers to different resource folders such as values, drawable, and layout.\nThese qualifiers may represent a language (-en,-hi) ,an orientation (-land,-port), or screen size range (-ldpi,-hdpi).\nThe naming convention has a hierarchy of qualifiers and qualifiers must be added in that order only.\n    values-en-hdpi (it cannot be values-hdpi-en)\n    values-en-land-hdpi (it cannot be values values-hdpi-en-land or any other order)\nlayout-hdpi\n\nThe order is documented in Table 2 at this page\nSome qualifiers represent absolute values whereas others like ldpi,hdpi represent breakpoints.\n\n\nA18:\n\nYour object can also implement the Parcelable interface. Then you can use the Bundle.putParcelable() method and pass your object between activities within intent.\nThe Photostream application uses this approach and may be used as a reference.\n\n\nA19:\n\nIt depends on the type of data you need access to.  If you have some kind of data pool that needs to persist across Activitys then Erich's answer is the way to go.  If you just need to pass a few objects from one activity to another then you can have them implement Serializable and pass them in the extras of the Intent to start the new Activity.\n\n\nA20:\n\nMaybe it's an unpopular answer, but in the past I've simply used a class that has a static reference to the object I want to persist through activities.  So,\npublic class PersonHelper\n{\n    public static Person person;\n}\n\nI tried going down the Parcelable interface path, but ran into a number of issues with it and the overhead in your code was unappealing to me.\n\n\nA21:\n\nMove the long task to a seperate class. Implement it as a subject-observer pattern. Whenever the activity is created register and while closing unregister with the task class. Task class can use AsyncTask.\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " X, so we can get the next step. (, ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 54, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nIs there a way to notify people on change of some certain files? Specifically, I would like to track change of *.sql files and and notify our developers on a change. How can I configure post commit hooks to notify?\n\n\nCandidate answers:\nA1:\n\nI had a very similar issue with the same error message. \"Changes not staged for commit\", yet when I do a diff it shows differences.  I finally figured out that a while back I had changed a directories case. ex. \"PostgeSQL\" to \"postgresql\".  As I remember now sometimes git will leave a file or two behind in the old case directory.  Then you will commit a new version to the new case.\nThus git doesn't know which one to rely on.  So to resolve it, I had to go onto the github's website. Then you're able to view both cases.  And you must delete all the files in the incorrect cased directory. Be sure that you have the correct version saved off or in the correct cased directory.\nOnce you have deleted all the files in the old case directory, that whole directory will disappear. Then do a commit.\nAt this point you should be able to do a Pull on your local computer and not see the conflicts any more.  Thus being able to commit again.  :)\n\n\nA2:\n\nSo i had this project that wasnt under source control i made some changes to and wanted to keep stuff i changed.\ngit init\ngit remote add origin <url>\ngit fetch\ngit branch master origin/master\ngit restore --staged .\n\n\n\nA3:\n\nMake sure that your user account is added to the repository as a collaborator.\nSetting --> Collaborators\n\n\nA4:\n\nI received this same error when I attempted to git clone something from Kiln what was actually a Mercurial repository.\n\n\nA5:\n\nFor future references, I solved this problem by changing the GIT system used in Sourcetree, from Embedded to System.\nHow to change the git system:\n \n1. Find the Options submenu (located under Tools menu) and click on it to view the Options popup \n2. On the Options setting popup, find the Git tab, then click on it to view the Git setting \n3. Find Git version setting, then check the version used in it. In my case, I changed it to system because previously I'm using Git bash cmd, and it looks like that the git embedded in Sourcetree clashes with each other. \n4. Restart Sourcetree\n\n\nA6:\n\nIt looks like that's a private (or deleted) repository; if you visit the repository page while logged it'll give you the real URL, which'll probably be https://TeaCodie@github.com/TeaCodie/TeaCodie-Website.git , i.e. with a username specified?\n\n\nA7:\n\nIn my case I needed to put my ssh password instead of my personal access token. Then it worked.\n\n\nA8:\n\nI finally was able to solve this after months of no luck. On a Mac :\n\nOpen 'Keychain Access' on a Mac by using Launchpad or Spotlight Search\nNarrow down your search by selecting \"login\" under Keychains and \"Passwords\" under the Category (both filters on the left side of the window)\nIn the top right hand corner of the window, do a search for \"git\"\nYou'll likely see a keychain there. I right-clicked and deleted the one entry I had.\nGo into sourcetree and try to do a pull/fetch/anything. You'll be prompted for a password. This should re-initiate your password with Sourcetree. To double check this, go into your keychain and repeat steps 3 & 4. Click on the keychain and look at the access control tab. SourcetreeLogin should be listed as always having access now.\n\n\n\n\nA9:\n\nIn my case, I cloned the repo using HTTPS based url. Then I configured my githib account with 2-factor authentication using SSH key. So whenever I was pulling/pushing changes, it was showing me dialog to provide my password.\nI fixed it by enabling SSH access by right clicking on project repo-menu on SourceTree.\n\n\n\nA10:\n\nLets say you have remote repo with files and local repo with the same files.\nAnd you want to add Git to local files, and dont want to push.\nThen you can do such commands on local repo:\ngit init\ngit remote add origin <url>\ngit fetch --all\ngit reset --hard origin/master\n\nAfter that your repository is synced with remote repo.\nYou can change files, git add, push and so on.\n\n\nA11:\n\nIn my case my github account did not have permissions to the repo. Added the github account as a collaborator for the repo and that fixed it.\n\n\nA12:\n\nI got the same problem while using a github repository, and connecting to it via https, while using the OS X Keychain Credential helper.\nMy problem was that I had the wrong credentials stored in OS X's Keychain (I was using the email address that I used to sign up for github.com rather than the [username]@github.com address it provides you). I deleted the old account in the keychain and only left the @github.com one and it fixed the problem.\nNot sure if it is related, but when I checked the user.email local config:\ngit config -l\n\nit showed the incorrect email address as well, so I updated the local git user.email to use the correct account too:\ngit config user.email <username>@github.com\n\n\n\nA13:\n\nThis is what I discovered (after searching for 'sourcetree' in my drive C:):\n\nUninstall SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\Atlassian\\SourceTree\nDelete folder C:\\Users[username]\\AppData\\Local\\SourceTree\nReinstall SourceTree\n\n\n\nA14:\n\nif you have more files in my case i have 7000 image files when i try to add them from project's route folder it hasn't added them but when i go to the image folder everything is ok. Go through the target folder and command like abows\ngit add .\ngit commit -am \"image uploading\"\ngit push origin master\n\n\n\ngit push origin master Enumerating objects: 6574, done. Counting objects: 100% (6574/6574), done. Delta compression using up to 4\n    threads Compressing objects: 100% (6347/6347), done. Writing objects: \n    28% (1850/6569), 142.17 MiB | 414.00 KiB/s\n\n\n\n\nA15:\n\nI find this problem appearing when I've done a git add . in a subdirectory below where my .gitignore file lives (the home directory of my repository, so to speak).  Try changing directories to your uppermost directory and running git add . followed by git commit -m \"my commit message\".\n\n\nA16:\n\n\ngit init\ngit add .\ngit commit -m \"initial commit\"\ngit remote add origin [URL]\ngit push origin masteror\n\ngit push -f origin master\n\ngit push --force.\nIt does force the update\n\n\n\nA17:\n\nThis error mostly caused by WRONG URL, please check:\n\nhttp or https\nURL Name\nusername@git_url\nwrong git name\n\n\n\nA18:\n\nUse git diff-tree in your post-receive hook:\n git diff-tree --name-status -rz\n\nYou can grep the result to check if certain files are modified (status 'M'), as described in this answer.\nyou can find many examples on gist.github.com, with this one using the --name-status option.\n\n\nA19:\n\nOpen your Terminal, access to this folder and write:\ngit init\ngit add .\ngit commit -m \"my commit\"\ngit remote set-url origin git@github.com:username/repo.git\ngit push origin master\n\n\n\nA20:\n\nFor windows user\n\nchange SSH Client from Putty/Plink to OpenSSH\n\nin Tools -> Option -> SSHClient\n\n\n\nA21:\n\nopen terminal and clone your repo. repository url must have your usedid,\nFor ex:\n$ git clone https://<username>@github.com/<userId>/<reponame>.git\n$ git clone https://mark@github.com/mak123/<reponame>.git\n\nThen drag and drop the cloned folder into the sourceTree window.You can also do by clicking \"+New Repository\" button and from the menu by selecting \"Add Existing Local Repository\".\nIt may ask for password again but this time it will get added to your keychain.\n\n\nA22:\n\nIn my case, I was trying to clone a private repository in our group Github account to me laptop:\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone https://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nfatal: https://github.com/nyusngroup/velocity_CCSNe.git/info/refs?service=git-upload-pack not found: did you run git update-server-info on the server?\n\nI found two ways can solve this.\n(1) Use http instead of https. It asks me the name and password of our group Github account. After I entered the information, I can clone it.\nYuqians-MacBook-Air:regenerateSNID yuqianliu$ git clone http://github.com/nyusngroup/velocity_CCSNe.git\nCloning into 'velocity_CCSNe'...\nUsername for 'http://github.com':nyusngroup\nPassword for 'http://nyusngroup@github.com': \n\n(2) Add my Github account to the collaborators of the private repository in our group Github account, as pointed by Monkey King's answer above.\n\n\nA23:\n\nThis happened to me and at first it was not apparent what had gone wrong. The previous day I had renamed my repo on github.com only changing the first character of the repo name from a lowercase letter to uppercase. I was not aware that the repo name in the origin URL was case-sensitive unit I received the error which contained the text, \"did you run git update-server-info on the server.\" Once I edited the .git/config file to correct the case of the same letter in the origin URL, the problem was solved, the error no longer occurred and I was once again able to push my changes up to the repo on github.com. Another bit of advice: if you make changes to your repo on github, make sure to test sync'ing your local repo right away. In my case, I did not and it was the next day when I had tried to sync and couldn't because of the error and I forgot that I had renamed the repo, so I was scratching my head for a bit. Had I tested sync'ing right away, I would have caught this problem right away.\n\n\nA24:\n\nAs the message says:\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nGit has a \"staging area\" where files need to be added before being committed, you can read an explanation of it here.\n\nFor your specific example, you can use:\ngit commit -am \"save arezzo files\"\n\n(note the extra a in the flags, can also be written as git commit -a -m \"message\" - both do the same thing)\nAlternatively, if you want to be more selective about what you add to the commit, you use the git add command to add the appropriate files to the staging area, and git status to preview what is about to be added (remembering to pay attention to the wording used).\nYou can also find general documentation and tutorials for how to use git on the git documentation page which will give more detail about the concept of staging/adding files.\n\nOne other thing worth knowing about is interactive staging - this allows you to add parts of a file to the staging area, so if you've made three distinct code changes (for related but different functionality), you can use interactive mode to split the changes and add/commit each part in turn. Having smaller specific commits like this can be helpful.\n\n\nA25:\n\nI had a similar thing after an update; I tried re-adding my bitbucket credentials, changing the protocols, etc. with no luck, until...\n\nI opened my keychain and searched for bitbucket (probably the same for github, just search for github instead) under the Passwords category. \nI found that I had multiple Access Keys for my user, so I closed my repository, deleted all the keys of \"application password\" Kind, opened the SourceTree preferences > Accounts > removed my account and re-added it (login as usual - I used Basic with HTTPS). \nKeychain asks for permission to save the password, which I said yes and now I only see one key of the kind \"application password\". \n\nI opened my repository from the repo browser and hey presto, no more password popups!\nThis bugged me for a whole week! If this helped you, you can do a little dance with me now :)\n\n\nA26:\n\nYou might have changed your repository name\nIn your local repository edit the file:\n.git/config\n\nThen check:\n[remote \"origin\"]\n   url = \n\nthat the URL matches your remote repository\n\n\nA27:\n\nI copied a small sub project I had that was under Git source control into another project and forgot to delete the .git folder. When I went to commit I got the same message as above and couldn't clear it until I deleted the .git folder.\nIt is a bit silly, but it is worth checking you don't have a .git folder under the folder that doesn't commit.\n\n\nA28:\n\nYou should do:\ngit commit . -m \"save arezzo files\"\n\n\n\nA29:\n\nIn addition to setting up the Personal Access Token on GitHub, and doing what A_01 states above, this may be helpful (it's how I finally got it working)\n\nI made sure had the latest version of Sourcetree \nI upgraded to the latest version of and used System Git (Tools -> Options -> Git)\nUpgraded to the latest Git Credential Manager for Windows (this was via an installer I downloaded and double-clicked, very easy)\nEnsured I was using OAuth for my GitHub account stored in Tools -> Options -> Authentication\nMade sure I checked off the box \"Allow Sourcetree to manage my credentials using the Git Credential Manager\" (this was important!!) on Tools -> Options -> Git\nThen when I triggered the GitHub dialog (via pushing a new branch, just an example), I logged in there (this is where it used to fail).\nI next got the 2 factor auth dialog, and the code was sent to my phone as expected. I was able to enter the code from my phone into this dialog.\n\n\n\nA30:\n\nI met up with the same problem.\nHow I solved this problem is:\nI use git bash to create a new repo, when I typed \"git push origin master\"\nIt reported \n\"fatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\"\nFinally, I found there was not a repo on the github at all.\nYou'd better create a new repo first on github. \nMaybe this experience can help somebody.\n\n\nA31:\n\nReally struggled with this on Mac, couldn't get it to work with SSH. Ended up following this guide which worked: https://www.youtube.com/watch?v=otRHRPnHDeI.\nThere were a couple things which could have been the cause, it may not have been the SSH itself. For instance, Oauth apparently doesn't work on SourceTree despite nothing appearing wrong immediately when using it. Making sure you use a Personal Access Token and not your real password among others.\nA good rule of thumb is, if after connecting your account it only shows your public repos, it's already broken. https://github.com/settings/tokens\n\n\nA32:\n\nThe easiest way is to create an app password and use that password.\nHere you can see how you can create an app password on Bitbucket:\nhttps://docs.snyk.io/tutorials/getting-started/atlassian-integrations/atlassian-bitbucket-app-password\n\n\nA33:\n\nMy issue was that I used the clone https url widget provided by github. That URL doesn't work for private repositories as you need to add a username to the front of it.  \nExample: a private repo owned by john and named widget with collaborator sam the correct url would be: \nhttps://sam@github.com/john/widget.git\nThe github provided url: \nhttps://github.com/john/widget.git\nThe error message leaves much to be desired.\n\n\nA34:\n\nThis error could also happen if the repository you are attempting to reach was deleted.\n\n\nA35:\n\nGo to Sourctree -> Tools -> Options -> Git Select \n\n\nA36:\n\n1. first create a git repostry.\n2. second open git bash in  existing or uploading project.\n3. perform git init \n4. git add .\n5. git commit -m \"print message\"\n6. git remote add github<repostry url>\n7. git remote -v\n8. git push github master\n\nOR\ngit push origin master\n\nif  you get any error, you may use it\ngit push -f origin master\n\n\n\nA37:\n\nIf you want folks to get notified via email, and you want them to manage their own notifications, then https://app.github-file-watcher.com/ should do the trick - it monitors any public repo, and will notify you via email of changes to any file, a specific file, or some files that match your criteria. \n\n\nA38:\n\nI had more luck with navigating in my terminal to the directory I wanted to add to the repository, then (assuming you're working on a branch called master):\n    git init\n    git add .\n    git commit -m \"my commit\"\n    git remote add origin <remote repository URL>\n    git push origin master\n\nHere's a link to an article explaining how to do it in more detail: https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/\nNote that you won't be able to run the \"git add .\" line if the directory in question is open.\n\n\nA39:\n\nGo to the directory where you code is,\ngit init\ngit add .\ngit commit -m \"Your message\"\n\nNow add your address go to your git hub copy the clone address,\ngit remote add origin <remote repository URL>\n\nNow add push your code with,\ngit push -u -f origin master\n\nAnd you are done.\n\n\nA40:\n\nDid you create a new repository on the http://github.com with the same name? \nIf not, do it! And make sure each letter is correct and case sensitive.\n\n\nA41:\n\nYou could have done a:\ngit add -u -n\n\nTo check which files you modified and are going to be added (dry run: -n option), and then\ngit add -u\n\nTo add just modified files\n\n\nA42:\n\nprobably you were trying to clone like this: \ngit clone https://github.com/TeaCodie/TeaCodie-Website.git\nThen you got this error:\nfatal: https://github.com/TeaCodie/TeaCodie-Website.git/info/refs not found: \ndid you run git update-server-info on the server?\nHere is what worked for me:\ngit clone https://github.com/TeaCodie/TeaCodie-Website.git/.git\nbecause the directory \"info/refs\" is in the .git directory.\n\n\nA43:\n\nSourceTree version 2.4.8.0\nThe problem for me seemed to be the order of setup between SourceTree and GitHub. I setup SourceTree first and just clicked all the \"setup later\" options.\nAn easy fix - Uninstall and re-install SourceTree. The non-obvious bit was needing to delete the actual program files manually.\nFrom any \"SourceTree.exe\" shortcut, open file location. Mine forked at \"C:\\Users\\myusername\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Atlassian\" where the shortcut pointed to another shortcut. \nOpen that shortcut's file location. I eventually ended up at \"C:\\Users\\myusername\\AppData\\Local\\SourceTree\". Delete the whole \"SourceTree\" folder. Go up one level to \"...\\Local\" and delete the \"Atlassian\" folder as well.\nNow uninstall SourceTree. Make sure your GitHub is setup the way you desire (for me, I also created and saved an SSH key that I used for SourceTree setup). Re-install SourceTree using all the correct credentials, point to the right repository and such. \nAfter all this, SourceTree asked for my GitHub credentials twice, with two different dialog boxes, then stopped and worked! No more endless loop. Good luck!\n\n\nA44:\n\nAll the answers above seems to guide about creating a new repository in git but the question is about adding a folder to existing repo. To do that following steps can be followed.\n\nClone your existing repo using following command:\ngit clone https://github.com/company/our_repo.git \nManually take your project folder to the desired location i.e. trunk/bin\nNow commit and then push in the repo using the commands:\ngit commit -m \"message\" and git push origin master \n\n\n\nA45:\n\nI am using a SSH key, but SourceTree was asking me a password every time.\nFound on the community of Atlassian, this solution worked for me:\n\nOpen the terminal, get into your project directory\nType git config credential.helper store\nType git pull\nInput username/password (if asked)\n\nVoil\u00e0 !\n\n\nA46:\n\nMac:\nThis solution works for me.\nAfter clone code successfully, you open terminal and run this code\ngit config --global credential.helper osxkeychain\n\n\n\nA47:\n\nYou didn't add the changes. Either specifically add them via\ngit add filename1 filename2\n\nor add all changes (from root path of the project)\ngit add .\n\nor use the shorthand -a while commiting:\ngit commit -a -m \"message\".\n\n\n\nA48:\n\nFollow to @Chris answer in case you've enabled 2FA, to use Personal Access Token in SourceTree, you need to add your acc with this token in SourceTree > Preferences with following information:\n\nAuth Type: Basic  \nUsername: {your-github-account} \nPassword: {your-personal-access-token} \nProtocol: HTTPS\n\nAfter that, you are able to clone your Github repo.\n\n\n\nA49:\n\nI think it is very preferable if you first pull the existing Github repo on the local and then add the new files to the Github repo\nThis link will help: https://stackoverflow.com/a/61285076/5840973\n\n\nA50:\n\nI had an issue where I was doing commit --amend even after issuing a git add . and it still wasn't working.  Turns out I made some .vimrc customizations and my editor wasn't working correctly.  Fixing these errors so that vim returns the correct code resolved the issue.\n\n\nA51:\n\nAssume that I would like to add FreeRTOS repository, which URL is https://github.com/FreeRTOS/FreeRTOS-Kernel.git, into my repository, example URL is https://github.com/username/example as a submodule\ngit submodule add https://github.com/FreeRTOS/FreeRTOS-Kernel.git\ngit add .\ngit commit -m 'add a submodule'\ngit push\n\nTo clone using HTTPS:\ngit clone https://github.com/username/example.git --recurse-submodules\n\nUsing SSH:\ngit clone git@github.com:username/example.git --recurse-submodules\n\nIf you have downloaded the repo without using the --recurse-submodules argument, you need to run:\ngit submodule update --init --recursive\n\n\n\nA52:\n\nJust another stupid reason: you may have changed credentials on your laptop, but there is another development PC running but not frequently used, that has SourceTree started and uses the old credentials frequently to poll status.\nSounds stupid, but took me some time to solve :-)\n\n\nA53:\n\nIf you are not using GitHub 2FA, another reason for asking you your password is:  \nYour GitHub repo url is an ssh one, not an https one.\nSince git does not find your ssh keys (or your ssh key is not registered to your GitHub account), it fails back to account/password mechanism. \nIf you are using 2FA (and an https url), then you need a personal token as a password.\n\n\nA54:\n\nAlso make sure the repo you've entered is cased correctly (it's case sensitive).\n\n\nA55:\n\nif you have a subfolder, which was cloned from other git-Repository, first you have to remove the $.git$ file from the child-Repository:\nrm -rf .git\nafter that you can change to parent folder and use git add -A. \n\n\nA56:\n\nI encountered this message when using Jenkins 2.176.1 and git plugin 3.10.0 using a very old command line git version (1.7.1) as included on CentOS 6.\nThe message does not occur on newer versions of command line git (1.8 or later) with the Jenkins git plugin.\nUpgrading to a newer command line git version resolves the problem.\nOfficially, the Jenkins git plugin does not support command line git 1.7.1.  The minimum supported version is command line git 1.7.10. Many Jenkins git plugin use cases require at least command line git 1.9.0.\nCommand line git 1.7.1 behaves differently than later versions when a repository has been initialized (with git init) and then a git fetch is performed with a refspec which references 'origin'.  Later versions of command line git do not have the same problem.\n\n\nA57:\n\nIn my case, this was happening with a submodule that had defaulted to https even though the main project was using ssh. I fixed it by changing the submodule's repo path in SourceTree's Repository Settings to use the ssh path instead of https.\n\n\nA58:\n\nIn my case I had old version of the git. Upgrade to latest version fixed this issue.\n\n\nA59:\n\ndelete each .git file in all projects\nyou can use this command\nrm -rf .git\n\n\n\nA60:\n\nIn my case, the Sourcetree login screen repeatedly showed when pushing to GitHub. Logging in from that screen failed. I finally determined it was due to an expired OAuth token, which is trivial to fix.\n\nNavigate to the GitHub account config screen: Tools > Options > Authentication\nClick your GitHub account and then click edit\nOn the Edit Hosting Account dialog click the Refresh OAuth Token button\n\n\n\nA61:\n\nSince this question is specifically about SourceTree and Github, the answer in 2021 is simple. Quoting bcr:\n\nEnsured I was using OAuth for my GitHub account stored in Tools ->\nOptions -> Authentication\n\n\nGo there. Select Add (new account).\nSelect OAuth.\nClick Refresh OAuth Token. A browser page will open (github.com) requesting renewal of your token.\nLogin to accept. SourceTree will immediately respond by acknowledging the token provided by github.com.\n\n\n\nA62:\n\nThe reason why this is happening is because you have a folder that is already being tracked by Git inside another folder that is also tracked by Git. For example, I had a project and I added a subfolder to it. Both of them were being tracked by Git before I put one inside the other. In order to stop tracking the one inside, find it and remove the Git file with:\nrm -rf .git\n\nIn my case I had a WordPress application and the folder I added inside was a theme. So I had to go to the theme root, and remove the Git file, so that the whole project would now be tracked by the parent, the WordPress application.\n\n\nA63:\n\nIf you are using \"Username & Password\" authentication,you can try to change to use SSH keys for the authentication.\nI have met this kind of situation when I commit to GitLab everytime.When I changed the way of authentication:SSH keys,the problem has been solved.\n\n\nA64:\n\nIn my case, the password that Sourcetree was asking for was for the ssh private key password. If I remove the password from the ssh key, this prompt doesn't come up.\nHowever since I wanted the password to be there, I kept investigating why it wasn't accepting my ssh key password.\nTurns out there is a wrinkle. If you have multiple ssh private keys which have different passwords, Sourcetree will give you a password prompt for each password.\nThink of it this way - if you have a fresh shell in your system, and you just started ssh-agent up, and ran ssh-add, what are the password prompts that you get? For every such password prompt, Sourcetree will give you this dialog box. In fact, if you add passwords to the dialog in the same order as you would in the shell to an ssh-add, this would work. In fact, I am guessing Sourcetree is doing exactly the same in the background.\nSo my final solution was 1) keep the passwords on the ssh keys 2) make all these passwords the same. So now Sourcetree will give me a single password prompt, and entering the ssh private key password will work.\n\n\nA65:\n\nIn my case I was using the wrong protocol in the repository URL (http instead of https)\n\n\nA66:\n\nIn my repo's directory on my machine in the terminal I had to reset the origin url:\ngit remote set-url origin git@github.com:repoaccountname/repo-name.git\n\n\n\nA67:\n\nYou have to use -f when you are going to push on already existing repo.\ngit init\ngit add *\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin <repo url>\ngit push -f origin main\n\n\n\nA68:\n\nMaybe an obvious thing, but...\nIf you have problem with the index, use git-gui. You get a very good view how the index (staging area) actually works.\nAnother source of information that helped me understand the index was Scott Chacons \"Getting Git\" page 259 and forward.\nI started off using the command line because most documentation only showed that...\nI think git-gui and gitk actually make me work faster, and I got rid of bad habits like \"git pull\" for example... Now I always fetch first... See what the new changes really are before I merge.\n\n\nA69:\n\nI got this issue when using GitStack. I looked into C:/GitStack/apache/logs/error.log and found that GitStack was looking for the repository in the wrong location although its settings said D:/Repositories (my custom location), GitStack looked for the repositories in its default location. So I simply went and save the settings again from GitStack admin page. It solved the problem.\n\n\nA70:\n\nA solution worked for me given by Andrew Magill, Copied from Source : link\nThe problem is that SourceTree's embedded Git client comes with git-credential-manager v1.12, which no longer works with Github since they disabled TLS 1.1 on their site.  You need to get git-credential-manager v1.14.  Easiest way to do that is to install a current copy of the Git client separately, and then switch SourceTree over to use that (\"system git\") instead of its embedded client.  Alternatively, you can update git-credential-manager in your embedded client by replacing its files with the newest version from Microsoft.\n\n\nA71:\n\nIf you are using two-factor authentication with GitHub you will need to create a personal access token and use it with SourceTree:\n\nTo work with GitHub's two-factor authentication in SourceTree you can simply use your access token instead of your password. The steps to do this are as follows:\n\nGo to your Personal Access Tokens settings in GitHub.\nClick on the Generate new token button.\nName the token something descriptive.\nSelect which scopes you wish to grant this token.\nClick the Generate token button.\nCopy the token and use it as a password in your hosted repositories.\n\nYou can find more information about this on GitHub's help here.\n\nMac Users:\nIf your SourceTree keeps on asking for the password, go to the Terminal and type this:\ngit config --global credential.helper osxkeychain\n\n\n\nA72:\n\nHere is what worked for me.\nI tried almost all the solutions mentioned here and on the web a lot of times. I gave up multiple times before and was using it this way for more than a year I think. Finally, I was able to solve this annoying issue.\n\nAs suggested by user \"AEQ\" in his answer, I deleted all the passwords related to GitHub from my Keychain Access.\nTried deleting and re-adding the Github account. But still, the problem persisted. (generated a personal access token from \"https://github.com/settings/tokens\" and use it as the password)\n\nWhat made it work was\nMake sure to use HTTPS and not SSH when adding the new user account. If you have already added the account, just edit it. That's it.\nFinally, It feels good.\nAttaching screenshots for ease.\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " V, and the two"}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 55, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nI forked a project on github and am successfully making changes to my local master and pushing to origin on github. I want to send a pull request, but only want to include the last commit.  The pull request UI on github.com shows the last 9 commits and I don't know how to filter that down.\nI was trying to understand if I should create a new local branch, check that out and somehow reset or rebase to upstream?  Then apply my last commit from my master by id to the new local branch and use that for the pull request?\nI'm trying to get the concepts right and figure out the right command lines to do what I need.\n\n\nCandidate answers:\nA1:\n\nI found this helpful.\nhttp://jqr.github.com/2009/04/25/deploying-multiple-environments-on-heroku.html\n\n\nA2:\n\nThere is a solution, w/o exposing your email. The error occurred because you have configured your own email address in the git config --(global|system|local) user.email.\nIf the same email is your Github email and you have selected the option to make your email private, this error gets flagged by git.\nHelpfully, Github provides you with a no-reply email address which you can use for command line actions. Just check your Email settings on your Github Profile.\nYou can simply remove or undo the commit done with the user.name and before committing changes again, set another email for\ngit config --(global|system|local) user.email \"<no-reply-email-here>\".\nNow when you try to push changes to your remote repo, the error should be gone.\n\n\nA3:\n\n--orphan is good for creating an empty branch locally, however, in order to push it or interact with other branches, you will need a commit.\nCreating a new commit on an orphan branch is not a good idea because you won't be able to interact with other branches. I.e.\ngit checkout --orphan test\ngit commit --allow-empty -m \"init test branch\"\ngit merge master\nfatal: refusing to merge unrelated histories\n\nInstead, you should prefer creating a new branch from the first commit of master. If the commit is not empty you can add an empty commit before the first one, as explained by @houtanb.\n\n\nA4:\n\nI think it should be\npush = refs/heads/*:refs/heads/*\ninstead...\n\n\nA5:\n\nNovember 2021 Update: As of git version 2.27, you can now use git switch --orphan <new branch> to create an empty branch with no history.\nUnlike git checkout --orphan <new branch>, this branch won't have any files from your current branch (save for those which git doesn't track).\nThis should be the preferred way to create empty branches with no prior history.\nOnce you actually have commits on this branch, it can be pushed to github via git push -u origin <branch name>:\ngit switch --orphan <new branch>\ngit commit --allow-empty -m \"Initial commit on orphan branch\"\ngit push -u origin <new branch>\n\nOriginal answer:\nWhat's wrong with the --orphan option? If you want a branch that is empty and have no history, this is the way to go...\ngit checkout --orphan empty-branch\n\nThen you can remove all the files you'll have in the staging area (so that they don't get committed):\ngit rm -rf .\n\nAt this point you have an empty branch, on your machine.\nBefore you can push to GitHub (or any other Git repository), you will need at least one commit, even if it does not have any content on it (i.e. empty commit), as you cannot push an empty branch\ngit commit --allow-empty -m \"root commit\"\n\nFinally, push it to the remote, and crack open a beer\ngit push origin empty-branch\n\n\n\nA6:\n\ndelete each .git file in all projects\nyou can use this command\nrm -rf .git\n\n\n\nA7:\n\nYou should check out heroku_san, it solves this problem quite nicely.\nFor example, you could:\ngit checkout BRANCH\nrake qa deploy\n\nIt also makes it easy to spin up new Heroku instances to deploy a topic branch to new servers:\ngit checkout BRANCH\n# edit config/heroku.yml with new app instance and shortname\nrake shortname heroku:create deploy # auto creates deploys and migrates\n\nAnd of course you can make simpler rake tasks if you do something frequently.\n\n\nA8:\n\nHeroku labs now offers a github add-on that let's you specify which branch to push. \nSee Heroku's write up on this beta feature. \nYou'll need to sign-up as a beta tester for the time-being.\n\n\nA9:\n\ngit push heroku $(git branch --show-current):master\n\nAlternately:\ngit push heroku HEAD:master\n\n\n\nA10:\n\nMaybe an obvious thing, but...\nIf you have problem with the index, use git-gui. You get a very good view how the index (staging area) actually works.\nAnother source of information that helped me understand the index was Scott Chacons \"Getting Git\" page 259 and forward.\nI started off using the command line because most documentation only showed that...\nI think git-gui and gitk actually make me work faster, and I got rid of bad habits like \"git pull\" for example... Now I always fetch first... See what the new changes really are before I merge.\n\n\nA11:\n\nThe accepted answer led me to some problems, so I did this:\n$ git branch\n* staging\n$ git branch master c74d99cf46f6ed23e742f2617e9908294b4a608b\n$ git checkout master\nSwitched to branch 'master'\n\nAnd got what I wanted without and merge / pull-request issues. I just had to pick a base commit to create my second branch from.\n\n\nA12:\n\nAs the message says:\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nGit has a \"staging area\" where files need to be added before being committed, you can read an explanation of it here.\n\nFor your specific example, you can use:\ngit commit -am \"save arezzo files\"\n\n(note the extra a in the flags, can also be written as git commit -a -m \"message\" - both do the same thing)\nAlternatively, if you want to be more selective about what you add to the commit, you use the git add command to add the appropriate files to the staging area, and git status to preview what is about to be added (remembering to pay attention to the wording used).\nYou can also find general documentation and tutorials for how to use git on the git documentation page which will give more detail about the concept of staging/adding files.\n\nOne other thing worth knowing about is interactive staging - this allows you to add parts of a file to the staging area, so if you've made three distinct code changes (for related but different functionality), you can use interactive mode to split the changes and add/commit each part in turn. Having smaller specific commits like this can be helpful.\n\n\nA13:\n\ngit push -f heroku local_branch_name:master\n\n\n\nA14:\n\n\nWhat's the difference between them?\n\nA tag is a pointer to a specific commit. This pointer can be super charged with some additional information (identity of the creator of the tag, a description, a GPG signature, ...).\nA tag is a git concept whereas a Release is GitHub higher level concept.\nAs stated in the official announcement post from the GitHub blog: \"Releases are first-class objects with changelogs and binary assets that present a full project history beyond Git artifacts.\"\nA Release is created from an existing tag and exposes release notes and links to download the software or source code from GitHub.\n\nUsing GitHub's API, I can't get the releases list, but I can get the tags list.\n\nThe current version of the GitHub API doesn't expose a way to manage Releases (create, update, ...). I'm pretty sure that this shouldn't take too long before it's exposed.\nHowever, it's currently possible to list the releases and tags of a specific repository.\nFor instance, the url below will list the 6 releases (as of today) from ReactiveUI\n\nhttps://api.github.com/repos/reactiveui/ReactiveUI/releases\n\nWhereas this one will list the 54 tags (as of today) from the same repository\n\nhttps://api.github.com/repos/reactiveui/ReactiveUI/tags\n\nCreating a release is currently in a process which involves a manual action (adding the release notes, uploading the packages, ...). This explains why tags are not transparently seen as Releases.\n\nIf you want to be notified of the API changes, you can subscribe to the GitHub API Changes blog.\nHowever, if you can't wait, the GitHub API home page states \"If you have any problems or requests please contact support.\". This can be done through an email to support@github.com or this contact form.\nUpdate:\nThe GitHub API now allows to manipulate Releases. See the announcement.\n\n\nA15:\n\nWhen enabling the \u201cBlock command line pushes that expose my email\u201d feature, you\u2019ll also want to configure Git to use your no-reply email address. Don\u2019t worry\u2014this won\u2019t affect your contribution graph. All commits will still be associated with your account.\n\nOpen Terminal.\n\nChange the current working directory to the local repository where you want to configure the email address that you associate with your Git commits.\n\nFind your GitHub noreply address in your GitHub's Personal Settings \u2192 Emails. It's mentioned in the description of the Keep my email address private checkbox. Usually, it starts with a unique identifier, plus your username.\n\nSet an email address in Git. Use your GitHub-provided no-reply email address.\n\nSetting your email address for every repository on your computer\n  git config --global user.email \"{ID}+{username}@users.noreply.github.com\"\n\n\nSetting your email address for a single repository\n  git config user.email \"{ID}+{username}@users.noreply.github.com\"\n\n\n\n\nReset the author information on your last commit:\ngit commit --amend --reset-author --no-edit\n\n\n\nIf you have multiple commits with your private e-mail address, see this answer.\n\nNow you can push the commit with the noreply e-mail address, and future commits will have the noreply e-mail address as well.\ngit push\n\n\n\nOnce you configure Git, commits will use your alternate \u201cnoreply\u201d email address, and any pushes that don\u2019t will be rejected.\n\n\nA16:\n\n\nOpen Emails section of github.com. Visit https://github.com/settings/emails.\n\nGo to Keep my email addresses private section and note down your donotreply email id.\n\nOpen git terminal and set your donotreply email id as your email id using following command:\n\n\ngit config --global user.email \"<your_donotreply_email_id\"\n\n\nRevert your recent local commits (with your private email) which are getting failed to be pushed into repository.\n\ngit reset --soft HEAD~1 \n\n\nStage and push those commits\n\ngit add .\ngit commit \u2013m \"<commit_message>\"\ngit push\n\n\n\nA17:\n\nYou can also follow the instructions here to create an empty commit at the root of your master branch. Then just create your release branch where that empty root commit is.\n\n\nA18:\n\nThe accepted answer is indeed the way to go. However if you already have other branches that cannot be merged to de newly created fresh development branch, my personal favorite solution is to perform a rebase, execute the following on the branch that needs to merge to develop:\ngit rebase develop\nThat way the initial commit will be the first commit on the branch to merge as well. There is of course a caveat :) You are rewriting history on the branch by performing a rebase. So you will have to force push the branch to origin:\ngit push -f\nOthers that also use the branch, will be unable to pull now, so they will have to perform the rebase as well, locally, after fetching develop. Assuming they are on the branch that needs to be rebased:\ngit fetch\ngit rebase origin/develop\n\n\n\nA19:\n\nYou didn't add the changes. Either specifically add them via\ngit add filename1 filename2\n\nor add all changes (from root path of the project)\ngit add .\n\nor use the shorthand -a while commiting:\ngit commit -a -m \"message\".\n\n\n\nA20:\n\nFor me, it works,\ngit push -f heroku otherBranch:master\n\n\nThe -f (force flag) is recommended in order to avoid conflicts with other developers\u2019 pushes. Since you are not using Git for your revision control, but as a transport only, using the force flag is a reasonable practice.\n\nsource :- offical docs\n\n\nA21:\n\nWarning: This will expose your email address! Each commit includes the email address of the committer and for public repositories, this information is publicly available.\n--\nI experienced the same error: GH007 message as well and used the following to resolve the issue.\n\nGo to Setting your commit email address.\nFollow the Setting your email address for every repository on your computer.\nOpen your GitHub account, and go to Settings \u2192 Emails.\nSelect the Keep my email address private check box.\nUnselect the Block command line pushes that expose my email check box.\n\n\n\nA22:\n\nI wrote a bash program that does all the work of setting up a PR branch for you. It performs forking if needed, syncing with the upstream, setting up upstream remote, etc. and you just need to commit your modifications, push and submit a PR. \nHere is how you run it:\ngithub-make-pr-branch ssh your-github-username orig_repo_user orig_repo_name new-feature\n\nYou will find the program here and its repository also includes a step-by-step guide to performing the same process manually if you'd like to understand how it works, and also extra information on how to keep your feature branch up-to-date with the upstream master and other useful tidbits.\n\n\nA23:\n\nYou have probably enabled (or it is enabled now by default) the email privacy feature of GitHub.\nIt is detailed on this page at GitHub help.\nIn short, there is a toggle that you can enable (which is enabled on your account) that prevents any push where the user is your actual email address. You can use an anonymized address instead (or choose to disable the feature).\nDetails for both options are available at the same page.\n\n\nA24:\n\nI've started a project to help people making their first GitHub pull request. You can do the hands-on tutorial to make your first PR here\nThe workflow is simple as\n\nFork the repo in github\nGet clone url by clicking on clone repo button\nGo to terminal and run git clone <clone url you copied earlier>\nMake a branch for changes you're makeing git checkout -b branch-name\nMake necessary changes\nCommit your changes git commit\nPush your changes to your fork on GitHub git push origin branch-name\nGo to your fork on GitHub to see a Compare and pull request button\nClick on it and give necessary details\n\n\n\nA25:\n\nWhen using a wildcard, it had to be present on both sides of the refspec, so +refs/heads/*:refs/heads/master will not work. But you can use +HEAD:refs/heads/master:\ngit config remote.heroku.push +HEAD:refs/heads/master\n\nAlso, you can do this directly with git push:\ngit push heroku +HEAD:master\ngit push -f heroku HEAD:master\n\n\n\nA26:\n\nI had an issue where I was doing commit --amend even after issuing a git add . and it still wasn't working.  Turns out I made some .vimrc customizations and my editor wasn't working correctly.  Fixing these errors so that vim returns the correct code resolved the issue.\n\n\nA27:\n\nI had a very similar issue with the same error message. \"Changes not staged for commit\", yet when I do a diff it shows differences.  I finally figured out that a while back I had changed a directories case. ex. \"PostgeSQL\" to \"postgresql\".  As I remember now sometimes git will leave a file or two behind in the old case directory.  Then you will commit a new version to the new case.\nThus git doesn't know which one to rely on.  So to resolve it, I had to go onto the github's website. Then you're able to view both cases.  And you must delete all the files in the incorrect cased directory. Be sure that you have the correct version saved off or in the correct cased directory.\nOnce you have deleted all the files in the old case directory, that whole directory will disappear. Then do a commit.\nAt this point you should be able to do a Pull on your local computer and not see the conflicts any more.  Thus being able to commit again.  :)\n\n\nA28:\n\nIn order to make a pull request you need to do the following steps:\n\nFork a repository (to which you want to make a pull request). Just click the fork button the the repository page and you will have a separate github repository preceded with your github username.\nClone the repository to your local machine. The Github software that you installed on your local machine can do this for you. Click the clone button beside the repository name.\nMake local changes/commits to the files\nsync the changes\ngo to your github forked repository and click the \"Compare & Review\" green button besides the branch button. (The button has icon - no text)\nA new page will open showing your changes and then click the pull request link, that will send the request to the original owner of the repository you forked.\n\nIt took me a while to figure this, hope this will help someone.\n\n\nA29:\n\nThe Simplest GitHub Pull Request is from the web interface without using git.\n\nRegister a GitHub account, login then go to the page in the repository you want to change.\nClick the pencil icon,\n\nsearch for text near the location, make any edits you want then preview them to confirm. Give the proposed change a description up to 50 characters and optionally an extended description then click the Propose file Change button.\nIf you're reading this you won't have write access to the repository (project folders) so GitHub will create a copy of the repository (actually a branch) in your account. Click the Create pull request button.\nGive the Pull Request a description and add any comments then click Create pull request button.\n\n\n\nA30:\n\nSee https://devcenter.heroku.com/articles/git#deploying-code\n$ git push heroku yourbranch:master\n\n\n\nA31:\n\nAt some point in 2022, the syntax 'git push heroku otherbranchname:master' stopped working for me. It would always just return \"Everything up-to-date\".  After consulting the Heroku docs, I found that \"master\" had been changed to \"main\", so the syntax is now like this:\ngit push heroku otherbranchname:main\n\n\nA32:\n\nYou could have done a:\ngit add -u -n\n\nTo check which files you modified and are going to be added (dry run: -n option), and then\ngit add -u\n\nTo add just modified files\n\n\nA33:\n\nWarning, If you are Using Git 2.40 or 2.41 (Q1/Q2 2023), even a git commit -am can still generate:\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nThis is reported in git-for-windows/git issue 4462\n\nThe crucial part to make it fail was to ensure that no tracked file is left after staging the deletions\nmkdir test_repo\ncd test_repo\ngit init\necho test > file1.txt\necho test > file2.txt\ngit add --all\ngit commit -m \"first commit\"\ndel file1.txt\ndel file2.txt\ngit commit -m \"remove files\" -a\n\n\nThis is fixed with Git 2.42 (Q3 2023).\nA few places failed to differentiate the case where the index is truly empty (nothing added) and we haven't yet read from the on-disk index file, which have been corrected with Git 2.42 (Q3 2023).\nSee commit 2ee045e, commit 7667f4f, commit 866b43e (29 Jun 2023) by Johannes Schindelin (dscho).\n(Merged by Junio C Hamano -- gitster -- in commit 7f5ad0c, 08 Jul 2023)\n\ncommit -a -m: allow the top-level tree to become empty again\nSigned-off-by: Johannes Schindelin\n\n\nIn 03267e8 (\"commit: discard partial cache before (re-)reading it\", 2022-11-08, Git v2.40.0-rc0 -- merge listed in batch #1), a memory leak was plugged by discarding any partial index before re-reading it.\nThe problem with this memory leak fix is that it was based on an incomplete understanding of the logic introduced in 7168624 (\"Do not generate full commit log message if it is not going to be used\", 2007-11-28, Git v1.5.4-rc0 -- merge).\nThat logic was introduced to add a shortcut when committing without editing the commit message interactively.\nA part of that logic was to ensure that the index was read into memory:\nif (!active_nr && read_cache() < 0)\n  die(...)\n\nTranslation to English: If the index has not yet been read, read it, and if that fails, error out.\nThat logic was incorrect, though: It used !active_nr as an indicator that the index was not yet read.\nUsually this is not a problem because in the vast majority of instances, the index contains at least one entry.\nAnd it was natural to do it this way because at the time that condition was introduced, the index_state structure had no explicit flag to indicate that it was initialized: This flag was only introduced in 913e0e9 (unpack_trees(): protect the handcrafted in-core index from read_cache(), 2008-08-23, Git v1.6.1-rc1 -- merge) (unpack_trees(): protect the handcrafted in-core index from read_cache(), 2008-08-23), but that commit did not adjust the code path where no index file was found and a new, pristine index was initialized.\nNow, when the index does not contain any entry (which is quite common in Git's test suite because it starts quite a many repositories from scratch), subsequent calls to do_read_index() will mistake the index not to be initialized, and read it again unnecessarily.\nThis is a problem because after initializing the empty index e.g. the cache_tree in that index could have been initialized before a subsequent call to do_read_index() wants to ensure an initialized index.\nAnd if that subsequent call mistakes the index not to have been initialized, it would lead to leaked memory.\nThe correct fix for that memory leak is to adjust the condition so that it does not mistake active_nr == 0 to mean that the index has not yet been read.\nUsing the initialized flag instead, we avoid that mistake, and as a bonus we can fix a bug at the same time that was introduced by the memory leak fix: When deleting all tracked files and then asking git commit -a -m ...(man) to commit the result, Git would internally update the index, then discard and re-read the index undoing the update, and fail to commit anything.\n\n\n\nA34:\n\nI copied a small sub project I had that was under Git source control into another project and forgot to delete the .git folder. When I went to commit I got the same message as above and couldn't clear it until I deleted the .git folder.\nIt is a bit silly, but it is worth checking you don't have a .git folder under the folder that doesn't commit.\n\n\nA35:\n\nThe reason why this is happening is because you have a folder that is already being tracked by Git inside another folder that is also tracked by Git. For example, I had a project and I added a subfolder to it. Both of them were being tracked by Git before I put one inside the other. In order to stop tracking the one inside, find it and remove the Git file with:\nrm -rf .git\n\nIn my case I had a WordPress application and the folder I added inside was a theme. So I had to go to the theme root, and remove the Git file, so that the whole project would now be tracked by the parent, the WordPress application.\n\n\nA36:\n\nCreate a new branch starting from the latest commit, which is also in the origin repository:\ngit branch new-branch origin/master\ngit checkout new-branch\n\nThen use git cherry-pick to get the single commit you want the pull request for. If the branch with this commit is called feature and the commit you want is the latest commit in this branch, this will be\ngit cherry-pick feature\n\nAssuming this patch applies without conflict, you got now a branch for which you can do your pull request.\nIn a second step, you now need to decide what to do with your feature branch. If you haven't published your changes on this branch yet, the best procedure is probably rebasing this branch upon new-branch (and removing the last commit, if this is not done automatically by git rebase).\n\n\nA37:\n\nThe safest command to push different local Git branches to Heroku/master.\ngit push -f heroku branch_name:master\n\nNote: Although, you can push without using the -f, the -f (force flag) is recommended in order to avoid conflicts with other developers\u2019 pushes.\n\n\nA38:\n\nif you have more files in my case i have 7000 image files when i try to add them from project's route folder it hasn't added them but when i go to the image folder everything is ok. Go through the target folder and command like abows\ngit add .\ngit commit -am \"image uploading\"\ngit push origin master\n\n\n\ngit push origin master Enumerating objects: 6574, done. Counting objects: 100% (6574/6574), done. Delta compression using up to 4\n    threads Compressing objects: 100% (6347/6347), done. Writing objects: \n    28% (1850/6569), 142.17 MiB | 414.00 KiB/s\n\n\n\n\nA39:\n\nYou should do:\ngit commit . -m \"save arezzo files\"\n\n\n\nA40:\n\nI find this problem appearing when I've done a git add . in a subdirectory below where my .gitignore file lives (the home directory of my repository, so to speak).  Try changing directories to your uppermost directory and running git add . followed by git commit -m \"my commit message\".\n\n\nA41:\n\nAlso note that if your using the git flow system and your feature branch might be called \nfeature/mobile_additions\n\nand with a git remote called stagingtwo, then the command to push to heroku would be\ngit push stagingtwo feature/mobile_additions:master\n\n\n\nA42:\n\nif you have a subfolder, which was cloned from other git-Repository, first you have to remove the $.git$ file from the child-Repository:\nrm -rf .git\nafter that you can change to parent folder and use git add -A. \n\n\nA43:\n\nI followed tim peterson's instructions but I created a local branch for my changes. However, after pushing I was not seeing the new branch in GitHub. The solution was to add -u to the push command:\ngit push -u origin <branch>\n\n\n\nA44:\n\nFor those of us who have a github.com account, but only get a nasty error message when we type \"git\" into the command-line, here's how to do it all in your browser :)\n\nSame as Tim and Farhan wrote: Fork your own copy of the project:\n\nAfter a few seconds, you'll be redirected to your own forked copy of the project:\n\nNavigate to the file(s) you need to change and click \"Edit this file\" in the toolbar:\n\nAfter editing, write a few words describing the changes and then \"Commit changes\", just as well to the master branch (since this is only your own copy and not the \"main\" project).\n\nRepeat steps 3 and 4 for all files you need to edit, and then go back to the root of your copy of the project. There, click the green \"Compare, review...\" button:\n\nFinally, click \"Create pull request\" ..and then \"Create pull request\" again after you've double-checked your request's heading and description:\n\n\n\n\nA45:\n\n(In addition to the official \"GitHub Help 'Using pull requests' page\",\nsee also \"Forking vs. Branching in GitHub\", \"What is the difference between origin and upstream in GitHub\")\nCouple tips on pull-requests:\nAssuming that you have first forked a repo, here is what you should do in that fork that you own:\n\ncreate a branch: isolate your modifications in a branch. Don't create a pull request from master, where you could be tempted to accumulate and mix several modifications at once.\nrebase that branch: even if you already did a pull request from that branch, rebasing it on top of origin/master (making sure your patch is still working) will update the pull request automagically (no need to click on anything)\nupdate that branch: if your pull request is rejected, you simply can add new commits, and/or redo your history completely: it will activate your existing pull request again.\n\"focus\" that branch: i.e., make its topic \"tight\", don't modify thousands of class and the all app, only add or fix a well-defined feature, keeping the changes small.\ndelete that branch: once accepted, you can safely delete that branch on your fork (and git remote prune origin). The GitHub GUI will propose for you to delete your branch in your pull-request page.\n\nNote: to write the Pull-Request itself, see \"How to write the perfect pull request\" (January 2015, GitHub)\n\nMarch 2016: New PR merge button option: see \"Github squash commits from web interface on pull request after review comments?\".\n\nThe maintainer of the repo can choose to merge --squash those PR commits.\n\nAfter a Pull Request\nRegarding the last point, since April, 10th 2013, \"Redesigned merge button\", the branch is deleted for you:\n\n\nDeleting branches after you merge has also been simplified.\nInstead of confirming the delete with an extra step, we immediately remove the branch when you delete it and provide a convenient link to restore the branch in the event you need it again.\n\nThat confirms the best practice of deleting the branch after merging a pull request.\n\npull-request vs. request-pull\n\npull request isn't an official \"git\" term.\nGit uses the request-pull(!) command to build a request for merging:\nIt \"summarizes the changes between two commits to the standard output, and includes the given URL in the generated summary.\"\nGithub launches its own version on day one (February 2008), but redesigned that feature in May 2010, stating that:\n  Pull Request = Compare View + Issues + Commit comments\n\n\n\n\ne-notes for \"reposotory\" (sic)\n<humour>\nThat (pull request) isn't even defined properly by GitHub!\nFortunately, a true business news organization would know, and there is an e-note in order to replace pull-replace by 'e-note':\n\nSo if your reposotory needs a e-note... ask Fox Business. They are in the know.\n</humour>\n\n\nA46:\n\nYou need to basically create a new branch & cherry-pick the commits you want to add to it.\n\nNote: you might need these before the checkout/cherry-pick commands\ngit remote add upstream <git repository>\ngit remote update\n\ngit checkout -b <new-branch-name> upstream/master\n\ngit cherry-pick <SHA hash of commit>\n\ngit push origin <new-branch-name>\n\nAfterwards, you will see <new-branch-name> branch on github, switch to it and can submit the pull request with the changes you want.\n\n\nA47:\n\nTo learn how to make a pull request I just followed two separate help pages on Github (linked below as bullet points). The following command line commands are for Part 1. Part 2, the actual pull request, is done entirely on Github's website.\n$ git clone https://github.com/tim-peterson/dwolla-php.git\n$ cd dwolla-php\n$ git remote add upstream https://github.com/Dwolla/dwolla-php.git\n$ git fetch upstream\n// make your changes to this newly cloned, local repo \n$ git add .\n$ git commit -m '1st commit to dwolla'\n$ git push origin master\n\n\nPart 1: fork someone's repo: https://help.github.com/articles/fork-a-repo\n\nclick the 'fork' button on the repo you want to contribute to, in this case: Dwolla's PHP repo (Dwolla/dwolla-php)\nget the URL for your newly created fork, in this case: https://github.com/tim-peterson/dwolla-php.git (tim-peterson/dwolla-php) \ntype the git clone->cd dwolla-php->git remote->git fetch sequence above to clone your fork somewhere in your computer (i.e., \"copy/paste\" it to, in this case: third_party TimPeterson$)  and sync it with the master repo (Dwolla/dwolla-php)\nmake your changes to your local repo\ntype the git add->git commit->git push sequence above to push your changes to the remote repo, i.e., your fork on Github (tim-peterson/dwolla-php)\n\nPart 2: make pull-request: https://help.github.com/articles/using-pull-requests\n\ngo to your fork's webpage on Github (https://github.com/tim-peterson/dwolla-php)\nclick 'pull-request' button \ngive pull-request a name, fill in details of what changes you made, click submit button.\nyou're done!!\n\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": " ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ("}
{"tag": "ada_bestanswer_128k.mamba-2.8b", "model_key": "mamba-2.8b", "model_id": "state-spaces/mamba-2.8b-hf", "revision": "main", "context_len": 8192, "max_new_tokens": 128, "precision": "bfloat16", "gpu_mem_util": 0.9, "seed": 42, "num_prompts": 57, "batch_size": 1, "input_tokens": 445252, "output_tokens": 1686, "total_tokens": 446938, "total_time_s": 106.49631953239441, "tokens_per_s": 4196.745971714533, "start_vram_gb": 42.8212890625, "peak_vram_gb": 42.8212890625, "end_vram_gb": 43.8212890625, "peak_allocated_gb": 0.0, "device_name": "NVIDIA RTX A6000", "sample_idx": 56, "prompt": "You are a helpful programming assistant.\n\nYou are given a Stack Overflow question and several candidate answers.\nEach answer is labeled with an ID like A1, A2, ..., An.\nYour job is to choose which candidate answer best answers the question.\n\nQuestion:\n\nHow can I use spaces in the Windows Command Line?\ncmd /C C:\\Program Files (x86)\\WinRar\\Rar.exe a D:\\Hello 2\\File.rar D:\\Hello 2\\*.*\n\n\n\nCandidate answers:\nA1:\n\nBatch Files automatically pass the text after the program so long as their are variables to assign them to. They are passed in order they are sent; e.g. %1 will be the first string sent after the program is called, etc.\nIf you have Hello.bat and the contents are:\n@echo off\necho.Hello, %1 thanks for running this batch file (%2)\npause\n\nand you invoke the batch in command via\n\nhello.bat APerson241 %date%\n\nyou should receive this message back:\nHello, APerson241 thanks for running this batch file (01/11/2013)\n\n\nA2:\n\nTo answer the how I was recently debugging the same problem in a python script and captured a stack trace using windbg.\n\nThe call to WriteConsole eventually leads to the NtDeviceIoControlFile syscall and the kernel does not return until a keypress happens or the QuickEdit mode is changed. So if you never write to the console, your process is safe from QuickEdit mode freeze. And your user will never need to copy anything. So wait what is QuickEdit mode for again?\n\n\nA3:\n\nUse PowerShell to do anything smarter for a DOS prompt. Here, I've shown how to batch rename all the files and directories in the current directory that contain spaces by replacing them with _ underscores.\nDir |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\nEDIT :\nOptionally, the Where-Object command can be used to filter out ineligible objects for the successive cmdlet (command-let). The following are some examples to illustrate the flexibility it can afford you:\n\nTo skip any document files\nDir |\nWhere-Object { $_.Name -notmatch \"\\.(doc|xls|ppt)x?$\" } |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\nTo process only directories (pre-3.0 version)\nDir |\nWhere-Object { $_.Mode -match \"^d\" } |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\nPowerShell v3.0 introduced new Dir flags. You can also use Dir -Directory there.\nTo skip any files already containing an underscore (or some other character)\nDir |\nWhere-Object { -not $_.Name.Contains(\"_\") } |\nRename-Item -NewName { $_.Name -replace \" \",\"_\" }\n\n\n\n\nA4:\n\nYou should try using quotes.\ncmd /C \"C:\\Program Files (x86)\\WinRar\\Rar.exe\" a \"D:\\Hello 2\\File.rar\" \"D:\\Hello 2\\*.*\"\n\n\n\nA5:\n\nFirst you can reach till that folder:\ncd 'C:\\MyDirectory'\nand then use:\n./MyCommand.cmd\n\n\nA6:\n\nThere is a cool little program you can use to redirect the output to a file and the console\nsome_command  ^|  TEE.BAT  [ -a ]  filename \n\n\n\n@ECHO OFF\r\n:: Check Windows version\r\nIF NOT \"%OS%\"==\"Windows_NT\" GOTO Syntax\r\n\r\n:: Keep variables local\r\nSETLOCAL\r\n\r\n:: Check command line arguments\r\nSET Append=0\r\nIF /I [%1]==[-a] (\r\n\tSET Append=1\r\n\tSHIFT\r\n)\r\nIF     [%1]==[] GOTO Syntax\r\nIF NOT [%2]==[] GOTO Syntax\r\n\r\n:: Test for invalid wildcards\r\nSET Counter=0\r\nFOR /F %%A IN ('DIR /A /B %1 2^>NUL') DO CALL :Count \"%%~fA\"\r\nIF %Counter% GTR 1 (\r\n\tSET Counter=\r\n\tGOTO Syntax\r\n)\r\n\r\n:: A valid filename seems to have been specified\r\nSET File=%1\r\n\r\n:: Check if a directory with the specified name exists\r\nDIR /AD %File% >NUL 2>NUL\r\nIF NOT ERRORLEVEL 1 (\r\n\tSET File=\r\n\tGOTO Syntax\r\n)\r\n\r\n:: Specify /Y switch for Windows 2000 / XP COPY command\r\nSET Y=\r\nVER | FIND \"Windows NT\" > NUL\r\nIF ERRORLEVEL 1 SET Y=/Y\r\n\r\n:: Flush existing file or create new one if -a wasn't specified\r\nIF %Append%==0 (COPY %Y% NUL %File% > NUL 2>&1)\r\n\r\n:: Actual TEE\r\nFOR /F \"tokens=1* delims=]\" %%A IN ('FIND /N /V \"\"') DO (\r\n\t>  CON    ECHO.%%B\r\n\t>> %File% ECHO.%%B\r\n)\r\n\r\n:: Done\r\nENDLOCAL\r\nGOTO:EOF\r\n\r\n:Count\r\nSET /A Counter += 1\r\nSET File=%1\r\nGOTO:EOF\r\n\r\n:Syntax\r\nECHO.\r\nECHO Tee.bat,  Version 2.11a for Windows NT 4 / 2000 / XP\r\nECHO Display text on screen and redirect it to a file simultaneously\r\nECHO.\r\nIF NOT \"%OS%\"==\"Windows_NT\" ECHO Usage:  some_command  \u00b3  TEE.BAT  [ -a ]  filename\r\nIF NOT \"%OS%\"==\"Windows_NT\" GOTO Skip\r\nECHO Usage:  some_command  ^|  TEE.BAT  [ -a ]  filename\r\n:Skip\r\nECHO.\r\nECHO Where:  \"some_command\" is the command whose output should be redirected\r\nECHO         \"filename\"     is the file the output should be redirected to\r\nECHO         -a             appends the output of the command to the file,\r\nECHO                        rather than overwriting the file\r\nECHO.\r\nECHO Written by Rob van der Woude\r\nECHO http://www.robvanderwoude.com\r\nECHO Modified by Kees Couprie\r\nECHO http://kees.couprie.org\r\nECHO and Andrew Cameron\n\n\n\n\n\nA7:\n\n@echo off\n>output.txt (\necho Checking your system infor, Please wating...\n\nsysteminfo | findstr /c:\"Host Name\" \nsysteminfo | findstr /c:\"Domain\"\n\nipconfig /all | find \"Physical Address\" \n\nipconfig | find \"IPv4\" \nipconfig | find \"Default Gateway\"\n\n)\n\n@pause\n\n\n\nA8:\n\nThis batch file can help, but it has some limitations. The filename characters  = and % cannot be replaced (going from memory here) and an ^ in the filenames might be a problem too.\nIn this portion %newname: =_% on every line in the lower block it replaces the character after : with the character after = so as it stands the bunch of characters are going to be replaced with an underscore.\nRemove the echo to activate the ren command as it will merely print the commands to the console window until you do.\nIt will only process the current folder, unless you add /s to the DIR command portion and then it will process all folders under the current one too.\nTo delete a certain character, remove the character from after the = sign. In %newname:z=% an entry like this would remove all z characters (case insensitive).\n@echo off\nfor /f \"delims=\" %%a in ('dir /a:-d /o:n /b') do call :next \"%%a\"\npause\nGOTO:EOF\n:next\nset \"newname=%~nx1\"\n\nset \"newname=%newname: =_%\"\nset \"newname=%newname:)=_%\"\nset \"newname=%newname:(=_%\"\nset \"newname=%newname:&=_%\"\nset \"newname=%newname:^=_%\"\nset \"newname=%newname:$=_%\"\nset \"newname=%newname:#=_%\"\nset \"newname=%newname:@=_%\"\nset \"newname=%newname:!=_%\"\nset \"newname=%newname:-=_%\"\nset \"newname=%newname:+=_%\"\nset \"newname=%newname:}=_%\"\nset \"newname=%newname:{=_%\"\nset \"newname=%newname:]=_%\"\nset \"newname=%newname:[=_%\"\nset \"newname=%newname:;=_%\"\nset \"newname=%newname:'=_%\"\nset \"newname=%newname:`=_%\"\nset \"newname=%newname:,=_%\"\n\necho ren %1 \"%newname%\n\n\n\nA9:\n\nGo to C:\\MyDirectory and try this:\n.\\MyCommand.cmd\n\n\n\nA10:\n\nInvoke-Item will look up the default handler for the file type and tell it to run it.\nIt's basically the same as double-clicking the file in Explorer, or using start.exe.\n\n\nA11:\n\n@echo OFF\n[your command] >> [Your log file name].txt\n\nI used the command above in my batch file and it works. In the log file, it shows the results of my command.\n\n\nA12:\n\nEnclose the paths containing spaces with double quotes.\ncmd /C \"C:\\Program Files (x86)\\WinRar\\Rar.exe\" a \"D:\\Hello 2\\File.rar\" \"D:\\Hello 2\\*.*\"\n\n\n\nA13:\n\nTo run or convert batch files to PowerShell (particularly if you wish to sign all your scheduled task scripts with a certificate) I simply create a PowerShell script, for example, deletefolders.ps1.\nInput the following into the script:\ncmd.exe /c \"rd /s /q C:\\#TEMP\\test1\"\n\ncmd.exe /c \"rd /s /q C:\\#TEMP\\test2\"\n\ncmd.exe /c \"rd /s /q C:\\#TEMP\\test3\"\n\n*Each command needs to be put on a new line, calling cmd.exe again.\nThis script can now be signed and run from PowerShell outputting the commands to command prompt / cmd directly.\nIt is a much safer way than running batch files!\n\n\nA14:\n\nSingle quotation marks won't do in that case. You have to add quotation marks around each path and also enclose the whole command in quotation marks:\ncmd /C \"\"C:\\Program Files (x86)\\WinRar\\Rar.exe\" a \"D:\\Hello 2\\File.rar\" \"D:\\Hello 2\\*.*\"\"\n\nSee also the cmd.exe remarks section.\n\n\nA15:\n\nFor me, for Visual Studio 2022, it worked when I executed this \".reg\" command.\nWindows Registry Editor Version 5.00\n\n[HKEY_CURRENT_USER\\Console\\C:_Program Files_Microsoft Visual Studio_2022_Professional_Common7_IDE_CommonExtensions_Platform_Debugger_VsDebugConsole.exe]\n\"CodePage\"=dword:0000fde9\n\nIt is based on @Shaohua Li's answer: https://stackoverflow.com/a/24711864/2941313. It does the same thing but for different path (specifically for VS2022 console).\n\n\nA16:\n\nAdding the following lines at the bottom of your batch file will grab everything just as displayed inside the CMD window and export into a text file:\npowershell -c \"$wshell = New-Object -ComObject wscript.shell; $wshell.SendKeys('^a')\npowershell -c \"$wshell = New-Object -ComObject wscript.shell; $wshell.SendKeys('^c')\npowershell Get-Clipboard > MyLog.txt\n\nIt basically performs a select all -> copy into clipboard -> paste into text file.\n\n\nA17:\n\ntry this:\nif 3 gtr 2 @echo \"biggger\"\n\nThis outputs:\n\"biggger\"\n\n\nThe other operators are:\n\nEQU - equal\nNEQ - not equal\nLSS - less than\nLEQ - less than or equal\nGTR - greater than\nGEQ - greater than or equal\n\nReference\n\nMicrosoft TechNet | Windows Server 2012 | Command-line Reference: If (Archived here)\nSS64: If (Archived here)\n\n\n\nA18:\n\necho some output >\"your logfile\"\n\nor\n(\n echo some output\n echo more output\n)>\"Your logfile\"\n\nshould fill the bill.\nIf you want to APPEND the output, use >> instead of >. > will start a new logfile.\n\n\nA19:\n\nThis is very much by design.  There's no reasonable way a user can select text when your program keeps scrolling the content of the console window.  So the console host program simply stops reading your stdout/stderr output and your program hangs until the user completes the operation. This can be changed, you'll have to call Get+SetConsoleMode() and turn off the ENABLE_QUICK_EDIT_MODE option.\nDo note that this \"hang\" isn't fundamentally different from the execution pauses you get when your program generates stdout output at a rate far higher than the console host can consume it.  Albeit that those delays are finite.\nAnd it isn't the only way the user can stop your program, they can also simply press Ctrl+S.  Pressing Ctrl+Q resumes it again.  If you're old enough then you might recognize these control codes as Xon/Xoff, handshake characters for a terminal.  Which is what a console really is, a simple emulation of a terminal the way they were used back in the 1970s.  This can be changed too, you'll have to stop relying on the built-in buffered console input and switch to ReadConsole().  Or by turning off the ENABLE_LINE_INPUT console option, not so sure what side-effects that has since you didn't mention any language runtime, you'll have to try.\nAnd of course terminating your program is very easy.  You get EOF on stdin when the user types Ctrl+Z, that ought to end your program.  And there is Ctrl+C and Ctrl+Break for an instant termination, regardless what your program is doing.  You can get a notification for these with SetConsoleCtrlHandler() but you can't block it.\nIf the default behavior is dangerous and risk the health of a human then I'd strongly suggest you hire a consultant.  And won't know who wrote this answer.\n\n\nA20:\n\n    if %var% geq 1\n\nis the easiest way\n\n\nA21:\n\nUse variables i.e. the .BAT variables and called %0 to %9\n\n\nA22:\n\nOpen an elevated Command Prompt (run cmd as administrator).\nquery your registry for available TT fonts to the console by:\n    REG query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Console\\TrueTypeFont\"\n\nYou'll see an output like :\n    0    REG_SZ    Lucida Console\n    00    REG_SZ    Consolas\n    936    REG_SZ    *\u65b0\u5b8b\u4f53\n    932    REG_SZ    *\uff2d\uff33 \u30b4\u30b7\u30c3\u30af\n\nNow we need to add a TT font that supports the characters you need like Courier New, we do this by adding zeros to the string name, so in this case the next one would be \"000\" :\n    REG ADD \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Console\\TrueTypeFont\" /v 000 /t REG_SZ /d \"Courier New\"\n\nNow we implement UTF-8 support:\n    REG ADD HKCU\\Console /v CodePage /t REG_DWORD /d 65001 /f\n\nSet default font to \"Courier New\":\n    REG ADD HKCU\\Console /v FaceName /t REG_SZ /d \"Courier New\" /f\n\nSet font size to 20 :\n    REG ADD HKCU\\Console /v FontSize /t REG_DWORD /d 20 /f\n\nEnable quick edit if you like :\n    REG ADD HKCU\\Console /v QuickEdit /t REG_DWORD /d 1 /f\n\n\n\nA23:\n\nThis may fail in the case of \"toxic\" characters in the input.\nConsidering an input like thisIsAnIn^^^^put is a good way how to get understand what is going on.\nSure there is a rule that an input string MUST be inside double quoted marks but I have a feeling that this rule is a valid rule only if the meaning of the input is a location on a NTFS partition (maybe it is a rule for URLs I am not sure).\nBut it is not a rule for an arbitrary input string of course (it is \"a good practice\" but you cannot count with it).\n\n\nA24:\n\nif you want both out and err streams redirected\ndir >> a.txt 2>&1\n\n\n\nA25:\n\nUsing parameters in batch files: %0 and %9\nBatch files can refer to the words passed in as parameters with the tokens: %0 to %9.\n%0 is the program name as it was called.\n%1 is the first command line parameter\n%2 is the second command line parameter\nand so on till %9.\n\nparameters passed in on the commandline must be alphanumeric characters and delimited by spaces.  Since %0 is the program name as it was called, in DOS %0 will be empty for AUTOEXEC.BAT if started at boot time.\nExample:\nPut the following command in a batch file called mybatch.bat:\n@echo off\n@echo hello %1 %2\npause\n\nInvoking the batch file like this: mybatch john billy would output:\nhello john billy\n\nGet more than 9 parameters for a batch file, use: %*\nThe Percent Star token %* means \"the rest of the parameters\".  You can use a for loop to grab them, as defined here:\nhttp://www.robvanderwoude.com/parameters.php\nNotes about delimiters for batch parameters\nSome characters in the command line parameters are ignored by batch files, depending on the DOS version, whether they are \"escaped\" or not, and often depending on their location in the command line:\ncommas (\",\") are replaced by spaces, unless they are part of a string in \ndouble quotes\n\nsemicolons (\";\") are replaced by spaces, unless they are part of a string in \ndouble quotes\n\n\"=\" characters are sometimes replaced by spaces, not if they are part of a \nstring in double quotes\n\nthe first forward slash (\"/\") is replaced by a space only if it immediately \nfollows the command, without a leading space\n\nmultiple spaces are replaced by a single space, unless they are part of a \nstring in double quotes\n\ntabs are replaced by a single space\n\nleading spaces before the first command line argument are ignored\n\n\n\nA26:\n\nYou need to use:\nfor /f \"delims=\" %%a IN ('dir /b /s build\\release\\*.dll') do echo \"%%a\"\nThis overrides the default delimiters which are TAB and SPACE\n\n\nA27:\n\nSave the following into a file with \".reg\" suffix:\nWindows Registry Editor Version 5.00\n\n[HKEY_CURRENT_USER\\Console\\%SystemRoot%_system32_cmd.exe]\n\"CodePage\"=dword:0000fde9\n\nDouble click this file, and regedit will import it.\nIt basically sets the key HKEY_CURRENT_USER\\Console\\%SystemRoot%_system32_cmd.exe\\CodePage to 0xfde9 (65001 in decimal system).\n\n\nA28:\n\nI know this is an older post, but someone will stumble across it in a Google search and it also looks like some questions the OP asked in comments weren't specifically addressed. Also, please go easy on me since this is my first answer posted on SO. :)\nTo redirect the output to a file using a dynamically generated file name, my go-to (read: quick & dirty) approach is the second solution offered by @dbenham. So for example, this:\n@echo off\n> filename_prefix-%DATE:~-4%-%DATE:~4,2%-%DATE:~7,2%_%time:~0,2%%time:~3,2%%time:~6,2%.log (\necho Your Name Here\necho Beginning Date/Time: %DATE:~-4%-%DATE:~4,2%-%DATE:~7,2%_%time:~0,2%%time:~3,2%%time:~6,2%.log\nREM do some stuff here\necho Your Name Here\necho Ending Date/Time: %DATE:~-4%-%DATE:~4,2%-%DATE:~7,2%_%time:~0,2%%time:~3,2%%time:~6,2%.log\n)\n\nWill create a file like what you see in this screenshot of the file in the target directory\nThat will contain this output:\nYour Name Here\nBeginning Date/Time: 2016-09-16_141048.log\nYour Name Here\nEnding Date/Time: 2016-09-16_141048.log\n\nAlso keep in mind that this solution is locale-dependent, so be careful how/when you use it.\n\n\nA29:\n\nIf you don't want to deal with \"quotes\" you can use the \"s\" switch in %~dpnx[]...\nthis will output the short filenames that are easy to work with.\nfrom the Command line...\nfor /f \"delims=\" %f IN ('dir /b /s \"C:\\Program Files\\*.dll\"') do echo %~sdpnxf\n\ninside a .CMD/.BAT file you need to \"escape\" the [%] e.g., double-up [%%]\nfor /f \"delims=\" %%f IN ('dir /b /s \"C:\\Program Files\\*.dll\"') do echo %%~sdpnxf\n\n\n\nA30:\n\nActually, you don't even need a greater feature. All you need to do is add\ngoto homepagename\n\nThen you will be taken there if none of the if commands execute a goto command.\nFor example, this will fix your code:\n@echo off\n:Start2 \ncls\ngoto Start\n:Start\ntitle Frequently Used Websites\necho Please select a website from the list\necho with the corresponding key\necho --------------------------------------\necho [1] Google\necho [2] Wikipedia\necho [3] Facebook\necho [4] Youtube\necho [5] Yahoo\nset input=\nset /p input= Choice:\nif %input%==1 goto Z if NOT goto Start2\nif %input%==2 goto X if NOT goto Start2\nif %input%==3 goto C if NOT goto Start2\nif %input%==4 goto V if NOT goto Start2\nif %input%==5 goto B if NOT goto Start2\nif %input%>=6 goto N\ngoto Start\n\n\n\nA31:\n\nYou can write this (easier)\n@echo off\n\n:Start2\ncls\ngoto Start\n\n:Start\ntitle Frequently Used Websites\necho Please select a website from the list\necho with the corresponding key\necho --------------------------------------\necho [1] Google\necho [2] Wikipedia\necho [3] Facebook\necho [4] Youtube\necho [5] Yahoo\n\nset /p input= Choice: \n\nif %input%==1 goto Z\nif %input%==2 goto X\nif %input%==3 goto C\nif %input%==4 goto V\nif %input%==5 goto B\necho Invalid selection!\necho.\necho Press any key to go back!\npause >nul\ncls\ngoto start2\n\n:Z\ncls\necho You have selected Google\npause\nstart www.google.com\nexit\n\n:X\ncls\necho You have selected Wikipedia\npause\nstart www.wikipedia.com\nexit\n\n:C\ncls\necho You have selected Facebook\npause\nstart www.facebook.com\nexit\n\n:V\ncls\necho You have selected Youtube\npause\nstart www.youtube.com\nexit\n\n:B\ncls\necho You have selected Yahoo\npause\nstart www.Yahoo.com\nexit\n\n:N\ncls\necho Invalid Selection! Try again\npause\ngoto start2\n\n\n\nA32:\n\nThe problem is there's a wildcard in the string that gets interpreted as a filename.  You also need double quotes for a string with spaces.  I'm not sure if there's a way to escape the wildcard.\nfor %a IN (\"dir /b /s build\\release\\.dll\") do echo %a\n\n\"dir /b /s build\\release\\.dll\"\n\n\n\nA33:\n\nA one-liner command in Windows PowerShell to delete or rename certain characters will be as below. (here the whitespace is being replaced with underscore)\nDir | Rename-Item \u2013NewName { $_.name \u2013replace \" \",\"_\" }\n\n\n\nA34:\n\nAdd these two lines near the top of your batch file, all stdout and stderr after will be redirected to log.txt:\nif not \"%1\"==\"STDOUT_TO_FILE\"  %0 STDOUT_TO_FILE %*  >log.txt 2>&1\nshift /1\n\n\n\nA35:\n\nAs others have already said, parameters passed through the command line can be accessed in batch files with the notation %1 to %9. There are also two other tokens that you can use:\n\n%0 is the executable (batch file) name as specified in the command line.\n%* is all parameters specified in the command line -- this is very useful if you want to forward the parameters to another program.\n\nThere are also lots of important techniques to be aware of in addition to simply how to access the parameters.\nChecking if a parameter was passed\nThis is done with constructs like IF \"%~1\"==\"\", which is true if and only if no arguments were passed at all. Note the tilde character which causes any surrounding quotes to be removed from the value of %1; without a tilde you will get unexpected results if that value includes double quotes, including the possibility of syntax errors.\nHandling more than 9 arguments (or just making life easier)\nIf you need to access more than 9 arguments you have to use the command SHIFT. This command shifts the values of all arguments one place, so that %0 takes the value of %1, %1 takes the value of %2, etc. %9 takes the value of the tenth argument (if one is present), which was not available through any variable before calling SHIFT (enter command SHIFT /? for more options).\nSHIFT is also useful when you want to easily process parameters without requiring that they are presented in a specific order. For example, a script may recognize the flags -a and -b in any order. A good way to parse the command line in such cases is\n:parse\nIF \"%~1\"==\"\" GOTO endparse\nIF \"%~1\"==\"-a\" REM do something\nIF \"%~1\"==\"-b\" REM do something else\nSHIFT\nGOTO parse\n:endparse\nREM ready for action!\n\nThis scheme allows you to parse pretty complex command lines without going insane.\nSubstitution of batch parameters\nFor parameters that represent file names the shell provides lots of functionality related to working with files that is not accessible in any other way. This functionality is accessed with constructs that begin with %~.\nFor example, to get the size of the file passed in as an argument use\nECHO %~z1\n\nTo get the path of the directory where the batch file was launched from (very useful!) you can use\nECHO %~dp0\n\nYou can view the full range of these capabilities by typing CALL /? in the command prompt.\n\n\nA36:\n\nTry invoking cmd /c C:\\MyDirectory\\MyCommand.cmd \u2013 that should work.\n\n\nA37:\n\nThe simple naive way that is slow because it opens and positions the file pointer to End-Of-File multiple times.\n@echo off\ncommand1 >output.txt\ncommand2 >>output.txt\n...\ncommandN >>output.txt\n\nA better way - easier to write, and faster because the file is opened and positioned only once.\n@echo off\n>output.txt (\n  command1\n  command2\n  ...\n  commandN\n)\n\nAnother good and fast way that only opens and positions the file once\n@echo off\ncall :sub >output.txt\nexit /b\n\n:sub\ncommand1\ncommand2\n...\ncommandN\n\nEdit 2020-04-17\nEvery now and then you may want to repeatedly write to two or more files. You might also want different messages on the screen. It is still possible to to do this efficiently by redirecting to undefined handles outside a parenthesized block or subroutine, and then use the & notation to reference the already opened files.\ncall :sub 9>File1.txt 8>File2.txt\nexit /b\n\n:sub\necho Screen message 1\n>&9 echo File 1 message 1\n>&8 echo File 2 message 1\necho Screen message 2\n>&9 echo File 1 message 2\n>&8 echo File 2 message 2\nexit /b\n\nI chose to use handles 9 and 8 in reverse order because that way is more likely to avoid potential permanent redirection due to a Microsoft redirection implementation design flaw when performing multiple redirections on the same command. It is highly unlikely, but even that approach could expose the bug if you try hard enough. If you stage the redirection than you are guaranteed to avoid the problem.\n3>File1.txt ( 4>File2.txt call :sub)\nexit /b\n\n:sub\netc.\n\n\n\nA38:\n\nI got around this by prepending \"type\" and putting double quotes surrounding the path in the IN clause\nFOR /F %%A IN ('type \"c:\\A Path With Spaces\\A File.txt\"') DO (\n    ECHO %%A\n)\n\nThis article gave me the idea to use \"type\" in the IN clause.\n\n\nA39:\n\n@Jon's :parse/:endparse scheme is a great start, and he has my gratitude for the initial pass, but if you think that the Windows torturous batch system would let you off that easy\u2026 well, my friend, you are in for a shock. I have spent the whole day with this devilry, and after much painful research and experimentation I finally managed something viable for a real-life utility.\nLet us say that we want to implement a utility foobar. It requires an initial command. It has an optional parameter --foo which takes an optional value (which cannot be another parameter, of course); if the value is missing it defaults to default. It also has an optional parameter --bar which takes a required value. Lastly it can take a flag --baz with no value allowed. Oh, and these parameters can come in any order.\nIn other words, it looks like this:\nfoobar <command> [--foo [<fooval>]] [--bar <barval>] [--baz]\n\nComplicated? No, that seems pretty typical of real life utilities. (git anyone?)\nWithout further ado, here is a solution:\n@ECHO OFF\nSETLOCAL\nREM FooBar parameter demo\nREM By Garret Wilson\n\nSET CMD=%~1\n\nIF \"%CMD%\" == \"\" (\n  GOTO usage\n)\nSET FOO=\nSET DEFAULT_FOO=default\nSET BAR=\nSET BAZ=\n\nSHIFT\n:args\nSET PARAM=%~1\nSET ARG=%~2\nIF \"%PARAM%\" == \"--foo\" (\n  SHIFT\n  IF NOT \"%ARG%\" == \"\" (\n    IF NOT \"%ARG:~0,2%\" == \"--\" (\n      SET FOO=%ARG%\n      SHIFT\n    ) ELSE (\n      SET FOO=%DEFAULT_FOO%\n    )\n  ) ELSE (\n    SET FOO=%DEFAULT_FOO%\n  )\n) ELSE IF \"%PARAM%\" == \"--bar\" (\n  SHIFT\n  IF NOT \"%ARG%\" == \"\" (\n    SET BAR=%ARG%\n    SHIFT\n  ) ELSE (\n    ECHO Missing bar value. 1>&2\n    ECHO:\n    GOTO usage\n  )\n) ELSE IF \"%PARAM%\" == \"--baz\" (\n  SHIFT\n  SET BAZ=true\n) ELSE IF \"%PARAM%\" == \"\" (\n  GOTO endargs\n) ELSE (\n  ECHO Unrecognized option %1. 1>&2\n  ECHO:\n  GOTO usage\n)\nGOTO args\n:endargs\n\nECHO Command: %CMD%\nIF NOT \"%FOO%\" == \"\" (\n  ECHO Foo: %FOO%\n)\nIF NOT \"%BAR%\" == \"\" (\n  ECHO Bar: %BAR%\n)\nIF \"%BAZ%\" == \"true\" (\n  ECHO Baz\n)\n\nREM TODO do something with FOO, BAR, and/or BAZ\nGOTO :eof\n\n:usage\nECHO FooBar\nECHO Usage: foobar ^<command^> [--foo [^<fooval^>]] [--bar ^<barval^>] [--baz]\nEXIT /B 1\n\nYes, it really is that bad. See my similar post at https://stackoverflow.com/a/50653047/421049, where I provide more analysis of what is going on in the logic, and why I used certain constructs.\nHideous. Most of that I had to learn today. And it hurt.\n\n\nA40:\n\nAfter I tried algirdas' solution, my Windows crashed (Win 7 Pro 64bit) so I decided to try a different solution:\n\nStart Run (Win+R)\nType cmd /K chcp 65001\n\nYou will get mostly what you want. To start it from the taskbar or anywhere else, make a shortcut (you can name it cmd.unicode.exe or whatever you like) and change its Target to C:\\Windows\\System32\\cmd.exe /K chcp 65001.\n\n\nA41:\n\nThe PowerShell answers are good, but the Rename-Item command doesn't work in the same target directory unless ALL of your files have the unwanted character in them (fails if it finds duplicates).\nIf you're like me and had a mix of good names and bad names, try this script instead (will replace spaces with an underscore):\nGet-ChildItem -recurse -name | ForEach-Object { Move-Item $_ $_.replace(\" \", \"_\") }\n\n\n\n\nThink briefly about which candidate is the best answer.\nThen respond with ONLY the ID of the best answer, for example: A3.\n", "completion": "t,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,"}
